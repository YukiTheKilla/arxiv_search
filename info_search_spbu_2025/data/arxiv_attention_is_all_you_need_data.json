[
  {
    "id": "http://arxiv.org/abs/2412.09925v2",
    "title": "Simulating Hard Attention Using Soft Attention",
    "summary": "We study conditions under which transformers using soft attention can\nsimulate hard attention, that is, effectively focus all attention on a subset\nof positions. First, we examine several subclasses of languages recognized by\nhard-attention transformers, which can be defined in variants of linear\ntemporal logic. We demonstrate how soft-attention transformers can compute\nformulas of these logics using unbounded positional embeddings or temperature\nscaling. Second, we demonstrate how temperature scaling allows softmax\ntransformers to simulate general hard-attention transformers, using a\ntemperature that depends on the minimum gap between the maximum attention\nscores and other attention scores.",
    "published": "2024-12-13T07:27:42Z",
    "updated": "2025-06-26T13:41:24Z",
    "authors": [
      "Andy Yang",
      "Lena Strobl",
      "David Chiang",
      "Dana Angluin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.08593v1",
    "title": "Attention-Only Transformers and Implementing MLPs with Attention Heads",
    "summary": "The transformer architecture is widely used in machine learning models and\nconsists of two alternating sublayers: attention heads and MLPs. We prove that\nan MLP neuron can be implemented by a masked attention head with internal\ndimension 1 so long as the MLP's activation function comes from a restricted\nclass including SiLU and close approximations of ReLU and GeLU. This allows one\nto convert an MLP-and-attention transformer into an attention-only transformer\nat the cost of greatly increasing the number of attention heads. We also prove\nthat attention heads can perform the components of an MLP (linear\ntransformations and activation functions) separately. Finally, we prove that\nattention heads can encode arbitrary masking patterns in their weight matrices\nto within arbitrarily small error.",
    "published": "2023-09-15T17:47:45Z",
    "updated": "2023-09-15T17:47:45Z",
    "authors": [
      "Robert Huben",
      "Valerie Morris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.15679v1",
    "title": "Generic Attention-model Explainability for Interpreting Bi-Modal and\n  Encoder-Decoder Transformers",
    "summary": "Transformers are increasingly dominating multi-modal reasoning tasks, such as\nvisual question answering, achieving state-of-the-art results thanks to their\nability to contextualize information using the self-attention and co-attention\nmechanisms. These attention modules also play a role in other computer vision\ntasks including object detection and image segmentation. Unlike Transformers\nthat only use self-attention, Transformers with co-attention require to\nconsider multiple attention maps in parallel in order to highlight the\ninformation that is relevant to the prediction in the model's input. In this\nwork, we propose the first method to explain prediction by any\nTransformer-based architecture, including bi-modal Transformers and\nTransformers with co-attentions. We provide generic solutions and apply these\nto the three most commonly used of these architectures: (i) pure\nself-attention, (ii) self-attention combined with co-attention, and (iii)\nencoder-decoder attention. We show that our method is superior to all existing\nmethods which are adapted from single modality explainability.",
    "published": "2021-03-29T15:03:11Z",
    "updated": "2021-03-29T15:03:11Z",
    "authors": [
      "Hila Chefer",
      "Shir Gur",
      "Lior Wolf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.14000v1",
    "title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped\n  Attention",
    "summary": "Recently, Transformers have shown promising performance in various vision\ntasks. To reduce the quadratic computation complexity caused by the global\nself-attention, various methods constrain the range of attention within a local\nregion to improve its efficiency. Consequently, their receptive fields in a\nsingle attention layer are not large enough, resulting in insufficient context\nmodeling. To address this issue, we propose a Pale-Shaped self-Attention\n(PS-Attention), which performs self-attention within a pale-shaped region.\nCompared to the global self-attention, PS-Attention can reduce the computation\nand memory costs significantly. Meanwhile, it can capture richer contextual\ninformation under the similar computation complexity with previous local\nself-attention mechanisms. Based on the PS-Attention, we develop a general\nVision Transformer backbone with a hierarchical architecture, named Pale\nTransformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the\nmodel size of 22M, 48M, and 85M respectively for 224 ImageNet-1K\nclassification, outperforming the previous Vision Transformer backbones. For\ndownstream tasks, our Pale Transformer backbone performs better than the recent\nstate-of-the-art CSWin Transformer by a large margin on ADE20K semantic\nsegmentation and COCO object detection & instance segmentation. The code will\nbe released on https://github.com/BR-IDL/PaddleViT.",
    "published": "2021-12-28T05:37:24Z",
    "updated": "2021-12-28T05:37:24Z",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.15176v1",
    "title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic\n  Speech Recognition",
    "summary": "The Transformer architecture model, based on self-attention and multi-head\nattention, has achieved remarkable success in offline end-to-end Automatic\nSpeech Recognition (ASR). However, self-attention and multi-head attention\ncannot be easily applied for streaming or online ASR. For self-attention in\nTransformer ASR, the softmax normalization function-based attention mechanism\nmakes it impossible to highlight important speech information. For multi-head\nattention in Transformer ASR, it is not easy to model monotonic alignments in\ndifferent heads. To overcome these two limits, we integrate sparse attention\nand monotonic attention into Transformer-based ASR. The sparse mechanism\nintroduces a learned sparsity scheme to enable each self-attention structure to\nfit the corresponding head better. The monotonic attention deploys\nregularization to prune redundant heads for the multi-head attention structure.\nThe experiments show that our method can effectively improve the attention\nmechanism on widely used benchmarks of speech recognition.",
    "published": "2022-09-30T01:55:57Z",
    "updated": "2022-09-30T01:55:57Z",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Wen qi Wei",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.01778v1",
    "title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
    "summary": "Attention-based transformer networks have demonstrated promising potential as\ntheir applications extend from natural language processing to vision. However,\ndespite the recent improvements, such as sub-quadratic attention approximation\nand various training enhancements, the compact vision transformers to date\nusing the regular attention still fall short in comparison with its convnet\ncounterparts, in terms of \\textit{accuracy,} \\textit{model size}, \\textit{and}\n\\textit{throughput}. This paper introduces a compact self-attention mechanism\nthat is fundamental and highly generalizable. The proposed method reduces\nredundancy and improves efficiency on top of the existing attention\noptimizations. We show its drop-in applicability for both the regular attention\nmechanism and some most recent variants in vision transformers. As a result, we\nproduced smaller and faster models with the same or better accuracies.",
    "published": "2021-08-03T22:33:58Z",
    "updated": "2021-08-03T22:33:58Z",
    "authors": [
      "Lingchuan Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.13027v2",
    "title": "BOAT: Bilateral Local Attention Vision Transformer",
    "summary": "Vision Transformers achieved outstanding performance in many computer vision\ntasks. Early Vision Transformers such as ViT and DeiT adopt global\nself-attention, which is computationally expensive when the number of patches\nis large. To improve efficiency, recent Vision Transformers adopt local\nself-attention mechanisms, where self-attention is computed within local\nwindows. Despite the fact that window-based local self-attention significantly\nboosts efficiency, it fails to capture the relationships between distant but\nsimilar patches in the image plane. To overcome this limitation of image-space\nlocal attention, in this paper, we further exploit the locality of patches in\nthe feature space. We group the patches into multiple clusters using their\nfeatures, and self-attention is computed within every cluster. Such\nfeature-space local attention effectively captures the connections between\npatches across different local windows but still relevant. We propose a\nBilateral lOcal Attention vision Transformer (BOAT), which integrates\nfeature-space local attention with image-space local attention. We further\nintegrate BOAT with both Swin and CSWin models, and extensive experiments on\nseveral benchmark datasets demonstrate that our BOAT-CSWin model clearly and\nconsistently outperforms existing state-of-the-art CNN models and vision\nTransformers.",
    "published": "2022-01-31T07:09:50Z",
    "updated": "2022-10-19T16:10:44Z",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.01989v3",
    "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence\n  Learning Ability",
    "summary": "Transformer and its variants are fundamental neural architectures in deep\nlearning. Recent works show that learning attention in the Fourier space can\nimprove the long sequence learning capability of Transformers. We argue that\nwavelet transform shall be a better choice because it captures both position\nand frequency information with linear time complexity. Therefore, in this\npaper, we systematically study the synergy between wavelet transform and\nTransformers. We propose Wavelet Space Attention (WavSpA) that facilitates\nattention learning in a learnable wavelet coefficient space which replaces the\nattention in Transformers by (1) applying forward wavelet transform to project\nthe input sequences to multi-resolution bases, (2) conducting attention\nlearning in the wavelet coefficient space, and (3) reconstructing the\nrepresentation in input space via backward wavelet transform. Extensive\nexperiments on the Long Range Arena demonstrate that learning attention in the\nwavelet space using either fixed or adaptive wavelets can consistently improve\nTransformer's performance and also significantly outperform learning in Fourier\nspace. We further show our method can enhance Transformer's reasoning\nextrapolation capability over distance on the LEGO chain-of-reasoning task.",
    "published": "2022-10-05T02:37:59Z",
    "updated": "2023-05-22T22:42:47Z",
    "authors": [
      "Yufan Zhuang",
      "Zihan Wang",
      "Fangbo Tao",
      "Jingbo Shang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.01542v1",
    "title": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not\n  Attention",
    "summary": "Recently, a considerable number of studies in computer vision involves deep\nneural architectures called vision transformers. Visual processing in these\nmodels incorporates computational models that are claimed to implement\nattention mechanisms. Despite an increasing body of work that attempts to\nunderstand the role of attention mechanisms in vision transformers, their\neffect is largely unknown. Here, we asked if the attention mechanisms in vision\ntransformers exhibit similar effects as those known in human visual attention.\nTo answer this question, we revisited the attention formulation in these models\nand found that despite the name, computationally, these models perform a\nspecial class of relaxation labeling with similarity grouping effects.\nAdditionally, whereas modern experimental findings reveal that human visual\nattention involves both feed-forward and feedback mechanisms, the purely\nfeed-forward architecture of vision transformers suggests that attention in\nthese models will not have the same effects as those known in humans. To\nquantify these observations, we evaluated grouping performance in a family of\nvision transformers. Our results suggest that self-attention modules group\nfigures in the stimuli based on similarity in visual features such as color.\nAlso, in a singleton detection experiment as an instance of saliency detection,\nwe studied if these models exhibit similar effects as those of feed-forward\nvisual salience mechanisms utilized in human visual attention. We found that\ngenerally, the transformer-based attention modules assign more salience either\nto distractors or the ground. Together, our study suggests that the attention\nmechanisms in vision transformers perform similarity grouping and not\nattention.",
    "published": "2023-03-02T19:18:11Z",
    "updated": "2023-03-02T19:18:11Z",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.02703v2",
    "title": "Selective Attention Improves Transformer",
    "summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention consistently improves language modeling and downstream task\nperformance in a variety of model sizes and context lengths. For example,\ntransformers trained with the language modeling objective on C4 with selective\nattention perform language modeling equivalently to standard transformers with\n~2X more heads and parameters in their attention modules. Selective attention\nalso allows decreasing the size of the attention's context buffer, leading to\nmeaningful reductions in the memory and compute requirements during inference.\nFor example, transformers trained on C4 with context sizes of 512, 1,024, and\n2,048 need 16X, 25X, and 47X less memory for their attention module,\nrespectively, when equipped with selective attention, as those without\nselective attention, with the same validation perplexity.",
    "published": "2024-10-03T17:27:30Z",
    "updated": "2025-04-24T02:44:54Z",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.04347v1",
    "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry",
    "summary": "Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.",
    "published": "2024-02-06T19:31:26Z",
    "updated": "2024-02-06T19:31:26Z",
    "authors": [
      "Michael Zhang",
      "Kush Bhatia",
      "Hermann Kumbong",
      "Christopher RÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.14615v2",
    "title": "Unique Hard Attention: A Tale of Two Sides",
    "summary": "Understanding the expressive power of transformers has recently attracted\nattention, as it offers insights into their abilities and limitations. Many\nstudies analyze unique hard attention transformers, where attention selects a\nsingle position that maximizes the attention scores. When multiple positions\nachieve the maximum score, either the rightmost or the leftmost of those is\nchosen. In this paper, we highlight the importance of this seeming triviality.\nRecently, finite-precision transformers with both leftmost- and rightmost-hard\nattention were shown to be equivalent to Linear Temporal Logic (LTL). We show\nthat this no longer holds with only leftmost-hard attention -- in that case,\nthey correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we\nshow that models with leftmost-hard attention are equivalent to \\emph{soft}\nattention, suggesting they may better approximate real-world transformers than\nright-attention models. These findings refine the landscape of transformer\nexpressivity and underscore the role of attention directionality.",
    "published": "2025-03-18T18:12:09Z",
    "updated": "2025-06-02T18:30:46Z",
    "authors": [
      "Selim Jerad",
      "Anej Svete",
      "Jiaoda Li",
      "Ryan Cotterell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.05786v1",
    "title": "CAT: Cross Attention in Vision Transformer",
    "summary": "Since Transformer has found widespread use in NLP, the potential of\nTransformer in CV has been realized and has inspired many new approaches.\nHowever, the computation required for replacing word tokens with image patches\nfor Transformer after the tokenization of the image is vast(e.g., ViT), which\nbottlenecks model training and inference. In this paper, we propose a new\nattention mechanism in Transformer termed Cross Attention, which alternates\nattention inner the image patch instead of the whole image to capture local\ninformation and apply attention between image patches which are divided from\nsingle-channel feature maps capture global information. Both operations have\nless computation than standard self-attention in Transformer. By alternately\napplying attention inner patch and between patches, we implement cross\nattention to maintain the performance with lower computational cost and build a\nhierarchical network called Cross Attention Transformer(CAT) for other vision\ntasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves\nthe performance of other methods on COCO and ADE20K, illustrating that our\nnetwork has the potential to serve as general backbones. The code and models\nare available at \\url{https://github.com/linhezheng19/CAT}.",
    "published": "2021-06-10T14:38:32Z",
    "updated": "2021-06-10T14:38:32Z",
    "authors": [
      "Hezheng Lin",
      "Xing Cheng",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Qing Song",
      "Wei Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.00427v1",
    "title": "You Only Need Less Attention at Each Stage in Vision Transformers",
    "summary": "The advent of Vision Transformers (ViTs) marks a substantial paradigm shift\nin the realm of computer vision. ViTs capture the global information of images\nthrough self-attention modules, which perform dot product computations among\npatchified image tokens. While self-attention modules empower ViTs to capture\nlong-range dependencies, the computational complexity grows quadratically with\nthe number of tokens, which is a major hindrance to the practical application\nof ViTs. Moreover, the self-attention mechanism in deep ViTs is also\nsusceptible to the attention saturation issue. Accordingly, we argue against\nthe necessity of computing the attention scores in every layer, and we propose\nthe Less-Attention Vision Transformer (LaViT), which computes only a few\nattention operations at each stage and calculates the subsequent feature\nalignments in other layers via attention transformations that leverage the\npreviously calculated attention scores. This novel approach can mitigate two\nprimary issues plaguing traditional self-attention modules: the heavy\ncomputational burden and attention saturation. Our proposed architecture offers\nsuperior efficiency and ease of implementation, merely requiring matrix\nmultiplications that are highly optimized in contemporary deep learning\nframeworks. Moreover, our architecture demonstrates exceptional performance\nacross various vision tasks including classification, detection and\nsegmentation.",
    "published": "2024-06-01T12:49:16Z",
    "updated": "2024-06-01T12:49:16Z",
    "authors": [
      "Shuoxi Zhang",
      "Hanpeng Liu",
      "Stephen Lin",
      "Kun He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.15578v1",
    "title": "Generalized Probabilistic Attention Mechanism in Transformers",
    "summary": "The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.",
    "published": "2024-10-21T01:55:52Z",
    "updated": "2024-10-21T01:55:52Z",
    "authors": [
      "DongNyeong Heo",
      "Heeyoul Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.05495v1",
    "title": "Is Attentional Channel Processing Design Required? Comprehensive\n  Analysis Of Robustness Between Vision Transformers And Fully Attentional\n  Networks",
    "summary": "The robustness testing has been performed for standard CNN models and Vision\nTransformers, however there is a lack of comprehensive study between the\nrobustness of traditional Vision Transformers without an extra attentional\nchannel design and the latest fully attentional network(FAN) models. So in this\npaper, we use the ImageNet dataset to compare the robustness of fully\nattentional network(FAN) models with traditional Vision Transformers to\nunderstand the role of an attentional channel processing design using white box\nattacks and also study the transferability between the same using black box\nattacks.",
    "published": "2023-06-08T18:33:12Z",
    "updated": "2023-06-08T18:33:12Z",
    "authors": [
      "Abhishri Ajit Medewar",
      "Swanand Ashokrao Kavitkar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20666v1",
    "title": "Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence\n  Transformers",
    "summary": "We propose a novel framework, Continuous_Time Attention, which infuses\npartial differential equations (PDEs) into the Transformer's attention\nmechanism to address the challenges of extremely long input sequences. Instead\nof relying solely on a static attention matrix, we allow attention weights to\nevolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion\ndynamics. This mechanism systematically smooths local noise, enhances\nlong_range dependencies, and stabilizes gradient flow. Theoretically, our\nanalysis shows that PDE_based attention leads to better optimization landscapes\nand polynomial rather than exponential decay of distant interactions.\nEmpirically, we benchmark our method on diverse experiments_demonstrating\nconsistent gains over both standard and specialized long sequence Transformer\nvariants. Our findings highlight the potential of PDE_based formulations to\nenrich attention mechanisms with continuous_time dynamics and global coherence.",
    "published": "2025-05-27T03:30:10Z",
    "updated": "2025-05-27T03:30:10Z",
    "authors": [
      "Yukun Zhang",
      "Xueqing Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.02636v1",
    "title": "Boosting Crowd Counting via Multifaceted Attention",
    "summary": "This paper focuses on the challenging crowd counting task. As large-scale\nvariations often exist within crowd images, neither fixed-size convolution\nkernel of CNN nor fixed-size attention of recent vision transformers can well\nhandle this kind of variation. To address this problem, we propose a\nMultifaceted Attention Network (MAN) to improve transformer models in local\nspatial relation encoding. MAN incorporates global attention from a vanilla\ntransformer, learnable local attention, and instance attention into a counting\nmodel. Firstly, the local Learnable Region Attention (LRA) is proposed to\nassign attention exclusively for each feature location dynamically. Secondly,\nwe design the Local Attention Regularization to supervise the training of LRA\nby minimizing the deviation among the attention for different feature\nlocations. Finally, we provide an Instance Attention mechanism to focus on the\nmost important instances dynamically during training. Extensive experiments on\nfour challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++,\nand NWPU have validated the proposed method. Codes:\nhttps://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.",
    "published": "2022-03-05T01:36:43Z",
    "updated": "2022-03-05T01:36:43Z",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Rongrong Ji",
      "Yaowei Wang",
      "Xiaopeng Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1907.06607v1",
    "title": "Agglomerative Attention",
    "summary": "Neural networks using transformer-based architectures have recently\ndemonstrated great power and flexibility in modeling sequences of many types.\nOne of the core components of transformer networks is the attention layer,\nwhich allows contextual information to be exchanged among sequence elements.\nWhile many of the prevalent network structures thus far have utilized full\nattention -- which operates on all pairs of sequence elements -- the quadratic\nscaling of this attention mechanism significantly constrains the size of models\nthat can be trained. In this work, we present an attention model that has only\nlinear requirements in memory and computation time. We show that, despite the\nsimpler attention model, networks using this attention mechanism can attain\ncomparable performance to full attention networks on language modeling tasks.",
    "published": "2019-07-15T17:11:05Z",
    "updated": "2019-07-15T17:11:05Z",
    "authors": [
      "Matthew Spellings"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18387v1",
    "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little",
    "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.",
    "published": "2025-08-25T18:19:21Z",
    "updated": "2025-08-25T18:19:21Z",
    "authors": [
      "Ivan Kobyzev",
      "Abbas Ghaddar",
      "Dingtao Hu",
      "Boxing Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.09193v3",
    "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective\n  Transformer",
    "summary": "Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.",
    "published": "2021-08-20T14:22:00Z",
    "updated": "2021-09-02T06:44:38Z",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Binxing Jiao",
      "Daxin Jiang",
      "Yongfeng Huang",
      "Xing Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.08874v3",
    "title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "summary": "The attention module is the key component in Transformers. While the global\nattention mechanism offers high expressiveness, its excessive computational\ncost restricts its applicability in various scenarios. In this paper, we\npropose a novel attention paradigm, Agent Attention, to strike a favorable\nbalance between computational efficiency and representation power.\nSpecifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$,\nintroduces an additional set of agent tokens $A$ into the conventional\nattention module. The agent tokens first act as the agent for the query tokens\n$Q$ to aggregate information from $K$ and $V$, and then broadcast the\ninformation back to $Q$. Given the number of agent tokens can be designed to be\nmuch smaller than the number of query tokens, the agent attention is\nsignificantly more efficient than the widely adopted Softmax attention, while\npreserving global context modelling capability. Interestingly, we show that the\nproposed agent attention is equivalent to a generalized form of linear\nattention. Therefore, agent attention seamlessly integrates the powerful\nSoftmax attention and the highly efficient linear attention. Extensive\nexperiments demonstrate the effectiveness of agent attention with various\nvision Transformers and across diverse vision tasks, including image\nclassification, object detection, semantic segmentation and image generation.\nNotably, agent attention has shown remarkable performance in high-resolution\nscenarios, owning to its linear attention nature. For instance, when applied to\nStable Diffusion, our agent attention accelerates generation and substantially\nenhances image generation quality without any additional training. Code is\navailable at https://github.com/LeapLabTHU/Agent-Attention.",
    "published": "2023-12-14T16:26:29Z",
    "updated": "2024-07-15T09:42:48Z",
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.10102v2",
    "title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
    "summary": "Attention is a key component of Transformers, which have recently achieved\nconsiderable success in natural language processing. Hence, attention is being\nextensively studied to investigate various linguistic capabilities of\nTransformers, focusing on analyzing the parallels between attention weights and\nspecific linguistic phenomena. This paper shows that attention weights alone\nare only one of the two factors that determine the output of attention and\nproposes a norm-based analysis that incorporates the second factor, the norm of\nthe transformed input vectors. The findings of our norm-based analyses of BERT\nand a Transformer-based neural machine translation system include the\nfollowing: (i) contrary to previous studies, BERT pays poor attention to\nspecial tokens, and (ii) reasonable word alignment can be extracted from\nattention mechanisms of Transformer. These findings provide insights into the\ninner workings of Transformers.",
    "published": "2020-04-21T15:22:27Z",
    "updated": "2020-10-06T15:15:38Z",
    "authors": [
      "Goro Kobayashi",
      "Tatsuki Kuribayashi",
      "Sho Yokoi",
      "Kentaro Inui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.09431v1",
    "title": "Striped Attention: Faster Ring Attention for Causal Transformers",
    "summary": "To help address the growing demand for ever-longer sequence lengths in\ntransformer models, Liu et al. recently proposed Ring Attention, an exact\nattention algorithm capable of overcoming per-device memory bottle- necks by\ndistributing self-attention across multiple devices. In this paper, we study\nthe performance characteristics of Ring Attention in the important special case\nof causal transformer models, and identify a key workload imbal- ance due to\ntriangular structure of causal attention computations. We propose a simple\nextension to Ring Attention, which we call Striped Attention to fix this\nimbalance. Instead of devices having contiguous subsequences, each device has a\nsubset of tokens distributed uniformly throughout the sequence, which we\ndemonstrate leads to more even workloads. In experiments running Striped\nAttention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x\nend-to-end throughput improvements over the original Ring Attention algorithm\non causal transformer training at a sequence length of 256k. Furthermore, on 16\nTPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of\n786k. We release the code for our experiments as open source",
    "published": "2023-11-15T23:01:02Z",
    "updated": "2023-11-15T23:01:02Z",
    "authors": [
      "William Brandon",
      "Aniruddha Nrusimha",
      "Kevin Qian",
      "Zachary Ankner",
      "Tian Jin",
      "Zhiye Song",
      "Jonathan Ragan-Kelley"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.13781v1",
    "title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "summary": "Self-attention is key to the remarkable success of transformers in sequence\nmodeling tasks including many applications in natural language processing and\ncomputer vision. Like neural network layers, these attention mechanisms are\noften developed by heuristics and experience. To provide a principled framework\nfor constructing attention layers in transformers, we show that the\nself-attention corresponds to the support vector expansion derived from a\nsupport vector regression problem, whose primal formulation has the form of a\nneural network layer. Using our framework, we derive popular attention layers\nused in practice and propose two new attentions: 1) the Batch Normalized\nAttention (Attention-BN) derived from the batch normalization layer and 2) the\nAttention with Scaled Head (Attention-SH) derived from using less training data\nto fit the SVR model. We empirically demonstrate the advantages of the\nAttention-BN and Attention-SH in reducing head redundancy, increasing the\nmodel's accuracy, and improving the model's efficiency in a variety of\npractical applications including image and time-series classification.",
    "published": "2024-06-19T19:11:22Z",
    "updated": "2024-06-19T19:11:22Z",
    "authors": [
      "Tan M. Nguyen",
      "Tam Nguyen",
      "Nhat Ho",
      "Andrea L. Bertozzi",
      "Richard G. Baraniuk",
      "Stanley J. Osher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.04082v1",
    "title": "Curve Your Attention: Mixed-Curvature Transformers for Graph\n  Representation Learning",
    "summary": "Real-world graphs naturally exhibit hierarchical or cyclical structures that\nare unfit for the typical Euclidean space. While there exist graph neural\nnetworks that leverage hyperbolic or spherical spaces to learn representations\nthat embed such structures more accurately, these methods are confined under\nthe message-passing paradigm, making the models vulnerable against side-effects\nsuch as oversmoothing and oversquashing. More recent work have proposed global\nattention-based graph Transformers that can easily model long-range\ninteractions, but their extensions towards non-Euclidean geometry are yet\nunexplored. To bridge this gap, we propose Fully Product-Stereographic\nTransformer, a generalization of Transformers towards operating entirely on the\nproduct of constant curvature spaces. When combined with tokenized graph\nTransformers, our model can learn the curvature appropriate for the input graph\nin an end-to-end fashion, without the need of additional tuning on different\ncurvature initializations. We also provide a kernelized approach to\nnon-Euclidean attention, which enables our model to run in time and memory cost\nlinear to the number of nodes and edges while respecting the underlying\ngeometry. Experiments on graph reconstruction and node classification\ndemonstrate the benefits of generalizing Transformers to the non-Euclidean\ndomain.",
    "published": "2023-09-08T02:44:37Z",
    "updated": "2023-09-08T02:44:37Z",
    "authors": [
      "Sungjun Cho",
      "Seunghyuk Cho",
      "Sungwoo Park",
      "Hankook Lee",
      "Honglak Lee",
      "Moontae Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.00662v1",
    "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
    "summary": "The current paradigm of large-scale pre-training and fine-tuning Transformer\nlarge language models has lead to significant improvements across the board in\nnatural language processing. However, such large models are susceptible to\noverfitting to their training data, and as a result the models perform poorly\nwhen the domain changes. Also, due to the model's scale, the cost of\nfine-tuning the model to the new domain is large. Nonparametric Variational\nInformation Bottleneck (NVIB) has been proposed as a regulariser for training\ncross-attention in Transformers, potentially addressing the overfitting\nproblem. We extend the NVIB framework to replace all types of attention\nfunctions in Transformers, and show that existing pretrained Transformers can\nbe reinterpreted as Nonparametric Variational (NV) models using a proposed\nidentity initialisation. We then show that changing the initialisation\nintroduces a novel, information-theoretic post-training regularisation in the\nattention mechanism, which improves out-of-domain generalisation without any\ntraining. This success supports the hypothesis that pretrained Transformers are\nimplicitly NV Bayesian models.",
    "published": "2023-12-01T15:40:30Z",
    "updated": "2023-12-01T15:40:30Z",
    "authors": [
      "Fabio Fehr",
      "James Henderson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.01324v2",
    "title": "MABViT -- Modified Attention Block Enhances Vision Transformers",
    "summary": "Recent studies have demonstrated the effectiveness of Gated Linear Units\n(GLU) in enhancing transformer models, particularly in Large Language Models\n(LLMs). Additionally, utilizing a parallel configuration within each\nTransformer block rather than the conventional serialized method has been\nrevealed to accelerate the training of LLMs without significantly impacting\nperformance. However, when the MLP and attention block were run in parallel for\nthe image classification task, we observed a noticeable decline in performance.\nWe propose a novel transformer variant that integrates non-linearity within the\nattention block to tackle this problem. We implemented the GLU-based activation\nfunction on the Value tensor, and this new technique surpasses the current\nstate-of-the-art S/16 variant of Vision Transformers by 0.6% on the ImageNet-1K\ndataset while utilizing fewer parameters. It also supersedes the B/16 variant\nwhile using only half the parameters. Furthermore, we provide results with the\nGELU activation function variant to confirm our assertions. Lastly, we showcase\nthat the MABViT variants exhibit greater potential when utilized in deep\ntransformers compared to the standard architecture.",
    "published": "2023-12-03T09:00:31Z",
    "updated": "2024-01-01T13:27:15Z",
    "authors": [
      "Mahesh Ramesh",
      "Aswinkumar Ramkumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.04953v1",
    "title": "Adaptive-avg-pooling based Attention Vision Transformer for Face\n  Anti-spoofing",
    "summary": "Traditional vision transformer consists of two parts: transformer encoder and\nmulti-layer perception (MLP). The former plays the role of feature learning to\nobtain better representation, while the latter plays the role of\nclassification. Here, the MLP is constituted of two fully connected (FC)\nlayers, average value computing, FC layer and softmax layer. However, due to\nthe use of average value computing module, some useful information may get\nlost, which we plan to preserve by the use of alternative framework. In this\nwork, we propose a novel vision transformer referred to as adaptive-avg-pooling\nbased attention vision transformer (AAViT) that uses modules of adaptive\naverage pooling and attention to replace the module of average value computing.\nWe explore the proposed AAViT for the studies on face anti-spoofing using\nReplay-Attack database. The experiments show that the AAViT outperforms vision\ntransformer in face anti-spoofing by producing a reduced equal error rate. In\naddition, we found that the proposed AAViT can perform much better than some\ncommonly used neural networks such as ResNet and some other known systems on\nthe Replay-Attack corpus.",
    "published": "2024-01-10T06:26:28Z",
    "updated": "2024-01-10T06:26:28Z",
    "authors": [
      "Jichen Yang",
      "Fangfan Chen",
      "Rohan Kumar Das",
      "Zhengyu Zhu",
      "Shunsi Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.11273v1",
    "title": "Training Transformer Models by Wavelet Losses Improves Quantitative and\n  Visual Performance in Single Image Super-Resolution",
    "summary": "Transformer-based models have achieved remarkable results in low-level vision\ntasks including image super-resolution (SR). However, early Transformer-based\napproaches that rely on self-attention within non-overlapping windows encounter\nchallenges in acquiring global information. To activate more input pixels\nglobally, hybrid attention models have been proposed. Moreover, training by\nsolely minimizing pixel-wise RGB losses, such as L1, have been found inadequate\nfor capturing essential high-frequency details. This paper presents two\ncontributions: i) We introduce convolutional non-local sparse attention (NLSA)\nblocks to extend the hybrid transformer architecture in order to further\nenhance its receptive field. ii) We employ wavelet losses to train Transformer\nmodels to improve quantitative and subjective performance. While wavelet losses\nhave been explored previously, showing their power in training\nTransformer-based SR models is novel. Our experimental results demonstrate that\nthe proposed model provides state-of-the-art PSNR results as well as superior\nvisual performance across various benchmark datasets.",
    "published": "2024-04-17T11:25:19Z",
    "updated": "2024-04-17T11:25:19Z",
    "authors": [
      "Cansu Korkmaz",
      "A. Murat Tekalp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.01102v2",
    "title": "Less is More: on the Over-Globalizing Problem in Graph Transformers",
    "summary": "Graph Transformer, due to its global attention mechanism, has emerged as a\nnew tool in dealing with graph-structured data. It is well recognized that the\nglobal attention mechanism considers a wider receptive field in a fully\nconnected graph, leading many to believe that useful information can be\nextracted from all the nodes. In this paper, we challenge this belief: does the\nglobalizing property always benefit Graph Transformers? We reveal the\nover-globalizing problem in Graph Transformer by presenting both empirical\nevidence and theoretical analysis, i.e., the current attention mechanism overly\nfocuses on those distant nodes, while the near nodes, which actually contain\nmost of the useful information, are relatively weakened. Then we propose a\nnovel Bi-Level Global Graph Transformer with Collaborative Training\n(CoBFormer), including the inter-cluster and intra-cluster Transformers, to\nprevent the over-globalizing problem while keeping the ability to extract\nvaluable information from distant nodes. Moreover, the collaborative training\nis proposed to improve the model's generalization ability with a theoretical\nguarantee. Extensive experiments on various graphs well validate the\neffectiveness of our proposed CoBFormer.",
    "published": "2024-05-02T09:12:22Z",
    "updated": "2024-05-24T08:53:13Z",
    "authors": [
      "Yujie Xing",
      "Xiao Wang",
      "Yibo Li",
      "Hai Huang",
      "Chuan Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.09161v1",
    "title": "Complex Image-Generative Diffusion Transformer for Audio Denoising",
    "summary": "The audio denoising technique has captured widespread attention in the deep\nneural network field. Recently, the audio denoising problem has been converted\ninto an image generation task, and deep learning-based approaches have been\napplied to tackle this problem. However, its performance is still limited,\nleaving room for further improvement. In order to enhance audio denoising\nperformance, this paper introduces a complex image-generative diffusion\ntransformer that captures more information from the complex Fourier domain. We\nexplore a novel diffusion transformer by integrating the transformer with a\ndiffusion model. Our proposed model demonstrates the scalability of the\ntransformer and expands the receptive field of sparse attention using attention\ndiffusion. Our work is among the first to utilize diffusion transformers to\ndeal with the image generation task for audio denoising. Extensive experiments\non two benchmark datasets demonstrate that our proposed model outperforms\nstate-of-the-art methods.",
    "published": "2024-06-13T14:23:19Z",
    "updated": "2024-06-13T14:23:19Z",
    "authors": [
      "Junhui Li",
      "Pu Wang",
      "Jialu Li",
      "Youshan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.08711v1",
    "title": "On-Chip Learning via Transformer In-Context Learning",
    "summary": "Autoregressive decoder-only transformers have become key components for\nscalable sequence processing and generation models. However, the transformer's\nself-attention mechanism requires transferring prior token projections from the\nmain memory at each time step (token), thus severely limiting their performance\non conventional processors. Self-attention can be viewed as a dynamic\nfeed-forward layer, whose matrix is input sequence-dependent similarly to the\nresult of local synaptic plasticity. Using this insight, we present a\nneuromorphic decoder-only transformer model that utilizes an on-chip plasticity\nprocessor to compute self-attention. Interestingly, the training of\ntransformers enables them to ``learn'' the input context during inference. We\ndemonstrate this in-context learning ability of transformers on the Loihi 2\nprocessor by solving a few-shot classification problem. With this we emphasize\nthe importance of pretrained models especially their ability to find simple,\nlocal, backpropagation free, learning rules enabling on-chip learning and\nadaptation in a hardware friendly manner.",
    "published": "2024-10-11T10:54:09Z",
    "updated": "2024-10-11T10:54:09Z",
    "authors": [
      "Jan Finkbeiner",
      "Emre Neftci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.04073v1",
    "title": "TransAdapter: Vision Transformer for Feature-Centric Unsupervised Domain\n  Adaptation",
    "summary": "Unsupervised Domain Adaptation (UDA) aims to utilize labeled data from a\nsource domain to solve tasks in an unlabeled target domain, often hindered by\nsignificant domain gaps. Traditional CNN-based methods struggle to fully\ncapture complex domain relationships, motivating the shift to vision\ntransformers like the Swin Transformer, which excel in modeling both local and\nglobal dependencies. In this work, we propose a novel UDA approach leveraging\nthe Swin Transformer with three key modules. A Graph Domain Discriminator\nenhances domain alignment by capturing inter-pixel correlations through graph\nconvolutions and entropy-based attention differentiation. An Adaptive Double\nAttention module combines Windows and Shifted Windows attention with dynamic\nreweighting to align long-range and local features effectively. Finally, a\nCross-Feature Transform modifies Swin Transformer blocks to improve\ngeneralization across domains. Extensive benchmarks confirm the\nstate-of-the-art performance of our versatile method, which requires no\ntask-specific alignment modules, establishing its adaptability to diverse\napplications.",
    "published": "2024-12-05T11:11:39Z",
    "updated": "2024-12-05T11:11:39Z",
    "authors": [
      "A. Enes Doruk",
      "Erhan Oztop",
      "Hasan F. Ates"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.17571v1",
    "title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with\n  Transformer Attention for FPGA-based Particle Physics",
    "summary": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of\nSpiking Neural Networks (SNNs), Transformers, and high-performance computing\ntailored for particle physics, particularly in particle identification from\ndetector responses. Our approach leverages SNNs' intrinsic temporal dynamics\nand Transformers' robust attention mechanisms to enhance performance when\ndiscerning intricate particle interactions. At the heart of HPCNeuroNet lies\nthe integration of the sequential dynamism inherent in SNNs with the\ncontext-aware attention capabilities of Transformers, enabling the model to\nprecisely decode and interpret complex detector data. HPCNeuroNet is realized\nthrough the HLS4ML framework and optimized for deployment in FPGA environments.\nThe model accuracy and scalability are also enhanced by this architectural\nchoice. Benchmarked against machine learning models, HPCNeuroNet showcases\nbetter performance metrics, underlining its transformative potential in\nhigh-energy physics. We demonstrate that the combination of SNNs, Transformers,\nand FPGA-based high-performance computing in particle physics signifies a\nsignificant step forward and provides a strong foundation for future research.",
    "published": "2024-12-23T13:44:29Z",
    "updated": "2024-12-23T13:44:29Z",
    "authors": [
      "Murat Isik",
      "Hiruna Vishwamith",
      "Jonathan Naoukin",
      "I. Can Dikmen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.16955v1",
    "title": "CASK: A Gauge Covariant Transformer for Lattice Gauge Theory",
    "summary": "We propose a Transformer neural network architecture specifically designed\nfor lattice QCD, focusing on preserving the fundamental symmetries required in\nlattice gauge theory. The proposed architecture is gauge covariant/equivariant,\nensuring it respects gauge symmetry on the lattice, and is also equivariant\nunder spacetime symmetries such as rotations and translations on the lattice. A\nkey feature of our approach lies in the attention matrix, which forms the core\nof the Transformer architecture. To preserve symmetries, we define the\nattention matrix using a Frobenius inner product between link variables and\nextended staples. This construction ensures that the attention matrix remains\ninvariant under gauge transformations, thereby making the entire Transformer\narchitecture covariant. We evaluated the performance of the gauge covariant\nTransformer in the context of self-learning HMC. Numerical experiments show\nthat the proposed architecture achieves higher performance compared to the\ngauge covariant neural networks, demonstrating its potential to improve lattice\nQCD calculations.",
    "published": "2025-01-28T13:56:42Z",
    "updated": "2025-01-28T13:56:42Z",
    "authors": [
      "Yuki Nagai",
      "Hiroshi Ohno",
      "Akio Tomiya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01519v1",
    "title": "Speed-up of Vision Transformer Models by Attention-aware Token Filtering",
    "summary": "Vision Transformer (ViT) models have made breakthroughs in image embedding\nextraction, which provide state-of-the-art performance in tasks such as\nzero-shot image classification. However, the models suffer from a high\ncomputational burden. In this paper, we propose a novel speed-up method for ViT\nmodels called Attention-aware Token Filtering (ATF). ATF consists of two main\nideas: a novel token filtering module and a filtering strategy. The token\nfiltering module is introduced between a tokenizer and a transformer encoder of\nthe ViT model, without modifying or fine-tuning of the transformer encoder. The\nmodule filters out tokens inputted to the encoder so that it keeps tokens in\nregions of specific object types dynamically and keeps tokens in regions that\nstatically receive high attention in the transformer encoder. This filtering\nstrategy maintains task accuracy while filtering out tokens inputted to the\ntransformer encoder. Evaluation results on retrieval tasks show that ATF\nprovides $2.8\\times$ speed-up to a ViT model, SigLIP, while maintaining the\nretrieval recall rate.",
    "published": "2025-06-02T10:34:55Z",
    "updated": "2025-06-02T10:34:55Z",
    "authors": [
      "Takahiro Naruko",
      "Hiroaki Akutsu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03790v1",
    "title": "Attention-Only Transformers via Unrolled Subspace Denoising",
    "summary": "Despite the popularity of transformers in practice, their architectures are\nempirically designed and neither mathematically justified nor interpretable.\nMoreover, as indicated by many empirical studies, some components of\ntransformer architectures may be redundant. To derive a fully interpretable\ntransformer architecture with only necessary components, we contend that the\ngoal of representation learning is to compress a set of noisy initial token\nrepresentations towards a mixture of low-dimensional subspaces. To compress\nthese noisy token representations, an associated denoising operation naturally\ntakes the form of a multi-head (subspace) self-attention. By unrolling such\niterative denoising operations into a deep network, we arrive at a highly\ncompact architecture that consists of \\textit{only} self-attention operators\nwith skip connections at each layer. Moreover, we show that each layer performs\nhighly efficient denoising: it improves the signal-to-noise ratio of token\nrepresentations \\textit{at a linear rate} with respect to the number of layers.\nDespite its simplicity, extensive experiments on vision and language tasks\ndemonstrate that such a transformer achieves performance close to that of\nstandard transformer architectures such as GPT-2 and CRATE.",
    "published": "2025-06-04T09:53:14Z",
    "updated": "2025-06-04T09:53:14Z",
    "authors": [
      "Peng Wang",
      "Yifu Lu",
      "Yaodong Yu",
      "Druv Pai",
      "Qing Qu",
      "Yi Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.15150v2",
    "title": "Improving BERT with Syntax-aware Local Attention",
    "summary": "Pre-trained Transformer-based neural language models, such as BERT, have\nachieved remarkable results on varieties of NLP tasks. Recent works have shown\nthat attention-based models can benefit from more focused attention over local\nregions. Most of them restrict the attention scope within a linear span, or\nconfine to certain tasks such as machine translation and question answering. In\nthis paper, we propose a syntax-aware local attention, where the attention\nscopes are restrained based on the distances in the syntactic structure. The\nproposed syntax-aware local attention can be integrated with pretrained\nlanguage models, such as BERT, to render the model to focus on syntactically\nrelevant words. We conduct experiments on various single-sentence benchmarks,\nincluding sentence classification and sequence labeling tasks. Experimental\nresults show consistent gains over BERT on all benchmark datasets. The\nextensive studies verify that our model achieves better performance owing to\nmore focused attention over syntactically relevant words.",
    "published": "2020-12-30T13:29:58Z",
    "updated": "2021-05-24T12:59:51Z",
    "authors": [
      "Zhongli Li",
      "Qingyu Zhou",
      "Chao Li",
      "Ke Xu",
      "Yunbo Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.13139v1",
    "title": "Multimodal Integration of Human-Like Attention in Visual Question\n  Answering",
    "summary": "Human-like attention as a supervisory signal to guide neural attention has\nshown significant promise but is currently limited to uni-modal integration -\neven for inherently multimodal tasks such as visual question answering (VQA).\nWe present the Multimodal Human-like Attention Network (MULAN) - the first\nmethod for multimodal integration of human-like attention on image and text\nduring training of VQA models. MULAN integrates attention predictions from two\nstate-of-the-art text and image saliency models into neural self-attention\nlayers of a recent transformer-based VQA model. Through evaluations on the\nchallenging VQAv2 dataset, we show that MULAN achieves a new state-of-the-art\nperformance of 73.98% accuracy on test-std and 73.72% on test-dev and, at the\nsame time, has approximately 80% fewer trainable parameters than prior work.\nOverall, our work underlines the potential of integrating multimodal human-like\nand neural attention for VQA",
    "published": "2021-09-27T15:56:54Z",
    "updated": "2021-09-27T15:56:54Z",
    "authors": [
      "Ekta Sood",
      "Fabian KÃ¶gel",
      "Philipp MÃ¼ller",
      "Dominike Thomas",
      "Mihai Bace",
      "Andreas Bulling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.10962v3",
    "title": "Visual Attention Emerges from Recurrent Sparse Reconstruction",
    "summary": "Visual attention helps achieve robust perception under noise, corruption, and\ndistribution shifts in human vision, which are areas where modern neural\nnetworks still fall short. We present VARS, Visual Attention from Recurrent\nSparse reconstruction, a new attention formulation built on two prominent\nfeatures of the human visual attention mechanism: recurrency and sparsity.\nRelated features are grouped together via recurrent connections between\nneurons, with salient objects emerging via sparse regularization. VARS adopts\nan attractor network with recurrent connections that converges toward a stable\npattern over time. Network layers are represented as ordinary differential\nequations (ODEs), formulating attention as a recurrent attractor network that\nequivalently optimizes the sparse reconstruction of input using a dictionary of\n\"templates\" encoding underlying patterns of data. We show that self-attention\nis a special case of VARS with a single-step optimization and no sparsity\nconstraint. VARS can be readily used as a replacement for self-attention in\npopular vision transformers, consistently improving their robustness across\nvarious benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).",
    "published": "2022-04-23T00:35:02Z",
    "updated": "2022-06-12T23:57:50Z",
    "authors": [
      "Baifeng Shi",
      "Yale Song",
      "Neel Joshi",
      "Trevor Darrell",
      "Xin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.05144v1",
    "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token",
    "summary": "Mixture-of-Experts (MoE) networks have been proposed as an efficient way to\nscale up model capacity and implement conditional computing. However, the study\nof MoE components mostly focused on the feedforward layer in Transformer\narchitecture. This paper proposes the Mixture of Attention Heads (MoA), a new\narchitecture that combines multi-head attention with the MoE mechanism. MoA\nincludes a set of attention heads that each has its own set of parameters.\nGiven an input, a router dynamically selects a subset of $k$ attention heads\nper token. This conditional computation schema allows MoA to achieve stronger\nperformance than the standard multi-head attention layer. Furthermore, the\nsparsely gated MoA can easily scale up the number of attention heads and the\nnumber of parameters while preserving computational efficiency. In addition to\nthe performance improvements, MoA also automatically differentiates heads'\nutilities, providing a new perspective to discuss the model's interpretability.\nWe conducted experiments on several important tasks, including Machine\nTranslation and Masked Language Modeling. Experiments have shown promising\nresults on several tasks against strong baselines that involve large and very\ndeep models.",
    "published": "2022-10-11T04:54:05Z",
    "updated": "2022-10-11T04:54:05Z",
    "authors": [
      "Xiaofeng Zhang",
      "Yikang Shen",
      "Zeyu Huang",
      "Jie Zhou",
      "Wenge Rong",
      "Zhang Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1905.02719v2",
    "title": "Intentional Attention Mask Transformation for Robust CNN Classification",
    "summary": "Convolutional Neural Networks have achieved impressive results in various\ntasks, but interpreting the internal mechanism is a challenging problem. To\ntackle this problem, we exploit a multi-channel attention mechanism in feature\nspace. Our network architecture allows us to obtain an attention mask for each\nfeature while existing CNN visualization methods provide only a common\nattention mask for all features. We apply the proposed multi-channel attention\nmechanism to multi-attribute recognition task. We can obtain different\nattention mask for each feature and for each attribute. Those analyses give us\ndeeper insight into the feature space of CNNs. Furthermore, our proposed\nattention mechanism naturally derives a method for improving the robustness of\nCNNs. From the observation of feature space based on the proposed attention\nmask, we demonstrate that we can obtain robust CNNs by intentionally\nemphasizing features that are important for attributes. The experimental\nresults for the benchmark dataset show that the proposed method gives high\nhuman interpretability while accurately grasping the attributes of the data,\nand improves network robustness.",
    "published": "2019-05-07T08:16:46Z",
    "updated": "2019-05-20T06:22:10Z",
    "authors": [
      "Masanari Kimura",
      "Masayuki Tanaka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.03766v1",
    "title": "Improving Attention Mechanism with Query-Value Interaction",
    "summary": "Attention mechanism has played critical roles in various state-of-the-art NLP\nmodels such as Transformer and BERT. It can be formulated as a ternary function\nthat maps the input queries, keys and values into an output by using a\nsummation of values weighted by the attention weights derived from the\ninteractions between queries and keys. Similar with query-key interactions,\nthere is also inherent relatedness between queries and values, and\nincorporating query-value interactions has the potential to enhance the output\nby learning customized values according to the characteristics of queries.\nHowever, the query-value interactions are ignored by existing attention\nmethods, which may be not optimal. In this paper, we propose to improve the\nexisting attention mechanism by incorporating query-value interactions. We\npropose a query-value interaction function which can learn query-aware\nattention values, and combine them with the original values and attention\nweights to form the final output. Extensive experiments on four datasets for\ndifferent tasks show that our approach can consistently improve the performance\nof many attention-based models by incorporating query-value interactions.",
    "published": "2020-10-08T05:12:52Z",
    "updated": "2020-10-08T05:12:52Z",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.05315v1",
    "title": "SMYRF: Efficient Attention using Asymmetric Clustering",
    "summary": "We propose a novel type of balanced clustering algorithm to approximate\nattention. Attention complexity is reduced from $O(N^2)$ to $O(N \\log N)$,\nwhere $N$ is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive\nHashing (LSH) in a novel way by defining new Asymmetric transformations and an\nadaptive scheme that produces balanced clusters. The biggest advantage of SMYRF\nis that it can be used as a drop-in replacement for dense attention layers\nwithout any retraining. On the contrary, prior fast attention methods impose\nconstraints (e.g. queries and keys share the same vector representations) and\nrequire re-training from scratch. We apply our method to pre-trained\nstate-of-the-art Natural Language Processing and Computer Vision models and we\nreport significant memory and speed benefits. Notably, SMYRF-BERT outperforms\n(slightly) BERT on GLUE, while using $50\\%$ less memory. We also show that\nSMYRF can be used interchangeably with dense attention before and after\ntraining. Finally, we use SMYRF to train GANs with attention in high\nresolutions. Using a single TPU, we were able to scale attention to 128x128=16k\nand 256x256=65k tokens on BigGAN on CelebA-HQ.",
    "published": "2020-10-11T18:49:17Z",
    "updated": "2020-10-11T18:49:17Z",
    "authors": [
      "Giannis Daras",
      "Nikita Kitaev",
      "Augustus Odena",
      "Alexandros G. Dimakis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.12416v1",
    "title": "SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning",
    "summary": "In recent years, the attention mechanism contributes significantly to\nhypergraph based neural networks. However, these methods update the attention\nweights with the network propagating. That is to say, this type of attention\nmechanism is only suitable for deep learning-based methods while not applicable\nto the traditional machine learning approaches. In this paper, we propose a\nhypergraph based sparse attention mechanism to tackle this issue and embed it\ninto dictionary learning. More specifically, we first construct a sparse\nattention hypergraph, asset attention weights to samples by employing the\n$\\ell_1$-norm sparse regularization to mine the high-order relationship among\nsample features. Then, we introduce the hypergraph Laplacian operator to\npreserve the local structure for subspace transformation in dictionary\nlearning. Besides, we incorporate the discriminative information into the\nhypergraph as the guidance to aggregate samples. Unlike previous works, our\nmethod updates attention weights independently, does not rely on the deep\nnetwork. We demonstrate the efficacy of our approach on four benchmark\ndatasets.",
    "published": "2020-10-23T14:07:00Z",
    "updated": "2020-10-23T14:07:00Z",
    "authors": [
      "Shuai Shao",
      "Rui Xu",
      "Yan-Jiang Wang",
      "Weifeng Liu",
      "Bao-Di Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1909.09595v1",
    "title": "SANVis: Visual Analytics for Understanding Self-Attention Networks",
    "summary": "Attention networks, a deep neural network architecture inspired by humans'\nattention mechanism, have seen significant success in image captioning, machine\ntranslation, and many other applications. Recently, they have been further\nevolved into an advanced approach called multi-head self-attention networks,\nwhich can encode a set of input vectors, e.g., word vectors in a sentence, into\nanother set of vectors. Such encoding aims at simultaneously capturing diverse\nsyntactic and semantic features within a set, each of which corresponds to a\nparticular attention head, forming altogether multi-head attention. Meanwhile,\nthe increased model complexity prevents users from easily understanding and\nmanipulating the inner workings of models. To tackle the challenges, we present\na visual analytics system called SANVis, which helps users understand the\nbehaviors and the characteristics of multi-head self-attention networks. Using\na state-of-the-art self-attention model called Transformer, we demonstrate\nusage scenarios of SANVis in machine translation tasks. Our system is available\nat http://short.sanvis.org",
    "published": "2019-09-13T05:59:40Z",
    "updated": "2019-09-13T05:59:40Z",
    "authors": [
      "Cheonbok Park",
      "Inyoup Na",
      "Yongjang Jo",
      "Sungbok Shin",
      "Jaehyo Yoo",
      "Bum Chul Kwon",
      "Jian Zhao",
      "Hyungjong Noh",
      "Yeonsoo Lee",
      "Jaegul Choo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1909.12406v1",
    "title": "Monotonic Multihead Attention",
    "summary": "Simultaneous machine translation models start generating a target sequence\nbefore they have encoded or read the source sequence. Recent approaches for\nthis task either apply a fixed policy on a state-of-the art Transformer model,\nor a learnable monotonic attention on a weaker recurrent neural network-based\nstructure. In this paper, we propose a new attention mechanism, Monotonic\nMultihead Attention (MMA), which extends the monotonic attention mechanism to\nmultihead attention. We also introduce two novel and interpretable approaches\nfor latency control that are specifically designed for multiple attentions\nheads. We apply MMA to the simultaneous machine translation task and\ndemonstrate better latency-quality tradeoffs compared to MILk, the previous\nstate-of-the-art approach. We also analyze how the latency controls affect the\nattention span and we motivate the introduction of our model by analyzing the\neffect of the number of decoder layers and heads on quality and latency.",
    "published": "2019-09-26T21:32:50Z",
    "updated": "2019-09-26T21:32:50Z",
    "authors": [
      "Xutai Ma",
      "Juan Pino",
      "James Cross",
      "Liezl Puzon",
      "Jiatao Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.12246v1",
    "title": "Do Attention Heads in BERT Track Syntactic Dependencies?",
    "summary": "We investigate the extent to which individual attention heads in pretrained\ntransformer language models, such as BERT and RoBERTa, implicitly capture\nsyntactic dependency relations. We employ two methods---taking the maximum\nattention weight and computing the maximum spanning tree---to extract implicit\ndependency relations from the attention weights of each layer/head, and compare\nthem to the ground-truth Universal Dependency (UD) trees. We show that, for\nsome UD relation types, there exist heads that can recover the dependency type\nsignificantly better than baselines on parsed English text, suggesting that\nsome self-attention heads act as a proxy for syntactic structure. We also\nanalyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the\nsemantics-oriented MNLI---to investigate whether fine-tuning affects the\npatterns of their self-attention, but we do not observe substantial differences\nin the overall dependency relations extracted using our methods. Our results\nsuggest that these models have some specialist attention heads that track\nindividual dependency types, but no generalist head that performs holistic\nparsing significantly better than a trivial baseline, and that analyzing\nattention weights directly may not reveal much of the syntactic knowledge that\nBERT-style models are known to learn.",
    "published": "2019-11-27T16:09:11Z",
    "updated": "2019-11-27T16:09:11Z",
    "authors": [
      "Phu Mon Htut",
      "Jason Phang",
      "Shikha Bordia",
      "Samuel R. Bowman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.09115v1",
    "title": "The heads hypothesis: A unifying statistical approach towards\n  understanding multi-headed attention in BERT",
    "summary": "Multi-headed attention heads are a mainstay in transformer-based models.\nDifferent methods have been proposed to classify the role of each attention\nhead based on the relations between tokens which have high pair-wise attention.\nThese roles include syntactic (tokens with some syntactic relation), local\n(nearby tokens), block (tokens in the same sentence) and delimiter (the special\n[CLS], [SEP] tokens). There are two main challenges with existing methods for\nclassification: (a) there are no standard scores across studies or across\nfunctional roles, and (b) these scores are often average quantities measured\nacross sentences without capturing statistical significance. In this work, we\nformalize a simple yet effective score that generalizes to all the roles of\nattention heads and employs hypothesis testing on this score for robust\ninference. This provides us the right lens to systematically analyze attention\nheads and confidently comment on many commonly posed questions on analyzing the\nBERT model. In particular, we comment on the co-location of multiple functional\nroles in the same attention head, the distribution of attention heads across\nlayers, and effect of fine-tuning for specific NLP tasks on these functional\nroles.",
    "published": "2021-01-22T14:10:59Z",
    "updated": "2021-01-22T14:10:59Z",
    "authors": [
      "Madhura Pande",
      "Aakriti Budhraja",
      "Preksha Nema",
      "Pratyush Kumar",
      "Mitesh M. Khapra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.09003v2",
    "title": "Invertible Attention",
    "summary": "Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks. The code is available at\nhttps://github.com/Schwartz-Zha/InvertibleAttention",
    "published": "2021-06-16T17:55:02Z",
    "updated": "2021-06-27T13:01:09Z",
    "authors": [
      "Jiajun Zha",
      "Yiran Zhong",
      "Jing Zhang",
      "Richard Hartley",
      "Liang Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.09236v1",
    "title": "Efficient Conformer with Prob-Sparse Attention Mechanism for\n  End-to-EndSpeech Recognition",
    "summary": "End-to-end models are favored in automatic speech recognition (ASR) because\nof their simplified system structure and superior performance. Among these\nmodels, Transformer and Conformer have achieved state-of-the-art recognition\naccuracy in which self-attention plays a vital role in capturing important\nglobal information. However, the time and memory complexity of self-attention\nincreases squarely with the length of the sentence. In this paper, a\nprob-sparse self-attention mechanism is introduced into Conformer to sparse the\ncomputing process of self-attention in order to accelerate inference speed and\nreduce space consumption. Specifically, we adopt a Kullback-Leibler divergence\nbased sparsity measurement for each query to decide whether we compute the\nattention function on this query. By using the prob-sparse attention mechanism,\nwe achieve impressively 8% to 45% inference speed-up and 15% to 45% memory\nusage reduction of the self-attention module of Conformer Transducer while\nmaintaining the same level of error rate.",
    "published": "2021-06-17T04:04:04Z",
    "updated": "2021-06-17T04:04:04Z",
    "authors": [
      "Xiong Wang",
      "Sining Sun",
      "Lei Xie",
      "Long Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.07224v2",
    "title": "Local Multi-Head Channel Self-Attention for Facial Expression\n  Recognition",
    "summary": "Since the Transformer architecture was introduced in 2017 there has been many\nattempts to bring the self-attention paradigm in the field of computer vision.\nIn this paper we propose a novel self-attention module that can be easily\nintegrated in virtually every convolutional neural network and that is\nspecifically designed for computer vision, the LHC: Local (multi) Head Channel\n(self-attention). LHC is based on two main ideas: first, we think that in\ncomputer vision the best way to leverage the self-attention paradigm is the\nchannel-wise application instead of the more explored spatial attention and\nthat convolution will not be replaced by attention modules like recurrent\nnetworks were in NLP; second, a local approach has the potential to better\novercome the limitations of convolution than global attention. With LHC-Net we\nmanaged to achieve a new state of the art in the famous FER2013 dataset with a\nsignificantly lower complexity and impact on the \"host\" architecture in terms\nof computational cost when compared with the previous SOTA.",
    "published": "2021-11-14T02:38:01Z",
    "updated": "2021-11-18T17:09:03Z",
    "authors": [
      "Roberto Pecoraro",
      "Valerio Basile",
      "Viviana Bono",
      "Sara Gallo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.00091v1",
    "title": "Dynamic N:M Fine-grained Structured Sparse Attention Mechanism",
    "summary": "Transformers are becoming the mainstream solutions for various tasks like NLP\nand Computer vision. Despite their success, the high complexity of the\nattention mechanism hinders them from being applied to latency-sensitive tasks.\nTremendous efforts have been made to alleviate this problem, and many of them\nsuccessfully reduce the asymptotic complexity to linear. Nevertheless, most of\nthem fail to achieve practical speedup over the original full attention under\nmoderate sequence lengths and are unfriendly to finetuning. In this paper, we\npresent DFSS, an attention mechanism that dynamically prunes the full attention\nweight matrix to N:M fine-grained structured sparse pattern. We provide both\ntheoretical and empirical evidence that demonstrates DFSS is a good\napproximation of the full attention mechanism. We propose a dedicated CUDA\nkernel design that completely eliminates the dynamic pruning overhead and\nachieves speedups under arbitrary sequence length. We evaluate the 1:2 and 2:4\nsparsity under different configurations and achieve 1.27~ 1.89x speedups over\nthe full-attention mechanism. It only takes a couple of finetuning epochs from\nthe pretrained model to achieve on par accuracy with full attention mechanism\non tasks from various domains under different sequence lengths from 384 to\n4096.",
    "published": "2022-02-28T20:52:24Z",
    "updated": "2022-02-28T20:52:24Z",
    "authors": [
      "Zhaodong Chen",
      "Yuying Quan",
      "Zheng Qu",
      "Liu Liu",
      "Yufei Ding",
      "Yuan Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.13730v1",
    "title": "Understanding Long Programming Languages with Structure-Aware Sparse\n  Attention",
    "summary": "Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have\nachieved great success in many downstream code-related tasks. Since the memory\nand computational complexity of self-attention in the Transformer grow\nquadratically with the sequence length, PPLMs typically limit the code length\nto 512. However, codes in real-world applications are generally long, such as\ncode searches, which cannot be processed efficiently by existing PPLMs. To\nsolve this problem, in this paper, we present SASA, a Structure-Aware Sparse\nAttention mechanism, which reduces the complexity and improves performance for\nlong code understanding tasks. The key components in SASA are top-$k$ sparse\nattention and Abstract Syntax Tree (AST)-based structure-aware attention. With\ntop-$k$ sparse attention, the most crucial attention relation can be obtained\nwith a lower computational cost. As the code structure represents the logic of\nthe code statements, which is a complement to the code sequence\ncharacteristics, we further introduce AST structures into attention. Extensive\nexperiments on CodeXGLUE tasks show that SASA achieves better performance than\nthe competing baselines.",
    "published": "2022-05-27T02:50:57Z",
    "updated": "2022-05-27T02:50:57Z",
    "authors": [
      "Tingting Liu",
      "Chengyu Wang",
      "Cen Chen",
      "Ming Gao",
      "Aoying Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.01844v1",
    "title": "Efficient Representation Learning via Adaptive Context Pooling",
    "summary": "Self-attention mechanisms model long-range context by using pairwise\nattention between all input tokens. In doing so, they assume a fixed attention\ngranularity defined by the individual tokens (e.g., text characters or image\npixels), which may not be optimal for modeling complex dependencies at higher\nlevels. In this paper, we propose ContextPool to address this problem by\nadapting the attention granularity for each token. Inspired by the success of\nConvNets that are combined with pooling to capture long-range dependencies, we\nlearn to pool neighboring features for each token before computing attention in\na given attention layer. The pooling weights and support size are adaptively\ndetermined, allowing the pooled features to encode meaningful context with\nvarying scale. We show that ContextPool makes attention models more expressive,\nachieving strong performance often with fewer layers and thus significantly\nreduced cost. Experiments validate that our ContextPool module, when plugged\ninto transformer models, matches or surpasses state-of-the-art performance\nusing less compute on several language and image benchmarks, outperforms recent\nworks with learned context sizes or sparse attention patterns, and is also\napplicable to ConvNets for efficient feature learning.",
    "published": "2022-07-05T07:10:31Z",
    "updated": "2022-07-05T07:10:31Z",
    "authors": [
      "Chen Huang",
      "Walter Talbott",
      "Navdeep Jaitly",
      "Josh Susskind"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.06096v1",
    "title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic\n  Speech Recognition",
    "summary": "Attention layers are an integral part of modern end-to-end automatic speech\nrecognition systems, for instance as part of the Transformer or Conformer\narchitecture. Attention is typically multi-headed, where each head has an\nindependent set of learned parameters and operates on the same input feature\nsequence. The output of multi-headed attention is a fusion of the outputs from\nthe individual heads. We empirically analyze the diversity between\nrepresentations produced by the different attention heads and demonstrate that\nthe heads become highly correlated during the course of training. We\ninvestigate a few approaches to increasing attention head diversity, including\nusing different attention mechanisms for each head and auxiliary training loss\nfunctions to promote head diversity. We show that introducing\ndiversity-promoting auxiliary loss functions during training is a more\neffective approach, and obtain WER improvements of up to 6% relative on the\nLibrispeech corpus. Finally, we draw a connection between the diversity of\nattention heads and the similarity of the gradients of head parameters.",
    "published": "2022-09-13T15:50:03Z",
    "updated": "2022-09-13T15:50:03Z",
    "authors": [
      "Kartik Audhkhasi",
      "Yinghui Huang",
      "Bhuvana Ramabhadran",
      "Pedro J. Moreno"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.14145v3",
    "title": "Multi-scale Attention Network for Single Image Super-Resolution",
    "summary": "ConvNets can compete with transformers in high-level tasks by exploiting\nlarger receptive fields. To unleash the potential of ConvNet in\nsuper-resolution, we propose a multi-scale attention network (MAN), by coupling\nclassical multi-scale mechanism with emerging large kernel attention. In\nparticular, we proposed multi-scale large kernel attention (MLKA) and gated\nspatial attention unit (GSAU). Through our MLKA, we modify large kernel\nattention with multi-scale and gate schemes to obtain the abundant attention\nmap at various granularity levels, thereby aggregating global and local\ninformation and avoiding potential blocking artifacts. In GSAU, we integrate\ngate mechanism and spatial attention to remove the unnecessary linear layer and\naggregate informative spatial context. To confirm the effectiveness of our\ndesigns, we evaluate MAN with multiple complexities by simply stacking\ndifferent numbers of MLKA and GSAU. Experimental results illustrate that our\nMAN can perform on par with SwinIR and achieve varied trade-offs between\nstate-of-the-art performance and computations.",
    "published": "2022-09-28T14:49:28Z",
    "updated": "2024-04-13T03:36:29Z",
    "authors": [
      "Yan Wang",
      "Yusen Li",
      "Gang Wang",
      "Xiaoguang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.16289v2",
    "title": "Lightweight Structure-Aware Attention for Visual Understanding",
    "summary": "Attention operator has been widely used as a basic brick in visual\nunderstanding since it provides some flexibility through its adjustable\nkernels. However, this operator suffers from inherent limitations: (1) the\nattention kernel is not discriminative enough, resulting in high redundancy,\nand (2) the complexity in computation and memory is quadratic in the sequence\nlength. In this paper, we propose a novel attention operator, called\nLightweight Structure-aware Attention (LiSA), which has a better representation\npower with log-linear complexity. Our operator transforms the attention kernels\nto be more discriminative by learning structural patterns. These structural\npatterns are encoded by exploiting a set of relative position embeddings (RPEs)\nas multiplicative weights, thereby improving the representation power of the\nattention kernels. Additionally, the RPEs are approximated to obtain log-linear\ncomplexity. Our experiments and analyses demonstrate that the proposed operator\noutperforms self-attention and other existing operators, achieving\nstate-of-the-art results on ImageNet-1K and other downstream tasks such as\nvideo action recognition on Kinetics-400, object detection \\& instance\nsegmentation on COCO, and semantic segmentation on ADE-20K.",
    "published": "2022-11-29T15:20:14Z",
    "updated": "2025-07-03T12:08:30Z",
    "authors": [
      "Heeseung Kwon",
      "Francisco M. Castro",
      "Manuel J. Marin-Jimenez",
      "Nicolas Guil",
      "Karteek Alahari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.11185v1",
    "title": "Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns\n  Predict Reading Times Over and Above GPT-2 Surprisal",
    "summary": "Transformer-based large language models are trained to make predictions about\nthe next word by aggregating representations of previous tokens through their\nself-attention mechanism. In the field of cognitive modeling, such attention\npatterns have recently been interpreted as embodying the process of cue-based\nretrieval, in which attention over multiple targets is taken to generate\ninterference and latency during retrieval. Under this framework, this work\nfirst defines an entropy-based predictor that quantifies the diffuseness of\nself-attention, as well as distance-based predictors that capture the\nincremental change in attention patterns across timesteps. Moreover, following\nrecent studies that question the informativeness of attention weights, we also\nexperiment with alternative methods for incorporating vector norms into\nattention weights. Regression experiments using predictors calculated from the\nGPT-2 language model show that these predictors deliver a substantially better\nfit to held-out self-paced reading and eye-tracking data over a rigorous\nbaseline including GPT-2 surprisal. Additionally, the distance-based predictors\ngenerally demonstrated higher predictive power, with effect sizes of up to 6.59\nms per standard deviation on self-paced reading times (compared to 2.82 ms for\nsurprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared\nto 3.81 ms for surprisal).",
    "published": "2022-12-21T16:56:07Z",
    "updated": "2022-12-21T16:56:07Z",
    "authors": [
      "Byung-Doh Oh",
      "William Schuler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.04556v1",
    "title": "Attention: Marginal Probability is All You Need?",
    "summary": "Attention mechanisms are a central property of cognitive systems allowing\nthem to selectively deploy cognitive resources in a flexible manner. Attention\nhas been long studied in the neurosciences and there are numerous\nphenomenological models that try to capture its core properties. Recently\nattentional mechanisms have become a dominating architectural choice of machine\nlearning and are the central innovation of Transformers. The dominant intuition\nand formalism underlying their development has drawn on ideas of keys and\nqueries in database management systems. In this work, we propose an alternative\nBayesian foundation for attentional mechanisms and show how this unifies\ndifferent attentional architectures in machine learning. This formulation\nallows to to identify commonality across different attention ML architectures\nas well as suggest a bridge to those developed in neuroscience. We hope this\nwork will guide more sophisticated intuitions into the key properties of\nattention architectures and suggest new ones.",
    "published": "2023-04-07T14:38:39Z",
    "updated": "2023-04-07T14:38:39Z",
    "authors": [
      "Ryan Singh",
      "Christopher L. Buckley"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.07911v1",
    "title": "Pit One Against Many: Leveraging Attention-head Embeddings for\n  Parameter-efficient Multi-head Attention",
    "summary": "Scaling pre-trained language models has resulted in large performance gains\nin various natural language processing tasks but comes with a large cost in\nmemory requirements. Inspired by the position embeddings in transformers, we\naim to simplify and reduce the memory footprint of the multi-head attention\n(MHA) mechanism. We propose an alternative module that uses only a single\nshared projection matrix and multiple head embeddings (MHE), i.e. one per head.\nWe empirically demonstrate that our MHE attention is substantially more memory\nefficient compared to alternative attention mechanisms while achieving high\npredictive performance retention ratio to vanilla MHA on several downstream\ntasks. MHE attention only requires a negligible fraction of additional\nparameters ($3nd$, where $n$ is the number of attention heads and $d$ the size\nof the head embeddings) compared to a single-head attention, while MHA requires\n$(3n^2-3n)d^2-3nd$ additional parameters.",
    "published": "2023-10-11T21:38:40Z",
    "updated": "2023-10-11T21:38:40Z",
    "authors": [
      "Huiyin Xue",
      "Nikolaos Aletras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.14820v2",
    "title": "How Smooth Is Attention?",
    "summary": "Self-attention and masked self-attention are at the heart of Transformers'\noutstanding success. Still, our mathematical understanding of attention, in\nparticular of its Lipschitz properties - which are key when it comes to\nanalyzing robustness and expressive power - is incomplete. We provide a\ndetailed study of the Lipschitz constant of self-attention in several practical\nscenarios, discussing the impact of the sequence length $n$ and layer\nnormalization on the local Lipschitz constant of both unmasked and masked\nself-attention. In particular, we show that for inputs of length $n$ in any\ncompact set, the Lipschitz constant of self-attention is bounded by $\\sqrt{n}$\nup to a constant factor and that this bound is tight for reasonable sequence\nlengths. When the sequence length $n$ is too large for the previous bound to be\ntight, which we refer to as the mean-field regime, we provide an upper bound\nand a matching lower bound which are independent of $n$. Our mean-field\nframework for masked self-attention is novel and of independent interest. Our\nexperiments on pretrained and randomly initialized BERT and GPT-2 support our\ntheoretical findings.",
    "published": "2023-12-22T16:47:10Z",
    "updated": "2024-06-04T15:51:36Z",
    "authors": [
      "ValÃ©rie Castin",
      "Pierre Ablin",
      "Gabriel PeyrÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.14652v2",
    "title": "AEANet: Affinity Enhanced Attentional Networks for Arbitrary Style\n  Transfer",
    "summary": "Arbitrary artistic style transfer is a research area that combines rational\nacademic study with emotive artistic creation. It aims to create a new image\nfrom a content image according to a target artistic style, maintaining the\ncontent's textural structural information while incorporating the artistic\ncharacteristics of the style image. However, existing style transfer methods\noften significantly damage the texture lines of the content image during the\nstyle transformation. To address these issues, we propose affinity-enhanced\nattentional network, which include the content affinity-enhanced attention\n(CAEA) module, the style affinity-enhanced attention (SAEA) module, and the\nhybrid attention (HA) module. The CAEA and SAEA modules first use attention to\nenhance content and style representations, followed by a detail enhanced (DE)\nmodule to reinforce detail features. The hybrid attention module adjusts the\nstyle feature distribution based on the content feature distribution. We also\nintroduce the local dissimilarity loss based on affinity attention, which\nbetter preserves the affinity with content and style images. Experiments\ndemonstrate that our work achieves better results in arbitrary style transfer\nthan other state-of-the-art methods.",
    "published": "2024-09-23T01:39:11Z",
    "updated": "2024-09-24T10:46:00Z",
    "authors": [
      "Gen Li",
      "Xianqiu Zheng",
      "Yujian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.14846v2",
    "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
    "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
    "published": "2024-09-23T09:22:59Z",
    "updated": "2025-02-07T13:09:17Z",
    "authors": [
      "Junyang Zhang",
      "Mu Yuan",
      "Ruiguang Zhong",
      "Puhan Luo",
      "Huiyou Zhan",
      "Ningkang Zhang",
      "Chengchen Hu",
      "Xiangyang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.15097v2",
    "title": "Efficiently Dispatching Flash Attention For Partially Filled Attention\n  Masks",
    "summary": "Transformers are widely used across various applications, many of which yield\nsparse or partially filled attention matrices. Examples include attention masks\ndesigned to reduce the quadratic complexity of attention, sequence packing\ntechniques, and recent innovations like tree masking for fast validation in\nMEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art\nalgorithm Flash Attention still processes them with quadratic complexity as\nthough they were dense. In this paper, we introduce Binary Block Masking, a\nhighly efficient modification that enhances Flash Attention by making it\nmask-aware. We further propose two optimizations: one tailored for masks with\ncontiguous non-zero patterns and another for extremely sparse masks. Our\nexperiments on attention masks derived from real-world scenarios demonstrate up\nto a 9x runtime improvement. The implementation will be publicly released to\nfoster further research and application.",
    "published": "2024-09-23T15:11:07Z",
    "updated": "2024-09-24T12:56:13Z",
    "authors": [
      "Agniv Sharma",
      "Jonas Geiping"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.00359v2",
    "title": "Does Self-Attention Need Separate Weights in Transformers?",
    "summary": "The success of self-attention lies in its ability to capture long-range\ndependencies and enhance context understanding, but it is limited by its\ncomputational complexity and challenges in handling sequential data with\ninherent directionality. This work introduces a shared weight\nself-attention-based BERT model that only learns one weight matrix for (Key,\nValue, and Query) representations instead of three individual matrices for each\nof them. Our shared weight attention reduces the training parameter size by\nmore than half and training time by around one-tenth. Furthermore, we\ndemonstrate higher prediction accuracy on small tasks of GLUE over the BERT\nbaseline and in particular a generalization power on noisy and out-of-domain\ndata. Experimental results indicate that our shared self-attention method\nachieves a parameter size reduction of 66.53% in the attention block. In the\nGLUE dataset, the shared weight self-attention-based BERT model demonstrates\naccuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric,\nand pairwise attention-based BERT models, respectively. The model and source\ncode are available at Anonymous.",
    "published": "2024-11-30T04:46:20Z",
    "updated": "2025-05-02T04:24:25Z",
    "authors": [
      "Md Kowsher",
      "Nusrat Jahan Prottasha",
      "Chun-Nam Yu",
      "Ozlem Ozmen Garibay",
      "Niloofar Yousefi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.15061v2",
    "title": "PolaFormer: Polarity-aware Linear Attention for Vision Transformers",
    "summary": "Linear attention has emerged as a promising alternative to softmax-based\nattention, leveraging kernelized feature maps to reduce complexity from\nquadratic to linear in sequence length. However, the non-negative constraint on\nfeature maps and the relaxed exponential function used in approximation lead to\nsignificant information loss compared to the original query-key dot products,\nresulting in less discriminative attention maps with higher entropy. To address\nthe missing interactions driven by negative values in query-key pairs, we\npropose a polarity-aware linear attention mechanism that explicitly models both\nsame-signed and opposite-signed query-key interactions, ensuring comprehensive\ncoverage of relational information. Furthermore, to restore the spiky\nproperties of attention maps, we provide a theoretical analysis proving the\nexistence of a class of element-wise functions (with positive first and second\nderivatives) that can reduce entropy in the attention distribution. For\nsimplicity, and recognizing the distinct contributions of each dimension, we\nemploy a learnable power function for rescaling, allowing strong and weak\nattention signals to be effectively separated. Extensive experiments\ndemonstrate that the proposed PolaFormer improves performance on various vision\ntasks, enhancing both expressiveness and efficiency by up to 4.6%.",
    "published": "2025-01-25T03:46:35Z",
    "updated": "2025-03-04T07:00:07Z",
    "authors": [
      "Weikang Meng",
      "Yadan Luo",
      "Xin Li",
      "Dongmei Jiang",
      "Zheng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.13981v1",
    "title": "CacheFormer: High Attention-Based Segment Caching",
    "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
    "published": "2025-04-18T06:34:57Z",
    "updated": "2025-04-18T06:34:57Z",
    "authors": [
      "Sushant Singh",
      "Ausif Mahmood"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.15897v1",
    "title": "SUPRA: Subspace Parameterized Attention for Neural Operator on General\n  Domains",
    "summary": "Neural operators are efficient surrogate models for solving partial\ndifferential equations (PDEs), but their key components face challenges: (1) in\norder to improve accuracy, attention mechanisms suffer from computational\ninefficiency on large-scale meshes, and (2) spectral convolutions rely on the\nFast Fourier Transform (FFT) on regular grids and assume a flat geometry, which\ncauses accuracy degradation on irregular domains. To tackle these problems, we\nregard the matrix-vector operations in the standard attention mechanism on\nvectors in Euclidean space as bilinear forms and linear operators in vector\nspaces and generalize the attention mechanism to function spaces. This new\nattention mechanism is fully equivalent to the standard attention but\nimpossible to compute due to the infinite dimensionality of function spaces. To\naddress this, inspired by model reduction techniques, we propose a Subspace\nParameterized Attention (SUPRA) neural operator, which approximates the\nattention mechanism within a finite-dimensional subspace. To construct a\nsubspace on irregular domains for SUPRA, we propose using the Laplacian\neigenfunctions, which naturally adapt to domains' geometry and guarantee the\noptimal approximation for smooth functions. Experiments show that the SUPRA\nneural operator reduces error rates by up to 33% on various PDE datasets while\nmaintaining state-of-the-art computational efficiency.",
    "published": "2025-04-22T13:40:04Z",
    "updated": "2025-04-22T13:40:04Z",
    "authors": [
      "Zherui Yang",
      "Zhengyang Xue",
      "Ligang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.02161v1",
    "title": "Focus What Matters: Matchability-Based Reweighting for Local Feature\n  Matching",
    "summary": "Since the rise of Transformers, many semi-dense matching methods have adopted\nattention mechanisms to extract feature descriptors. However, the attention\nweights, which capture dependencies between pixels or keypoints, are often\nlearned from scratch. This approach can introduce redundancy and noisy\ninteractions from irrelevant regions, as it treats all pixels or keypoints\nequally. Drawing inspiration from keypoint selection processes, we propose to\nfirst classify all pixels into two categories: matchable and non-matchable.\nMatchable pixels are expected to receive higher attention weights, while\nnon-matchable ones are down-weighted. In this work, we propose a novel\nattention reweighting mechanism that simultaneously incorporates a learnable\nbias term into the attention logits and applies a matchability-informed\nrescaling to the input value features. The bias term, injected prior to the\nsoftmax operation, selectively adjusts attention scores based on the confidence\nof query-key interactions. Concurrently, the feature rescaling acts\npost-attention by modulating the influence of each value vector in the final\noutput. This dual design allows the attention mechanism to dynamically adjust\nboth its internal weighting scheme and the magnitude of its output\nrepresentations. Extensive experiments conducted on three benchmark datasets\nvalidate the effectiveness of our method, consistently outperforming existing\nstate-of-the-art approaches.",
    "published": "2025-05-04T15:50:28Z",
    "updated": "2025-05-04T15:50:28Z",
    "authors": [
      "Dongyue Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07260v2",
    "title": "UMoE: Unifying Attention and FFN with Shared Experts",
    "summary": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising\napproach for scaling Transformer models. While initial works primarily\nincorporated MoE into feed-forward network (FFN) layers, recent studies have\nexplored extending the MoE paradigm to attention layers to enhance model\nperformance. However, existing attention-based MoE layers require specialized\nimplementations and demonstrate suboptimal performance compared to their\nFFN-based counterparts. In this paper, we aim to unify MoE designs in attention\nand FFN layers by introducing a novel reformulation of the attention mechanism,\nthat reveals an underlying FFN-like structure within attention modules. Our\nproposed architecture, UMoE, achieves superior performance through\nattention-based MoE layers while enabling efficient parameter sharing between\nFFN and attention components.",
    "published": "2025-05-12T06:21:44Z",
    "updated": "2025-10-23T09:59:10Z",
    "authors": [
      "Yuanhang Yang",
      "Chaozheng Wang",
      "Jing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.19595v2",
    "title": "Efficient Attention Mechanisms for Large Language Models: A Survey",
    "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models.",
    "published": "2025-07-25T18:08:10Z",
    "updated": "2025-08-07T10:08:17Z",
    "authors": [
      "Yutao Sun",
      "Zhenyu Li",
      "Yike Zhang",
      "Tengyu Pan",
      "Bowen Dong",
      "Yuyi Guo",
      "Jianyong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25223v1",
    "title": "Enhancing Linear Attention with Residual Learning",
    "summary": "Linear attention offers a linear-time alternative to self-attention but often\nstruggles to capture long-range patterns. We revisit linear attention through a\nprediction-correction lens and show that prevalent variants can be written as a\ncombination of a historical prediction and a single-token correction, which\ncreates an expressivity bottleneck. To address this bottleneck, we introduce\nResidual Linear Attention (RLA), a framework that equips linear attention with\nan explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent\nstate that learns to accumulate residual errors over time and correct the base\nprediction. We further instantiate a delta-rule version, Residual Delta Net\n(RDN), incorporating adaptive gating and residual clipping for enhanced\ncorrection control and stability. Our implementation leverages highly optimized\nlinear attention kernels and preserves linear time and memory. Across language\nmodeling and recall-intensive evaluations, RLA and RDN consistently outperform\ntheir respective baselines and other modern linear-attention methods, narrowing\nthe gap to standard Transformers while retaining linear scaling.",
    "published": "2025-09-24T07:36:08Z",
    "updated": "2025-09-24T07:36:08Z",
    "authors": [
      "Xunhao Lai",
      "Jialiang Kang",
      "Jianqiao Lu",
      "Tong Lin",
      "Pengyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11602v1",
    "title": "Deconstructing Attention: Investigating Design Principles for Effective\n  Language Modeling",
    "summary": "The success of Transformer language models is widely credited to their\ndot-product attention mechanism, which interweaves a set of key design\nprinciples: mixing information across positions (enabling multi-token\ninteractions), sequence-dependent activations (where attention weights adapt to\neach input), a specific mathematical form (dot-product similarities plus\nsoftmax weighting), and coupling of queries and keys to evolving hidden states\n(grounding attention in the current layer). However, the necessity of each of\nthese principles remains largely untested. In this work, we systematically\ndeconstruct attention by designing controlled variants that selectively relax\nthese principles, applied both uniformly across all layers and in hybrid\narchitectures where only some layers retain standard attention. Our empirical\nanalysis reveals that mechanisms for mixing tokens are indispensable, as their\nabsence collapses models to near-random behavior, while the exact mathematical\nform and sequence dependency can be substantially relaxed, especially when\npreserved in just a subset of layers. Surprisingly, even variants that fail in\nisolation can achieve robust performance when interleaved with standard\nattention, highlighting a cooperative effect. These findings deepen our\nunderstanding of what truly underpins attention's effectiveness and open new\navenues for simplifying language models without sacrificing performance.",
    "published": "2025-10-13T16:42:14Z",
    "updated": "2025-10-13T16:42:14Z",
    "authors": [
      "Huiyin Xue",
      "Nafise Sadat Moosavi",
      "Nikolaos Aletras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1804.10752v2",
    "title": "Syllable-Based Sequence-to-Sequence Speech Recognition with the\n  Transformer in Mandarin Chinese",
    "summary": "Sequence-to-sequence attention-based models have recently shown very\npromising results on automatic speech recognition (ASR) tasks, which integrate\nan acoustic, pronunciation and language model into a single neural network. In\nthese models, the Transformer, a new sequence-to-sequence attention-based model\nrelying entirely on self-attention without using RNNs or convolutions, achieves\na new single-model state-of-the-art BLEU on neural machine translation (NMT)\ntasks. Since the outstanding performance of the Transformer, we extend it to\nspeech and concentrate on it as the basic architecture of sequence-to-sequence\nattention-based model on Mandarin Chinese ASR tasks. Furthermore, we\ninvestigate a comparison between syllable based model and context-independent\nphoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese.\nAdditionally, a greedy cascading decoder with the Transformer is proposed for\nmapping CI-phoneme sequences and syllable sequences into word sequences.\nExperiments on HKUST datasets demonstrate that syllable based model with the\nTransformer performs better than CI-phoneme based counterpart, and achieves a\ncharacter error rate (CER) of \\emph{$28.77\\%$}, which is competitive to the\nstate-of-the-art CER of $28.0\\%$ by the joint CTC-attention based\nencoder-decoder network.",
    "published": "2018-04-28T06:54:11Z",
    "updated": "2018-06-04T07:46:02Z",
    "authors": [
      "Shiyu Zhou",
      "Linhao Dong",
      "Shuang Xu",
      "Bo Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.00640v2",
    "title": "Wide Attention Is The Way Forward For Transformers?",
    "summary": "The Transformer is an extremely powerful and prominent deep learning\narchitecture. In this work, we challenge the commonly held belief in deep\nlearning that going deeper is better, and show an alternative design approach\nthat is building wider attention Transformers. We demonstrate that wide single\nlayer Transformer models can compete with or outperform deeper ones in a\nvariety of Natural Language Processing (NLP) tasks when both are trained from\nscratch. The impact of changing the model aspect ratio on Transformers is then\nstudied systematically. This ratio balances the number of layers and the number\nof attention heads per layer while keeping the total number of attention heads\nand all other hyperparameters constant. On average, across 4 NLP tasks and 10\nattention types, single layer wide models perform 0.3% better than their deep\ncounterparts. We show an in-depth evaluation and demonstrate how wide models\nrequire a far smaller memory footprint and can run faster on commodity\nhardware, in addition, these wider models are also more interpretable. For\nexample, a single layer Transformer on the IMDb byte level text classification\nhas 3.1x faster inference latency on a CPU than its equally accurate deeper\ncounterpart, and is half the size. We therefore put forward wider and shallower\nmodels as a viable and desirable alternative for small models on NLP tasks, and\nas an important area of research for domains beyond this.",
    "published": "2022-10-02T21:49:54Z",
    "updated": "2022-11-08T22:24:09Z",
    "authors": [
      "Jason Ross Brown",
      "Yiren Zhao",
      "Ilia Shumailov",
      "Robert D Mullins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.03036v3",
    "title": "Structure-Aware Transformer for Graph Representation Learning",
    "summary": "The Transformer architecture has gained growing attention in graph\nrepresentation learning recently, as it naturally overcomes several limitations\nof graph neural networks (GNNs) by avoiding their strict structural inductive\nbiases and instead only encoding the graph structure via positional encoding.\nHere, we show that the node representations generated by the Transformer with\npositional encoding do not necessarily capture structural similarity between\nthem. To address this issue, we propose the Structure-Aware Transformer, a\nclass of simple and flexible graph Transformers built upon a new self-attention\nmechanism. This new self-attention incorporates structural information into the\noriginal self-attention by extracting a subgraph representation rooted at each\nnode before computing the attention. We propose several methods for\nautomatically generating the subgraph representation and show theoretically\nthat the resulting representations are at least as expressive as the subgraph\nrepresentations. Empirically, our method achieves state-of-the-art performance\non five graph prediction benchmarks. Our structure-aware framework can leverage\nany existing GNN to extract the subgraph representation, and we show that it\nsystematically improves performance relative to the base GNN model,\nsuccessfully combining the advantages of GNNs and Transformers. Our code is\navailable at https://github.com/BorgwardtLab/SAT.",
    "published": "2022-02-07T09:53:39Z",
    "updated": "2022-06-13T12:15:38Z",
    "authors": [
      "Dexiong Chen",
      "Leslie O'Bray",
      "Karsten Borgwardt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.02562v2",
    "title": "Transformer Transducer: A Streamable Speech Recognition Model with\n  Transformer Encoders and RNN-T Loss",
    "summary": "In this paper we present an end-to-end speech recognition model with\nTransformer encoders that can be used in a streaming speech recognition system.\nTransformer computation blocks based on self-attention are used to encode both\naudio and label sequences independently. The activations from both audio and\nlabel encoders are combined with a feed-forward layer to compute a probability\ndistribution over the label space for every combination of acoustic frame\nposition and label history. This is similar to the Recurrent Neural Network\nTransducer (RNN-T) model, which uses RNNs for information encoding instead of\nTransformer encoders. The model is trained with the RNN-T loss well-suited to\nstreaming decoding. We present results on the LibriSpeech dataset showing that\nlimiting the left context for self-attention in the Transformer layers makes\ndecoding computationally tractable for streaming, with only a slight\ndegradation in accuracy. We also show that the full attention version of our\nmodel beats the-state-of-the art accuracy on the LibriSpeech benchmarks. Our\nresults also show that we can bridge the gap between full attention and limited\nattention versions of our model by attending to a limited number of future\nframes.",
    "published": "2020-02-07T00:04:04Z",
    "updated": "2020-02-14T21:47:10Z",
    "authors": [
      "Qian Zhang",
      "Han Lu",
      "Hasim Sak",
      "Anshuman Tripathi",
      "Erik McDermott",
      "Stephen Koo",
      "Shankar Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.04263v5",
    "title": "On the Connection between Local Attention and Dynamic Depth-wise\n  Convolution",
    "summary": "Vision Transformer (ViT) attains state-of-the-art performance in visual\nrecognition, and the variant, Local Vision Transformer, makes further\nimprovements. The major component in Local Vision Transformer, local attention,\nperforms the attention separately over small local windows. We rephrase local\nattention as a channel-wise locally-connected layer and analyze it from two\nnetwork regularization manners, sparse connectivity and weight sharing, as well\nas weight computation. Sparse connectivity: there is no connection across\nchannels, and each position is connected to the positions within a small local\nwindow. Weight sharing: the connection weights for one position are shared\nacross channels or within each group of channels. Dynamic weight: the\nconnection weights are dynamically predicted according to each image instance.\nWe point out that local attention resembles depth-wise convolution and its\ndynamic version in sparse connectivity. The main difference lies in weight\nsharing - depth-wise convolution shares connection weights (kernel weights)\nacross spatial positions. We empirically observe that the models based on\ndepth-wise convolution and the dynamic variant with lower computation\ncomplexity perform on-par with or sometimes slightly better than Swin\nTransformer, an instance of Local Vision Transformer, for ImageNet\nclassification, COCO object detection and ADE semantic segmentation. These\nobservations suggest that Local Vision Transformer takes advantage of two\nregularization forms and dynamic weight to increase the network capacity. Code\nis available at https://github.com/Atten4Vis/DemystifyLocalViT.",
    "published": "2021-06-08T11:47:44Z",
    "updated": "2022-08-04T09:27:15Z",
    "authors": [
      "Qi Han",
      "Zejia Fan",
      "Qi Dai",
      "Lei Sun",
      "Ming-Ming Cheng",
      "Jiaying Liu",
      "Jingdong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.07239v1",
    "title": "TransCAM: Transformer Attention-based CAM Refinement for Weakly\n  Supervised Semantic Segmentation",
    "summary": "Weakly supervised semantic segmentation (WSSS) with only image-level\nsupervision is a challenging task. Most existing methods exploit Class\nActivation Maps (CAM) to generate pixel-level pseudo labels for supervised\ntraining. However, due to the local receptive field of Convolution Neural\nNetworks (CNN), CAM applied to CNNs often suffers from partial activation --\nhighlighting the most discriminative part instead of the entire object area. In\norder to capture both local features and global representations, the Conformer\nhas been proposed to combine a visual transformer branch with a CNN branch. In\nthis paper, we propose TransCAM, a Conformer-based solution to WSSS that\nexplicitly leverages the attention weights from the transformer branch of the\nConformer to refine the CAM generated from the CNN branch. TransCAM is\nmotivated by our observation that attention weights from shallow transformer\nblocks are able to capture low-level spatial feature similarities while\nattention weights from deep transformer blocks capture high-level semantic\ncontext. Despite its simplicity, TransCAM achieves a new state-of-the-art\nperformance of 69.3% and 69.6% on the respective PASCAL VOC 2012 validation and\ntest sets, showing the effectiveness of transformer attention-based refinement\nof CAM for WSSS.",
    "published": "2022-03-14T16:17:18Z",
    "updated": "2022-03-14T16:17:18Z",
    "authors": [
      "Ruiwen Li",
      "Zheda Mai",
      "Chiheb Trabelsi",
      "Zhibo Zhang",
      "Jongseong Jang",
      "Scott Sanner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.05526v2",
    "title": "Long-term Leap Attention, Short-term Periodic Shift for Video\n  Classification",
    "summary": "Video transformer naturally incurs a heavier computation burden than a static\nvision transformer, as the former processes $T$ times longer sequence than the\nlatter under the current attention of quadratic complexity $(T^2N^2)$. The\nexisting works treat the temporal axis as a simple extension of spatial axes,\nfocusing on shortening the spatio-temporal sequence by either generic pooling\nor local windowing without utilizing temporal redundancy.\n  However, videos naturally contain redundant information between neighboring\nframes; thereby, we could potentially suppress attention on visually similar\nframes in a dilated manner. Based on this hypothesis, we propose the LAPS, a\nlong-term ``\\textbf{\\textit{Leap Attention}}'' (LA), short-term\n``\\textbf{\\textit{Periodic Shift}}'' (\\textit{P}-Shift) module for video\ntransformers, with $(2TN^2)$ complexity. Specifically, the ``LA'' groups\nlong-term frames into pairs, then refactors each discrete pair via attention.\nThe ``\\textit{P}-Shift'' exchanges features between temporal neighbors to\nconfront the loss of short-term dynamics. By replacing a vanilla 2D attention\nwith the LAPS, we could adapt a static transformer into a video one, with zero\nextra parameters and neglectable computation overhead ($\\sim$2.6\\%).\nExperiments on the standard Kinetics-400 benchmark demonstrate that our LAPS\ntransformer could achieve competitive performances in terms of accuracy, FLOPs,\nand Params among CNN and transformer SOTAs. We open-source our project in\n\\sloppy\n\\href{https://github.com/VideoNetworks/LAPS-transformer}{\\textit{\\color{magenta}{https://github.com/VideoNetworks/LAPS-transformer}}} .",
    "published": "2022-07-12T13:30:15Z",
    "updated": "2022-07-23T07:06:32Z",
    "authors": [
      "Hao Zhang",
      "Lechao Cheng",
      "Yanbin Hao",
      "Chong-Wah Ngo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.07470v1",
    "title": "Reviving Shift Equivariance in Vision Transformers",
    "summary": "Shift equivariance is a fundamental principle that governs how we perceive\nthe world - our recognition of an object remains invariant with respect to\nshifts. Transformers have gained immense popularity due to their effectiveness\nin both language and vision tasks. While the self-attention operator in vision\ntransformers (ViT) is permutation-equivariant and thus shift-equivariant, patch\nembedding, positional encoding, and subsampled attention in ViT variants can\ndisrupt this property, resulting in inconsistent predictions even under small\nshift perturbations. Although there is a growing trend in incorporating the\ninductive bias of convolutional neural networks (CNNs) into vision\ntransformers, it does not fully address the issue. We propose an adaptive\npolyphase anchoring algorithm that can be seamlessly integrated into vision\ntransformer models to ensure shift-equivariance in patch embedding and\nsubsampled attention modules, such as window attention and global subsampled\nattention. Furthermore, we utilize depth-wise convolution to encode positional\ninformation. Our algorithms enable ViT, and its variants such as Twins to\nachieve 100% consistency with respect to input shift, demonstrate robustness to\ncropping, flipping, and affine transformations, and maintain consistent\npredictions even when the original models lose 20 percentage points on average\nwhen shifted by just a few pixels with Twins' accuracy dropping from 80.57% to\n62.40%.",
    "published": "2023-06-13T00:13:11Z",
    "updated": "2023-06-13T00:13:11Z",
    "authors": [
      "Peijian Ding",
      "Davit Soselia",
      "Thomas Armstrong",
      "Jiahao Su",
      "Furong Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.11307v3",
    "title": "Transforming Graphs for Enhanced Attribute Clustering: An Innovative\n  Graph Transformer-Based Method",
    "summary": "Graph Representation Learning (GRL) is an influential methodology, enabling a\nmore profound understanding of graph-structured data and aiding graph\nclustering, a critical task across various domains. The recent incursion of\nattention mechanisms, originally an artifact of Natural Language Processing\n(NLP), into the realm of graph learning has spearheaded a notable shift in\nresearch trends. Consequently, Graph Attention Networks (GATs) and Graph\nAttention Auto-Encoders have emerged as preferred tools for graph clustering\ntasks. Yet, these methods primarily employ a local attention mechanism, thereby\ncurbing their capacity to apprehend the intricate global dependencies between\nnodes within graphs. Addressing these impediments, this study introduces an\ninnovative method known as the Graph Transformer Auto-Encoder for Graph\nClustering (GTAGC). By melding the Graph Auto-Encoder with the Graph\nTransformer, GTAGC is adept at capturing global dependencies between nodes.\nThis integration amplifies the graph representation and surmounts the\nconstraints posed by the local attention mechanism. The architecture of GTAGC\nencompasses graph embedding, integration of the Graph Transformer within the\nautoencoder structure, and a clustering component. It strategically alternates\nbetween graph embedding and clustering, thereby tailoring the Graph Transformer\nfor clustering tasks, whilst preserving the graph's global structural\ninformation. Through extensive experimentation on diverse benchmark datasets,\nGTAGC has exhibited superior performance against existing state-of-the-art\ngraph clustering methodologies.",
    "published": "2023-06-20T06:04:03Z",
    "updated": "2023-08-12T14:37:23Z",
    "authors": [
      "Shuo Han",
      "Jiacheng Liu",
      "Jiayun Wu",
      "Yinan Chen",
      "Li Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.16030v1",
    "title": "VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections",
    "summary": "Graph transformer has been proven as an effective graph learning method for\nits adoption of attention mechanism that is capable of capturing expressive\nrepresentations from complex topological and feature information of graphs.\nGraph transformer conventionally performs dense attention (or global attention)\nfor every pair of nodes to learn node representation vectors, resulting in\nquadratic computational costs that are unaffordable for large-scale graph data.\nTherefore, mini-batch training for graph transformers is a promising direction,\nbut limited samples in each mini-batch can not support effective dense\nattention to encode informative representations. Facing this bottleneck, (1) we\nstart by assigning each node a token list that is sampled by personalized\nPageRank (PPR) and then apply standard multi-head self-attention only on this\nlist to compute its node representations. This PPR tokenization method\ndecouples model training from complex graph topological information and makes\nheavy feature engineering offline and independent, such that mini-batch\ntraining of graph transformers is possible by loading each node's token list in\nbatches. We further prove this PPR tokenization is viable as a graph\nconvolution network with a fixed polynomial filter and jumping knowledge.\nHowever, only using personalized PageRank may limit information carried by a\ntoken list, which could not support different graph inductive biases for model\ntraining. To this end, (2) we rewire graphs by introducing multiple types of\nvirtual connections through structure- and content-based super nodes that\nenable PPR tokenization to encode local and global contexts, long-range\ninteraction, and heterophilous information into each node's token list, and\nthen formalize our Virtual Connection Ranking based Graph Transformer\n(VCR-Graphormer).",
    "published": "2024-03-24T06:10:56Z",
    "updated": "2024-03-24T06:10:56Z",
    "authors": [
      "Dongqi Fu",
      "Zhigang Hua",
      "Yan Xie",
      "Jin Fang",
      "Si Zhang",
      "Kaan Sancak",
      "Hao Wu",
      "Andrey Malevich",
      "Jingrui He",
      "Bo Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.10101v4",
    "title": "Learning Linear Attention in Polynomial Time",
    "summary": "Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.",
    "published": "2024-10-14T02:41:01Z",
    "updated": "2025-10-23T21:29:09Z",
    "authors": [
      "Morris Yau",
      "Ekin AkyÃ¼rek",
      "Jiayuan Mao",
      "Joshua B. Tenenbaum",
      "Stefanie Jegelka",
      "Jacob Andreas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.13467v1",
    "title": "Multi-Level Attention and Contrastive Learning for Enhanced Text\n  Classification with an Optimized Transformer",
    "summary": "This paper studies a text classification algorithm based on an improved\nTransformer to improve the performance and efficiency of the model in text\nclassification tasks. Aiming at the shortcomings of the traditional Transformer\nmodel in capturing deep semantic relationships and optimizing computational\ncomplexity, this paper introduces a multi-level attention mechanism and a\ncontrastive learning strategy. The multi-level attention mechanism effectively\nmodels the global semantics and local features in the text by combining global\nattention with local attention; the contrastive learning strategy enhances the\nmodel's ability to distinguish between different categories by constructing\npositive and negative sample pairs while improving the classification effect.\nIn addition, in order to improve the training and inference efficiency of the\nmodel on large-scale text data, this paper designs a lightweight module to\noptimize the feature transformation process and reduce the computational cost.\nExperimental results on the dataset show that the improved Transformer model\noutperforms the comparative models such as BiLSTM, CNN, standard Transformer,\nand BERT in terms of classification accuracy, F1 score, and recall rate,\nshowing stronger semantic representation ability and generalization\nperformance. The method proposed in this paper provides a new idea for\nalgorithm optimization in the field of text classification and has good\napplication potential and practical value. Future work will focus on studying\nthe performance of this model in multi-category imbalanced datasets and\ncross-domain tasks and explore the integration wi",
    "published": "2025-01-23T08:32:27Z",
    "updated": "2025-01-23T08:32:27Z",
    "authors": [
      "Jia Gao",
      "Guiran Liu",
      "Binrong Zhu",
      "Shicheng Zhou",
      "Hongye Zheng",
      "Xiaoxuan Liao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.00926v3",
    "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study\n  on Training Dynamics and Implicit Bias",
    "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results.",
    "published": "2025-05-02T00:07:35Z",
    "updated": "2025-05-28T23:17:46Z",
    "authors": [
      "Ruiquan Huang",
      "Yingbin Liang",
      "Jing Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.03761v1",
    "title": "Adaptive Transformers in RL",
    "summary": "Recent developments in Transformers have opened new interesting areas of\nresearch in partially observable reinforcement learning tasks. Results from\nlate 2019 showed that Transformers are able to outperform LSTMs on both memory\nintense and reactive tasks. In this work we first partially replicate the\nresults shown in Stabilizing Transformers in RL on both reactive and memory\nbased environments. We then show performance improvement coupled with reduced\ncomputation when adding adaptive attention span to this Stable Transformer on a\nchallenging DMLab30 environment. The code for all our experiments and models is\navailable at https://github.com/jerrodparker20/adaptive-transformers-in-rl.",
    "published": "2020-04-08T01:03:10Z",
    "updated": "2020-04-08T01:03:10Z",
    "authors": [
      "Shakti Kumar",
      "Jerrod Parker",
      "Panteha Naderian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.05989v1",
    "title": "Towards Understanding Enablers of Digital Transformation in Small and\n  Medium-Sized Enterprises",
    "summary": "Even though, digital transformation has attracted much attention of both\nacademics and practitioners, a very limited number of studies have investigated\nthe digital transformation process in small and medium-sized enterprises (SMEs)\nand the findings remain fragmented. Given the accessibility and availability of\ndigital technologies to launch digital transformation initiatives and the\nimportance of SMEs in the economy, a profound understanding of enablers of the\ndigital transformation process in SMEs is much needed. As such, to address\nthis, in this paper we conducted a comprehensive review of related literature\nin information systems, management, and business disciplines, to identify key\nenablers that facilitate the digital transformation process in SMEs.",
    "published": "2021-11-10T23:38:30Z",
    "updated": "2021-11-10T23:38:30Z",
    "authors": [
      "Sachithra Lokuge",
      "Sophia Xiaoxia Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.07056v1",
    "title": "Transformer Scale Gate for Semantic Segmentation",
    "summary": "Effectively encoding multi-scale contextual information is crucial for\naccurate semantic segmentation. Existing transformer-based segmentation models\ncombine features across scales without any selection, where features on\nsub-optimal scales may degrade segmentation outcomes. Leveraging from the\ninherent properties of Vision Transformers, we propose a simple yet effective\nmodule, Transformer Scale Gate (TSG), to optimally combine multi-scale\nfeatures.TSG exploits cues in self and cross attentions in Vision Transformers\nfor the scale selection. TSG is a highly flexible plug-and-play module, and can\neasily be incorporated with any encoder-decoder-based hierarchical vision\nTransformer architecture. Extensive experiments on the Pascal Context and\nADE20K datasets demonstrate that our feature selection strategy achieves\nconsistent gains.",
    "published": "2022-05-14T13:11:39Z",
    "updated": "2022-05-14T13:11:39Z",
    "authors": [
      "Hengcan Shi",
      "Munawar Hayat",
      "Jianfei Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.05557v1",
    "title": "On Convolutional Vision Transformers for Yield Prediction",
    "summary": "While a variety of methods offer good yield prediction on histogrammed remote\nsensing data, vision Transformers are only sparsely represented in the\nliterature. The Convolution vision Transformer (CvT) is being tested to\nevaluate vision Transformers that are currently achieving state-of-the-art\nresults in many other vision tasks. CvT combines some of the advantages of\nconvolution with the advantages of dynamic attention and global context fusion\nof Transformers. It performs worse than widely tested methods such as XGBoost\nand CNNs, but shows that Transformers have potential to improve yield\nprediction.",
    "published": "2024-02-08T10:50:12Z",
    "updated": "2024-02-08T10:50:12Z",
    "authors": [
      "Alvin Inderka",
      "Florian Huber",
      "Volker Steinhage"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.16784v1",
    "title": "The Progression of Transformers from Language to Vision to MOT: A\n  Literature Review on Multi-Object Tracking with Transformers",
    "summary": "The transformer neural network architecture allows for autoregressive\nsequence-to-sequence modeling through the use of attention layers. It was\noriginally created with the application of machine translation but has\nrevolutionized natural language processing. Recently, transformers have also\nbeen applied across a wide variety of pattern recognition tasks, particularly\nin computer vision. In this literature review, we describe major advances in\ncomputer vision utilizing transformers. We then focus specifically on\nMulti-Object Tracking (MOT) and discuss how transformers are increasingly\nbecoming competitive in state-of-the-art MOT works, yet still lag behind\ntraditional deep learning methods.",
    "published": "2024-06-24T16:45:28Z",
    "updated": "2024-06-24T16:45:28Z",
    "authors": [
      "Abhi Kamboj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22719v1",
    "title": "IBiT: Utilizing Inductive Biases to Create a More Data Efficient\n  Attention Mechanism",
    "summary": "In recent years, Transformer-based architectures have become the dominant\nmethod for Computer Vision applications. While Transformers are explainable and\nscale well with dataset size, they lack the inductive biases of Convolutional\nNeural Networks. While these biases may be learned on large datasets, we show\nthat introducing these inductive biases through learned masks allow Vision\nTransformers to learn on much smaller datasets without Knowledge Distillation.\nThese Transformers, which we call Inductively Biased Image Transformers (IBiT),\nare significantly more accurate on small datasets, while retaining the\nexplainability Transformers.",
    "published": "2025-09-24T17:19:23Z",
    "updated": "2025-09-24T17:19:23Z",
    "authors": [
      "Adithya Giri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.09419v2",
    "title": "Compositional Attention: Disentangling Search and Retrieval",
    "summary": "Multi-head, key-value attention is the backbone of the widely successful\nTransformer model and its variants. This attention mechanism uses multiple\nparallel key-value attention blocks (called heads), each performing two\nfundamental computations: (1) search - selection of a relevant entity from a\nset via query-key interactions, and (2) retrieval - extraction of relevant\nfeatures from the selected entity via a value matrix. Importantly, standard\nattention heads learn a rigid mapping between search and retrieval. In this\nwork, we first highlight how this static nature of the pairing can potentially:\n(a) lead to learning of redundant parameters in certain tasks, and (b) hinder\ngeneralization. To alleviate this problem, we propose a novel attention\nmechanism, called Compositional Attention, that replaces the standard head\nstructure. The proposed mechanism disentangles search and retrieval and\ncomposes them in a dynamic, flexible and context-dependent manner through an\nadditional soft competition stage between the query-key combination and value\npairing. Through a series of numerical experiments, we show that it outperforms\nstandard multi-head attention on a variety of tasks, including some\nout-of-distribution settings. Through our qualitative analysis, we demonstrate\nthat Compositional Attention leads to dynamic specialization based on the type\nof retrieval needed. Our proposed mechanism generalizes multi-head attention,\nallows independent scaling of search and retrieval, and can easily be\nimplemented in lieu of standard attention heads in any network architecture.",
    "published": "2021-10-18T15:47:38Z",
    "updated": "2022-02-13T20:31:45Z",
    "authors": [
      "Sarthak Mittal",
      "Sharath Chandra Raparthy",
      "Irina Rish",
      "Yoshua Bengio",
      "Guillaume Lajoie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.11208v2",
    "title": "KS-DETR: Knowledge Sharing in Attention Learning for Detection\n  Transformer",
    "summary": "Scaled dot-product attention applies a softmax function on the scaled\ndot-product of queries and keys to calculate weights and then multiplies the\nweights and values. In this work, we study how to improve the learning of\nscaled dot-product attention to improve the accuracy of DETR. Our method is\nbased on the following observations: using ground truth foreground-background\nmask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables\nlearning much better weights/values; with better weights/values, better\nvalues/weights can be learned. We propose a triple-attention module in which\nthe first attention is a plain scaled dot-product attention, the second/third\nattention generates high-quality weights/values (with the assistance of GT\nFg-Bg Mask) and shares the values/weights with the first attention to improve\nthe quality of values/weights. The second and third attentions are removed\nduring inference. We call our method knowledge-sharing DETR (KS-DETR), which is\nan extension of knowledge distillation (KD) in the way that the improved\nweights and values of the teachers (the second and third attentions) are\ndirectly shared, instead of mimicked, by the student (the first attention) to\nenable more efficient knowledge transfer from the teachers to the student.\nExperiments on various DETR-like methods show consistent improvements over the\nbaseline methods on the MS COCO benchmark. Code is available at\nhttps://github.com/edocanonymous/KS-DETR.",
    "published": "2023-02-22T08:48:08Z",
    "updated": "2023-03-16T04:54:05Z",
    "authors": [
      "Kaikai Zhao",
      "Norimichi Ukita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.04057v2",
    "title": "Bidirectional Attention as a Mixture of Continuous Word Experts",
    "summary": "Bidirectional attention $\\unicode{x2013}$ composed of self-attention with\npositional encodings and the masked language model (MLM) objective\n$\\unicode{x2013}$ has emerged as a key component of modern large language\nmodels (LLMs). Despite its empirical success, few studies have examined its\nstatistical underpinnings: What statistical model is bidirectional attention\nimplicitly fitting? What sets it apart from its non-attention predecessors? We\nexplore these questions in this paper. The key observation is that fitting a\nsingle-layer single-head bidirectional attention, upon reparameterization, is\nequivalent to fitting a continuous bag of words (CBOW) model with\nmixture-of-experts (MoE) weights. Further, bidirectional attention with\nmultiple heads and multiple layers is equivalent to stacked MoEs and a mixture\nof MoEs, respectively. This statistical viewpoint reveals the distinct use of\nMoE in bidirectional attention, which aligns with its practical effectiveness\nin handling heterogeneous data. It also suggests an immediate extension to\ncategorical tabular data, if we view each word location in a sentence as a\ntabular feature. Across empirical studies, we find that this extension\noutperforms existing tabular extensions of transformers in out-of-distribution\n(OOD) generalization. Finally, this statistical perspective of bidirectional\nattention enables us to theoretically characterize when linear word analogies\nare present in its word embeddings. These analyses show that bidirectional\nattention can require much stronger assumptions to exhibit linear word\nanalogies than its non-attention predecessors.",
    "published": "2023-07-08T23:25:55Z",
    "updated": "2023-12-11T05:18:57Z",
    "authors": [
      "Kevin Christian Wibisono",
      "Yixin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.17381v2",
    "title": "Various Lengths, Constant Speed: Efficient Language Modeling with\n  Lightning Attention",
    "summary": "We present Lightning Attention, the first linear attention implementation\nthat maintains a constant training speed for various sequence lengths under\nfixed memory consumption. Due to the issue with cumulative summation operations\n(cumsum), previous linear attention implementations cannot achieve their\ntheoretical advantage in a casual setting. However, this issue can be\neffectively solved by utilizing different attention calculation strategies to\ncompute the different parts of attention. Specifically, we split the attention\ncalculation into intra-blocks and inter-blocks and use conventional attention\ncomputation for intra-blocks and linear attention kernel tricks for\ninter-blocks. This eliminates the need for cumsum in the linear attention\ncalculation. Furthermore, a tiling technique is adopted through both forward\nand backward procedures to take full advantage of the GPU hardware. To enhance\naccuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new\narchitecture that is tailored to our lightning attention. We conduct rigorous\ntesting on standard and self-collected datasets with varying model sizes and\nsequence lengths. TNL is notably more efficient than other language models. In\naddition, benchmark results indicate that TNL performs on par with\nstate-of-the-art LLMs utilizing conventional transformer structures. The source\ncode is released at github.com/OpenNLPLab/TransnormerLLM.",
    "published": "2024-05-27T17:38:13Z",
    "updated": "2024-06-20T09:12:42Z",
    "authors": [
      "Zhen Qin",
      "Weigao Sun",
      "Dong Li",
      "Xuyang Shen",
      "Weixuan Sun",
      "Yiran Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.03321v1",
    "title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
    "summary": "Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.",
    "published": "2025-03-05T09:55:07Z",
    "updated": "2025-03-05T09:55:07Z",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23436v1",
    "title": "LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal\n  Transport",
    "summary": "Transformers have proven highly effective across a wide range of modalities.\nHowever, the quadratic complexity of the standard softmax attention mechanism\nposes a fundamental barrier to scaling them to long context windows. A large\nbody of work addresses this with linear attention, which reformulates attention\nas a kernel function and approximates it with finite feature maps to achieve\nlinear-time computation. Orthogonal to computational scaling, most attention\nmechanisms -- both quadratic and linear -- produce row-normalized maps that can\nover-focus on a few tokens, degrading robustness and information flow.\nEnforcing doubly-stochastic attention alleviates this by balancing token\nparticipation across rows and columns, but existing doubly-stochastic attention\nmechanisms typically introduce substantial overhead, undermining scalability.\nWe propose LOTFormer, a principled attention mechanism that is simultaneously\nlinear-time and doubly-stochastic. Our approach exploits the connection between\nattention maps and transportation plans between query and key measures. The\ncentral idea is to constrain the transport plan to be low-rank by conditioning\nit on a learnable pivot measure with small support. Concretely, we solve two\nentropic optimal transport problems (queries $\\to$ pivot and pivot $\\to$ keys)\nand compose them into a conditional (glued) coupling. This yields an attention\nmatrix that is provably doubly-stochastic, has rank at most $r \\ll n$, and\napplies to values in $O(nr)$ time without forming the full $n \\times n$ map.\nThe pivot locations and masses are learned end-to-end. Empirically, LOTFormer\nachieves state-of-the-art results on the Long Range Arena benchmark, surpassing\nprior linear and transport-based attention methods in both accuracy and\nefficiency.",
    "published": "2025-09-27T18:11:09Z",
    "updated": "2025-09-27T18:11:09Z",
    "authors": [
      "Ashkan Shahbazi",
      "Chayne Thrash",
      "Yikun Bai",
      "Keaton Hamm",
      "Navid NaderiAlizadeh",
      "Soheil Kolouri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04476v1",
    "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
    "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
    "published": "2025-10-06T04:24:23Z",
    "updated": "2025-10-06T04:24:23Z",
    "authors": [
      "Tomas Figliolia",
      "Nicholas Alonso",
      "Rishi Iyer",
      "Quentin Anthony",
      "Beren Millidge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.13895v1",
    "title": "When Can Self-Attention Be Replaced by Feed Forward Layers?",
    "summary": "Recently, self-attention models such as Transformers have given competitive\nresults compared to recurrent neural network systems in speech recognition. The\nkey factor for the outstanding performance of self-attention models is their\nability to capture temporal relationships without being limited by the distance\nbetween two related events. However, we note that the range of the learned\ncontext progressively increases from the lower to upper self-attention layers,\nwhilst acoustic events often happen within short time spans in a left-to-right\norder. This leads to a question: for speech recognition, is a global view of\nthe entire sequence still important for the upper self-attention layers in the\nencoder of Transformers? To investigate this, we replace these self-attention\nlayers with feed forward layers. In our speech recognition experiments (Wall\nStreet Journal and Switchboard), we indeed observe an interesting result:\nreplacing the upper self-attention layers in the encoder with feed forward\nlayers leads to no performance drop, and even minor gains. Our experiments\noffer insights to how self-attention layers process the speech signal, leading\nto the conclusion that the lower self-attention layers of the encoder encode a\nsufficiently wide range of inputs, hence learning further contextual\ninformation in the upper layers is unnecessary.",
    "published": "2020-05-28T10:35:49Z",
    "updated": "2020-05-28T10:35:49Z",
    "authors": [
      "Shucong Zhang",
      "Erfan Loweimi",
      "Peter Bell",
      "Steve Renals"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.13755v1",
    "title": "Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient\n  Vision Transformers",
    "summary": "Vision Transformers (ViT) have shown their competitive advantages\nperformance-wise compared to convolutional neural networks (CNNs) though they\noften come with high computational costs. To this end, previous methods explore\ndifferent attention patterns by limiting a fixed number of spatially nearby\ntokens to accelerate the ViT's multi-head self-attention (MHSA) operations.\nHowever, such structured attention patterns limit the token-to-token\nconnections to their spatial relevance, which disregards learned semantic\nconnections from a full attention mask. In this work, we propose a novel\napproach to learn instance-dependent attention patterns, by devising a\nlightweight connectivity predictor module to estimate the connectivity score of\neach pair of tokens. Intuitively, two tokens have high connectivity scores if\nthe features are considered relevant either spatially or semantically. As each\ntoken only attends to a small number of other tokens, the binarized\nconnectivity masks are often very sparse by nature and therefore provide the\nopportunity to accelerate the network via sparse computations. Equipped with\nthe learned unstructured attention pattern, sparse attention ViT (Sparsifiner)\nproduces a superior Pareto-optimal trade-off between FLOPs and top-1 accuracy\non ImageNet compared to token sparsity. Our method reduces 48% to 69% FLOPs of\nMHSA while the accuracy drop is within 0.4%. We also show that combining\nattention and token sparsity reduces ViT FLOPs by over 60%.",
    "published": "2023-03-24T02:12:28Z",
    "updated": "2023-03-24T02:12:28Z",
    "authors": [
      "Cong Wei",
      "Brendan Duke",
      "Ruowei Jiang",
      "Parham Aarabi",
      "Graham W. Taylor",
      "Florian Shkurti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.09483v1",
    "title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning",
    "summary": "In sequence to sequence learning, the self-attention mechanism proves to be\nhighly effective, and achieves significant improvements in many tasks. However,\nthe self-attention mechanism is not without its own flaws. Although\nself-attention can model extremely long dependencies, the attention in deep\nlayers tends to overconcentrate on a single token, leading to insufficient use\nof local information and difficultly in representing long sequences. In this\nwork, we explore parallel multi-scale representation learning on sequence data,\nstriving to capture both long-range and short-range language structures. To\nthis end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple.\nMUSE-simple contains the basic idea of parallel multi-scale sequence\nrepresentation learning, and it encodes the sequence in parallel, in terms of\ndifferent scales with the help from self-attention, and pointwise\ntransformation. MUSE builds on MUSE-simple and explores combining convolution\nand self-attention for learning sequence representations from more different\nscales. We focus on machine translation and the proposed approach achieves\nsubstantial performance improvements over Transformer, especially on long\nsequences. More importantly, we find that although conceptually simple, its\nsuccess in practice requires intricate considerations, and the multi-scale\nattention must build on unified semantic space. Under common setting, the\nproposed model achieves substantial performance and outperforms all previous\nmodels on three main machine translation tasks. In addition, MUSE has potential\nfor accelerating inference due to its parallelism. Code will be available at\nhttps://github.com/lancopku/MUSE",
    "published": "2019-11-17T09:36:07Z",
    "updated": "2019-11-17T09:36:07Z",
    "authors": [
      "Guangxiang Zhao",
      "Xu Sun",
      "Jingjing Xu",
      "Zhiyuan Zhang",
      "Liangchen Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.12786v1",
    "title": "ELSA: Enhanced Local Self-Attention for Vision Transformer",
    "summary": "Self-attention is powerful in modeling long-range dependencies, but it is\nweak in local finer-level feature learning. The performance of local\nself-attention (LSA) is just on par with convolution and inferior to dynamic\nfilters, which puzzles researchers on whether to use LSA or its counterparts,\nwhich one is better, and what makes LSA mediocre. To clarify these, we\ncomprehensively investigate LSA and its counterparts from two sides:\n\\emph{channel setting} and \\emph{spatial processing}. We find that the devil\nlies in the generation and application of spatial attention, where relative\nposition embeddings and the neighboring filter application are key factors.\nBased on these findings, we propose the enhanced local self-attention (ELSA)\nwith Hadamard attention and the ghost head. Hadamard attention introduces the\nHadamard product to efficiently generate attention in the neighboring case,\nwhile maintaining the high-order mapping. The ghost head combines attention\nmaps with static matrices to increase channel capacity. Experiments demonstrate\nthe effectiveness of ELSA. Without architecture / hyperparameter modification,\ndrop-in replacing LSA with ELSA boosts Swin Transformer \\cite{swin} by up to\n+1.4 on top-1 accuracy. ELSA also consistently benefits VOLO \\cite{volo} from\nD1 to D5, where ELSA-VOLO-D5 achieves 87.2 on the ImageNet-1K without extra\ntraining images. In addition, we evaluate ELSA in downstream tasks. ELSA\nsignificantly improves the baseline by up to +1.9 box Ap / +1.3 mask Ap on the\nCOCO, and by up to +1.9 mIoU on the ADE20K. Code is available at\n\\url{https://github.com/damo-cv/ELSA}.",
    "published": "2021-12-23T18:59:48Z",
    "updated": "2021-12-23T18:59:48Z",
    "authors": [
      "Jingkai Zhou",
      "Pichao Wang",
      "Fan Wang",
      "Qiong Liu",
      "Hao Li",
      "Rong Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.00606v1",
    "title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and\n  On-Chip Recomputation",
    "summary": "As its core computation, a self-attention mechanism gauges pairwise\ncorrelations across the entire input sequence. Despite favorable performance,\ncalculating pairwise correlations is prohibitively costly. While recent work\nhas shown the benefits of runtime pruning of elements with low attention\nscores, the quadratic complexity of self-attention mechanisms and their on-chip\nmemory capacity demands are overlooked. This work addresses these constraints\nby architecting an accelerator, called SPRINT, which leverages the inherent\nparallelism of ReRAM crossbar arrays to compute attention scores in an\napproximate manner. Our design prunes the low attention scores using a\nlightweight analog thresholding circuitry within ReRAM, enabling SPRINT to\nfetch only a small subset of relevant data to on-chip memory. To mitigate\npotential negative repercussions for model accuracy, SPRINT re-computes the\nattention scores for the few fetched data in digital. The combined in-memory\npruning and on-chip recompute of the relevant attention scores enables SPRINT\nto transform quadratic complexity to a merely linear one. In addition, we\nidentify and leverage a dynamic spatial locality between the adjacent attention\noperations even after pruning, which eliminates costly yet redundant data\nfetches. We evaluate our proposed technique on a wide range of state-of-the-art\ntransformer models. On average, SPRINT yields 7.5x speedup and 19.6x energy\nreduction when total 16KB on-chip memory is used, while virtually on par with\niso-accuracy of the baseline models (on average 0.36% degradation).",
    "published": "2022-09-01T17:18:19Z",
    "updated": "2022-09-01T17:18:19Z",
    "authors": [
      "Amir Yazdanbakhsh",
      "Ashkan Moradifirouzabadi",
      "Zheng Li",
      "Mingu Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.12043v2",
    "title": "MixPro: Data Augmentation with MaskMix and Progressive Attention\n  Labeling for Vision Transformer",
    "summary": "The recently proposed data augmentation TransMix employs attention labels to\nhelp visual transformers (ViT) achieve better robustness and performance.\nHowever, TransMix is deficient in two aspects: 1) The image cropping method of\nTransMix may not be suitable for ViTs. 2) At the early stage of training, the\nmodel produces unreliable attention maps. TransMix uses unreliable attention\nmaps to compute mixed attention labels that can affect the model. To address\nthe aforementioned issues, we propose MaskMix and Progressive Attention\nLabeling (PAL) in image and label space, respectively. In detail, from the\nperspective of image space, we design MaskMix, which mixes two images based on\na patch-like grid mask. In particular, the size of each mask patch is\nadjustable and is a multiple of the image patch size, which ensures each image\npatch comes from only one image and contains more global contents. From the\nperspective of label space, we design PAL, which utilizes a progressive factor\nto dynamically re-weight the attention weights of the mixed attention label.\nFinally, we combine MaskMix and Progressive Attention Labeling as our new data\naugmentation method, named MixPro. The experimental results show that our\nmethod can improve various ViT-based models at scales on ImageNet\nclassification (73.8\\% top-1 accuracy based on DeiT-T for 300 epochs). After\nbeing pre-trained with MixPro on ImageNet, the ViT-based models also\ndemonstrate better transferability to semantic segmentation, object detection,\nand instance segmentation. Furthermore, compared to TransMix, MixPro also shows\nstronger robustness on several benchmarks. The code is available at\nhttps://github.com/fistyee/MixPro.",
    "published": "2023-04-24T12:38:09Z",
    "updated": "2023-08-07T10:20:59Z",
    "authors": [
      "Qihao Zhao",
      "Yangyu Huang",
      "Wei Hu",
      "Fan Zhang",
      "Jun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.19882v1",
    "title": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques\n  and Insights",
    "summary": "Intrigued by the inherent ability of the human visual system to identify\nsalient regions in complex scenes, attention mechanisms have been seamlessly\nintegrated into various Computer Vision (CV) tasks. Building upon this\nparadigm, Vision Transformer (ViT) networks exploit attention mechanisms for\nimproved efficiency. This review navigates the landscape of redesigned\nattention mechanisms within ViTs, aiming to enhance their performance. This\npaper provides a comprehensive exploration of techniques and insights for\ndesigning attention mechanisms, systematically reviewing recent literature in\nthe field of CV. This survey begins with an introduction to the theoretical\nfoundations and fundamental concepts underlying attention mechanisms. We then\npresent a systematic taxonomy of various attention mechanisms within ViTs,\nemploying redesigned approaches. A multi-perspective categorization is proposed\nbased on their application, objectives, and the type of attention applied. The\nanalysis includes an exploration of the novelty, strengths, weaknesses, and an\nin-depth evaluation of the different proposed strategies. This culminates in\nthe development of taxonomies that highlight key properties and contributions.\nFinally, we gather the reviewed studies along with their available open-source\nimplementations at our\n\\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}.\nWe aim to regularly update it with the most recent relevant papers.",
    "published": "2024-03-28T23:31:59Z",
    "updated": "2024-03-28T23:31:59Z",
    "authors": [
      "Moein Heidari",
      "Reza Azad",
      "Sina Ghorbani Kolahi",
      "RenÃ© Arimond",
      "Leon Niggemeier",
      "Alaa Sulaiman",
      "Afshin Bozorgpour",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Ilker Hacihaliloglu",
      "Dorit Merhof"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01659v2",
    "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse\n  Graph Processing Techniques",
    "summary": "Transformers have demonstrated great success in numerous domains including\nnatural language processing and bioinformatics. This success stems from the use\nof the attention mechanism by these models in order to represent and propagate\npairwise interactions between individual tokens of sequential data. However,\nthe primary limitation of this operation is its quadratic memory and time\ncomplexity in relation to the input's context length - the length of a sequence\nover which the interactions need to be captured. This significantly limits the\nlength of sequences that can be inferred upon by these models. Extensive\nresearch has been conducted to reduce the number of pairwise interactions to\nsub-quadratic in relation to the context length by introducing sparsity into\nthe attention mechanism through the development of sparse attention masks.\nHowever, efficient implementations that achieve \"true sparsity\" are lacking.\n  In this work, we address this issue by proposing a graph computing view of\nattention where tokens are perceived as nodes of the graph and the attention\nmask determines the edges of the graph. Using this view, we develop graph\nprocessing algorithms to implement the attention mechanism. Both theoretically\nand empirically, we demonstrate that our algorithms only perform the needed\ncomputations, i.e., they are work optimal. We also perform extensive\nexperimentation using popular attention masks to explore the impact of sparsity\non execution time and achievable context length. Our experiments demonstrate\nsignificant speedups in execution times compared to state-of-the-art attention\nimplementations such as FlashAttention for large sequence lengths. We also\ndemonstrate that our algorithms are able to achieve extremely long sequence\nlengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
    "published": "2025-01-31T22:05:00Z",
    "updated": "2025-02-07T13:44:24Z",
    "authors": [
      "Nathaniel Tomczak",
      "Sanmukh Kuppannagari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.09820v1",
    "title": "Vi-LAD: Vision-Language Attention Distillation for Socially-Aware Robot\n  Navigation in Dynamic Environments",
    "summary": "We introduce Vision-Language Attention Distillation (Vi-LAD), a novel\napproach for distilling socially compliant navigation knowledge from a large\nVision-Language Model (VLM) into a lightweight transformer model for real-time\nrobotic navigation. Unlike traditional methods that rely on expert\ndemonstrations or human-annotated datasets, Vi-LAD performs knowledge\ndistillation and fine-tuning at the intermediate layer representation level\n(i.e., attention maps) by leveraging the backbone of a pre-trained\nvision-action model. These attention maps highlight key navigational regions in\na given scene, which serve as implicit guidance for socially aware motion\nplanning. Vi-LAD fine-tunes a transformer-based model using intermediate\nattention maps extracted from the pre-trained vision-action model, combined\nwith attention-like semantic maps constructed from a large VLM. To achieve\nthis, we introduce a novel attention-level distillation loss that fuses\nknowledge from both sources, generating augmented attention maps with enhanced\nsocial awareness. These refined attention maps are then utilized as a\ntraversability costmap within a socially aware model predictive controller\n(MPC) for navigation. We validate our approach through real-world experiments\non a Husky wheeled robot, demonstrating significant improvements over\nstate-of-the-art (SOTA) navigation methods. Our results show up to 14.2% - 50%\nimprovement in success rate, which highlights the effectiveness of Vi-LAD in\nenabling socially compliant and efficient robot navigation.",
    "published": "2025-03-12T20:38:23Z",
    "updated": "2025-03-12T20:38:23Z",
    "authors": [
      "Mohamed Elnoor",
      "Kasun Weerakoon",
      "Gershom Seneviratne",
      "Jing Liang",
      "Vignesh Rajagopal",
      "Dinesh Manocha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.06457v1",
    "title": "A Systematic Analysis of Hybrid Linear Attention",
    "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
    "published": "2025-07-08T23:54:11Z",
    "updated": "2025-07-08T23:54:11Z",
    "authors": [
      "Dustin Wang",
      "Rui-Jie Zhu",
      "Steven Abreu",
      "Yong Shan",
      "Taylor Kergan",
      "Yuqi Pan",
      "Yuhong Chou",
      "Zheng Li",
      "Ge Zhang",
      "Wenhao Huang",
      "Jason Eshraghian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17892v1",
    "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality\n  Image Restoration",
    "summary": "Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.",
    "published": "2025-07-23T19:41:49Z",
    "updated": "2025-07-23T19:41:49Z",
    "authors": [
      "Hanzhou Liu",
      "Binghan Li",
      "Chengkai Liu",
      "Mi Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12969v1",
    "title": "Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for\n  Fast Video Generation",
    "summary": "The computational demands of self-attention mechanisms pose a critical\nchallenge for transformer-based video generation, particularly in synthesizing\nultra-long sequences. Current approaches, such as factorized attention and\nfixed sparse patterns, fail to fully exploit the inherent spatio-temporal\nredundancies in video data. Through systematic analysis of video diffusion\ntransformers (DiT), we uncover a key insight: Attention matrices exhibit\nstructured, yet heterogeneous sparsity patterns, where specialized heads\ndynamically attend to distinct spatiotemporal regions (e.g., local pattern,\ncross-shaped pattern, or global pattern). Existing sparse attention methods\neither impose rigid constraints or introduce significant overhead, limiting\ntheir effectiveness. To address this, we propose Compact Attention, a\nhardware-aware acceleration framework featuring three innovations: 1) Adaptive\ntiling strategies that approximate diverse spatial interaction patterns via\ndynamic tile grouping, 2) Temporally varying windows that adjust sparsity\nlevels based on frame proximity, and 3) An automated configuration search\nalgorithm that optimizes sparse patterns while preserving critical attention\npathways. Our method achieves 1.6~2.5x acceleration in attention computation on\nsingle-GPU setups while maintaining comparable visual quality with\nfull-attention baselines. This work provides a principled approach to unlocking\nefficient long-form video generation through structured sparsity exploitation.\nProject Page: https://yo-ava.github.io/Compact-Attention.github.io/",
    "published": "2025-08-18T14:45:42Z",
    "updated": "2025-08-18T14:45:42Z",
    "authors": [
      "Qirui Li",
      "Guangcong Zheng",
      "Qi Zhao",
      "Jie Li",
      "Bin Dong",
      "Yiwu Yao",
      "Xi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01817v1",
    "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
    "summary": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA)\nmechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number of\nfloating-point operations (FLOPs) required for the attention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introduces Sparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcoming Reactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models",
    "published": "2025-10-02T09:01:38Z",
    "updated": "2025-10-02T09:01:38Z",
    "authors": [
      "Adam Filipek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.06981v2",
    "title": "Thinking Like Transformers",
    "summary": "What is the computational model behind a Transformer? Where recurrent neural\nnetworks have direct parallels in finite state machines, allowing clear\ndiscussion and thought around architecture variants or trained models,\nTransformers have no such familiar parallel. In this paper we aim to change\nthat, proposing a computational model for the transformer-encoder in the form\nof a programming language. We map the basic components of a transformer-encoder\n-- attention and feed-forward computation -- into simple primitives, around\nwhich we form a programming language: the Restricted Access Sequence Processing\nLanguage (RASP). We show how RASP can be used to program solutions to tasks\nthat could conceivably be learned by a Transformer, and how a Transformer can\nbe trained to mimic a RASP solution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We further use our model to relate\ntheir difficulty in terms of the number of required layers and attention heads:\nanalyzing a RASP program implies a maximum number of heads and layers necessary\nto encode a task in a transformer. Finally, we see how insights gained from our\nabstraction might be used to explain phenomena seen in recent works.",
    "published": "2021-06-13T13:04:46Z",
    "updated": "2021-07-19T11:22:34Z",
    "authors": [
      "Gail Weiss",
      "Yoav Goldberg",
      "Eran Yahav"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.12678v1",
    "title": "Interpretation of the Transformer and Improvement of the Extractor",
    "summary": "It has been over six years since the Transformer architecture was put\nforward. Surprisingly, the vanilla Transformer architecture is still widely\nused today. One reason is that the lack of deep understanding and comprehensive\ninterpretation of the Transformer architecture makes it more challenging to\nimprove the Transformer architecture. In this paper, we first interpret the\nTransformer architecture comprehensively in plain words based on our\nunderstanding and experiences. The interpretations are further proved and\nverified. These interpretations also cover the Extractor, a family of drop-in\nreplacements for the multi-head self-attention in the Transformer architecture.\nThen, we propose an improvement on a type of the Extractor that outperforms the\nself-attention, without introducing additional trainable parameters.\nExperimental results demonstrate that the improved Extractor performs even\nbetter, showing a way to improve the Transformer architecture.",
    "published": "2023-11-21T15:36:20Z",
    "updated": "2023-11-21T15:36:20Z",
    "authors": [
      "Zhe Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.17406v1",
    "title": "Dependency Transformer Grammars: Integrating Dependency Structures into\n  Transformer Language Models",
    "summary": "Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars.",
    "published": "2024-07-24T16:38:38Z",
    "updated": "2024-07-24T16:38:38Z",
    "authors": [
      "Yida Zhao",
      "Chao Lou",
      "Kewei Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.08794v2",
    "title": "Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking\n  Transformers",
    "summary": "The integration of neuromorphic computing and transformers through spiking\nneural networks (SNNs) offers a promising path to energy-efficient sequence\nmodeling, with the potential to overcome the energy-intensive nature of the\nartificial neural network (ANN)-based transformers. However, the algorithmic\nefficiency of SNN-based transformers cannot be fully exploited on GPUs due to\narchitectural incompatibility. This paper introduces Xpikeformer, a hybrid\nanalog-digital hardware architecture designed to accelerate SNN-based\ntransformer models. The architecture integrates analog in-memory computing\n(AIMC) for feedforward and fully connected layers, and a stochastic spiking\nattention (SSA) engine for efficient attention mechanisms. We detail the\ndesign, implementation, and evaluation of Xpikeformer, demonstrating\nsignificant improvements in energy consumption and computational efficiency.\nThrough image classification tasks and wireless communication symbol detection\ntasks, we show that Xpikeformer can achieve inference accuracy comparable to\nthe GPU implementation of ANN-based transformers. Evaluations reveal that\nXpikeformer achieves $13\\times$ reduction in energy consumption at\napproximately the same throughput as the state-of-the-art (SOTA) digital\naccelerator for ANN-based transformers. Additionally, Xpikeformer achieves up\nto $1.9\\times$ energy reduction compared to the optimal digital ASIC projection\nof SOTA SNN-based transformers.",
    "published": "2024-08-16T15:07:54Z",
    "updated": "2025-04-03T13:22:17Z",
    "authors": [
      "Zihang Song",
      "Prabodh Katti",
      "Osvaldo Simeone",
      "Bipin Rajendran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07134v2",
    "title": "Bringing Attention to CAD: Boundary Representation Learning via\n  Transformer",
    "summary": "The recent rise of generative artificial intelligence (AI), powered by\nTransformer networks, has achieved remarkable success in natural language\nprocessing, computer vision, and graphics. However, the application of\nTransformers in computer-aided design (CAD), particularly for processing\nboundary representation (B-rep) models, remains largely unexplored. To bridge\nthis gap, we propose a novel approach for adapting Transformers to B-rep\nlearning, called the Boundary Representation Transformer (BRT). B-rep models\npose unique challenges due to their irregular topology and continuous geometric\ndefinitions, which are fundamentally different from the structured and discrete\ndata Transformers are designed for. To address this, BRT proposes a continuous\ngeometric embedding method that encodes B-rep surfaces (trimmed and untrimmed)\ninto Bezier triangles, preserving their shape and continuity without\ndiscretization. Additionally, BRT employs a topology-aware embedding method\nthat organizes these geometric embeddings into a sequence of discrete tokens\nsuitable for Transformers, capturing both geometric and topological\ncharacteristics within B-rep models. This enables the Transformer's attention\nmechanism to effectively learn shape patterns and contextual semantics of\nboundary elements in a B-rep model. Extensive experiments demonstrate that BRT\nachieves state-of-the-art performance in part classification and feature\nrecognition tasks.",
    "published": "2025-04-07T07:04:02Z",
    "updated": "2025-08-29T04:28:36Z",
    "authors": [
      "Qiang Zou",
      "Lizhen Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12787v2",
    "title": "Wavy Transformer",
    "summary": "Transformers have achieved remarkable success across natural language\nprocessing (NLP) and computer vision (CV). However, deep transformer models\noften suffer from an over-smoothing issue, in which token representations\nconverge to similar values as they pass through successive transformer blocks.\nIn this paper, we establish an equivalence between the hidden-state dynamics\ninduced by stacked attention layers and graph neural diffusion on a complete\ngraph. From this perspective, over-smoothing can be interpreted as a\nconsequence of the dissipative nature of the underlying diffusion dynamics.\nMotivated by this physical interpretation, we propose Wavy Transformer, which\nconsists of a novel attention layer based on second-order wavy dynamics. We\nalso introduce a feed-forward network and a normalization layer designed to\npreserve the physical state-velocity relationship under the chain rule, thereby\nextending the transformer architecture. We further validate our proposed\ntechniques on various transformer models for NLP and CV tasks. The results\nconsistently demonstrate that Wavy Transformer improves performance with\nminimal additional parameters and no extra hyperparameter tuning.",
    "published": "2025-08-18T10:03:38Z",
    "updated": "2025-10-20T15:22:31Z",
    "authors": [
      "Satoshi Noguchi",
      "Yoshinobu Kawahara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1605.00716v1",
    "title": "Radio Transformer Networks: Attention Models for Learning to Synchronize\n  in Wireless Systems",
    "summary": "We introduce learned attention models into the radio machine learning domain\nfor the task of modulation recognition by leveraging spatial transformer\nnetworks and introducing new radio domain appropriate transformations. This\nattention model allows the network to learn a localization network capable of\nsynchronizing and normalizing a radio signal blindly with zero knowledge of the\nsignals structure based on optimization of the network for classification\naccuracy, sparse representation, and regularization. Using this architecture we\nare able to outperform our prior results in accuracy vs signal to noise ratio\nagainst an identical system without attention, however we believe such an\nattention model has implication far beyond the task of modulation recognition.",
    "published": "2016-05-03T00:45:35Z",
    "updated": "2016-05-03T00:45:35Z",
    "authors": [
      "Timothy J O'Shea",
      "Latha Pemula",
      "Dhruv Batra",
      "T. Charles Clancy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.09561v1",
    "title": "Normalized Attention Without Probability Cage",
    "summary": "Attention architectures are widely used; they recently gained renewed\npopularity with Transformers yielding a streak of state of the art results.\nYet, the geometrical implications of softmax-attention remain largely\nunexplored. In this work we highlight the limitations of constraining attention\nweights to the probability simplex and the resulting convex hull of value\nvectors. We show that Transformers are sequence length dependent biased towards\ntoken isolation at initialization and contrast Transformers to simple max- and\nsum-pooling - two strong baselines rarely reported. We propose to replace the\nsoftmax in self-attention with normalization, yielding a hyperparameter and\ndata-bias robust, generally applicable architecture. We support our insights\nwith empirical results from more than 25,000 trained models. All results and\nimplementations are made available.",
    "published": "2020-05-19T16:26:34Z",
    "updated": "2020-05-19T16:26:34Z",
    "authors": [
      "Oliver Richter",
      "Roger Wattenhofer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.06618v1",
    "title": "Formal Language Recognition by Hard Attention Transformers: Perspectives\n  from Circuit Complexity",
    "summary": "This paper analyzes three formal models of Transformer encoders that differ\nin the form of their self-attention mechanism: unique hard attention (UHAT);\ngeneralized unique hard attention (GUHAT), which generalizes UHAT; and\naveraging hard attention (AHAT). We show that UHAT and GUHAT Transformers,\nviewed as string acceptors, can only recognize formal languages in the\ncomplexity class AC$^0$, the class of languages recognizable by families of\nBoolean circuits of constant depth and polynomial size. This upper bound\nsubsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages\nor the PARITY language, since those languages are outside AC$^0$ (Furst et al.,\n1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are\nrecognizable by AHAT networks, implying that AHAT can recognize languages that\nUHAT and GUHAT cannot.",
    "published": "2022-04-13T19:25:42Z",
    "updated": "2022-04-13T19:25:42Z",
    "authors": [
      "Yiding Hao",
      "Dana Angluin",
      "Robert Frank"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.08688v1",
    "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal\n  Attention Masks",
    "summary": "Since 2017, the Transformer-based models play critical roles in various\ndownstream Natural Language Processing tasks. However, a common limitation of\nthe attention mechanism utilized in Transformer Encoder is that it cannot\nautomatically capture the information of word order, so explicit position\nembeddings are generally required to be fed into the target model. In contrast,\nTransformer Decoder with the causal attention masks is naturally sensitive to\nthe word order. In this work, we focus on improving the position encoding\nability of BERT with the causal attention masks. Furthermore, we propose a new\npre-trained language model DecBERT and evaluate it on the GLUE benchmark.\nExperimental results show that (1) the causal attention mask is effective for\nBERT on the language understanding tasks; (2) our DecBERT model without\nposition embeddings achieve comparable performance on the GLUE benchmark; and\n(3) our modification accelerates the pre-training process and DecBERT w/ PE\nachieves better overall performance than the baseline systems when pre-training\nwith the same amount of computational resources.",
    "published": "2022-04-19T06:12:48Z",
    "updated": "2022-04-19T06:12:48Z",
    "authors": [
      "Ziyang Luo",
      "Yadong Xi",
      "Jing Ma",
      "Zhiwei Yang",
      "Xiaoxi Mao",
      "Changjie Fan",
      "Rongsheng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.12451v4",
    "title": "Understanding The Robustness in Vision Transformers",
    "summary": "Recent studies show that Vision Transformers(ViTs) exhibit strong robustness\nagainst various corruptions. Although this property is partly attributed to the\nself-attention mechanism, there is still a lack of systematic understanding. In\nthis paper, we examine the role of self-attention in learning robust\nrepresentations. Our study is motivated by the intriguing properties of the\nemerging visual grouping in Vision Transformers, which indicates that\nself-attention may promote robustness through improved mid-level\nrepresentations. We further propose a family of fully attentional networks\n(FANs) that strengthen this capability by incorporating an attentional channel\nprocessing design. We validate the design comprehensively on various\nhierarchical backbones. Our model achieves a state-of-the-art 87.1% accuracy\nand 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also\ndemonstrate state-of-the-art accuracy and robustness in two downstream tasks:\nsemantic segmentation and object detection. Code is available at:\nhttps://github.com/NVlabs/FAN.",
    "published": "2022-04-26T17:16:32Z",
    "updated": "2022-11-08T15:52:39Z",
    "authors": [
      "Daquan Zhou",
      "Zhiding Yu",
      "Enze Xie",
      "Chaowei Xiao",
      "Anima Anandkumar",
      "Jiashi Feng",
      "Jose M. Alvarez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.15436v1",
    "title": "Transformer Tracking",
    "summary": "Correlation acts as a critical role in the tracking field, especially in\nrecent popular Siamese-based trackers. The correlation operation is a simple\nfusion manner to consider the similarity between the template and the search\nregion. However, the correlation operation itself is a local linear matching\nprocess, leading to lose semantic information and fall into local optimum\neasily, which may be the bottleneck of designing high-accuracy tracking\nalgorithms. Is there any better feature fusion method than correlation? To\naddress this issue, inspired by Transformer, this work presents a novel\nattention-based feature fusion network, which effectively combines the template\nand search region features solely using attention. Specifically, the proposed\nmethod includes an ego-context augment module based on self-attention and a\ncross-feature augment module based on cross-attention. Finally, we present a\nTransformer tracking (named TransT) method based on the Siamese-like feature\nextraction backbone, the designed attention-based fusion mechanism, and the\nclassification and regression head. Experiments show that our TransT achieves\nvery promising results on six challenging datasets, especially on large-scale\nLaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively\n50 fps on GPU. Code and models are available at\nhttps://github.com/chenxin-dlut/TransT.",
    "published": "2021-03-29T09:06:55Z",
    "updated": "2021-03-29T09:06:55Z",
    "authors": [
      "Xin Chen",
      "Bin Yan",
      "Jiawen Zhu",
      "Dong Wang",
      "Xiaoyun Yang",
      "Huchuan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1711.02132v1",
    "title": "Weighted Transformer Network for Machine Translation",
    "summary": "State-of-the-art results on neural machine translation often use attentional\nsequence-to-sequence models with some form of convolution or recursion. Vaswani\net al. (2017) propose a new architecture that avoids recurrence and convolution\ncompletely. Instead, it uses only self-attention and feed-forward layers. While\nthe proposed architecture achieves state-of-the-art results on several machine\ntranslation tasks, it requires a large number of parameters and training\niterations to converge. We propose Weighted Transformer, a Transformer with\nmodified attention layers, that not only outperforms the baseline network in\nBLEU score but also converges 15-40% faster. Specifically, we replace the\nmulti-head attention by multiple self-attention branches that the model learns\nto combine during the training process. Our model improves the state-of-the-art\nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation\ntask and by 0.4 on the English-to-French translation task.",
    "published": "2017-11-06T19:35:00Z",
    "updated": "2017-11-06T19:35:00Z",
    "authors": [
      "Karim Ahmed",
      "Nitish Shirish Keskar",
      "Richard Socher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.02680v1",
    "title": "Separable Self-attention for Mobile Vision Transformers",
    "summary": "Mobile vision transformers (MobileViT) can achieve state-of-the-art\nperformance across several mobile vision tasks, including classification and\ndetection. Though these models have fewer parameters, they have high latency as\ncompared to convolutional neural network-based models. The main efficiency\nbottleneck in MobileViT is the multi-headed self-attention (MHA) in\ntransformers, which requires $O(k^2)$ time complexity with respect to the\nnumber of tokens (or patches) $k$. Moreover, MHA requires costly operations\n(e.g., batch-wise matrix multiplication) for computing self-attention,\nimpacting latency on resource-constrained devices. This paper introduces a\nseparable self-attention method with linear complexity, i.e. $O(k)$. A simple\nyet effective characteristic of the proposed method is that it uses\nelement-wise operations for computing self-attention, making it a good choice\nfor resource-constrained devices. The improved model, MobileViTv2, is\nstate-of-the-art on several mobile vision tasks, including ImageNet object\nclassification and MS-COCO object detection. With about three million\nparameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet\ndataset, outperforming MobileViT by about 1% while running $3.2\\times$ faster\non a mobile device.\n  Our source code is available at: \\url{https://github.com/apple/ml-cvnets}",
    "published": "2022-06-06T15:31:35Z",
    "updated": "2022-06-06T15:31:35Z",
    "authors": [
      "Sachin Mehta",
      "Mohammad Rastegari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.08898v2",
    "title": "SimA: Simple Softmax-free Attention for Vision Transformers",
    "summary": "Recently, vision transformers have become very popular. However, deploying\nthem in many applications is computationally expensive partly due to the\nSoftmax layer in the attention block. We introduce a simple but effective,\nSoftmax-free attention block, SimA, which normalizes query and key matrices\nwith simple $\\ell_1$-norm instead of using Softmax layer. Then, the attention\nblock in SimA is a simple multiplication of three matrices, so SimA can\ndynamically change the ordering of the computation at the test time to achieve\nlinear computation on the number of tokens or the number of channels. We\nempirically show that SimA applied to three SOTA variations of transformers,\nDeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models,\nwithout any need for Softmax layer. Interestingly, changing SimA from\nmulti-head to single-head has only a small effect on the accuracy, which\nsimplifies the attention block further. The code is available here:\nhttps://github.com/UCDvision/sima",
    "published": "2022-06-17T17:15:01Z",
    "updated": "2024-03-23T06:43:47Z",
    "authors": [
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.15195v1",
    "title": "The Topological BERT: Transforming Attention into Topology for Natural\n  Language Processing",
    "summary": "In recent years, the introduction of the Transformer models sparked a\nrevolution in natural language processing (NLP). BERT was one of the first text\nencoders using only the attention mechanism without any recurrent parts to\nachieve state-of-the-art results on many NLP tasks.\n  This paper introduces a text classifier using topological data analysis. We\nuse BERT's attention maps transformed into attention graphs as the only input\nto that classifier. The model can solve tasks such as distinguishing spam from\nham messages, recognizing whether a sentence is grammatically correct, or\nevaluating a movie review as negative or positive. It performs comparably to\nthe BERT baseline and outperforms it on some tasks.\n  Additionally, we propose a new method to reduce the number of BERT's\nattention heads considered by the topological classifier, which allows us to\nprune the number of heads from 144 down to as few as ten with no reduction in\nperformance. Our work also shows that the topological model displays higher\nrobustness against adversarial attacks than the original BERT model, which is\nmaintained during the pruning process. To the best of our knowledge, this work\nis the first to confront topological-based models with adversarial attacks in\nthe context of NLP.",
    "published": "2022-06-30T11:25:31Z",
    "updated": "2022-06-30T11:25:31Z",
    "authors": [
      "Ilan Perez",
      "Raphael Reinauer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.07845v1",
    "title": "Parallel Hierarchical Transformer with Attention Alignment for\n  Abstractive Multi-Document Summarization",
    "summary": "In comparison to single-document summarization, abstractive Multi-Document\nSummarization (MDS) brings challenges on the representation and coverage of its\nlengthy and linked sources. This study develops a Parallel Hierarchical\nTransformer (PHT) with attention alignment for MDS. By incorporating word- and\nparagraph-level multi-head attentions, the hierarchical architecture of PHT\nallows better processing of dependencies at both token and document levels. To\nguide the decoding towards a better coverage of the source documents, the\nattention-alignment mechanism is then introduced to calibrate beam search with\npredicted optimal attention distributions. Based on the WikiSum data, a\ncomprehensive evaluation is conducted to test improvements on MDS by the\nproposed architecture. By better handling the inner- and cross-document\ninformation, results in both ROUGE and human evaluation suggest that our\nhierarchical model generates summaries of higher quality relative to other\nTransformer-based baselines at relatively low computational cost.",
    "published": "2022-08-16T17:02:48Z",
    "updated": "2022-08-16T17:02:48Z",
    "authors": [
      "Ye Ma",
      "Lu Zong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.10407v2",
    "title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for\n  Transformer-based LVCSR",
    "summary": "The Transformer has shown impressive performance in automatic speech\nrecognition. It uses the encoder-decoder structure with self-attention to learn\nthe relationship between the high-level representation of the source inputs and\nembedding of the target outputs. In this paper, we propose a novel decoder\nstructure that features a self-and-mixed attention decoder (SMAD) with a deep\nacoustic structure (DAS) to improve the acoustic representation of\nTransformer-based LVCSR. Specifically, we introduce a self-attention mechanism\nto learn a multi-layer deep acoustic structure for multiple levels of acoustic\nabstraction. We also design a mixed attention mechanism that learns the\nalignment between different levels of acoustic abstraction and its\ncorresponding linguistic information simultaneously in a shared embedding\nspace. The ASR experiments on Aishell-1 shown that the proposed structure\nachieves CERs of 4.8% on the dev set and 5.1% on the test set, which are the\nbest results obtained on this task to the best of our knowledge.",
    "published": "2020-06-18T10:24:18Z",
    "updated": "2020-09-15T09:12:10Z",
    "authors": [
      "Xinyuan Zhou",
      "Grandee Lee",
      "Emre YÄ±lmaz",
      "Yanhua Long",
      "Jiaen Liang",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.01335v1",
    "title": "On the Distribution, Sparsity, and Inference-time Quantization of\n  Attention Values in Transformers",
    "summary": "How much information do NLP tasks really need from a transformer's attention\nmechanism at application-time (inference)? From recent work, we know that there\nis sparsity in transformers and that the floating-points within its computation\ncan be discretized to fewer values with minimal loss to task accuracies.\nHowever, this requires retraining or even creating entirely new models, both of\nwhich can be expensive and carbon-emitting. Focused on optimizations that do\nnot require training, we systematically study the full range of typical\nattention values necessary. This informs the design of an inference-time\nquantization technique using both pruning and log-scaled mapping which produces\nonly a few (e.g. $2^3$) unique values. Over the tasks of question answering and\nsentiment analysis, we find nearly 80% of attention values can be pruned to\nzeros with minimal ($< 1.0\\%$) relative loss in accuracy. We use this pruning\ntechnique in conjunction with quantizing the attention values to only a 3-bit\nformat, without retraining, resulting in only a 0.8% accuracy reduction on\nquestion answering with fine-tuned RoBERTa.",
    "published": "2021-06-02T17:45:47Z",
    "updated": "2021-06-02T17:45:47Z",
    "authors": [
      "Tianchu Ji",
      "Shraddhan Jain",
      "Michael Ferdman",
      "Peter Milder",
      "H. Andrew Schwartz",
      "Niranjan Balasubramanian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.02873v1",
    "title": "SDA-GAN: Unsupervised Image Translation Using Spectral Domain\n  Attention-Guided Generative Adversarial Network",
    "summary": "This work introduced a novel GAN architecture for unsupervised image\ntranslation on the task of face style transform. A spectral attention-based\nmechanism is embedded into the design along with spatial attention on the image\ncontents. We proved that neural network has the potential of learning complex\ntransformations such as Fourier transform, within considerable computational\ncost. The model is trained and tested in comparison to the baseline model,\nwhich only uses spatial attention. The performance improvement of our approach\nis significant especially when the source and target domain include different\ncomplexity (reduced FID to 49.18 from 142.84). In the translation process, a\nspectra filling effect was introduced due to the implementation of FFT and\nspectral attention. Another style transfer task and real-world object\ntranslation are also studied in this paper.",
    "published": "2021-10-06T15:59:43Z",
    "updated": "2021-10-06T15:59:43Z",
    "authors": [
      "Qizhou Wang",
      "Maksim Makarenko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.05364v2",
    "title": "Human Guided Exploitation of Interpretable Attention Patterns in\n  Summarization and Topic Segmentation",
    "summary": "The multi-head self-attention mechanism of the transformer model has been\nthoroughly investigated recently. In one vein of study, researchers are\ninterested in understanding why and how transformers work. In another vein,\nresearchers propose new attention augmentation methods to make transformers\nmore accurate, efficient and interpretable. In this paper, we combine these two\nlines of research in a human-in-the-loop pipeline to first discover important\ntask-specific attention patterns. Then those patterns are injected, not only to\nsmaller models, but also to the original model. The benefits of our pipeline\nand discovered patterns are demonstrated in two case studies with extractive\nsummarization and topic segmentation. After discovering interpretable patterns\nin BERT-based models fine-tuned for the two downstream tasks, experiments\nindicate that when we inject the patterns into attention heads, the models show\nconsiderable improvements in accuracy and efficiency.",
    "published": "2021-12-10T07:15:09Z",
    "updated": "2022-10-26T22:48:02Z",
    "authors": [
      "Raymond Li",
      "Wen Xiao",
      "Linzi Xing",
      "Lanjun Wang",
      "Gabriel Murray",
      "Giuseppe Carenini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.08534v3",
    "title": "Trading with the Momentum Transformer: An Intelligent and Interpretable\n  Architecture",
    "summary": "We introduce the Momentum Transformer, an attention-based deep-learning\narchitecture, which outperforms benchmark time-series momentum and\nmean-reversion trading strategies. Unlike state-of-the-art Long Short-Term\nMemory (LSTM) architectures, which are sequential in nature and tailored to\nlocal processing, an attention mechanism provides our architecture with a\ndirect connection to all previous time-steps. Our architecture, an\nattention-LSTM hybrid, enables us to learn longer-term dependencies, improves\nperformance when considering returns net of transaction costs and naturally\nadapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the\nintroduction of multiple attention heads, we can capture concurrent regimes, or\ntemporal dynamics, which are occurring at different timescales. The Momentum\nTransformer is inherently interpretable, providing us with greater insights\ninto our deep-learning momentum trading strategy, including the importance of\ndifferent factors over time and the past time-steps which are of the greatest\nsignificance to the model.",
    "published": "2021-12-16T00:04:12Z",
    "updated": "2022-11-22T22:53:47Z",
    "authors": [
      "Kieran Wood",
      "Sven Giegerich",
      "Stephen Roberts",
      "Stefan Zohren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.14709v1",
    "title": "MSTR: Multi-Scale Transformer for End-to-End Human-Object Interaction\n  Detection",
    "summary": "Human-Object Interaction (HOI) detection is the task of identifying a set of\n<human, object, interaction> triplets from an image. Recent work proposed\ntransformer encoder-decoder architectures that successfully eliminated the need\nfor many hand-designed components in HOI detection through end-to-end training.\nHowever, they are limited to single-scale feature resolution, providing\nsuboptimal performance in scenes containing humans, objects and their\ninteractions with vastly different scales and distances. To tackle this\nproblem, we propose a Multi-Scale TRansformer (MSTR) for HOI detection powered\nby two novel HOI-aware deformable attention modules called Dual-Entity\nattention and Entity-conditioned Context attention. While existing deformable\nattention comes at a huge cost in HOI detection performance, our proposed\nattention modules of MSTR learn to effectively attend to sampling points that\nare essential to identify interactions. In experiments, we achieve the new\nstate-of-the-art performance on two HOI detection benchmarks.",
    "published": "2022-03-28T12:58:59Z",
    "updated": "2022-03-28T12:58:59Z",
    "authors": [
      "Bumsoo Kim",
      "Jonghwan Mun",
      "Kyoung-Woon On",
      "Minchul Shin",
      "Junhyun Lee",
      "Eun-Sol Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.05064v1",
    "title": "Adaptive Graph Spatial-Temporal Transformer Network for Traffic Flow\n  Forecasting",
    "summary": "Traffic flow forecasting on graphs has real-world applications in many\nfields, such as transportation system and computer networks. Traffic\nforecasting can be highly challenging due to complex spatial-temporal\ncorrelations and non-linear traffic patterns. Existing works mostly model such\nspatial-temporal dependencies by considering spatial correlations and temporal\ncorrelations separately and fail to model the direct spatial-temporal\ncorrelations. Inspired by the recent success of transformers in the graph\ndomain, in this paper, we propose to directly model the cross-spatial-temporal\ncorrelations on the spatial-temporal graph using local multi-head\nself-attentions. To reduce the time complexity, we set the attention receptive\nfield to the spatially neighboring nodes, and we also introduce an adaptive\ngraph to capture the hidden spatial-temporal dependencies. Based on these\nattention mechanisms, we propose a novel Adaptive Graph Spatial-Temporal\nTransformer Network (ASTTN), which stacks multiple spatial-temporal attention\nlayers to apply self-attention on the input graph, followed by linear layers\nfor predictions. Experimental results on public traffic network datasets,\nMETR-LA PEMS-BAY, PeMSD4, and PeMSD7, demonstrate the superior performance of\nour model.",
    "published": "2022-07-09T19:21:00Z",
    "updated": "2022-07-09T19:21:00Z",
    "authors": [
      "Aosong Feng",
      "Leandros Tassiulas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.03745v1",
    "title": "Prior Knowledge-Guided Attention in Self-Supervised Vision Transformers",
    "summary": "Recent trends in self-supervised representation learning have focused on\nremoving inductive biases from training pipelines. However, inductive biases\ncan be useful in settings when limited data are available or provide additional\ninsight into the underlying data distribution. We present spatial prior\nattention (SPAN), a framework that takes advantage of consistent spatial and\nsemantic structure in unlabeled image datasets to guide Vision Transformer\nattention. SPAN operates by regularizing attention masks from separate\ntransformer heads to follow various priors over semantic regions. These priors\ncan be derived from data statistics or a single labeled sample provided by a\ndomain expert. We study SPAN through several detailed real-world scenarios,\nincluding medical image analysis and visual quality assurance. We find that the\nresulting attention masks are more interpretable than those derived from\ndomain-agnostic pretraining. SPAN produces a 58.7 mAP improvement for lung and\nheart segmentation. We also find that our method yields a 2.2 mAUC improvement\ncompared to domain-agnostic pretraining when transferring the pretrained model\nto a downstream chest disease classification task. Lastly, we show that SPAN\npretraining leads to higher downstream classification performance in low-data\nregimes compared to domain-agnostic pretraining.",
    "published": "2022-09-07T02:30:36Z",
    "updated": "2022-09-07T02:30:36Z",
    "authors": [
      "Kevin Miao",
      "Akash Gokul",
      "Raghav Singh",
      "Suzanne Petryk",
      "Joseph Gonzalez",
      "Kurt Keutzer",
      "Trevor Darrell",
      "Colorado Reed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.05580v1",
    "title": "Hyperbolic Cosine Transformer for LiDAR 3D Object Detection",
    "summary": "Recently, Transformer has achieved great success in computer vision. However,\nit is constrained because the spatial and temporal complexity grows\nquadratically with the number of large points in 3D object detection\napplications. Previous point-wise methods are suffering from time consumption\nand limited receptive fields to capture information among points. In this\npaper, we propose a two-stage hyperbolic cosine transformer (ChTR3D) for 3D\nobject detection from LiDAR point clouds. The proposed ChTR3D refines proposals\nby applying cosh-attention in linear computation complexity to encode rich\ncontextual relationships among points. The cosh-attention module reduces the\nspace and time complexity of the attention operation. The traditional softmax\noperation is replaced by non-negative ReLU activation and\nhyperbolic-cosine-based operator with re-weighting mechanism. Extensive\nexperiments on the widely used KITTI dataset demonstrate that, compared with\nvanilla attention, the cosh-attention significantly improves the inference\nspeed with competitive performance. Experiment results show that, among\ntwo-stage state-of-the-art methods using point-level features, the proposed\nChTR3D is the fastest one.",
    "published": "2022-11-10T13:54:49Z",
    "updated": "2022-11-10T13:54:49Z",
    "authors": [
      "Jigang Tong",
      "Fanhang Yang",
      "Sen Yang",
      "Enzeng Dong",
      "Shengzhi Du",
      "Xing Wang",
      "Xianlin Yi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.11315v1",
    "title": "Beyond Attentive Tokens: Incorporating Token Importance and Diversity\n  for Efficient Vision Transformers",
    "summary": "Vision transformers have achieved significant improvements on various vision\ntasks but their quadratic interactions between tokens significantly reduce\ncomputational efficiency. Many pruning methods have been proposed to remove\nredundant tokens for efficient vision transformers recently. However, existing\nstudies mainly focus on the token importance to preserve local attentive tokens\nbut completely ignore the global token diversity. In this paper, we emphasize\nthe cruciality of diverse global semantics and propose an efficient token\ndecoupling and merging method that can jointly consider the token importance\nand diversity for token pruning. According to the class token attention, we\ndecouple the attentive and inattentive tokens. In addition to preserving the\nmost discriminative local tokens, we merge similar inattentive tokens and match\nhomogeneous attentive tokens to maximize the token diversity. Despite its\nsimplicity, our method obtains a promising trade-off between model complexity\nand classification accuracy. On DeiT-S, our method reduces the FLOPs by 35%\nwith only a 0.2% accuracy drop. Notably, benefiting from maintaining the token\ndiversity, our method can even improve the accuracy of DeiT-T by 0.1% after\nreducing its FLOPs by 40%.",
    "published": "2022-11-21T09:57:11Z",
    "updated": "2022-11-21T09:57:11Z",
    "authors": [
      "Sifan Long",
      "Zhen Zhao",
      "Jimin Pi",
      "Shengsheng Wang",
      "Jingdong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.02240v2",
    "title": "Skip-Attention: Improving Vision Transformers by Paying Less Attention",
    "summary": "This work aims to improve the efficiency of vision transformers (ViT). While\nViTs use computationally expensive self-attention operations in every layer, we\nidentify that these operations are highly correlated across layers -- a key\nredundancy that causes unnecessary computations. Based on this observation, we\npropose SkipAt, a method to reuse self-attention computation from preceding\nlayers to approximate attention at one or more subsequent layers. To ensure\nthat reusing self-attention blocks across layers does not degrade the\nperformance, we introduce a simple parametric function, which outperforms the\nbaseline transformer's performance while running computationally faster. We\nshow the effectiveness of our method in image classification and\nself-supervised learning on ImageNet-1K, semantic segmentation on ADE20K, image\ndenoising on SIDD, and video denoising on DAVIS. We achieve improved throughput\nat the same-or-higher accuracy levels in all these tasks.",
    "published": "2023-01-05T18:59:52Z",
    "updated": "2023-01-17T16:17:57Z",
    "authors": [
      "Shashanka Venkataramanan",
      "Amir Ghodrati",
      "Yuki M. Asano",
      "Fatih Porikli",
      "Amirhossein Habibian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.13804v3",
    "title": "Cross-Language Speech Emotion Recognition Using Multimodal Dual\n  Attention Transformers",
    "summary": "Despite the recent progress in speech emotion recognition (SER),\nstate-of-the-art systems are unable to achieve improved performance in\ncross-language settings. In this paper, we propose a Multimodal Dual Attention\nTransformer (MDAT) model to improve cross-language SER. Our model utilises\npre-trained models for multimodal feature extraction and is equipped with a\ndual attention mechanism including graph attention and co-attention to capture\ncomplex dependencies across different modalities and achieve improved\ncross-language SER results using minimal target language data. In addition, our\nmodel also exploits a transformer encoder layer for high-level feature\nrepresentation to improve emotion classification accuracy. In this way, MDAT\nperforms refinement of feature representation at various stages and provides\nemotional salient features to the classification layer. This novel approach\nalso ensures the preservation of modality-specific emotional information while\nenhancing cross-modality and cross-language interactions. We assess our model's\nperformance on four publicly available SER datasets and establish its superior\neffectiveness compared to recent approaches and baseline models.",
    "published": "2023-06-23T22:38:32Z",
    "updated": "2023-07-14T13:36:35Z",
    "authors": [
      "Syed Aun Muhammad Zaidi",
      "Siddique Latif",
      "Junaid Qadir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.07781v1",
    "title": "Learning Image Deraining Transformer Network with Dynamic Dual\n  Self-Attention",
    "summary": "Recently, Transformer-based architecture has been introduced into single\nimage deraining task due to its advantage in modeling non-local information.\nHowever, existing approaches tend to integrate global features based on a dense\nself-attention strategy since it tend to uses all similarities of the tokens\nbetween the queries and keys. In fact, this strategy leads to ignoring the most\nrelevant information and inducing blurry effect by the irrelevant\nrepresentations during the feature aggregation. To this end, this paper\nproposes an effective image deraining Transformer with dynamic dual\nself-attention (DDSA), which combines both dense and sparse attention\nstrategies to better facilitate clear image reconstruction. Specifically, we\nonly select the most useful similarity values based on top-k approximate\ncalculation to achieve sparse attention. In addition, we also develop a novel\nspatial-enhanced feed-forward network (SEFN) to further obtain a more accurate\nrepresentation for achieving high-quality derained results. Extensive\nexperiments on benchmark datasets demonstrate the effectiveness of our proposed\nmethod.",
    "published": "2023-08-15T13:59:47Z",
    "updated": "2023-08-15T13:59:47Z",
    "authors": [
      "Zhentao Fan",
      "Hongming Chen",
      "Yufeng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.18738v1",
    "title": "TLM: Token-Level Masking for Transformers",
    "summary": "Structured dropout approaches, such as attention dropout and DropHead, have\nbeen investigated to regularize the multi-head attention mechanism in\nTransformers. In this paper, we propose a new regularization scheme based on\ntoken-level rather than structure-level to reduce overfitting. Specifically, we\ndevise a novel Token-Level Masking (TLM) training strategy for Transformers to\nregularize the connections of self-attention, which consists of two masking\ntechniques that are effective and easy to implement. The underlying idea is to\nmanipulate the connections between tokens in the multi-head attention via\nmasking, where the networks are forced to exploit partial neighbors'\ninformation to produce a meaningful representation. The generality and\neffectiveness of TLM are thoroughly evaluated via extensive experiments on 4\ndiversified NLP tasks across 18 datasets, including natural language\nunderstanding benchmark GLUE, ChineseGLUE, Chinese Grammatical Error\nCorrection, and data-to-text generation. The results indicate that TLM can\nconsistently outperform attention dropout and DropHead, e.g., it increases by\n0.5 points relative to DropHead with BERT-large on GLUE. Moreover, TLM can\nestablish a new record on the data-to-text benchmark Rotowire (18.93 BLEU). Our\ncode will be publicly available at https://github.com/Young1993/tlm.",
    "published": "2023-10-28T15:42:47Z",
    "updated": "2023-10-28T15:42:47Z",
    "authors": [
      "Yangjun Wu",
      "Kebin Fang",
      "Dongxiang Zhang",
      "Han Wang",
      "Hao Zhang",
      "Gang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.09955v1",
    "title": "DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing",
    "summary": "Images acquired in hazy conditions have degradations induced in them.\nDehazing such images is a vexed and ill-posed problem. Scores of prior-based\nand learning-based approaches have been proposed to mitigate the effect of haze\nand generate haze-free images. Many conventional methods are constrained by\ntheir lack of awareness regarding scene depth and their incapacity to capture\nlong-range dependencies. In this paper, a method that uses residual learning\nand vision transformers in an attention module is proposed. It essentially\ncomprises two networks: In the first one, the network takes the ratio of a hazy\nimage and the approximated transmission matrix to estimate a residual map. The\nsecond network takes this residual image as input and passes it through\nconvolution layers before superposing it on the generated feature maps. It is\nthen passed through global context and depth-aware transformer encoders to\nobtain channel attention. The attention module then infers the spatial\nattention map before generating the final haze-free image. Experimental\nresults, including several quantitative metrics, demonstrate the efficiency and\nscalability of the suggested methodology.",
    "published": "2023-12-15T17:05:32Z",
    "updated": "2023-12-15T17:05:32Z",
    "authors": [
      "Abdul Wasi",
      "O. Jeba Shiney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.16886v2",
    "title": "CAFCT-Net: A CNN-Transformer Hybrid Network with Contextual and\n  Attentional Feature Fusion for Liver Tumor Segmentation",
    "summary": "Medical image semantic segmentation techniques can help identify tumors\nautomatically from computed tomography (CT) scans. In this paper, we propose a\nContextual and Attentional feature Fusions enhanced Convolutional Neural\nNetwork (CNN) and Transformer hybrid network (CAFCT-Net) for liver tumor\nsegmentation. We incorporate three novel modules in the CAFCT-Net architecture:\nAttentional Feature Fusion (AFF), Atrous Spatial Pyramid Pooling (ASPP) of\nDeepLabv3, and Attention Gates (AGs) to improve contextual information related\nto tumor boundaries for accurate segmentation. Experimental results show that\nthe proposed model achieves a mean Intersection over Union (IoU) of 76.54% and\nDice coefficient of 84.29%, respectively, on the Liver Tumor Segmentation\nBenchmark (LiTS) dataset, outperforming pure CNN or Transformer methods, e.g.,\nAttention U-Net and PVTFormer.",
    "published": "2024-01-30T10:42:11Z",
    "updated": "2024-10-04T18:16:26Z",
    "authors": [
      "Ming Kang",
      "Chee-Ming Ting",
      "Fung Fung Ting",
      "RaphaÃ«l Phan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.04779v1",
    "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
    "summary": "The decoder-only Transformer architecture with causal masking and relative\nposition encoding (RPE) has become the de facto choice in language modeling.\nDespite its exceptional performance across various tasks, we have identified\ntwo limitations: First, it requires all attention scores to be non-zero and sum\nup to 1, even if the current embedding has sufficient self-contained\ninformation. This compels the model to assign disproportional excessive\nattention to specific tokens. Second, RPE-based Transformers are not universal\napproximators due to their limited capacity at encoding absolute positional\ninformation, which limits their application in position-critical tasks. In this\nwork, we propose StableMask: a parameter-free method to address both\nlimitations by refining the causal mask. It introduces pseudo-attention values\nto balance attention distributions and encodes absolute positional information\nvia a progressively decreasing mask ratio. StableMask's effectiveness is\nvalidated both theoretically and empirically, showing significant enhancements\nin language models with parameter sizes ranging from 71M to 1.4B across diverse\ndatasets and encoding methods. We further show that it naturally supports (1)\nefficient extrapolation without special tricks such as StreamingLLM and (2)\neasy integration with existing attention optimization techniques.",
    "published": "2024-02-07T12:01:02Z",
    "updated": "2024-02-07T12:01:02Z",
    "authors": [
      "Qingyu Yin",
      "Xuzheng He",
      "Xiang Zhuang",
      "Yu Zhao",
      "Jianhua Yao",
      "Xiaoyu Shen",
      "Qiang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.05370v1",
    "title": "Attention as Robust Representation for Time Series Forecasting",
    "summary": "Time series forecasting is essential for many practical applications, with\nthe adoption of transformer-based models on the rise due to their impressive\nperformance in NLP and CV. Transformers' key feature, the attention mechanism,\ndynamically fusing embeddings to enhance data representation, often relegating\nattention weights to a byproduct role. Yet, time series data, characterized by\nnoise and non-stationarity, poses significant forecasting challenges. Our\napproach elevates attention weights as the primary representation for time\nseries, capitalizing on the temporal relationships among data points to improve\nforecasting accuracy. Our study shows that an attention map, structured using\nglobal landmarks and local windows, acts as a robust kernel representation for\ndata points, withstanding noise and shifts in distribution. Our method\noutperforms state-of-the-art models, reducing mean squared error (MSE) in\nmultivariate time series forecasting by a notable 3.6% without altering the\ncore neural network architecture. It serves as a versatile component that can\nreadily replace recent patching based embedding schemes in transformer-based\nmodels, boosting their performance.",
    "published": "2024-02-08T03:00:50Z",
    "updated": "2024-02-08T03:00:50Z",
    "authors": [
      "PeiSong Niu",
      "Tian Zhou",
      "Xue Wang",
      "Liang Sun",
      "Rong Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.11686v1",
    "title": "Crystalformer: Infinitely Connected Attention for Periodic Structure\n  Encoding",
    "summary": "Predicting physical properties of materials from their crystal structures is\na fundamental problem in materials science. In peripheral areas such as the\nprediction of molecular properties, fully connected attention networks have\nbeen shown to be successful. However, unlike these finite atom arrangements,\ncrystal structures are infinitely repeating, periodic arrangements of atoms,\nwhose fully connected attention results in infinitely connected attention. In\nthis work, we show that this infinitely connected attention can lead to a\ncomputationally tractable formulation, interpreted as neural potential\nsummation, that performs infinite interatomic potential summations in a deeply\nlearned feature space. We then propose a simple yet effective Transformer-based\nencoder architecture for crystal structures called Crystalformer. Compared to\nan existing Transformer-based model, the proposed model requires only 29.4% of\nthe number of parameters, with minimal modifications to the original\nTransformer architecture. Despite the architectural simplicity, the proposed\nmethod outperforms state-of-the-art methods for various property regression\ntasks on the Materials Project and JARVIS-DFT datasets.",
    "published": "2024-03-18T11:37:42Z",
    "updated": "2024-03-18T11:37:42Z",
    "authors": [
      "Tatsunori Taniai",
      "Ryo Igarashi",
      "Yuta Suzuki",
      "Naoya Chiba",
      "Kotaro Saito",
      "Yoshitaka Ushiku",
      "Kanta Ono"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.00633v1",
    "title": "IPT-V2: Efficient Image Processing Transformer using Hierarchical\n  Attentions",
    "summary": "Recent advances have demonstrated the powerful capability of transformer\narchitecture in image restoration. However, our analysis indicates that\nexisting transformerbased methods can not establish both exact global and local\ndependencies simultaneously, which are much critical to restore the details and\nmissing content of degraded images. To this end, we present an efficient image\nprocessing transformer architecture with hierarchical attentions, called IPTV2,\nadopting a focal context self-attention (FCSA) and a global grid self-attention\n(GGSA) to obtain adequate token interactions in local and global receptive\nfields. Specifically, FCSA applies the shifted window mechanism into the\nchannel self-attention, helps capture the local context and mutual interaction\nacross channels. And GGSA constructs long-range dependencies in the\ncross-window grid, aggregates global information in spatial dimension.\nMoreover, we introduce structural re-parameterization technique to feed-forward\nnetwork to further improve the model capability. Extensive experiments\ndemonstrate that our proposed IPT-V2 achieves state-of-the-art results on\nvarious image processing tasks, covering denoising, deblurring, deraining and\nobtains much better trade-off for performance and computational complexity than\nprevious methods. Besides, we extend our method to image generation as latent\ndiffusion backbone, and significantly outperforms DiTs.",
    "published": "2024-03-31T10:01:20Z",
    "updated": "2024-03-31T10:01:20Z",
    "authors": [
      "Zhijun Tu",
      "Kunpeng Du",
      "Hanting Chen",
      "Hailing Wang",
      "Wei Li",
      "Jie Hu",
      "Yunhe Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.06022v2",
    "title": "Band-Attention Modulated RetNet for Face Forgery Detection",
    "summary": "The transformer networks are extensively utilized in face forgery detection\ndue to their scalability across large datasets.Despite their success,\ntransformers face challenges in balancing the capture of global context, which\nis crucial for unveiling forgery clues, with computational complexity.To\nmitigate this issue, we introduce Band-Attention modulated RetNet (BAR-Net), a\nlightweight network designed to efficiently process extensive visual contexts\nwhile avoiding catastrophic forgetting.Our approach empowers the target token\nto perceive global information by assigning differential attention levels to\ntokens at varying distances. We implement self-attention along both spatial\naxes, thereby maintaining spatial priors and easing the computational\nburden.Moreover, we present the adaptive frequency Band-Attention Modulation\nmechanism, which treats the entire Discrete Cosine Transform spectrogram as a\nseries of frequency bands with learnable weights.Together, BAR-Net achieves\nfavorable performance on several face forgery datasets, outperforming current\nstate-of-the-art methods.",
    "published": "2024-04-09T05:11:28Z",
    "updated": "2024-07-02T01:19:01Z",
    "authors": [
      "Zhida Zhang",
      "Jie Cao",
      "Wenkui Yang",
      "Qihang Fan",
      "Kai Zhou",
      "Ran He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.04299v2",
    "title": "ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D\n  Occupancy Perception via View-Guided Transformers",
    "summary": "3D occupancy, an advanced perception technology for driving scenarios,\nrepresents the entire scene without distinguishing between foreground and\nbackground by quantifying the physical space into a grid map. The widely\nadopted projection-first deformable attention, efficient in transforming image\nfeatures into 3D representations, encounters challenges in aggregating\nmulti-view features due to sensor deployment constraints. To address this\nissue, we propose our learning-first view attention mechanism for effective\nmulti-view feature aggregation. Moreover, we showcase the scalability of our\nview attention across diverse multi-view 3D tasks, including map construction\nand 3D object detection. Leveraging the proposed view attention as well as an\nadditional multi-frame streaming temporal attention, we introduce ViewFormer, a\nvision-centric transformer-based framework for spatiotemporal feature\naggregation. To further explore occupancy-level flow representation, we present\nFlowOcc3D, a benchmark built on top of existing high-quality datasets.\nQualitative and quantitative analyses on this benchmark reveal the potential to\nrepresent fine-grained dynamic scenes. Extensive experiments show that our\napproach significantly outperforms prior state-of-the-art methods. The codes\nare available at \\url{https://github.com/ViewFormerOcc/ViewFormer-Occ}.",
    "published": "2024-05-07T13:15:07Z",
    "updated": "2024-07-12T08:43:20Z",
    "authors": [
      "Jinke Li",
      "Xiao He",
      "Chonghua Zhou",
      "Xiaoqiang Cheng",
      "Yang Wen",
      "Dan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.13985v2",
    "title": "LookHere: Vision Transformers with Directed Attention Generalize and\n  Extrapolate",
    "summary": "High-resolution images offer more information about scenes that can improve\nmodel accuracy. However, the dominant model architecture in computer vision,\nthe vision transformer (ViT), cannot effectively leverage larger images without\nfinetuning -- ViTs poorly extrapolate to more patches at test time, although\ntransformers offer sequence length flexibility. We attribute this shortcoming\nto the current patch position encoding methods, which create a distribution\nshift when extrapolating.\n  We propose a drop-in replacement for the position encoding of plain ViTs that\nrestricts attention heads to fixed fields of view, pointed in different\ndirections, using 2D attention masks. Our novel method, called LookHere,\nprovides translation-equivariance, ensures attention head diversity, and limits\nthe distribution shift that attention heads face when extrapolating. We\ndemonstrate that LookHere improves performance on classification (avg. 1.6%),\nagainst adversarial attack (avg. 5.4%), and decreases calibration error (avg.\n1.5%) -- on ImageNet without extrapolation. With extrapolation, LookHere\noutperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on\nImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we\nrelease a high-resolution test set to improve the evaluation of high-resolution\nimage classifiers, called ImageNet-HR.",
    "published": "2024-05-22T20:47:25Z",
    "updated": "2024-10-29T20:39:53Z",
    "authors": [
      "Anthony Fuller",
      "Daniel G. Kyrollos",
      "Yousef Yassin",
      "James R. Green"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.17025v1",
    "title": "SWAT: Scalable and Efficient Window Attention-based Transformers\n  Acceleration on FPGAs",
    "summary": "Efficiently supporting long context length is crucial for Transformer models.\nThe quadratic complexity of the self-attention computation plagues traditional\nTransformers. Sliding window-based static sparse attention mitigates the\nproblem by limiting the attention scope of the input tokens, reducing the\ntheoretical complexity from quadratic to linear. Although the sparsity induced\nby window attention is highly structured, it does not align perfectly with the\nmicroarchitecture of the conventional accelerators, leading to suboptimal\nimplementation. In response, we propose a dataflow-aware FPGA-based accelerator\ndesign, SWAT, that efficiently leverages the sparsity to achieve scalable\nperformance for long input. The proposed microarchitecture is based on a design\nthat maximizes data reuse by using a combination of row-wise dataflow, kernel\nfusion optimization, and an input-stationary design considering the distributed\nmemory and computation resources of FPGA. Consequently, it achieves up to\n22$\\times$ and 5.7$\\times$ improvement in latency and energy efficiency\ncompared to the baseline FPGA-based accelerator and 15$\\times$ energy\nefficiency compared to GPU-based solution.",
    "published": "2024-05-27T10:25:08Z",
    "updated": "2024-05-27T10:25:08Z",
    "authors": [
      "Zhenyu Bai",
      "Pranav Dangi",
      "Huize Li",
      "Tulika Mitra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.02761v1",
    "title": "Multi-layer Learnable Attention Mask for Multimodal Tasks",
    "summary": "While the Self-Attention mechanism in the Transformer model has proven to be\neffective in many domains, we observe that it is less effective in more diverse\nsettings (e.g. multimodality) due to the varying granularity of each token and\nthe high computational demands of lengthy sequences. To address the challenges,\nwe introduce the Learnable Attention Mask (LAM), strategically designed to\nglobally regulate attention maps and prioritize critical tokens within the\nsequence. Leveraging the Self-Attention module in a BERT-like transformer\nnetwork, our approach adeptly captures associations between tokens. The\nextension of the LAM to a multi-layer version accommodates the varied\ninformation aspects embedded at each layer of the Transformer network.\nComprehensive experimental validation on various datasets, such as MADv2,\nQVHighlights, ImageNet 1K, and MSRVTT, demonstrates the efficacy of the LAM,\nexemplifying its ability to enhance model performance while mitigating\nredundant computations. This pioneering approach presents a significant\nadvancement in enhancing the understanding of complex scenarios, such as in\nmovie understanding.",
    "published": "2024-06-04T20:28:02Z",
    "updated": "2024-06-04T20:28:02Z",
    "authors": [
      "Wayner Barrios",
      "SouYoung Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.12893v1",
    "title": "Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference",
    "summary": "In the world of deep learning, Transformer models have become very\nsignificant, leading to improvements in many areas from understanding language\nto recognizing images, covering a wide range of applications. Despite their\nsuccess, the deployment of these models in real-time applications, particularly\non edge devices, poses significant challenges due to their quadratic\ncomputational intensity and memory demands. To overcome these challenges we\nintroduce a novel Hybrid Dynamic Pruning (HDP), an efficient\nalgorithm-architecture co-design approach that accelerates transformers using\nhead sparsity, block sparsity and approximation opportunities to reduce\ncomputations in attention and reduce memory access. With the observation of the\nhuge redundancy in attention scores and attention heads, we propose a novel\ninteger-based row-balanced block pruning to prune unimportant blocks in the\nattention matrix at run time, also propose integer-based head pruning to detect\nand prune unimportant heads at an early stage at run time. Also we propose an\napproximation method that reduces attention computations. To efficiently\nsupport these methods with lower latency and power efficiency, we propose a HDP\nco-processor architecture.",
    "published": "2024-07-17T11:15:16Z",
    "updated": "2024-07-17T11:15:16Z",
    "authors": [
      "Ghadeer Jaradat",
      "Mohammed Tolba",
      "Ghada Alsuhli",
      "Hani Saleh",
      "Mahmoud Al-Qutayri",
      "Thanos Stouraitis",
      "Baker Mohammad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.12742v1",
    "title": "TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based\n  Computing",
    "summary": "Due to the high computation overhead of Vision Transformers (ViTs), In-memory\nComputing architectures are being researched towards energy-efficient\ndeployment in edge-computing scenarios. Prior works have proposed efficient\nalgorithm-hardware co-design and IMC-architectural improvements to improve the\nenergy-efficiency of IMC-implemented ViTs. However, all prior works have\nneglected the overhead and co-depencence of attention blocks on the\naccuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose\nTReX- an attention-reuse-driven ViT optimization framework that effectively\nperforms attention reuse in ViT models to achieve optimal\naccuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer\nencoders for attention reuse to achieve near iso-accuracy performance while\nmeeting the user-specified delay requirement. Based on our analysis on the\nImagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and\n1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S\n(LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP\nreduction compared to state-of-the-art token pruning and weight sharing\napproaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal\naccuracy compared to baseline at 1.6x lower EDAP.",
    "published": "2024-08-22T21:51:38Z",
    "updated": "2024-08-22T21:51:38Z",
    "authors": [
      "Abhishek Moitra",
      "Abhiroop Bhattacharjee",
      "Youngeun Kim",
      "Priyadarshini Panda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.07146v2",
    "title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling",
    "summary": "Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via\n$\\operatorname{softmax}$, utilizing context-aware memory reading and adaptive\nforgetting to improve memory capacity while maintaining compact recurrent state\nsize. This design greatly enhances both training and inference efficiency\nthrough GLA's hardware-efficient training algorithm and reduced state size.\nAdditionally, retaining the $\\operatorname{softmax}$ operation is particularly\nbeneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings,\nreducing the need for extensive training from scratch. Extensive experiments\nconfirm GSA's superior performance in scenarios requiring in-context recall and\nin T2R settings.",
    "published": "2024-09-11T09:49:50Z",
    "updated": "2024-10-31T13:54:35Z",
    "authors": [
      "Yu Zhang",
      "Songlin Yang",
      "Ruijie Zhu",
      "Yue Zhang",
      "Leyang Cui",
      "Yiqiao Wang",
      "Bolun Wang",
      "Freda Shi",
      "Bailin Wang",
      "Wei Bi",
      "Peng Zhou",
      "Guohong Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.14439v1",
    "title": "Mixed Attention Transformer Enhanced Channel Estimation for Extremely\n  Large-Scale MIMO Systems",
    "summary": "Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is one\nof the key technologies for next-generation wireless communication systems.\nHowever, acquiring the accurate high-dimensional channel matrix of XL-MIMO\nremains a pressing challenge due to the intractable channel property and the\nhigh complexity. In this paper, a Mixed Attention Transformer based Channel\nEstimation Neural Network (MAT-CENet) is developed, which is inspired by the\nTransformer encoder structure as well as organically integrates the feature map\nattention and spatial attention mechanisms to better grasp the unique\ncharacteristics of the XL-MIMO channel. By incorporating the multi-head\nattention layer as the core enabler, the insightful feature importance is\ncaptured and exploited effectively. A comprehensive complexity analysis for the\nproposed MAT-CENet is also provided. Simulation results show that MAT-CENet\noutperforms the state of the art in different propagation scenarios of near-,\nfar- and hybrid-fields.",
    "published": "2024-10-18T12:55:06Z",
    "updated": "2024-10-18T12:55:06Z",
    "authors": [
      "Shuang shuang Li",
      "Peihao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03214v4",
    "title": "Continual Low-Rank Scaled Dot-product Attention",
    "summary": "Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of Transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models.",
    "published": "2024-12-04T11:05:01Z",
    "updated": "2025-07-28T10:27:45Z",
    "authors": [
      "GinÃ©s Carreto PicÃ³n",
      "Illia Oleksiienko",
      "Lukas Hedegaard",
      "Arian Bakhtiarnia",
      "Alexandros Iosifidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05869v1",
    "title": "HyLiFormer: Hyperbolic Linear Attention for Skeleton-based Human Action\n  Recognition",
    "summary": "Transformers have demonstrated remarkable performance in skeleton-based human\naction recognition, yet their quadratic computational complexity remains a\nbottleneck for real-world applications. To mitigate this, linear attention\nmechanisms have been explored but struggle to capture the hierarchical\nstructure of skeleton data. Meanwhile, the Poincar\\'e model, as a typical\nhyperbolic geometry, offers a powerful framework for modeling hierarchical\nstructures but lacks well-defined operations for existing mainstream linear\nattention. In this paper, we propose HyLiFormer, a novel hyperbolic linear\nattention Transformer tailored for skeleton-based action recognition. Our\napproach incorporates a Hyperbolic Transformation with Curvatures (HTC) module\nto map skeleton data into hyperbolic space and a Hyperbolic Linear Attention\n(HLA) module for efficient long-range dependency modeling. Theoretical analysis\nand extensive experiments on NTU RGB+D and NTU RGB+D 120 datasets demonstrate\nthat HyLiFormer significantly reduces computational complexity while preserving\nmodel accuracy, making it a promising solution for efficiency-critical\napplications.",
    "published": "2025-02-09T12:08:03Z",
    "updated": "2025-02-09T12:08:03Z",
    "authors": [
      "Yue Li",
      "Haoxuan Qu",
      "Mengyuan Liu",
      "Jun Liu",
      "Yujun Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.18277v1",
    "title": "Self-Adjust Softmax",
    "summary": "The softmax function is crucial in Transformer attention, which normalizes\neach row of the attention scores with summation to one, achieving superior\nperformances over other alternative functions. However, the softmax function\ncan face a gradient vanishing issue when some elements of the attention scores\napproach extreme values, such as probabilities close to one or zero. In this\npaper, we propose Self-Adjust Softmax (SA-Softmax) to address this issue by\nmodifying $softmax(x)$ to $x \\cdot softmax(x)$ and its normalized variant\n$\\frac{(x - min(x_{\\min},0))}{max(0,x_{max})-min(x_{min},0)} \\cdot softmax(x)$.\nWe theoretically show that SA-Softmax provides enhanced gradient properties\ncompared to the vanilla softmax function. Moreover, SA-Softmax Attention can be\nseamlessly integrated into existing Transformer models to their attention\nmechanisms with minor adjustments. We conducted experiments to evaluate the\nempirical performance of Transformer models using SA-Softmax compared to the\nvanilla softmax function. These experiments, involving models with up to 2.7\nbillion parameters, are conducted across diverse datasets, language tasks, and\npositional encoding methods.",
    "published": "2025-02-25T15:07:40Z",
    "updated": "2025-02-25T15:07:40Z",
    "authors": [
      "Chuanyang Zheng",
      "Yihang Gao",
      "Guoxuan Chen",
      "Han Shi",
      "Jing Xiong",
      "Xiaozhe Ren",
      "Chao Huang",
      "Xin Jiang",
      "Zhenguo Li",
      "Yu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07937v1",
    "title": "Quantum Graph Transformer for NLP Sentiment Classification",
    "summary": "Quantum machine learning is a promising direction for building more efficient\nand expressive models, particularly in domains where understanding complex,\nstructured data is critical. We present the Quantum Graph Transformer (QGT), a\nhybrid graph-based architecture that integrates a quantum self-attention\nmechanism into the message-passing framework for structured language modeling.\nThe attention mechanism is implemented using parameterized quantum circuits\n(PQCs), which enable the model to capture rich contextual relationships while\nsignificantly reducing the number of trainable parameters compared to classical\nattention mechanisms. We evaluate QGT on five sentiment classification\nbenchmarks. Experimental results show that QGT consistently achieves higher or\ncomparable accuracy than existing quantum natural language processing (QNLP)\nmodels, including both attention-based and non-attention-based approaches. When\ncompared with an equivalent classical graph transformer, QGT yields an average\naccuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic\ndatasets. Additionally, QGT demonstrates improved sample efficiency, requiring\nnearly 50% fewer labeled samples to reach comparable performance on the Yelp\ndataset. These results highlight the potential of graph-based QNLP techniques\nfor advancing efficient and scalable language understanding.",
    "published": "2025-06-09T16:55:41Z",
    "updated": "2025-06-09T16:55:41Z",
    "authors": [
      "Shamminuj Aktar",
      "Andreas BÃ¤rtschi",
      "Abdel-Hameed A. Badawy",
      "Stephan Eidenbenz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.11465v1",
    "title": "RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer",
    "summary": "Multimodal learning faces challenges in effectively fusing information from\ndiverse modalities, especially when modality quality varies across samples.\nDynamic fusion strategies, such as attention mechanism in Transformers, aim to\naddress such challenge by adaptively emphasizing modalities based on the\ncharacteristics of input data. However, through amounts of carefully designed\nexperiments, we surprisingly observed that the dynamic adaptability of\nwidely-used self-attention models diminishes. Model tends to prefer one\nmodality regardless of data characteristics. This bias triggers a\nself-reinforcing cycle that progressively overemphasizes the favored modality,\nwidening the distribution gap in attention keys across modalities and\ndeactivating attention mechanism's dynamic properties. To revive adaptability,\nwe propose a simple yet effective method Rolling Query (RollingQ), which\nbalances attention allocation by rotating the query to break the\nself-reinforcing cycle and mitigate the key distribution gap. Extensive\nexperiments on various multimodal scenarios validate the effectiveness of\nRollingQ and the restoration of cooperation dynamics is pivotal for enhancing\nthe broader capabilities of widely deployed multimodal Transformers. The source\ncode is available at https://github.com/GeWu-Lab/RollingQ_ICML2025.",
    "published": "2025-06-13T04:39:58Z",
    "updated": "2025-06-13T04:39:58Z",
    "authors": [
      "Haotian Ni",
      "Yake Wei",
      "Hang Liu",
      "Gong Chen",
      "Chong Peng",
      "Hao Lin",
      "Di Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.00394v1",
    "title": "HelixPipe: Efficient Distributed Training of Long Sequence Transformers\n  with Attention Parallel Pipeline Parallelism",
    "summary": "As transformer sequence lengths grow, existing pipeline parallelisms incur\nsuboptimal performance due to the quadratic attention computation and the\nsubstantial memory overhead. To relieve these challenges, we propose HelixPipe,\na novel pipeline parallelism for long sequence transformer training. First,\nHelixPipe introduces attention parallel partition, which schedules attention\ncomputations of different micro batches across different pipeline stages in\nparallel, reducing pipeline bubbles. Second, it employs a two-fold\nfirst-in-last-out micro batch schedule to balance memory usage and overlap\ncommunication with computation. Additionally, HelixPipe utilizes recomputation\nwithout attention and chunked MLP to mitigate fragmentation and enable longer\nsequences. Experiments demonstrate that HelixPipe gains increasing advantages\nwith longer sequence lengths, and outperforms existing methods in throughput\nand scalability across varying pipeline sizes, model sizes, and cluster\nconfigurations. Notably, it achieves a 26\\% speedup over baseline methods when\ntraining a 7B model with 128k sequence length on 64 H20 GPUs. Code is available\nat https://github.com/code-tunnel/Megatron-LM/tree/dev.",
    "published": "2025-07-01T03:11:18Z",
    "updated": "2025-07-01T03:11:18Z",
    "authors": [
      "Geng Zhang",
      "Shenggan Cheng",
      "Xuanlei Zhao",
      "Ziming Liu",
      "Yang You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07814v1",
    "title": "Pay Attention to Attention Distribution: A New Local Lipschitz Bound for\n  Transformers",
    "summary": "We present a novel local Lipschitz bound for self-attention blocks of\ntransformers. This bound is based on a refined closed-form expression for the\nspectral norm of the softmax function. The resulting bound is not only more\naccurate than in the prior art, but also unveils the dependence of the\nLipschitz constant on attention score maps. Based on the new findings, we\nsuggest an explanation of the way distributions inside the attention map affect\nthe robustness from the Lipschitz constant perspective. We also introduce a new\nlightweight regularization term called JaSMin (Jacobian Softmax norm\nMinimization), which boosts the transformer's robustness and decreases local\nLipschitz constants of the whole network.",
    "published": "2025-07-10T14:45:31Z",
    "updated": "2025-07-10T14:45:31Z",
    "authors": [
      "Nikolay Yudin",
      "Alexander Gaponov",
      "Sergei Kudriashov",
      "Maxim Rakhuba"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16676v1",
    "title": "Custom Algorithm-based Fault Tolerance for Attention Layers in\n  Transformers",
    "summary": "Transformers and large language models (LLMs), powered by the attention\nmechanism, have transformed numerous AI applications, driving the need for\nspecialized hardware accelerators. A major challenge in these accelerators is\nefficiently detecting errors caused by random hardware faults. Traditional\nalgorithm-based fault tolerance (ABFT) techniques verify individual matrix\nmultiplications but fall short in handling the full attention mechanism,\nparticularly due to intermediate softmax normalization. This work proposes\nFlash-ABFT, a novel method that computes an online checksum across the entire\nthree-matrix product of query, key and value matrices, of an attention layer,\nincluding the softmax operation, with a single check. This approach\nsignificantly reduces overhead by eliminating redundant checks while\nmaintaining high fault-detection accuracy. Experimental results demonstrate\nthat Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%\nenergy overhead, making it a cost-effective and robust solution for error\ndetection in attention accelerators.",
    "published": "2025-07-22T15:11:13Z",
    "updated": "2025-07-22T15:11:13Z",
    "authors": [
      "Vasileios Titopoulos",
      "Kosmas Alexandridis",
      "Giorgos Dimitrakopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.07519v1",
    "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based\n  Image Editing",
    "summary": "Transformer-based diffusion models have recently superseded traditional U-Net\narchitectures, with multimodal diffusion transformers (MM-DiT) emerging as the\ndominant approach in state-of-the-art models like Stable Diffusion 3 and\nFlux.1. Previous approaches have relied on unidirectional cross-attention\nmechanisms, with information flowing from text embeddings to image latents. In\ncontrast, MMDiT introduces a unified attention mechanism that concatenates\ninput projections from both modalities and performs a single full attention\noperation, allowing bidirectional information flow between text and image\nbranches. This architectural shift presents significant challenges for existing\nediting techniques. In this paper, we systematically analyze MM-DiT's attention\nmechanism by decomposing attention matrices into four distinct blocks,\nrevealing their inherent characteristics. Through these analyses, we propose a\nrobust, prompt-based image editing method for MM-DiT that supports global to\nlocal edits across various MM-DiT variants, including few-step models. We\nbelieve our findings bridge the gap between existing U-Net-based methods and\nemerging architectures, offering deeper insights into MMDiT's behavioral\npatterns.",
    "published": "2025-08-11T00:40:12Z",
    "updated": "2025-08-11T00:40:12Z",
    "authors": [
      "Joonghyuk Shin",
      "Alchan Hwang",
      "Yujin Kim",
      "Daneul Kim",
      "Jaesik Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10918v2",
    "title": "ToMA: Token Merge with Attention for Diffusion Models",
    "summary": "Diffusion models excel in high-fidelity image generation but face scalability\nlimits due to transformers' quadratic attention complexity. Plug-and-play token\nreduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens\nin generated images but rely on GPU-inefficient operations (e.g., sorting,\nscattered writes), introducing overheads that negate theoretical speedups when\npaired with optimized attention implementations (e.g., FlashAttention). To\nbridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf\nmethod that redesigns token reduction for GPU-aligned efficiency, with three\nkey contributions: 1) a reformulation of token merge as a submodular\noptimization problem to select diverse tokens; 2) merge/unmerge as an\nattention-like linear transformation via GPU-friendly matrix operations; and 3)\nexploiting latent locality and sequential redundancy (pattern reuse) to\nminimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%,\nrespectively (with DINO $\\Delta < 0.07$), outperforming prior methods. This\nwork bridges the gap between theoretical and practical efficiency for\ntransformers in diffusion.",
    "published": "2025-09-13T17:35:00Z",
    "updated": "2025-09-23T02:10:29Z",
    "authors": [
      "Wenbo Lu",
      "Shaoyi Zheng",
      "Yuxuan Xia",
      "Shengjie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12888v1",
    "title": "Runge-Kutta Approximation and Decoupled Attention for Rectified Flow\n  Inversion and Semantic Editing",
    "summary": "Rectified flow (RF) models have recently demonstrated superior generative\nperformance compared to DDIM-based diffusion models. However, in real-world\napplications, they suffer from two major challenges: (1) low inversion accuracy\nthat hinders the consistency with the source image, and (2) entangled\nmultimodal attention in diffusion transformers, which hinders precise attention\ncontrol. To address the first challenge, we propose an efficient high-order\ninversion method for rectified flow models based on the Runge-Kutta solver of\ndifferential equations. To tackle the second challenge, we introduce Decoupled\nDiffusion Transformer Attention (DDTA), a novel mechanism that disentangles\ntext and image attention inside the multimodal diffusion transformers, enabling\nmore precise semantic control. Extensive experiments on image reconstruction\nand text-guided editing tasks demonstrate that our method achieves\nstate-of-the-art performance in terms of fidelity and editability. Code is\navailable at https://github.com/wmchen/RKSovler_DDTA.",
    "published": "2025-09-16T09:41:14Z",
    "updated": "2025-09-16T09:41:14Z",
    "authors": [
      "Weiming Chen",
      "Zhihan Zhu",
      "Yijia Wang",
      "Zhihai He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25252v2",
    "title": "Fact Grounded Attention: Eliminating Hallucination in Large Language\n  Models Through Attention Level Knowledge Integration",
    "summary": "\"The greatest enemy of knowledge is not ignorance, it is the illusion of\nknowledge.\" Large Language Models have conquered natural language but remain\nprisoners of their own probabilistic nature--confidently hallucinating facts\nthey never truly knew. We present Fact Grounded Attention (FGA), a novel\narchitectural modification that transforms unreliable language models into\ndeterministic truth tellers by injecting verifiable knowledge directly into the\nattention mechanism. Unlike existing approaches that patch hallucinations after\ngeneration or prepend retrieved text, FGA intervenes at the mathematical heart\nof the transformer--the pre-softmax attention scores--creating a model that\ncannot hallucinate when facts exist in its knowledge base. Our experiments\nacross 1,107 technical queries spanning smartphones, laptops, and electric\nvehicles demonstrate a transformation from 6.3% accuracy in vanilla Llama 3.2\nto 99.7% accuracy with FGA. More critically, knowledge updates occur in under\none second without retraining, compared to hours for parameter editing\napproaches. FGA doesn't just reduce hallucination--it eliminates it entirely\nfor verifiable facts, marking a fundamental shift from probabilistic\napproximation to deterministic precision in neural language generation.",
    "published": "2025-09-27T09:55:21Z",
    "updated": "2025-10-02T06:19:43Z",
    "authors": [
      "Aayush Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12704v1",
    "title": "Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray\n  Diagnosis",
    "summary": "Transformer-based deep learning models have demonstrated exceptional\nperformance in medical imaging by leveraging attention mechanisms for feature\nrepresentation and interpretability. However, these models are prone to\nlearning spurious correlations, leading to biases and limited generalization.\nWhile human-AI attention alignment can mitigate these issues, it often depends\non costly manual supervision. In this work, we propose a Hybrid\nExplanation-Guided Learning (H-EGL) framework that combines self-supervised and\nhuman-guided constraints to enhance attention alignment and improve\ngeneralization. The self-supervised component of H-EGL leverages\nclass-distinctive attention without relying on restrictive priors, promoting\nrobustness and flexibility. We validate our approach on chest X-ray\nclassification using the Vision Transformer (ViT), where H-EGL outperforms two\nstate-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating\nsuperior classification accuracy and generalization capability. Additionally,\nit produces attention maps that are better aligned with human expertise.",
    "published": "2025-10-14T16:39:02Z",
    "updated": "2025-10-14T16:39:02Z",
    "authors": [
      "Shelley Zixin Shu",
      "Haozhe Luo",
      "Alexander Poellinger",
      "Mauricio Reyes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.19321v1",
    "title": "Online Handwritten Signature Verification Based on Temporal-Spatial\n  Graph Attention Transformer",
    "summary": "Handwritten signature verification is a crucial aspect of identity\nauthentication, with applications in various domains such as finance and\ne-commerce. However, achieving high accuracy in signature verification remains\nchallenging due to intra-user variability and the risk of forgery. This paper\nintroduces a novel approach for dynamic signature verification: the\nTemporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the\nGraph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both\nspatial and temporal dependencies in signature data. TS-GATR enhances\nverification performance by representing signatures as graphs, where each node\ncaptures dynamic features (e.g. position, velocity, pressure), and by using\nattention mechanisms to model their complex relationships. The proposed method\nfurther employs a Dual-Graph Attention Transformer (DGATR) module, which\nutilizes k-step and k-nearest neighbor adjacency graphs to model local and\nglobal spatial features, respectively. To capture long-term temporal\ndependencies, the model integrates GRU, thereby enhancing its ability to learn\ndynamic features during signature verification. Comprehensive experiments\nconducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR\nsurpasses current state-of-the-art approaches, consistently achieving lower\nEqual Error Rates (EER) across various scenarios.",
    "published": "2025-10-22T07:32:55Z",
    "updated": "2025-10-22T07:32:55Z",
    "authors": [
      "Hai-jie Yuan",
      "Heng Zhang",
      "Fei Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.04309v1",
    "title": "3D Vision with Transformers: A Survey",
    "summary": "The success of the transformer architecture in natural language processing\nhas recently triggered attention in the computer vision field. The transformer\nhas been used as a replacement for the widely used convolution operators, due\nto its ability to learn long-range dependencies. This replacement was proven to\nbe successful in numerous tasks, in which several state-of-the-art methods rely\non transformers for better learning. In computer vision, the 3D field has also\nwitnessed an increase in employing the transformer for 3D convolution neural\nnetworks and multi-layer perceptron networks. Although a number of surveys have\nfocused on transformers in vision in general, 3D vision requires special\nattention due to the difference in data representation and processing when\ncompared to 2D vision. In this work, we present a systematic and thorough\nreview of more than 100 transformers methods for different 3D vision tasks,\nincluding classification, segmentation, detection, completion, pose estimation,\nand others. We discuss transformer design in 3D vision, which allows it to\nprocess data with various 3D representations. For each application, we\nhighlight key properties and contributions of proposed transformer-based\nmethods. To assess the competitiveness of these methods, we compare their\nperformance to common non-transformer methods on 12 3D benchmarks. We conclude\nthe survey by discussing different open directions and challenges for\ntransformers in 3D vision. In addition to the presented papers, we aim to\nfrequently update the latest relevant papers along with their corresponding\nimplementations at: https://github.com/lahoud/3d-vision-transformers.",
    "published": "2022-08-08T17:59:11Z",
    "updated": "2022-08-08T17:59:11Z",
    "authors": [
      "Jean Lahoud",
      "Jiale Cao",
      "Fahad Shahbaz Khan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.00960v1",
    "title": "Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy\n  for Image Recognition without Convolutions",
    "summary": "With the achievements of Transformer in the field of natural language\nprocessing, the encoder-decoder and the attention mechanism in Transformer have\nbeen applied to computer vision. Recently, in multiple tasks of computer vision\n(image classification, object detection, semantic segmentation, etc.),\nstate-of-the-art convolutional neural networks have introduced some concepts of\nTransformer. This proves that Transformer has a good prospect in the field of\nimage recognition. After Vision Transformer was proposed, more and more works\nbegan to use self-attention to completely replace the convolutional layer. This\nwork is based on Vision Transformer, combined with the pyramid architecture,\nusing Split-transform-merge to propose the group encoder and name the network\narchitecture Aggregated Pyramid Vision Transformer (APVT). We perform image\nclassification tasks on the CIFAR-10 dataset and object detection tasks on the\nCOCO 2017 dataset. Compared with other network architectures that use\nTransformer as the backbone, APVT has excellent results while reducing the\ncomputational cost. We hope this improved strategy can provide a reference for\nfuture Transformer research in computer vision.",
    "published": "2022-03-02T09:14:28Z",
    "updated": "2022-03-02T09:14:28Z",
    "authors": [
      "Rui-Yang Ju",
      "Ting-Yu Lin",
      "Jen-Shiun Chiang",
      "Jia-Hao Jian",
      "Yu-Shian Lin",
      "Liu-Rui-Yi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.13342v2",
    "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
    "summary": "In this paper, we introduce DropHead, a structured dropout method\nspecifically designed for regularizing the multi-head attention mechanism,\nwhich is a key component of transformer, a state-of-the-art model for various\nNLP tasks. In contrast to the conventional dropout mechanisms which randomly\ndrop units or connections, the proposed DropHead is a structured dropout\nmethod. It drops entire attention-heads during training and It prevents the\nmulti-head attention model from being dominated by a small portion of attention\nheads while also reduces the risk of overfitting the training data, thus making\nuse of the multi-head attention mechanism more efficiently. Motivated by recent\nstudies about the learning dynamic of the multi-head attention mechanism, we\npropose a specific dropout rate schedule to adaptively adjust the dropout rate\nof DropHead and achieve better regularization effect. Experimental results on\nboth machine translation and text classification benchmark datasets demonstrate\nthe effectiveness of the proposed approach.",
    "published": "2020-04-28T07:33:14Z",
    "updated": "2020-11-01T15:57:37Z",
    "authors": [
      "Wangchunshu Zhou",
      "Tao Ge",
      "Ke Xu",
      "Furu Wei",
      "Ming Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.05853v1",
    "title": "Attention Weights in Transformer NMT Fail Aligning Words Between\n  Sequences but Largely Explain Model Predictions",
    "summary": "This work proposes an extensive analysis of the Transformer architecture in\nthe Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder\nattention mechanism, we prove that attention weights systematically make\nalignment errors by relying mainly on uninformative tokens from the source\nsequence. However, we observe that NMT models assign attention to these tokens\nto regulate the contribution in the prediction of the two contexts, the source\nand the prefix of the target sequence. We provide evidence about the influence\nof wrong alignments on the model behavior, demonstrating that the\nencoder-decoder attention mechanism is well suited as an interpretability\nmethod for NMT. Finally, based on our analysis, we propose methods that largely\nreduce the word alignment error rate compared to standard induced alignments\nfrom attention weights.",
    "published": "2021-09-13T10:44:02Z",
    "updated": "2021-09-13T10:44:02Z",
    "authors": [
      "Javier Ferrando",
      "Marta R. Costa-jussÃ "
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.13167v2",
    "title": "Exploring Self-Attention for Crop-type Classification Explainability",
    "summary": "Transformer models have become a promising approach for crop-type\nclassification. Although their attention weights can be used to understand the\nrelevant time points for crop disambiguation, the validity of these insights\ndepends on how closely the attention weights approximate the actual workings of\nthese black-box models, which is not always clear. In this paper, we introduce\na novel explainability framework that systematically evaluates the explanatory\npower of the attention weights of a standard transformer encoder for crop-type\nclassification. Our results show that attention patterns strongly relate to key\ndates, which are often associated with critical phenological events for\ncrop-type classification. Further, the sensitivity analysis reveals the limited\ncapability of the attention weights to characterize crop phenology as the\nidentified phenological events depend on the other crops considered during\ntraining. This limitation highlights the relevance of future work towards the\ndevelopment of deep learning approaches capable of automatically learning the\ntemporal vegetation dynamics for accurate crop disambiguation",
    "published": "2022-10-24T12:36:40Z",
    "updated": "2025-04-20T17:55:15Z",
    "authors": [
      "Ivica Obadic",
      "Ribana Roscher",
      "Dario Augusto Borges Oliveira",
      "Xiao Xiang Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1803.10916v1",
    "title": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting",
    "summary": "In this paper, we propose an attention-based end-to-end neural approach for\nsmall-footprint keyword spotting (KWS), which aims to simplify the pipelines of\nbuilding a production-quality KWS system. Our model consists of an encoder and\nan attention mechanism. The encoder transforms the input signal into a high\nlevel representation using RNNs. Then the attention mechanism weights the\nencoder features and generates a fixed-length vector. Finally, by linear\ntransformation and softmax function, the vector becomes a score used for\nkeyword detection. We also evaluate the performance of different encoder\narchitectures, including LSTM, GRU and CRNN. Experiments on real-world wake-up\ndata show that our approach outperforms the recent Deep KWS approach by a large\nmargin and the best performance is achieved by CRNN. To be more specific, with\n~84K parameters, our attention-based model achieves 1.02% false rejection rate\n(FRR) at 1.0 false alarm (FA) per hour.",
    "published": "2018-03-29T03:32:59Z",
    "updated": "2018-03-29T03:32:59Z",
    "authors": [
      "Changhao Shan",
      "Junbo Zhang",
      "Yujun Wang",
      "Lei Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.01410v2",
    "title": "Synthesizer Based Efficient Self-Attention for Vision Tasks",
    "summary": "Self-attention module shows outstanding competence in capturing long-range\nrelationships while enhancing performance on vision tasks, such as image\nclassification and image captioning. However, the self-attention module highly\nrelies on the dot product multiplication and dimension alignment among\nquery-key-value features, which cause two problems: (1) The dot product\nmultiplication results in exhaustive and redundant computation. (2) Due to the\nvisual feature map often appearing as a multi-dimensional tensor, reshaping the\nscale of the tensor feature to adapt to the dimension alignment might destroy\nthe internal structure of the tensor feature map. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants, namely,\nSynthesizing Tensor Transformations (STT), for directly processing image tensor\nfeatures. Without computing the dot-product multiplication among\nquery-key-value, the basic STT is composed of the tensor transformation to\nlearn the synthetic attention weight from visual information. The effectiveness\nof STT series is validated on the image classification and image caption.\nExperiments show that the proposed STT achieves competitive performance while\nkeeping robustness compared to self-attention in the aforementioned vision\ntasks.",
    "published": "2022-01-05T02:07:32Z",
    "updated": "2024-09-29T06:15:08Z",
    "authors": [
      "Guangyang Zhu",
      "Jianfeng Zhang",
      "Yuanzhi Feng",
      "Hai Lan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.12398v4",
    "title": "Multiscale Attention via Wavelet Neural Operators for Vision\n  Transformers",
    "summary": "Transformers have achieved widespread success in computer vision. At their\nheart, there is a Self-Attention (SA) mechanism, an inductive bias that\nassociates each token in the input with every other token through a weighted\nbasis. The standard SA mechanism has quadratic complexity with the sequence\nlength, which impedes its utility to long sequences appearing in high\nresolution vision. Recently, inspired by operator learning for PDEs, Adaptive\nFourier Neural Operators (AFNO) were introduced for high resolution attention\nbased on global convolution that is efficiently implemented via FFT. However,\nthe AFNO global filtering cannot well represent small and moderate scale\nstructures that commonly appear in natural images. To leverage the\ncoarse-to-fine scale structures we introduce a Multiscale Wavelet Attention\n(MWA) by leveraging wavelet neural operators which incurs linear complexity in\nthe sequence size. We replace the attention in ViT with MWA and our experiments\nwith CIFAR and Tiny-ImageNet classification demonstrate significant improvement\nover alternative Fourier-based attentions such as AFNO and Global Filter\nNetwork (GFN).",
    "published": "2023-03-22T09:06:07Z",
    "updated": "2023-08-15T09:05:54Z",
    "authors": [
      "Anahita Nekoozadeh",
      "Mohammad Reza Ahmadzadeh",
      "Zahra Mardani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.07448v1",
    "title": "Fast Convergence of DETR with Spatially Modulated Co-Attention",
    "summary": "The recently proposed Detection Transformer (DETR) model successfully applies\nTransformer to objects detection and achieves comparable performance with\ntwo-stage object detection frameworks, such as Faster-RCNN. However, DETR\nsuffers from its slow convergence. Training DETR \\cite{carion2020end} from\nscratch needs 500 epochs to achieve a high accuracy. To accelerate its\nconvergence, we propose a simple yet effective scheme for improving the DETR\nframework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core\nidea of SMCA is to conduct regression-aware co-attention in DETR by\nconstraining co-attention responses to be high near initially estimated\nbounding box locations. Our proposed SMCA increases DETR's convergence speed by\nreplacing the original co-attention mechanism in the decoder while keeping\nother operations in DETR unchanged. Furthermore, by integrating multi-head and\nscale-selection attention designs into SMCA, our fully-fledged SMCA can achieve\nbetter performance compared to DETR with a dilated convolution-based backbone\n(45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive\nablation studies on COCO dataset to validate the effectiveness of the proposed\nSMCA.",
    "published": "2021-01-19T03:52:44Z",
    "updated": "2021-01-19T03:52:44Z",
    "authors": [
      "Peng Gao",
      "Minghang Zheng",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.01275v2",
    "title": "Relaxed Attention: A Simple Method to Boost Performance of End-to-End\n  Automatic Speech Recognition",
    "summary": "Recently, attention-based encoder-decoder (AED) models have shown high\nperformance for end-to-end automatic speech recognition (ASR) across several\ntasks. Addressing overconfidence in such models, in this paper we introduce the\nconcept of relaxed attention, which is a simple gradual injection of a uniform\ndistribution to the encoder-decoder attention weights during training that is\neasily implemented with two lines of code. We investigate the effect of relaxed\nattention across different AED model architectures and two prominent ASR tasks,\nWall Street Journal (WSJ) and Librispeech. We found that transformers trained\nwith relaxed attention outperform the standard baseline models consistently\nduring decoding with external language models. On WSJ, we set a new benchmark\nfor transformer-based end-to-end speech recognition with a word error rate of\n3.65%, outperforming state of the art (4.20%) by 13.1% relative, while\nintroducing only a single hyperparameter.",
    "published": "2021-07-02T21:01:17Z",
    "updated": "2021-12-15T11:10:05Z",
    "authors": [
      "Timo Lohrenz",
      "Patrick Schwarz",
      "Zhengyang Li",
      "Tim Fingscheidt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.01377v1",
    "title": "A Dynamic Head Importance Computation Mechanism for Neural Machine\n  Translation",
    "summary": "Multiple parallel attention mechanisms that use multiple attention heads\nfacilitate greater performance of the Transformer model for various\napplications e.g., Neural Machine Translation (NMT), text classification. In\nmulti-head attention mechanism, different heads attend to different parts of\nthe input. However, the limitation is that multiple heads might attend to the\nsame part of the input, resulting in multiple heads being redundant. Thus, the\nmodel resources are under-utilized. One approach to avoid this is to prune\nleast important heads based on certain importance score. In this work, we focus\non designing a Dynamic Head Importance Computation Mechanism (DHICM) to\ndynamically calculate the importance of a head with respect to the input. Our\ninsight is to design an additional attention layer together with multi-head\nattention, and utilize the outputs of the multi-head attention along with the\ninput, to compute the importance for each head. Additionally, we add an extra\nloss function to prevent the model from assigning same score to all heads, to\nidentify more important heads and improvise performance. We analyzed\nperformance of DHICM for NMT with different languages. Experiments on different\ndatasets show that DHICM outperforms traditional Transformer-based approach by\nlarge margin, especially, when less training data is available.",
    "published": "2021-08-03T09:16:55Z",
    "updated": "2021-08-03T09:16:55Z",
    "authors": [
      "Akshay Goindani",
      "Manish Shrivastava"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.02404v1",
    "title": "Fast Convergence of DETR with Spatially Modulated Co-Attention",
    "summary": "The recently proposed Detection Transformer (DETR) model successfully applies\nTransformer to objects detection and achieves comparable performance with\ntwo-stage object detection frameworks, such as Faster-RCNN. However, DETR\nsuffers from its slow convergence. Training DETR from scratch needs 500 epochs\nto achieve a high accuracy. To accelerate its convergence, we propose a simple\nyet effective scheme for improving the DETR framework, namely Spatially\nModulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct\nlocation-aware co-attention in DETR by constraining co-attention responses to\nbe high near initially estimated bounding box locations. Our proposed SMCA\nincreases DETR's convergence speed by replacing the original co-attention\nmechanism in the decoder while keeping other operations in DETR unchanged.\nFurthermore, by integrating multi-head and scale-selection attention designs\ninto SMCA, our fully-fledged SMCA can achieve better performance compared to\nDETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3\nmAP at 500 epochs). We perform extensive ablation studies on COCO dataset to\nvalidate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .",
    "published": "2021-08-05T06:53:19Z",
    "updated": "2021-08-05T06:53:19Z",
    "authors": [
      "Peng Gao",
      "Minghang Zheng",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.15509v1",
    "title": "Scene-Adaptive Attention Network for Crowd Counting",
    "summary": "In recent years, significant progress has been made on the research of crowd\ncounting. However, as the challenging scale variations and complex scenes\nexisted in crowds, neither traditional convolution networks nor recent\nTransformer architectures with fixed-size attention could handle the task well.\nTo address this problem, this paper proposes a scene-adaptive attention\nnetwork, termed SAANet. First of all, we design a deformable attention in-built\nTransformer backbone, which learns adaptive feature representations with\ndeformable sampling locations and dynamic attention weights. Then we propose\nthe multi-level feature fusion and count-attentive feature enhancement modules\nfurther, to strengthen feature representation under the global image context.\nThe learned representations could attend to the foreground and are adaptive to\ndifferent scales of crowds. We conduct extensive experiments on four\nchallenging crowd counting benchmarks, demonstrating that our method achieves\nstate-of-the-art performance. Especially, our method currently ranks No.1 on\nthe public leaderboard of the NWPU-Crowd benchmark. We hope our method could be\na strong baseline to support future research in crowd counting. The source code\nwill be released to the community.",
    "published": "2021-12-31T15:03:17Z",
    "updated": "2021-12-31T15:03:17Z",
    "authors": [
      "Xing Wei",
      "Yuanrui Kang",
      "Jihao Yang",
      "Yunfeng Qiu",
      "Dahu Shi",
      "Wenming Tan",
      "Yihong Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.00287v1",
    "title": "DALG: Deep Attentive Local and Global Modeling for Image Retrieval",
    "summary": "Deeply learned representations have achieved superior image retrieval\nperformance in a retrieve-then-rerank manner. Recent state-of-the-art single\nstage model, which heuristically fuses local and global features, achieves\npromising trade-off between efficiency and effectiveness. However, we notice\nthat efficiency of existing solutions is still restricted because of their\nmulti-scale inference paradigm. In this paper, we follow the single stage art\nand obtain further complexity-effectiveness balance by successfully getting rid\nof multi-scale testing. To achieve this goal, we abandon the widely-used\nconvolution network giving its limitation in exploring diverse visual patterns,\nand resort to fully attention based framework for robust representation\nlearning motivated by the success of Transformer. Besides applying Transformer\nfor global feature extraction, we devise a local branch composed of\nwindow-based multi-head attention and spatial attention to fully exploit local\nimage patterns. Furthermore, we propose to combine the hierarchical local and\nglobal features via a cross-attention module, instead of using heuristically\nfusion as previous art does. With our Deep Attentive Local and Global modeling\nframework (DALG), extensive experimental results show that efficiency can be\nsignificantly improved while maintaining competitive results with the state of\nthe arts.",
    "published": "2022-07-01T09:32:15Z",
    "updated": "2022-07-01T09:32:15Z",
    "authors": [
      "Yuxin Song",
      "Ruolin Zhu",
      "Min Yang",
      "Dongliang He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.07164v1",
    "title": "Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text\n  Generation via Concentrating Attention",
    "summary": "Recently, powerful Transformer architectures have proven superior in\ngenerating high-quality sentences. Nevertheless, these models tend to produce\ndull high-frequency phrases, severely hurting the diversity and novelty of\ngenerated text. In this work, we dig into the intrinsic mechanism of this\nproblem and found that sparser attention values in Transformer could improve\ndiversity. To understand such a phenomenon, we first conduct both empirical and\ntheoretical analysis and then attribute it to representation degeneration\ncaused by the attentive mixture of the hidden states during training. We term\nthis process the Trap of Mediocrity. To escape from such a trap, we introduce a\nnovel attention regularization loss to control the sharpness of the attention\ndistribution, which is transparent to model structures and can be easily\nimplemented within 20 lines of python code. We prove that this method could be\nmathematically regarded as learning a Bayesian approximation of posterior\nattention. Experiments show that our method improved the diversity and novelty\nof the generated text while maintaining comparable quality on a variety of\nconditional and unconditional generation tasks.",
    "published": "2022-11-14T07:53:16Z",
    "updated": "2022-11-14T07:53:16Z",
    "authors": [
      "Wenhao Li",
      "Xiaoyuan Yi",
      "Jinyi Hu",
      "Maosong Sun",
      "Xing Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.14278v2",
    "title": "Multi-Layer Attention-Based Explainability via Transformers for Tabular\n  Data",
    "summary": "We propose a graph-oriented attention-based explainability method for tabular\ndata. Tasks involving tabular data have been solved mostly using traditional\ntree-based machine learning models which have the challenges of feature\nselection and engineering. With that in mind, we consider a transformer\narchitecture for tabular data, which is amenable to explainability, and present\na novel way to leverage self-attention mechanism to provide explanations by\ntaking into account the attention matrices of all heads and layers as a whole.\nThe matrices are mapped to a graph structure where groups of features\ncorrespond to nodes and attention values to arcs. By finding the maximum\nprobability paths in the graph, we identify groups of features providing larger\ncontributions to explain the model's predictions. To assess the quality of\nmulti-layer attention-based explanations, we compare them with popular\nattention-, gradient-, and perturbation-based explanability methods.",
    "published": "2023-02-28T03:28:18Z",
    "updated": "2024-06-04T03:59:23Z",
    "authors": [
      "Andrea TreviÃ±o Gavito",
      "Diego Klabjan",
      "Jean Utke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.03175v1",
    "title": "Infusing Lattice Symmetry Priors in Attention Mechanisms for\n  Sample-Efficient Abstract Geometric Reasoning",
    "summary": "The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most\nrecent language-complete instantiation (LARC) has been postulated as an\nimportant step towards general AI. Yet, even state-of-the-art machine learning\nmodels struggle to achieve meaningful performance on these problems, falling\nbehind non-learning based approaches. We argue that solving these tasks\nrequires extreme generalization that can only be achieved by proper accounting\nfor core knowledge priors. As a step towards this goal, we focus on geometry\npriors and introduce LatFormer, a model that incorporates lattice symmetry\npriors in attention masks. We show that, for any transformation of the\nhypercubic lattice, there exists a binary attention mask that implements that\ngroup action. Hence, our study motivates a modification to the standard\nattention mechanism, where attention weights are scaled using soft masks\ngenerated by a convolutional network. Experiments on synthetic geometric\nreasoning show that LatFormer requires 2 orders of magnitude fewer data than\nstandard attention and transformers. Moreover, our results on ARC and LARC\ntasks that incorporate geometric priors provide preliminary evidence that these\ncomplex datasets do not lie out of the reach of deep learning models.",
    "published": "2023-06-05T18:32:53Z",
    "updated": "2023-06-05T18:32:53Z",
    "authors": [
      "Mattia Atzeni",
      "Mrinmaya Sachan",
      "Andreas Loukas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.12821v1",
    "title": "CST-former: Transformer with Channel-Spectro-Temporal Attention for\n  Sound Event Localization and Detection",
    "summary": "Sound event localization and detection (SELD) is a task for the\nclassification of sound events and the localization of direction of arrival\n(DoA) utilizing multichannel acoustic signals. Prior studies employ spectral\nand channel information as the embedding for temporal attention. However, this\nusage limits the deep neural network from extracting meaningful features from\nthe spectral or spatial domains. Therefore, our investigation in this paper\npresents a novel framework termed the Channel-Spectro-Temporal Transformer\n(CST-former) that bolsters SELD performance through the independent application\nof attention mechanisms to distinct domains. The CST-former architecture\nemploys distinct attention mechanisms to independently process channel,\nspectral, and temporal information. In addition, we propose an unfolded local\nembedding (ULE) technique for channel attention (CA) to generate informative\nembedding vectors including local spectral and temporal information. Empirical\nvalidation through experimentation on the 2022 and 2023 DCASE Challenge task3\ndatasets affirms the efficacy of employing attention mechanisms separated\nacross each domain and the benefit of ULE, in enhancing SELD performance.",
    "published": "2023-12-20T07:49:43Z",
    "updated": "2023-12-20T07:49:43Z",
    "authors": [
      "Yusun Shul",
      "Jung-Woo Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.08816v1",
    "title": "ToSA: Token Selective Attention for Efficient Vision Transformers",
    "summary": "In this paper, we propose a novel token selective attention approach, ToSA,\nwhich can identify tokens that need to be attended as well as those that can\nskip a transformer layer. More specifically, a token selector parses the\ncurrent attention maps and predicts the attention maps for the next layer,\nwhich are then used to select the important tokens that should participate in\nthe attention operation. The remaining tokens simply bypass the next layer and\nare concatenated with the attended ones to re-form a complete set of tokens. In\nthis way, we reduce the quadratic computation and memory costs as fewer tokens\nparticipate in self-attention while maintaining the features for all the image\npatches throughout the network, which allows it to be used for dense prediction\ntasks. Our experiments show that by applying ToSA, we can significantly reduce\ncomputation costs while maintaining accuracy on the ImageNet classification\nbenchmark. Furthermore, we evaluate on the dense prediction task of monocular\ndepth estimation on NYU Depth V2, and show that we can achieve similar depth\nprediction accuracy using a considerably lighter backbone with ToSA.",
    "published": "2024-06-13T05:17:21Z",
    "updated": "2024-06-13T05:17:21Z",
    "authors": [
      "Manish Kumar Singh",
      "Rajeev Yasarla",
      "Hong Cai",
      "Mingu Lee",
      "Fatih Porikli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.05136v2",
    "title": "MHS-STMA: Multimodal Hate Speech Detection via Scalable\n  Transformer-Based Multilevel Attention Framework",
    "summary": "Social media has a significant impact on people's lives. Hate speech on\nsocial media has emerged as one of society's most serious issues in recent\nyears. Text and pictures are two forms of multimodal data that are distributed\nwithin articles. Unimodal analysis has been the primary emphasis of earlier\napproaches. Additionally, when doing multimodal analysis, researchers neglect\nto preserve the distinctive qualities associated with each modality. To address\nthese shortcomings, the present article suggests a scalable architecture for\nmultimodal hate content detection called transformer-based multilevel attention\n(STMA). This architecture consists of three main parts: a combined\nattention-based deep learning mechanism, a vision attention-mechanism encoder,\nand a caption attention-mechanism encoder. To identify hate content, each\ncomponent uses various attention processes and handles multimodal data in a\nunique way. Several studies employing multiple assessment criteria on three\nhate speech datasets such as Hateful memes, MultiOff, and MMHS150K, validate\nthe suggested architecture's efficacy. The outcomes demonstrate that on all\nthree datasets, the suggested strategy performs better than the baseline\napproaches.",
    "published": "2024-09-08T15:42:18Z",
    "updated": "2024-09-17T09:50:45Z",
    "authors": [
      "Anusha Chhabra",
      "Dinesh Kumar Vishwakarma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.16112v1",
    "title": "Self-attention as an attractor network: transient memories without\n  backpropagation",
    "summary": "Transformers are one of the most successful architectures of modern neural\nnetworks. At their core there is the so-called attention mechanism, which\nrecently interested the physics community as it can be written as the\nderivative of an energy function in certain cases: while it is possible to\nwrite the cross-attention layer as a modern Hopfield network, the same is not\npossible for the self-attention, which is used in the GPT architectures and\nother autoregressive models. In this work we show that it is possible to obtain\nthe self-attention layer as the derivative of local energy terms, which\nresemble a pseudo-likelihood. We leverage the analogy with pseudo-likelihood to\ndesign a recurrent model that can be trained without backpropagation: the\ndynamics shows transient states that are strongly correlated with both train\nand test examples. Overall we present a novel framework to interpret\nself-attention as an attractor network, potentially paving the way for new\ntheoretical approaches inspired from physics to understand transformers.",
    "published": "2024-09-24T14:19:56Z",
    "updated": "2024-09-24T14:19:56Z",
    "authors": [
      "Francesco D'Amico",
      "Matteo Negri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.05565v1",
    "title": "Chain and Causal Attention for Efficient Entity Tracking",
    "summary": "This paper investigates the limitations of transformers for entity-tracking\ntasks in large language models. We identify a theoretical constraint, showing\nthat transformers require at least $\\log_2 (n+1)$ layers to handle entity\ntracking with $n$ state changes. To address this issue, we propose an efficient\nand frugal enhancement to the standard attention mechanism, enabling it to\nmanage long-term dependencies more efficiently. By considering attention as an\nadjacency matrix, our model can track entity states with a single layer.\nEmpirical results demonstrate significant improvements in entity tracking\ndatasets while keeping competitive performance on standard natural language\nmodeling. Our modified attention allows us to achieve the same performance with\ndrastically fewer layers. Additionally, our enhanced mechanism reveals\nstructured internal representations of attention. Extensive experiments on both\ntoy and complex datasets validate our approach. Our contributions include\ntheoretical insights, an improved attention mechanism, and empirical\nvalidation.",
    "published": "2024-10-07T23:54:10Z",
    "updated": "2024-10-07T23:54:10Z",
    "authors": [
      "Erwan Fagnou",
      "Paul Caillon",
      "Blaise Delattre",
      "Alexandre Allauzen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.10741v1",
    "title": "MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map",
    "summary": "Various linear complexity models, such as Linear Transformer (LinFormer),\nState Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace\nthe conventional softmax attention in Transformer structures. However, the\noptimal design of these linear models is still an open question. In this work,\nwe attempt to answer this question by finding the best linear approximation to\nsoftmax attention from a theoretical perspective. We start by unifying existing\nlinear complexity models as the linear attention form and then identify three\nconditions for the optimal linear attention design: 1) Dynamic memory ability;\n2) Static approximation ability; 3) Least parameter approximation. We find that\nnone of the current linear models meet all three conditions, resulting in\nsuboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a\nsolution that satisfies these conditions. Our experiments on Multi-Query\nAssociative Recall (MQAR) task, language modeling, image classification, and\nLong-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than\nthe existing linear models.",
    "published": "2024-11-16T08:47:32Z",
    "updated": "2024-11-16T08:47:32Z",
    "authors": [
      "Yuhong Chou",
      "Man Yao",
      "Kexin Wang",
      "Yuqi Pan",
      "Ruijie Zhu",
      "Yiran Zhong",
      "Yu Qiao",
      "Jibin Wu",
      "Bo Xu",
      "Guoqi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.19399v1",
    "title": "Scalable-Softmax Is Superior for Attention",
    "summary": "The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.",
    "published": "2025-01-31T18:55:35Z",
    "updated": "2025-01-31T18:55:35Z",
    "authors": [
      "Ken M. Nakanishi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18809v2",
    "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
    "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
    "published": "2025-05-24T17:46:47Z",
    "updated": "2025-10-12T10:09:53Z",
    "authors": [
      "Wenhao Sun",
      "Rong-Cheng Tu",
      "Yifu Ding",
      "Zhao Jin",
      "Jingyi Liao",
      "Shunyu Liu",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24544v3",
    "title": "Cross-Attention Speculative Decoding",
    "summary": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding.",
    "published": "2025-05-30T12:52:35Z",
    "updated": "2025-09-22T01:23:11Z",
    "authors": [
      "Wei Zhong",
      "Manasa Bharadwaj",
      "Yixiao Wang",
      "Nikhil Verma",
      "Yipeng Ji",
      "Chul Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.00453v1",
    "title": "Recurrent Memory-Augmented Transformers with Chunked Attention for\n  Long-Context Language Modeling",
    "summary": "We present a Transformer architecture for long-context language modeling that\ncombines global attention with two biologically inspired components: chunked\nlocal attention and a gated FIFO memory mechanism. This unified attention block\nallows the model to efficiently handle both short-range and long-range\ndependencies without increasing attention cost quadratically. The memory module\npersistently stores past token representations using a gated update mechanism\ninspired by recurrent networks. Rotary positional encoding is applied per\nattention head to enable directionally disentangled, scale-invariant positional\nsignals. The architecture is implemented entirely from scratch in PyTorch, with\nno reliance on high-level libraries, enabling transparent and modular\nexperimentation. Our model offers a lightweight and extensible design for tasks\nsuch as dialogue modeling, code completion, and document understanding.",
    "published": "2025-07-01T06:11:38Z",
    "updated": "2025-07-01T06:11:38Z",
    "authors": [
      "Ankit Kashyap"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.19175v1",
    "title": "Patch Pruning Strategy Based on Robust Statistical Measures of Attention\n  Weight Diversity in Vision Transformers",
    "summary": "Multi-head self-attention is a distinctive feature extraction mechanism of\nvision transformers that computes pairwise relationships among all input\npatches, contributing significantly to their high performance. However, it is\nknown to incur a quadratic computational complexity with respect to the number\nof patches. One promising approach to address this issue is patch pruning,\nwhich improves computational efficiency by identifying and removing redundant\npatches. In this work, we propose a patch pruning strategy that evaluates the\nimportance of each patch based on the variance of attention weights across\nmultiple attention heads. This approach is inspired by the design of multi-head\nself-attention, which aims to capture diverse attention patterns across\ndifferent subspaces of feature representations. The proposed method can be\neasily applied during both training and inference, and achieves improved\nthroughput while maintaining classification accuracy in scenarios such as\nfine-tuning with pre-trained models. In addition, we also found that using\nrobust statistical measures, such as the median absolute deviation in place of\nvariance, to assess patch importance can similarly lead to strong performance.\nFurthermore, by introducing overlapping patch embeddings, our method achieves\nbetter performance with comparable throughput to conventional approaches that\nutilize all patches.",
    "published": "2025-07-25T11:31:17Z",
    "updated": "2025-07-25T11:31:17Z",
    "authors": [
      "Yuki Igaue",
      "Hiroaki Aizawa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.19409v1",
    "title": "Modality Agnostic Efficient Long Range Encoder",
    "summary": "The long-context capability of recent large transformer models can be\nsurmised to rely on techniques such as attention/model parallelism, as well as\nhardware-level optimizations. While these strategies allow input lengths to\nscale to millions of tokens, they do not fundamentally mitigate the quadratic\ncomputational and memory complexity of the core attention mechanism. In this\npaper, we address the challenge of long-context processing on a single device\nusing generic implementations by reducing the quadratic memory footprint and\ninference cost. Existing approaches to extend the context length for generic\nsingle device implementations -- such as token merging and modified attentions\n-- are often modality specific and attain a suboptimal tradeoff between\naccuracy and efficiency. To overcome these limitations, we propose MAELRE\n(Modality Agnostic Efficient Long Range Encoder), a unified and efficient\ntransformer architecture designed for long-range encoding across diverse\nmodalities. MAELRE integrates token merging with attention approximation,\nprogressively merging tokens at different stages of internal computational\nblocks. It employs a lightweight attention approximation when the number of\ntokens is large, and switches to standard dot-product attention as the sequence\nbecomes shorter through successive aggregation. We demonstrate that MAELRE\nachieves superior accuracy while reducing computational cost compared to\nexisting long-context models on classification tasks spanning multiple\nmodalities, including text, time series, audio, and vision.",
    "published": "2025-07-25T16:19:47Z",
    "updated": "2025-07-25T16:19:47Z",
    "authors": [
      "Toufiq Parag",
      "Ahmed Elgammal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20453v3",
    "title": "Your Attention Matters: to Improve Model Robustness to Noise and\n  Spurious Correlations",
    "summary": "Self-attention mechanisms are foundational to Transformer architectures,\nsupporting their impressive success in a wide range of tasks. While there are\nmany self-attention variants, their robustness to noise and spurious\ncorrelations has not been well studied. This study evaluates Softmax, Sigmoid,\nLinear, Doubly Stochastic, and Cosine attention within Vision Transformers\nunder different data corruption scenarios. Through testing across the CIFAR-10,\nCIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is\nthe most robust. It consistently outperformed the next best mechanism by\n$0.1\\%-5.1\\%$ when training data, or both training and testing data, were\ncorrupted. Our findings inform self-attention selection in contexts with\nimperfect data. The code used is available at\nhttps://github.com/ctamayor/NeurIPS-Robustness-ViT.",
    "published": "2025-07-28T01:07:22Z",
    "updated": "2025-09-06T03:33:17Z",
    "authors": [
      "Camilo Tamayo-Rousseau",
      "Yunjia Zhao",
      "Yiqun Zhang",
      "Randall Balestriero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.00367v1",
    "title": "Representation Shift: Unifying Token Compression with FlashAttention",
    "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.",
    "published": "2025-08-01T06:53:55Z",
    "updated": "2025-08-01T06:53:55Z",
    "authors": [
      "Joonmyung Choi",
      "Sanghyeok Lee",
      "Byungoh Ko",
      "Eunseo Kim",
      "Jihyung Kil",
      "Hyunwoo J. Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16929v2",
    "title": "Attention Layers Add Into Low-Dimensional Residual Subspaces",
    "summary": "Transformer architectures, and their attention mechanisms in particular, form\nthe foundation of modern large language models. While transformer models are\nwidely believed to operate in high-dimensional hidden spaces, we show that\nattention outputs are confined to a surprisingly low-dimensional subspace,\nwhere about 60\\% of the directions account for 99\\% of the variance--a\nphenomenon that is consistently observed across diverse model families and\ndatasets, and is induced by the attention output projection matrix. Critically,\nwe find this low-rank structure as a key factor of the prevalent dead feature\nproblem in sparse dictionary learning, where it creates a mismatch between\nrandomly initialized features and the intrinsic geometry of the activation\nspace. Building on this insight, we propose a subspace-constrained training\nmethod for sparse autoencoders (SAEs), initializing feature directions into the\nactive subspace of activations. Our approach reduces dead features from 87\\% to\nbelow 1\\% in Attention Output SAEs with 1M features, and can further extend to\nother sparse dictionary learning methods. Our findings provide both new\ninsights into the geometry of attention and practical tools for improving\nsparse dictionary learning in large language models.",
    "published": "2025-08-23T07:27:00Z",
    "updated": "2025-09-28T11:00:01Z",
    "authors": [
      "Junxuan Wang",
      "Xuyang Ge",
      "Wentao Shu",
      "Zhengfu He",
      "Xipeng Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26207v1",
    "title": "The silence of the weights: an investigation of structural pruning\n  strategies for attention-based audio signal architectures",
    "summary": "Transformer-based models have become the state of the art across multiple\ndomains, from natural language processing to machine listening, thanks to\nattention mechanisms. However, the attention layers require a large number of\nparameters and high-end hardware for both training and inference. We propose a\nnovel pruning technique targeted explicitly at the attention mechanism, where\nwe decouple the pruning of the four layers in the attention block, namely:\nquery, keys, values and outputs' projection matrices. We also investigate\npruning strategies to prune along the head and channel dimensions, and compare\nthe performance of the Audio Spectrogram Transformer (AST) model under\ndifferent pruning scenarios. Our results show that even by pruning 50\\% of the\nattention parameters we incur in performance degradation of less than 1\\%",
    "published": "2025-09-30T13:10:19Z",
    "updated": "2025-09-30T13:10:19Z",
    "authors": [
      "Andrea Diecidue",
      "Carlo Alberto Barbano",
      "Piero Fraternali",
      "Mathieu Fontaine",
      "Enzo Tartaglione"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00365v2",
    "title": "Continual Learning with Query-Only Attention",
    "summary": "Continual learning involves learning from a stream of data without repetition\nof data points, a scenario that is inherently complex due to distributional\nshift across tasks. We propose a query-only attention mechanism that discards\nkeys and values, yet preserves the core inductive bias of transformer\narchitectures. In continual learning scenarios, this simplified mechanism\nsignificantly mitigates both loss of plasticity and catastrophic forgetting,\noutperforming baselines such as selective re-initialization. We establish a\nconceptual link between query-only attention, full transformer attention, and\nmodel agnostic meta-learning, framing them as instances of meta-learning. We\nfurther provide intuition for why query-based models and attention networks\nhelp preserve plasticity in continual settings. Finally, through preliminary\nHessian spectrum analysis, we observe that models maintaining higher curvature\nrank across tasks tend to retain plasticity. Our findings suggest that full\nattention may not be essential for capturing the benefits of meta-learning in\ncontinual learning.",
    "published": "2025-10-01T00:14:34Z",
    "updated": "2025-11-01T03:58:05Z",
    "authors": [
      "Gautham Bekal",
      "Ashish Pujari",
      "Scott David Kelly"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2001.02674v5",
    "title": "Streaming automatic speech recognition with the transformer model",
    "summary": "Encoder-decoder based sequence-to-sequence models have demonstrated\nstate-of-the-art results in end-to-end automatic speech recognition (ASR).\nRecently, the transformer architecture, which uses self-attention to model\ntemporal context information, has been shown to achieve significantly lower\nword error rates (WERs) compared to recurrent neural network (RNN) based system\narchitectures. Despite its success, the practical usage is limited to offline\nASR tasks, since encoder-decoder architectures typically require an entire\nspeech utterance as input. In this work, we propose a transformer based\nend-to-end ASR system for streaming ASR, where an output must be generated\nshortly after each spoken word. To achieve this, we apply time-restricted\nself-attention for the encoder and triggered attention for the encoder-decoder\nattention mechanism. Our proposed streaming transformer architecture achieves\n2.8% and 7.2% WER for the \"clean\" and \"other\" test data of LibriSpeech, which\nto our knowledge is the best published streaming end-to-end ASR result for this\ntask.",
    "published": "2020-01-08T18:58:02Z",
    "updated": "2020-06-30T18:29:07Z",
    "authors": [
      "Niko Moritz",
      "Takaaki Hori",
      "Jonathan Le Roux"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.00697v1",
    "title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question\n  Answering",
    "summary": "Transformer-based QA models use input-wide self-attention -- i.e. across both\nthe question and the input passage -- at all layers, causing them to be slow\nand memory-intensive. It turns out that we can get by without input-wide\nself-attention at all layers, especially in the lower layers. We introduce\nDeFormer, a decomposed transformer, which substitutes the full self-attention\nwith question-wide and passage-wide self-attentions in the lower layers. This\nallows for question-independent processing of the input text representations,\nwhich in turn enables pre-computing passage representations reducing runtime\ncompute drastically. Furthermore, because DeFormer is largely similar to the\noriginal model, we can initialize DeFormer with the pre-training weights of a\nstandard transformer, and directly fine-tune on the target QA dataset. We show\nDeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and\nwith simple distillation-based losses they incur only a 1% drop in accuracy. We\nopen source the code at https://github.com/StonyBrookNLP/deformer.",
    "published": "2020-05-02T04:28:22Z",
    "updated": "2020-05-02T04:28:22Z",
    "authors": [
      "Qingqing Cao",
      "Harsh Trivedi",
      "Aruna Balasubramanian",
      "Niranjan Balasubramanian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.03957v1",
    "title": "Points to Patches: Enabling the Use of Self-Attention for 3D Shape\n  Recognition",
    "summary": "While the Transformer architecture has become ubiquitous in the machine\nlearning field, its adaptation to 3D shape recognition is non-trivial. Due to\nits quadratic computational complexity, the self-attention operator quickly\nbecomes inefficient as the set of input points grows larger. Furthermore, we\nfind that the attention mechanism struggles to find useful connections between\nindividual points on a global scale. In order to alleviate these problems, we\npropose a two-stage Point Transformer-in-Transformer (Point-TnT) approach which\ncombines local and global attention mechanisms, enabling both individual points\nand patches of points to attend to each other effectively. Experiments on shape\nclassification show that such an approach provides more useful features for\ndownstream tasks than the baseline Transformer, while also being more\ncomputationally efficient. In addition, we also extend our method to feature\nmatching for scene reconstruction, showing that it can be used in conjunction\nwith existing scene reconstruction pipelines.",
    "published": "2022-04-08T09:31:24Z",
    "updated": "2022-04-08T09:31:24Z",
    "authors": [
      "Axel Berg",
      "Magnus Oskarsson",
      "Mark O'Connor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.02030v1",
    "title": "Point Cloud Recognition with Position-to-Structure Attention\n  Transformers",
    "summary": "In this paper, we present Position-to-Structure Attention Transformers\n(PS-Former), a Transformer-based algorithm for 3D point cloud recognition.\nPS-Former deals with the challenge in 3D point cloud representation where\npoints are not positioned in a fixed grid structure and have limited feature\ndescription (only 3D coordinates ($x, y, z$) for scattered points). Existing\nTransformer-based architectures in this domain often require a pre-specified\nfeature engineering step to extract point features. Here, we introduce two new\naspects in PS-Former: 1) a learnable condensation layer that performs point\ndownsampling and feature extraction; and 2) a Position-to-Structure Attention\nmechanism that recursively enriches the structural information with the\nposition attention branch. Compared with the competing methods, while being\ngeneric with less heuristics feature designs, PS-Former demonstrates\ncompetitive experimental results on three 3D point cloud tasks including\nclassification, part segmentation, and scene segmentation.",
    "published": "2022-10-05T05:40:33Z",
    "updated": "2022-10-05T05:40:33Z",
    "authors": [
      "Zheng Ding",
      "James Hou",
      "Zhuowen Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.07503v1",
    "title": "STAR-Transformer: A Spatio-temporal Cross Attention Transformer for\n  Human Action Recognition",
    "summary": "In action recognition, although the combination of spatio-temporal videos and\nskeleton features can improve the recognition performance, a separate model and\nbalancing feature representation for cross-modal data are required. To solve\nthese problems, we propose Spatio-TemporAl cRoss (STAR)-transformer, which can\neffectively represent two cross-modal features as a recognizable vector. First,\nfrom the input video and skeleton sequence, video frames are output as global\ngrid tokens and skeletons are output as joint map tokens, respectively. These\ntokens are then aggregated into multi-class tokens and input into\nSTAR-transformer. The STAR-transformer encoder layer consists of a full\nself-attention (FAttn) module and a proposed zigzag spatio-temporal attention\n(ZAttn) module. Similarly, the continuous decoder consists of a FAttn module\nand a proposed binary spatio-temporal attention (BAttn) module.\nSTAR-transformer learns an efficient multi-feature representation of the\nspatio-temporal features by properly arranging pairings of the FAttn, ZAttn,\nand BAttn modules. Experimental results on the Penn-Action, NTU RGB+D 60, and\n120 datasets show that the proposed method achieves a promising improvement in\nperformance in comparison to previous state-of-the-art methods.",
    "published": "2022-10-14T04:09:44Z",
    "updated": "2022-10-14T04:09:44Z",
    "authors": [
      "Dasom Ahn",
      "Sangwon Kim",
      "Hyunsu Hong",
      "Byoung Chul Ko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.14337v2",
    "title": "Deformable Graph Transformer",
    "summary": "Transformer-based models have recently shown success in representation\nlearning on graph-structured data beyond natural language processing and\ncomputer vision. However, the success is limited to small-scale graphs due to\nthe drawbacks of full dot-product attention on graphs such as the quadratic\ncomplexity with respect to the number of nodes and message aggregation from\nenormous irrelevant nodes. To address these issues, we propose Deformable Graph\nTransformer (DGT) that performs sparse attention via dynamically sampled\nrelevant nodes for efficiently handling large-scale graphs with a linear\ncomplexity in the number of nodes. Specifically, our framework first constructs\nmultiple node sequences with various criteria to consider both structural and\nsemantic proximity. Then, combining with our learnable Katz Positional\nEncodings, the sparse attention is applied to the node sequences for learning\nnode representations with a significantly reduced computational cost. Extensive\nexperiments demonstrate that our DGT achieves state-of-the-art performance on 7\ngraph benchmark datasets with 2.5 - 449 times less computational cost compared\nto transformer-based graph models with full attention.",
    "published": "2022-06-29T00:23:25Z",
    "updated": "2022-10-04T03:13:03Z",
    "authors": [
      "Jinyoung Park",
      "Seongjun Yun",
      "Hyeonjin Park",
      "Jaewoo Kang",
      "Jisu Jeong",
      "Kyung-Min Kim",
      "Jung-woo Ha",
      "Hyunwoo J. Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.03369v2",
    "title": "A Spatially Separable Attention Mechanism for massive MIMO CSI Feedback",
    "summary": "Channel State Information (CSI) Feedback plays a crucial role in achieving\nhigher gains through beamforming. However, for a massive MIMO system, this\nfeedback overhead is huge and grows linearly with the number of antennas. To\nreduce the feedback overhead several compressive sensing (CS) techniques were\nimplemented in recent years but these techniques are often iterative and are\ncomputationally complex to realize in power-constrained user equipment (UE).\nHence, a data-based deep learning approach took over in these recent years\nintroducing a variety of neural networks for CSI compression. Specifically,\ntransformer-based networks have been shown to achieve state-of-the-art\nperformance. However, the multi-head attention operation, which is at the core\nof transformers, is computationally complex making transformers difficult to\nimplement on a UE. In this work, we present a lightweight transformer named\nSTNet which uses a spatially separable attention mechanism that is\nsignificantly less complex than the traditional full-attention. Equipped with\nthis, STNet outperformed state-of-the-art models in some scenarios with\napproximately $1/10^{th}$ of the resources.",
    "published": "2022-08-05T19:40:24Z",
    "updated": "2022-10-12T11:20:45Z",
    "authors": [
      "Sharan Mourya",
      "SaiDhiraj Amuru",
      "Kiran Kumar Kuchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.01192v1",
    "title": "Improving Transformer-based End-to-End Speaker Diarization by Assigning\n  Auxiliary Losses to Attention Heads",
    "summary": "Transformer-based end-to-end neural speaker diarization (EEND) models utilize\nthe multi-head self-attention (SA) mechanism to enable accurate speaker label\nprediction in overlapped speech regions. In this study, to enhance the training\neffectiveness of SA-EEND models, we propose the use of auxiliary losses for the\nSA heads of the transformer layers. Specifically, we assume that the attention\nweight matrices of an SA layer are redundant if their patterns are similar to\nthose of the identity matrix. We then explicitly constrain such matrices to\nexhibit specific speaker activity patterns relevant to voice activity detection\nor overlapped speech detection tasks. Consequently, we expect the proposed\nauxiliary losses to guide the transformer layers to exhibit more diverse\npatterns in the attention weights, thereby reducing the assumed redundancies in\nthe SA heads. The effectiveness of the proposed method is demonstrated using\nthe simulated and CALLHOME datasets for two-speaker diarization tasks, reducing\nthe diarization error rate of the conventional SA-EEND model by 32.58% and\n17.11%, respectively.",
    "published": "2023-03-02T12:15:36Z",
    "updated": "2023-03-02T12:15:36Z",
    "authors": [
      "Ye-Rin Jeoung",
      "Joon-Young Yang",
      "Jeong-Hwan Choi",
      "Joon-Hyuk Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.02648v1",
    "title": "Comparative study of Transformer and LSTM Network with attention\n  mechanism on Image Captioning",
    "summary": "In a globalized world at the present epoch of generative intelligence, most\nof the manual labour tasks are automated with increased efficiency. This can\nsupport businesses to save time and money. A crucial component of generative\nintelligence is the integration of vision and language. Consequently, image\ncaptioning become an intriguing area of research. There have been multiple\nattempts by the researchers to solve this problem with different deep learning\narchitectures, although the accuracy has increased, but the results are still\nnot up to standard. This study buckles down to the comparison of Transformer\nand LSTM with attention block model on MS-COCO dataset, which is a standard\ndataset for image captioning. For both the models we have used pretrained\nInception-V3 CNN encoder for feature extraction of the images. The Bilingual\nEvaluation Understudy score (BLEU) is used to checked the accuracy of caption\ngenerated by both models. Along with the transformer and LSTM with attention\nblock models,CLIP-diffusion model, M2-Transformer model and the X-Linear\nAttention model have been discussed with state of the art accuracy.",
    "published": "2023-03-05T11:45:53Z",
    "updated": "2023-03-05T11:45:53Z",
    "authors": [
      "Pranav Dandwate",
      "Chaitanya Shahane",
      "Vandana Jagtap",
      "Shridevi C. Karande"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.09514v4",
    "title": "MATIS: Masked-Attention Transformers for Surgical Instrument\n  Segmentation",
    "summary": "We propose Masked-Attention Transformers for Surgical Instrument Segmentation\n(MATIS), a two-stage, fully transformer-based method that leverages modern\npixel-wise attention mechanisms for instrument segmentation. MATIS exploits the\ninstance-level nature of the task by employing a masked attention module that\ngenerates and classifies a set of fine instrument region proposals. Our method\nincorporates long-term video-level information through video transformers to\nimprove temporal consistency and enhance mask classification. We validate our\napproach in the two standard public benchmarks, Endovis 2017 and Endovis 2018.\nOur experiments demonstrate that MATIS' per-frame baseline outperforms previous\nstate-of-the-art methods and that including our temporal consistency module\nboosts our model's performance further.",
    "published": "2023-03-16T17:31:40Z",
    "updated": "2024-01-26T04:47:20Z",
    "authors": [
      "NicolÃ¡s Ayobi",
      "Alejandra PÃ©rez-RondÃ³n",
      "Santiago RodrÃ­guez",
      "Pablo ArbelÃ¡ez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.16892v1",
    "title": "Multi-scale Hierarchical Vision Transformer with Cascaded Attention\n  Decoding for Medical Image Segmentation",
    "summary": "Transformers have shown great success in medical image segmentation. However,\ntransformers may exhibit a limited generalization ability due to the underlying\nsingle-scale self-attention (SA) mechanism. In this paper, we address this\nissue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT)\nbackbone network, which improves the generalizability of the model by computing\nSA at multiple scales. We also incorporate an attention-based decoder, namely\nCascaded Attention Decoding (CASCADE), for further refinement of multi-stage\nfeatures generated by MERIT. Finally, we introduce an effective multi-stage\nfeature mixing loss aggregation (MUTATION) method for better model training via\nimplicit ensembling. Our experiments on two widely used medical image\nsegmentation benchmarks (i.e., Synapse Multi-organ, ACDC) demonstrate the\nsuperior performance of MERIT over state-of-the-art methods. Our MERIT\narchitecture and MUTATION loss aggregation can be used with downstream medical\nimage and semantic segmentation tasks.",
    "published": "2023-03-29T17:58:40Z",
    "updated": "2023-03-29T17:58:40Z",
    "authors": [
      "Md Mostafijur Rahman",
      "Radu Marculescu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.18177v2",
    "title": "STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action\n  Recognition",
    "summary": "We study the problem of human action recognition using motion capture (MoCap)\nsequences. Unlike existing techniques that take multiple manual steps to derive\nstandardized skeleton representations as model input, we propose a novel\nSpatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences.\nThe model uses a hierarchical transformer with intra-frame off-set attention\nand inter-frame self-attention. The attention mechanism allows the model to\nfreely attend between any two vertex patches to learn non-local relationships\nin the spatial-temporal domain. Masked vertex modeling and future frame\nprediction are used as two self-supervised tasks to fully activate the\nbi-directional and auto-regressive attention in our hierarchical transformer.\nThe proposed method achieves state-of-the-art performance compared to\nskeleton-based and point-cloud-based models on common MoCap benchmarks. Code is\navailable at https://github.com/zgzxy001/STMT.",
    "published": "2023-03-31T16:19:27Z",
    "updated": "2024-07-26T19:13:29Z",
    "authors": [
      "Xiaoyu Zhu",
      "Po-Yao Huang",
      "Junwei Liang",
      "Celso M. de Melo",
      "Alexander Hauptmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1903.08953v1",
    "title": "Learning Multi-Level Information for Dialogue Response Selection by\n  Highway Recurrent Transformer",
    "summary": "With the increasing research interest in dialogue response generation, there\nis an emerging branch formulating this task as selecting next sentences, where\ngiven the partial dialogue contexts, the goal is to determine the most probable\nnext sentence. Following the recent success of the Transformer model, this\npaper proposes (1) a new variant of attention mechanism based on multi-head\nattention, called highway attention, and (2) a recurrent model based on\ntransformer and the proposed highway attention, so-called Highway Recurrent\nTransformer. Experiments on the response selection task in the seventh Dialog\nSystem Technology Challenge (DSTC7) show the capability of the proposed model\nof modeling both utterance-level and dialogue-level information; the\neffectiveness of each module is further analyzed as well.",
    "published": "2019-03-21T12:39:02Z",
    "updated": "2019-03-21T12:39:02Z",
    "authors": [
      "Ting-Rui Chiang",
      "Chao-Wei Huang",
      "Shang-Yu Su",
      "Yun-Nung Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1903.10144v1",
    "title": "Predicting Multiple Demographic Attributes with Task Specific Embedding\n  Transformation and Attention Network",
    "summary": "Most companies utilize demographic information to develop their strategy in a\nmarket. However, such information is not available to most retail companies.\nSeveral studies have been conducted to predict the demographic attributes of\nusers from their transaction histories, but they have some limitations. First,\nthey focused on parameter sharing to predict all attributes but capturing\ntask-specific features is also important in multi-task learning. Second, they\nassumed that all transactions are equally important in predicting demographic\nattributes. However, some transactions are more useful than others for\npredicting a certain attribute. Furthermore, decision making process of models\ncannot be interpreted as they work in a black-box manner. To address the\nlimitations, we propose an Embedding Transformation Network with Attention\n(ETNA) model which shares representations at the bottom of the model structure\nand transforms them to task-specific representations using a simple linear\ntransformation method. In addition, we can obtain more informative transactions\nfor predicting certain attributes using the attention mechanism. The\nexperimental results show that our model outperforms the previous models on all\ntasks. In our qualitative analysis, we show the visualization of attention\nweights, which provides business managers with some useful insights.",
    "published": "2019-03-25T06:25:47Z",
    "updated": "2019-03-25T06:25:47Z",
    "authors": [
      "Raehyun Kim",
      "Hyunjae Kim",
      "Janghyuk Lee",
      "Jaewoo Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.00696v2",
    "title": "Deep Image Spatial Transformation for Person Image Generation",
    "summary": "Pose-guided person image generation is to transform a source person image to\na target pose. This task requires spatial manipulations of source data.\nHowever, Convolutional Neural Networks are limited by the lack of ability to\nspatially transform the inputs. In this paper, we propose a differentiable\nglobal-flow local-attention framework to reassemble the inputs at the feature\nlevel. Specifically, our model first calculates the global correlations between\nsources and targets to predict flow fields. Then, the flowed local patch pairs\nare extracted from the feature maps to calculate the local attention\ncoefficients. Finally, we warp the source features using a content-aware\nsampling method with the obtained local attention coefficients. The results of\nboth subjective and objective experiments demonstrate the superiority of our\nmodel. Besides, additional results in video animation and view synthesis show\nthat our model is applicable to other tasks requiring spatial transformation.\nOur source code is available at\nhttps://github.com/RenYurui/Global-Flow-Local-Attention.",
    "published": "2020-03-02T07:31:00Z",
    "updated": "2020-03-18T09:42:02Z",
    "authors": [
      "Yurui Ren",
      "Xiaoming Yu",
      "Junming Chen",
      "Thomas H. Li",
      "Ge Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.00769v3",
    "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
    "summary": "The Transformer architecture has been successful across many domains,\nincluding natural language processing, computer vision and speech recognition.\nIn keyword spotting, self-attention has primarily been used on top of\nconvolutional or recurrent encoders. We investigate a range of ways to adapt\nthe Transformer architecture to keyword spotting and introduce the Keyword\nTransformer (KWT), a fully self-attentional architecture that exceeds\nstate-of-the-art performance across multiple tasks without any pre-training or\nadditional data. Surprisingly, this simple architecture outperforms more\ncomplex models that mix convolutional, recurrent and attentive layers. KWT can\nbe used as a drop-in replacement for these models, setting two new benchmark\nrecords on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on\nthe 12 and 35-command tasks respectively.",
    "published": "2021-04-01T21:15:30Z",
    "updated": "2021-06-15T13:06:01Z",
    "authors": [
      "Axel Berg",
      "Mark O'Connor",
      "Miguel Tairum Cruz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.01569v2",
    "title": "Conversational Question Answering over Knowledge Graphs with Transformer\n  and Graph Attention Networks",
    "summary": "This paper addresses the task of (complex) conversational question answering\nover a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic\nparSing with trAnsformer and Graph atteNtion nEtworks). It is the first\napproach, which employs a transformer architecture extended with Graph\nAttention Networks for multi-task neural semantic parsing. LASAGNE uses a\ntransformer model for generating the base logical forms, while the Graph\nAttention model is used to exploit correlations between (entity) types and\npredicates to produce node representations. LASAGNE also includes a novel\nentity recognition module which detects, links, and ranks all relevant entities\nin the question context. We evaluate LASAGNE on a standard dataset for complex\nsequential question answering, on which it outperforms existing baseline\naverages on all question types. Specifically, we show that LASAGNE improves the\nF1-score on eight out of ten question types; in some cases, the increase in\nF1-score is more than 20% compared to the state of the art.",
    "published": "2021-04-04T09:21:50Z",
    "updated": "2021-06-24T11:41:05Z",
    "authors": [
      "Endri Kacupaj",
      "Joan Plepi",
      "Kuldeep Singh",
      "Harsh Thakkar",
      "Jens Lehmann",
      "Maria Maleshkova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.14921v1",
    "title": "3D Object Tracking with Transformer",
    "summary": "Feature fusion and similarity computation are two core problems in 3D object\ntracking, especially for object tracking using sparse and disordered point\nclouds. Feature fusion could make similarity computing more efficient by\nincluding target object information. However, most existing LiDAR-based\napproaches directly use the extracted point cloud feature to compute similarity\nwhile ignoring the attention changes of object regions during tracking. In this\npaper, we propose a feature fusion network based on transformer architecture.\nBenefiting from the self-attention mechanism, the transformer encoder captures\nthe inter- and intra- relations among different regions of the point cloud. By\nusing cross-attention, the transformer decoder fuses features and includes more\ntarget cues into the current point cloud feature to compute the region\nattentions, which makes the similarity computing more efficient. Based on this\nfeature fusion network, we propose an end-to-end point cloud object tracking\nframework, a simple yet effective method for 3D object tracking using point\nclouds. Comprehensive experimental results on the KITTI dataset show that our\nmethod achieves new state-of-the-art performance. Code is available at:\nhttps://github.com/3bobo/lttr.",
    "published": "2021-10-28T07:03:19Z",
    "updated": "2021-10-28T07:03:19Z",
    "authors": [
      "Yubo Cui",
      "Zheng Fang",
      "Jiayao Shan",
      "Zuoxu Gu",
      "Sifan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.15301v1",
    "title": "Can Transformer be Too Compositional? Analysing Idiom Processing in\n  Neural Machine Translation",
    "summary": "Unlike literal expressions, idioms' meanings do not directly follow from\ntheir parts, posing a challenge for neural machine translation (NMT). NMT\nmodels are often unable to translate idioms accurately and over-generate\ncompositional, literal translations. In this work, we investigate whether the\nnon-compositionality of idioms is reflected in the mechanics of the dominant\nNMT model, Transformer, by analysing the hidden states and attention patterns\nfor models with English as source language and one of seven European languages\nas target language. When Transformer emits a non-literal translation - i.e.\nidentifies the expression as idiomatic - the encoder processes idioms more\nstrongly as single lexical units compared to literal expressions. This\nmanifests in idioms' parts being grouped through attention and in reduced\ninteraction between idioms and their context. In the decoder's cross-attention,\nfigurative inputs result in reduced attention on source-side tokens. These\nresults suggest that Transformer's tendency to process idioms as compositional\nexpressions contributes to literal translations of idioms.",
    "published": "2022-05-30T17:59:32Z",
    "updated": "2022-05-30T17:59:32Z",
    "authors": [
      "Verna Dankers",
      "Christopher G. Lucas",
      "Ivan Titov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.06101v1",
    "title": "Global-local Motion Transformer for Unsupervised Skeleton-based Action\n  Learning",
    "summary": "We propose a new transformer model for the task of unsupervised learning of\nskeleton motion sequences. The existing transformer model utilized for\nunsupervised skeleton-based action learning is learned the instantaneous\nvelocity of each joint from adjacent frames without global motion information.\nThus, the model has difficulties in learning the attention globally over\nwhole-body motions and temporally distant joints. In addition, person-to-person\ninteractions have not been considered in the model. To tackle the learning of\nwhole-body motion, long-range temporal dynamics, and person-to-person\ninteractions, we design a global and local attention mechanism, where, global\nbody motions and local joint motions pay attention to each other. In addition,\nwe propose a novel pretraining strategy, multi-interval pose displacement\nprediction, to learn both global and local attention in diverse time ranges.\nThe proposed model successfully learns local dynamics of the joints and\ncaptures global context from the motion sequences. Our model outperforms\nstate-of-the-art models by notable margins in the representative benchmarks.\nCodes are available at https://github.com/Boeun-Kim/GL-Transformer.",
    "published": "2022-07-13T10:18:07Z",
    "updated": "2022-07-13T10:18:07Z",
    "authors": [
      "Boeun Kim",
      "Hyung Jin Chang",
      "Jungho Kim",
      "Jin Young Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.13354v1",
    "title": "Are Neighbors Enough? Multi-Head Neural n-gram can be Alternative to\n  Self-attention",
    "summary": "Impressive performance of Transformer has been attributed to self-attention,\nwhere dependencies between entire input in a sequence are considered at every\nposition. In this work, we reform the neural $n$-gram model, which focuses on\nonly several surrounding representations of each position, with the multi-head\nmechanism as in Vaswani et al.(2017). Through experiments on\nsequence-to-sequence tasks, we show that replacing self-attention in\nTransformer with multi-head neural $n$-gram can achieve comparable or better\nperformance than Transformer. From various analyses on our proposed method, we\nfind that multi-head neural $n$-gram is complementary to self-attention, and\ntheir combinations can further improve performance of vanilla Transformer.",
    "published": "2022-07-27T08:20:00Z",
    "updated": "2022-07-27T08:20:00Z",
    "authors": [
      "Mengsay Loem",
      "Sho Takase",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.08553v1",
    "title": "Hybrid Transformers for Music Source Separation",
    "summary": "A natural question arising in Music Source Separation (MSS) is whether long\nrange contextual information is useful, or whether local acoustic features are\nsufficient. In other fields, attention based Transformers have shown their\nability to integrate information over long sequences. In this work, we\nintroduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral\nbi-U-Net based on Hybrid Demucs, where the innermost layers are replaced by a\ncross-domain Transformer Encoder, using self-attention within one domain, and\ncross-attention across domains. While it performs poorly when trained only on\nMUSDB, we show that it outperforms Hybrid Demucs (trained on the same data) by\n0.45 dB of SDR when using 800 extra training songs. Using sparse attention\nkernels to extend its receptive field, and per source fine-tuning, we achieve\nstate-of-the-art results on MUSDB with extra training data, with 9.20 dB of\nSDR.",
    "published": "2022-11-15T22:48:16Z",
    "updated": "2022-11-15T22:48:16Z",
    "authors": [
      "Simon Rouard",
      "Francisco Massa",
      "Alexandre DÃ©fossez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.08136v1",
    "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer",
    "summary": "Transformer models have achieved superior performance in various natural\nlanguage processing tasks. However, the quadratic computational cost of the\nattention mechanism limits its practicality for long sequences. There are\nexisting attention variants that improve the computational efficiency, but they\nhave limited ability to effectively compute global information. In parallel to\nTransformer models, state space models (SSMs) are tailored for long sequences,\nbut they are not flexible enough to capture complicated local information. We\npropose SPADE, short for $\\underline{\\textbf{S}}$tate\ns$\\underline{\\textbf{P}}$ace\n$\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$\nTransform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the\nbottom layer of SPADE, and we employ efficient local attention methods for the\nother layers. The SSM augments global information, which complements the lack\nof long-range dependency issue in local attention methods. Experimental results\non the Long Range Arena benchmark and language modeling tasks demonstrate the\neffectiveness of the proposed method. To further demonstrate the scalability of\nSPADE, we pre-train large encoder-decoder models and present fine-tuning\nresults on natural language understanding and natural language generation\ntasks.",
    "published": "2022-12-15T20:51:27Z",
    "updated": "2022-12-15T20:51:27Z",
    "authors": [
      "Simiao Zuo",
      "Xiaodong Liu",
      "Jian Jiao",
      "Denis Charles",
      "Eren Manavoglu",
      "Tuo Zhao",
      "Jianfeng Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.17633v1",
    "title": "DPFormer: Learning Differentially Private Transformer on Long-Tailed\n  Data",
    "summary": "The Transformer has emerged as a versatile and effective architecture with\nbroad applications. However, it still remains an open problem how to\nefficiently train a Transformer model of high utility with differential privacy\nguarantees. In this paper, we identify two key challenges in learning\ndifferentially private Transformers, i.e., heavy computation overhead due to\nper-sample gradient clipping and unintentional attention distraction within the\nattention mechanism. In response, we propose DPFormer, equipped with Phantom\nClipping and Re-Attention Mechanism, to address these challenges. Our\ntheoretical analysis shows that DPFormer can reduce computational costs during\ngradient clipping and effectively mitigate attention distraction (which could\nobstruct the training process and lead to a significant performance drop,\nespecially in the presence of long-tailed data). Such analysis is further\ncorroborated by empirical results on two real-world datasets, demonstrating the\nefficiency and effectiveness of the proposed DPFormer.",
    "published": "2023-05-28T05:00:07Z",
    "updated": "2023-05-28T05:00:07Z",
    "authors": [
      "Youlong Ding",
      "Xueyang Wu",
      "Hao Wang",
      "Weike Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.00294v1",
    "title": "Affinity-based Attention in Self-supervised Transformers Predicts\n  Dynamics of Object Grouping in Humans",
    "summary": "The spreading of attention has been proposed as a mechanism for how humans\ngroup features to segment objects. However, such a mechanism has not yet been\nimplemented and tested in naturalistic images. Here, we leverage the feature\nmaps from self-supervised vision Transformers and propose a model of human\nobject-based attention spreading and segmentation. Attention spreads within an\nobject through the feature affinity signal between different patches of the\nimage. We also collected behavioral data on people grouping objects in natural\nimages by judging whether two dots are on the same object or on two different\nobjects. We found that our models of affinity spread that were built on feature\nmaps from the self-supervised Transformers showed significant improvement over\nbaseline and CNN based models on predicting reaction time patterns of humans,\ndespite not being trained on the task or with any other object labels. Our work\nprovides new benchmarks for evaluating models of visual representation learning\nincluding Transformers.",
    "published": "2023-06-01T02:25:55Z",
    "updated": "2023-06-01T02:25:55Z",
    "authors": [
      "Hossein Adeli",
      "Seoyoung Ahn",
      "Nikolaus Kriegeskorte",
      "Gregory Zelinsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.01775v2",
    "title": "Gated recurrent neural networks discover attention",
    "summary": "Recent architectural developments have enabled recurrent neural networks\n(RNNs) to reach and even surpass the performance of Transformers on certain\nsequence modeling tasks. These modern RNNs feature a prominent design pattern:\nlinear recurrent layers interconnected by feedforward paths with multiplicative\ngating. Here, we show how RNNs equipped with these two design elements can\nexactly implement (linear) self-attention, the main building block of\nTransformers. By reverse-engineering a set of trained RNNs, we find that\ngradient descent in practice discovers our construction. In particular, we\nexamine RNNs trained to solve simple in-context learning tasks on which\nTransformers are known to excel and find that gradient descent instills in our\nRNNs the same attention-based in-context learning algorithm used by\nTransformers. Our findings highlight the importance of multiplicative\ninteractions in neural networks and suggest that certain RNNs might be\nunexpectedly implementing attention under the hood.",
    "published": "2023-09-04T19:28:54Z",
    "updated": "2024-02-07T11:30:01Z",
    "authors": [
      "Nicolas Zucchet",
      "Seijin Kobayashi",
      "Yassir Akram",
      "Johannes von Oswald",
      "Maxime Larcher",
      "Angelika Steger",
      "JoÃ£o Sacramento"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.13220v2",
    "title": "Towards Understanding How Transformers Learn In-context Through a\n  Representation Learning Lens",
    "summary": "Pre-trained large language models based on Transformers have demonstrated\nremarkable in-context learning (ICL) abilities. With just a few demonstration\nexamples, the models can implement new tasks without any parameter updates.\nHowever, it is still an open question to understand the mechanism of ICL. In\nthis paper, we attempt to explore the ICL process in Transformers through a\nlens of representation learning. Initially, leveraging kernel methods, we\nfigure out a dual model for one softmax attention layer. The ICL inference\nprocess of the attention layer aligns with the training procedure of its dual\nmodel, generating token representation predictions that are equivalent to the\ndual model's test outputs. We delve into the training process of this dual\nmodel from a representation learning standpoint and further derive a\ngeneralization error bound related to the quantity of demonstration tokens.\nSubsequently, we extend our theoretical conclusions to more complicated\nscenarios, including one Transformer layer and multiple attention layers.\nFurthermore, drawing inspiration from existing representation learning methods\nespecially contrastive learning, we propose potential modifications for the\nattention layer. Finally, experiments are designed to support our findings.",
    "published": "2023-10-20T01:55:34Z",
    "updated": "2024-11-01T04:04:44Z",
    "authors": [
      "Ruifeng Ren",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.13824v1",
    "title": "Plausibility Processing in Transformer Language Models: Focusing on the\n  Role of Attention Heads in GPT",
    "summary": "The goal of this paper is to explore how Transformer language models process\nsemantic knowledge, especially regarding the plausibility of noun-verb\nrelations. First, I demonstrate GPT2 exhibits a higher degree of similarity\nwith humans in plausibility processing compared to other Transformer language\nmodels. Next, I delve into how knowledge of plausibility is contained within\nattention heads of GPT2 and how these heads causally contribute to GPT2's\nplausibility processing ability. Through several experiments, it was found\nthat: i) GPT2 has a number of attention heads that detect plausible noun-verb\nrelationships; ii) these heads collectively contribute to the Transformer's\nability to process plausibility, albeit to varying degrees; and iii) attention\nheads' individual performance in detecting plausibility does not necessarily\ncorrelate with how much they contribute to GPT2's plausibility processing\nability.",
    "published": "2023-10-20T21:31:19Z",
    "updated": "2023-10-20T21:31:19Z",
    "authors": [
      "Soo Hyun Ryu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.19089v1",
    "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language\n  Models",
    "summary": "Recursion is a prominent feature of human language, and fundamentally\nchallenging for self-attention due to the lack of an explicit recursive-state\ntracking mechanism. Consequently, Transformer language models poorly capture\nlong-tail recursive structure and exhibit sample-inefficient syntactic\ngeneralization. This work introduces Pushdown Layers, a new self-attention\nlayer that models recursive state via a stack tape that tracks estimated depths\nof every token in an incremental parse of the observed prefix. Transformer LMs\nwith Pushdown Layers are syntactic language models that autoregressively and\nsynchronously update this stack tape as they predict new tokens, in turn using\nthe stack tape to softly modulate attention over tokens -- for instance,\nlearning to \"skip\" over closed constituents. When trained on a corpus of\nstrings annotated with silver constituency parses, Transformers equipped with\nPushdown Layers achieve dramatically better and 3-5x more sample-efficient\nsyntactic generalization, while maintaining similar perplexities. Pushdown\nLayers are a drop-in replacement for standard self-attention. We illustrate\nthis by finetuning GPT2-medium with Pushdown Layers on an automatically parsed\nWikiText-103, leading to improvements on several GLUE text classification\ntasks.",
    "published": "2023-10-29T17:27:18Z",
    "updated": "2023-10-29T17:27:18Z",
    "authors": [
      "Shikhar Murty",
      "Pratyusha Sharma",
      "Jacob Andreas",
      "Christopher D. Manning"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.20307v1",
    "title": "Causal Interpretation of Self-Attention in Pre-Trained Transformers",
    "summary": "We propose a causal interpretation of self-attention in the Transformer\nneural network architecture. We interpret self-attention as a mechanism that\nestimates a structural equation model for a given input sequence of symbols\n(tokens). The structural equation model can be interpreted, in turn, as a\ncausal structure over the input symbols under the specific context of the input\nsequence. Importantly, this interpretation remains valid in the presence of\nlatent confounders. Following this interpretation, we estimate conditional\nindependence relations between input symbols by calculating partial\ncorrelations between their corresponding representations in the deepest\nattention layer. This enables learning the causal structure over an input\nsequence using existing constraint-based algorithms. In this sense, existing\npre-trained Transformers can be utilized for zero-shot causal-discovery. We\ndemonstrate this method by providing causal explanations for the outcomes of\nTransformers in two tasks: sentiment classification (NLP) and recommendation.",
    "published": "2023-10-31T09:27:12Z",
    "updated": "2023-10-31T09:27:12Z",
    "authors": [
      "Raanan Y. Rohekar",
      "Yaniv Gurwicz",
      "Shami Nisimov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.07753v2",
    "title": "Polynomial-based Self-Attention for Table Representation learning",
    "summary": "Structured data, which constitutes a significant portion of existing data\ntypes, has been a long-standing research topic in the field of machine\nlearning. Various representation learning methods for tabular data have been\nproposed, ranging from encoder-decoder structures to Transformers. Among these,\nTransformer-based methods have achieved state-of-the-art performance not only\nin tabular data but also in various other fields, including computer vision and\nnatural language processing. However, recent studies have revealed that\nself-attention, a key component of Transformers, can lead to an oversmoothing\nissue. We show that Transformers for tabular data also face this problem, and\nto address the problem, we propose a novel matrix polynomial-based\nself-attention layer as a substitute for the original self-attention layer,\nwhich enhances model scalability. In our experiments with three representative\ntable learning models equipped with our proposed layer, we illustrate that the\nlayer effectively mitigates the oversmoothing problem and enhances the\nrepresentation performance of the existing methods, outperforming the\nstate-of-the-art table representation methods.",
    "published": "2023-12-12T21:49:26Z",
    "updated": "2023-12-18T09:13:55Z",
    "authors": [
      "Jayoung Kim",
      "Yehjin Shin",
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Noseong Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.10325v2",
    "title": "An Attentive Inductive Bias for Sequential Recommendation beyond the\n  Self-Attention",
    "summary": "Sequential recommendation (SR) models based on Transformers have achieved\nremarkable successes. The self-attention mechanism of Transformers for computer\nvision and natural language processing suffers from the oversmoothing problem,\ni.e., hidden representations becoming similar to tokens. In the SR domain, we,\nfor the first time, show that the same problem occurs. We present pioneering\ninvestigations that reveal the low-pass filtering nature of self-attention in\nthe SR, which causes oversmoothing. To this end, we propose a novel method\ncalled $\\textbf{B}$eyond $\\textbf{S}$elf-$\\textbf{A}$ttention for Sequential\n$\\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i)\ninject an inductive bias by considering fine-grained sequential patterns and\nii) integrate low and high-frequency information to mitigate oversmoothing. Our\ndiscovery shows significant advancements in the SR domain and is expected to\nbridge the gap for existing Transformer-based SR models. We test our proposed\napproach through extensive experiments on 6 benchmark datasets. The\nexperimental results demonstrate that our model outperforms 7 baseline methods\nin terms of recommendation performance. Our code is available at\nhttps://github.com/yehjin-shin/BSARec.",
    "published": "2023-12-16T05:23:08Z",
    "updated": "2024-02-17T23:49:20Z",
    "authors": [
      "Yehjin Shin",
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Noseong Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.12742v1",
    "title": "Cached Transformers: Improving Transformers with Differentiable Memory\n  Cache",
    "summary": "This work introduces a new Transformer model called Cached Transformer, which\nuses Gated Recurrent Cached (GRC) attention to extend the self-attention\nmechanism with a differentiable memory cache of tokens. GRC attention enables\nattending to both past and current tokens, increasing the receptive field of\nattention and allowing for exploring long-range dependencies. By utilizing a\nrecurrent gating unit to continuously update the cache, our model achieves\nsignificant advancements in \\textbf{six} language and vision tasks, including\nlanguage modeling, machine translation, ListOPs, image classification, object\ndetection, and instance segmentation. Furthermore, our approach surpasses\nprevious memory-based techniques in tasks such as language modeling and\ndisplays the ability to be applied to a broader range of situations.",
    "published": "2023-12-20T03:30:51Z",
    "updated": "2023-12-20T03:30:51Z",
    "authors": [
      "Zhaoyang Zhang",
      "Wenqi Shao",
      "Yixiao Ge",
      "Xiaogang Wang",
      "Jinwei Gu",
      "Ping Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.16228v2",
    "title": "Deformable Audio Transformer for Audio Event Detection",
    "summary": "Transformers have achieved promising results on a variety of tasks. However,\nthe quadratic complexity in self-attention computation has limited the\napplications, especially in low-resource settings and mobile or edge devices.\nExisting works have proposed to exploit hand-crafted attention patterns to\nreduce computation complexity. However, such hand-crafted patterns are\ndata-agnostic and may not be optimal. Hence, it is likely that relevant keys or\nvalues are being reduced, while less important ones are still preserved. Based\non this key insight, we propose a novel deformable audio Transformer for audio\nrecognition, named DATAR, where a deformable attention equipping with a pyramid\ntransformer backbone is constructed and learnable. Such an architecture has\nbeen proven effective in prediction tasks,~\\textit{e.g.}, event classification.\nMoreover, we identify that the deformable attention map computation may\nover-simplify the input feature, which can be further enhanced. Hence, we\nintroduce a learnable input adaptor to alleviate this issue, and DATAR achieves\nstate-of-the-art performance.",
    "published": "2023-12-24T18:27:22Z",
    "updated": "2024-01-08T03:52:24Z",
    "authors": [
      "Wentao Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.11673v1",
    "title": "MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View\n  Stereo",
    "summary": "Recent advancements in learning-based Multi-View Stereo (MVS) methods have\nprominently featured transformer-based models with attention mechanisms.\nHowever, existing approaches have not thoroughly investigated the profound\ninfluence of transformers on different MVS modules, resulting in limited depth\nestimation capabilities. In this paper, we introduce MVSFormer++, a method that\nprudently maximizes the inherent characteristics of attention to enhance\nvarious components of the MVS pipeline. Formally, our approach involves\ninfusing cross-view information into the pre-trained DINOv2 model to facilitate\nMVS learning. Furthermore, we employ different attention mechanisms for the\nfeature encoder and cost volume regularization, focusing on feature and spatial\naggregations respectively. Additionally, we uncover that some design details\nwould substantially impact the performance of transformer modules in MVS,\nincluding normalized 3D positional encoding, adaptive attention scaling, and\nthe position of layer normalization. Comprehensive experiments on DTU,\nTanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the\nproposed method. Notably, MVSFormer++ achieves state-of-the-art performance on\nthe challenging DTU and Tanks-and-Temples benchmarks.",
    "published": "2024-01-22T03:22:49Z",
    "updated": "2024-01-22T03:22:49Z",
    "authors": [
      "Chenjie Cao",
      "Xinlin Ren",
      "Yanwei Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.12439v1",
    "title": "MAST: Video Polyp Segmentation with a Mixture-Attention Siamese\n  Transformer",
    "summary": "Accurate segmentation of polyps from colonoscopy videos is of great\nsignificance to polyp treatment and early prevention of colorectal cancer.\nHowever, it is challenging due to the difficulties associated with modelling\nlong-range spatio-temporal relationships within a colonoscopy video. In this\npaper, we address this challenging task with a novel Mixture-Attention Siamese\nTransformer (MAST), which explicitly models the long-range spatio-temporal\nrelationships with a mixture-attention mechanism for accurate polyp\nsegmentation. Specifically, we first construct a Siamese transformer\narchitecture to jointly encode paired video frames for their feature\nrepresentations. We then design a mixture-attention module to exploit the\nintra-frame and inter-frame correlations, enhancing the features with rich\nspatio-temporal relationships. Finally, the enhanced features are fed to two\nparallel decoders for predicting the segmentation maps. To the best of our\nknowledge, our MAST is the first transformer model dedicated to video polyp\nsegmentation. Extensive experiments on the large-scale SUN-SEG benchmark\ndemonstrate the superior performance of MAST in comparison with the\ncutting-edge competitors. Our code is publicly available at\nhttps://github.com/Junqing-Yang/MAST.",
    "published": "2024-01-23T02:18:53Z",
    "updated": "2024-01-23T02:18:53Z",
    "authors": [
      "Geng Chen",
      "Junqing Yang",
      "Xiaozhou Pu",
      "Ge-Peng Ji",
      "Huan Xiong",
      "Yongsheng Pan",
      "Hengfei Cui",
      "Yong Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.12362v2",
    "title": "KV-weights are all you need for skipless transformers",
    "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
    "published": "2024-04-18T17:45:19Z",
    "updated": "2025-10-27T17:31:15Z",
    "authors": [
      "Nils Graef"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.01033v2",
    "title": "CrossMPT: Cross-attention Message-Passing Transformer for Error\n  Correcting Codes",
    "summary": "Error correcting codes (ECCs) are indispensable for reliable transmission in\ncommunication systems. The recent advancements in deep learning have catalyzed\nthe exploration of ECC decoders based on neural networks. Among these,\ntransformer-based neural decoders have achieved state-of-the-art decoding\nperformance. In this paper, we propose a novel Cross-attention Message-Passing\nTransformer (CrossMPT), which shares key operational principles with\nconventional message-passing decoders. While conventional transformer-based\ndecoders employ self-attention mechanism without distinguishing between the\ntypes of input vectors (i.e., magnitude and syndrome vectors), CrossMPT updates\nthe two types of input vectors separately and iteratively using two masked\ncross-attention blocks. The mask matrices are determined by the code's\nparity-check matrix, which explicitly captures the irrelevant relationship\nbetween two input vectors. Our experimental results show that CrossMPT\nsignificantly outperforms existing neural network-based decoders for various\ncode classes. Notably, CrossMPT achieves this decoding performance improvement,\nwhile significantly reducing the memory usage, complexity, inference time, and\ntraining time.",
    "published": "2024-05-02T06:30:52Z",
    "updated": "2024-10-11T04:42:49Z",
    "authors": [
      "Seong-Joon Park",
      "Hee-Youl Kwak",
      "Sang-Hyo Kim",
      "Yongjune Kim",
      "Jong-Seon No"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.04028v1",
    "title": "Masked Graph Transformer for Large-Scale Recommendation",
    "summary": "Graph Transformers have garnered significant attention for learning\ngraph-structured data, thanks to their superb ability to capture long-range\ndependencies among nodes. However, the quadratic space and time complexity\nhinders the scalability of Graph Transformers, particularly for large-scale\nrecommendation. Here we propose an efficient Masked Graph Transformer, named\nMGFormer, capable of capturing all-pair interactions among nodes with a linear\ncomplexity. To achieve this, we treat all user/item nodes as independent\ntokens, enhance them with positional embeddings, and feed them into a\nkernelized attention module. Additionally, we incorporate learnable relative\ndegree information to appropriately reweigh the attentions. Experimental\nresults show the superior performance of our MGFormer, even with a single\nattention layer.",
    "published": "2024-05-07T06:00:47Z",
    "updated": "2024-05-07T06:00:47Z",
    "authors": [
      "Huiyuan Chen",
      "Zhe Xu",
      "Chin-Chia Michael Yeh",
      "Vivian Lai",
      "Yan Zheng",
      "Minghua Xu",
      "Hanghang Tong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.17670v2",
    "title": "Brain Tumor Classification using Vision Transformer with Selective\n  Cross-Attention Mechanism and Feature Calibration",
    "summary": "Brain tumor classification is a challenging task in medical image analysis.\nIn this paper, we propose a novel approach to brain tumor classification using\na vision transformer with a novel cross-attention mechanism. Our approach\nleverages the strengths of transformers in modeling long-range dependencies and\nmulti-scale feature fusion. We introduce two new mechanisms to improve the\nperformance of the cross-attention fusion module: Feature Calibration Mechanism\n(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from\ndifferent branches to make them more compatible, while SCA selectively attends\nto the most informative features. Our experiments demonstrate that the proposed\napproach outperforms other state-of-the-art methods in brain tumor\nclassification, achieving improved accuracy and efficiency. The proposed FCM\nand SCA mechanisms can be easily integrated into other vision transformer\narchitectures, making them a promising direction for future research in medical\nimage analysis. Experimental results confirm that our approach surpasses\nexisting methods, achieving state-of-the-art performance in brain tumor\nclassification tasks.",
    "published": "2024-06-25T15:58:56Z",
    "updated": "2024-11-25T19:02:57Z",
    "authors": [
      "Mohammad Ali Labbaf Khaniki",
      "Marzieh Mirzaeibonehkhater",
      "Mohammad Manthouri",
      "Elham Hasani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.05489v1",
    "title": "How Effective are State Space Models for Machine Translation?",
    "summary": "Transformers are the current architecture of choice for NLP, but their\nattention layers do not scale well to long contexts. Recent works propose to\nreplace attention with linear recurrent layers -- this is the case for state\nspace models, which enjoy efficient training and inference. However, it remains\nunclear whether these models are competitive with transformers in machine\ntranslation (MT). In this paper, we provide a rigorous and comprehensive\nexperimental comparison between transformers and linear recurrent models for\nMT. Concretely, we experiment with RetNet, Mamba, and hybrid versions of Mamba\nwhich incorporate attention mechanisms. Our findings demonstrate that Mamba is\nhighly competitive with transformers on sentence and paragraph-level datasets,\nwhere in the latter both models benefit from shifting the training distribution\ntowards longer sequences. Further analysis show that integrating attention into\nMamba improves translation quality, robustness to sequence length\nextrapolation, and the ability to recall named entities.",
    "published": "2024-07-07T20:21:49Z",
    "updated": "2024-07-07T20:21:49Z",
    "authors": [
      "Hugo Pitorro",
      "Pavlo Vasylenko",
      "Marcos Treviso",
      "AndrÃ© F. T. Martins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.05591v1",
    "title": "On the Power of Convolution Augmented Transformer",
    "summary": "The transformer architecture has catalyzed revolutionary advances in language\nmodeling. However, recent architectural recipes, such as state-space models,\nhave bridged the performance gap. Motivated by this, we examine the benefits of\nConvolution-Augmented Transformer (CAT) for recall, copying, and length\ngeneralization tasks. CAT incorporates convolutional filters in the K/Q/V\nembeddings of an attention layer. Through CAT, we show that the locality of the\nconvolution synergizes with the global view of the attention. Unlike comparable\narchitectures, such as Mamba or transformer, CAT can provably solve the\nassociative recall (AR) and copying tasks using a single layer while also\nenjoying guaranteed length generalization. We also establish computational\ntradeoffs between convolution and attention by characterizing how convolution\ncan mitigate the need for full attention by summarizing the context window and\ncreating salient summary tokens to attend. Evaluations on real datasets\ncorroborate our findings and demonstrate that CAT and its variations indeed\nenhance the language modeling performance.",
    "published": "2024-07-08T04:08:35Z",
    "updated": "2024-07-08T04:08:35Z",
    "authors": [
      "Mingchen Li",
      "Xuechen Zhang",
      "Yixiao Huang",
      "Samet Oymak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.17261v1",
    "title": "Embedding-Free Transformer with Inference Spatial Reduction for\n  Efficient Semantic Segmentation",
    "summary": "We present an Encoder-Decoder Attention Transformer, EDAFormer, which\nconsists of the Embedding-Free Transformer (EFT) encoder and the all-attention\ndecoder leveraging our Embedding-Free Attention (EFA) structure. The proposed\nEFA is a novel global context modeling mechanism that focuses on functioning\nthe global non-linearity, not the specific roles of the query, key and value.\nFor the decoder, we explore the optimized structure for considering the\nglobality, which can improve the semantic segmentation performance. In\naddition, we propose a novel Inference Spatial Reduction (ISR) method for the\ncomputational efficiency. Different from the previous spatial reduction\nattention methods, our ISR method further reduces the key-value resolution at\nthe inference phase, which can mitigate the computation-performance trade-off\ngap for the efficient semantic segmentation. Our EDAFormer shows the\nstate-of-the-art performance with the efficient computation compared to the\nexisting transformer-based semantic segmentation models in three public\nbenchmarks, including ADE20K, Cityscapes and COCO-Stuff. Furthermore, our ISR\nmethod reduces the computational cost by up to 61% with minimal mIoU\nperformance degradation on Cityscapes dataset. The code is available at\nhttps://github.com/hyunwoo137/EDAFormer.",
    "published": "2024-07-24T13:24:25Z",
    "updated": "2024-07-24T13:24:25Z",
    "authors": [
      "Hyunwoo Yu",
      "Yubin Cho",
      "Beoungwoo Kang",
      "Seunghun Moon",
      "Kyeongbo Kong",
      "Suk-Ju Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.01708v1",
    "title": "AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual\n  Segmentation",
    "summary": "Recently, transformer-based models have demonstrated remarkable performance\non audio-visual segmentation (AVS) tasks. However, their expensive\ncomputational cost makes real-time inference impractical. By characterizing\nattention maps of the network, we identify two key obstacles in AVS models: 1)\nattention dissipation, corresponding to the over-concentrated attention weights\nby Softmax within restricted frames, and 2) inefficient, burdensome transformer\ndecoder, caused by narrow focus patterns in early stages. In this paper, we\nintroduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation\ntransformer that achieves fast, efficient and light-weight simultaneously. Our\nmodel leverages an efficient prompt query generator to correct the behaviour of\ncross-attention. Additionally, we propose ELF decoder to bring greater\nefficiency by facilitating convolutions suitable for local features to reduce\ncomputational burdens. Extensive experiments demonstrate that our AVESFormer\nsignificantly enhances model performance, achieving 79.9% on S4, 57.9% on MS3\nand 31.2% on AVSS, outperforming previous state-of-the-art and achieving an\nexcellent trade-off between performance and speed. Code can be found at\nhttps://github.com/MarkXCloud/AVESFormer.git.",
    "published": "2024-08-03T08:25:26Z",
    "updated": "2024-08-03T08:25:26Z",
    "authors": [
      "Zili Wang",
      "Qi Yang",
      "Linsu Shi",
      "Jiazhong Yu",
      "Qinghua Liang",
      "Fei Li",
      "Shiming Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.13166v4",
    "title": "An Evolved Universal Transformer Memory",
    "summary": "Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.",
    "published": "2024-10-17T02:47:10Z",
    "updated": "2025-02-13T09:08:42Z",
    "authors": [
      "Edoardo Cetin",
      "Qi Sun",
      "Tianyu Zhao",
      "Yujin Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.14874v2",
    "title": "Improving Vision Transformers by Overlapping Heads in Multi-Head\n  Self-Attention",
    "summary": "Vision Transformers have made remarkable progress in recent years, achieving\nstate-of-the-art performance in most vision tasks. A key component of this\nsuccess is due to the introduction of the Multi-Head Self-Attention (MHSA)\nmodule, which enables each head to learn different representations by applying\nthe attention mechanism independently. In this paper, we empirically\ndemonstrate that Vision Transformers can be further enhanced by overlapping the\nheads in MHSA. We introduce Multi-Overlapped-Head Self-Attention (MOHSA), where\nheads are overlapped with their two adjacent heads for queries, keys, and\nvalues, while zero-padding is employed for the first and last heads, which have\nonly one neighboring head. Various paradigms for overlapping ratios are\nproposed to fully investigate the optimal performance of our approach. The\nproposed approach is evaluated using five Transformer models on four benchmark\ndatasets and yields a significant performance boost. The source code will be\nmade publicly available upon publication.",
    "published": "2024-10-18T21:41:55Z",
    "updated": "2025-02-01T03:56:14Z",
    "authors": [
      "Tianxiao Zhang",
      "Bo Luo",
      "Guanghui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.02285v1",
    "title": "GQWformer: A Quantum-based Transformer for Graph Representation Learning",
    "summary": "Graph Transformers (GTs) have demonstrated significant advantages in graph\nrepresentation learning through their global attention mechanisms. However, the\nself-attention mechanism in GTs tends to neglect the inductive biases inherent\nin graph structures, making it chanllenging to effectively capture essential\nstructural information. To address this issue, we propose a novel approach that\nintegrate graph inductive bias into self-attention mechanisms by leveraging\nquantum technology for structural encoding. In this paper, we introduce the\nGraph Quantum Walk Transformer (GQWformer), a groundbreaking GNN framework that\nutilizes quantum walks on attributed graphs to generate node quantum states.\nThese quantum states encapsulate rich structural attributes and serve as\ninductive biases for the transformer, thereby enabling the generation of more\nmeaningful attention scores. By subsequently incorporating a recurrent neural\nnetwork, our design amplifies the model's ability to focus on both local and\nglobal information. We conducted comprehensive experiments across five publicly\navailable datasets to evaluate the effectiveness of our model. These results\nclearly indicate that GQWformer outperforms existing state-of-the-art graph\nclassification algorithms. These findings highlight the significant potential\nof integrating quantum computing methodologies with traditional GNNs to advance\nthe field of graph representation learning, providing a promising direction for\nfuture research and applications.",
    "published": "2024-12-03T09:03:04Z",
    "updated": "2024-12-03T09:03:04Z",
    "authors": [
      "Lei Yu",
      "Hongyang Chen",
      "Jingsong Lv",
      "Linyao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03673v2",
    "title": "Interpreting Transformers for Jet Tagging",
    "summary": "Machine learning (ML) algorithms, particularly attention-based transformer\nmodels, have become indispensable for analyzing the vast data generated by\nparticle physics experiments like ATLAS and CMS at the CERN LHC. Particle\nTransformer (ParT), a state-of-the-art model, leverages particle-level\nattention to improve jet-tagging tasks, which are critical for identifying\nparticles resulting from proton collisions. This study focuses on interpreting\nParT by analyzing attention heat maps and particle-pair correlations on the\n$\\eta$-$\\phi$ plane, revealing a binary attention pattern where each particle\nattends to at most one other particle. At the same time, we observe that ParT\nshows varying focus on important particles and subjets depending on decay,\nindicating that the model learns traditional jet substructure observables.\nThese insights enhance our understanding of the model's internal workings and\nlearning process, offering potential avenues for improving the efficiency of\ntransformer architectures in future high-energy physics applications.",
    "published": "2024-12-04T19:06:40Z",
    "updated": "2024-12-09T03:47:39Z",
    "authors": [
      "Aaron Wang",
      "Abhijith Gandrakota",
      "Jennifer Ngadiuba",
      "Vivekanand Sahu",
      "Priyansh Bhatnagar",
      "Elham E Khoda",
      "Javier Duarte"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06902v1",
    "title": "Emergence of Episodic Memory in Transformers: Characterizing Changes in\n  Temporal Structure of Attention Scores During Training",
    "summary": "We investigate in-context temporal biases in attention heads and transformer\noutputs. Using cognitive science methodologies, we analyze attention scores and\noutputs of the GPT-2 models of varying sizes. Across attention heads, we\nobserve effects characteristic of human episodic memory, including temporal\ncontiguity, primacy and recency. Transformer outputs demonstrate a tendency\ntoward in-context serial recall. Importantly, this effect is eliminated after\nthe ablation of the induction heads, which are the driving force behind the\ncontiguity effect. Our findings offer insights into how transformers organize\ninformation temporally during in-context learning, shedding light on their\nsimilarities and differences with human memory and learning.",
    "published": "2025-02-09T20:20:37Z",
    "updated": "2025-02-09T20:20:37Z",
    "authors": [
      "Deven Mahesh Mistry",
      "Anooshka Bajaj",
      "Yash Aggarwal",
      "Sahaj Singh Maini",
      "Zoran Tiganj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07553v1",
    "title": "Attention Learning is Needed to Efficiently Learn Parity Function",
    "summary": "Transformers, with their attention mechanisms, have emerged as the\nstate-of-the-art architectures of sequential modeling and empirically\noutperform feed-forward neural networks (FFNNs) across many fields, such as\nnatural language processing and computer vision. However, their generalization\nability, particularly for low-sensitivity functions, remains less studied. We\nbridge this gap by analyzing transformers on the $k$-parity problem. Daniely\nand Malach (NeurIPS 2020) show that FFNNs with one hidden layer and $O(nk^7\n\\log k)$ parameters can learn $k$-parity, where the input length $n$ is\ntypically much larger than $k$. In this paper, we prove that FFNNs require at\nleast $\\Omega(n)$ parameters to learn $k$-parity, while transformers require\nonly $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs.\nWe further prove that this parameter efficiency cannot be achieved with fixed\nattention heads. Our work establishes transformers as theoretically superior to\nFFNNs in learning parity function, showing how their attention mechanisms\nenable parameter-efficient generalization in functions with low sensitivity.",
    "published": "2025-02-11T13:41:30Z",
    "updated": "2025-02-11T13:41:30Z",
    "authors": [
      "Yaomengxi Han",
      "Debarghya Ghoshdastidar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.10927v2",
    "title": "The underlying structures of self-attention: symmetry, directionality,\n  and emergent dynamics in Transformer training",
    "summary": "Self-attention is essential to Transformer architectures, yet how information\nis embedded in the self-attention matrices and how different objective\nfunctions impact this process remains unclear. We present a mathematical\nframework to analyze self-attention matrices by deriving the structures\ngoverning their weight updates. Using this framework, we demonstrate that\nbidirectional training induces symmetry in the weight matrices, while\nautoregressive training results in directionality and column dominance. Our\ntheoretical findings are validated across multiple Transformer models -\nincluding ModernBERT, GPT, LLaMA3, and Mistral - and input modalities like\ntext, vision, and audio. Finally, we apply these insights by showing that\nsymmetric initialization improves the performance of encoder-only models on\nlanguage tasks. This mathematical analysis offers a novel theoretical\nperspective on how information is embedded through self-attention, thereby\nimproving the interpretability of Transformer models.",
    "published": "2025-02-15T23:08:02Z",
    "updated": "2025-06-03T10:08:01Z",
    "authors": [
      "Matteo Saponati",
      "Pascal Sager",
      "Pau Vilimelis Aceituno",
      "Thilo Stadelmann",
      "Benjamin Grewe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.11340v1",
    "title": "S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time\n  Series Forecasting",
    "summary": "Time series forecasting has recently achieved significant progress with\nmulti-scale models to address the heterogeneity between long and short range\npatterns. Despite their state-of-the-art performance, we identify two potential\nareas for improvement. First, the variates of the multivariate time series are\nprocessed independently. Moreover, the multi-scale (long and short range)\nrepresentations are learned separately by two independent models without\ncommunication. In light of these concerns, we propose State Space Transformer\nwith cross-attention (S2TX). S2TX employs a cross-attention mechanism to\nintegrate a Mamba model for extracting long-range cross-variate context and a\nTransformer model with local window attention to capture short-range\nrepresentations. By cross-attending to the global context, the Transformer\nmodel further facilitates variate-level interactions as well as local/global\ncommunications. Comprehensive experiments on seven classic long-short range\ntime-series forecasting benchmark datasets demonstrate that S2TX can achieve\nhighly robust SOTA results while maintaining a low memory footprint.",
    "published": "2025-02-17T01:40:45Z",
    "updated": "2025-02-17T01:40:45Z",
    "authors": [
      "Zihao Wu",
      "Juncheng Dong",
      "Haoming Yang",
      "Vahid Tarokh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.23551v2",
    "title": "A unified framework for establishing the universal approximation of\n  transformer-type architectures",
    "summary": "We investigate the universal approximation property (UAP) of transformer-type\narchitectures, providing a unified theoretical framework that extends prior\nresults on residual networks to models incorporating attention mechanisms. Our\nwork identifies token distinguishability as a fundamental requirement for UAP\nand introduces a general sufficient condition that applies to a broad class of\narchitectures. Leveraging an analyticity assumption on the attention layer, we\ncan significantly simplify the verification of this condition, providing a\nnon-constructive approach in establishing UAP for such architectures. We\ndemonstrate the applicability of our framework by proving UAP for transformers\nwith various attention mechanisms, including kernel-based and sparse attention\nmechanisms. The corollaries of our results either generalize prior works or\nestablish UAP for architectures not previously covered. Furthermore, our\nframework offers a principled foundation for designing novel transformer\narchitectures with inherent UAP guarantees, including those with specific\nfunctional symmetries. We propose examples to illustrate these insights.",
    "published": "2025-06-30T06:50:39Z",
    "updated": "2025-10-21T15:34:53Z",
    "authors": [
      "Jingpu Cheng",
      "Ting Lin",
      "Zuowei Shen",
      "Qianxiao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08959v1",
    "title": "CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For\n  Small-Scale Vision",
    "summary": "Vision Transformers (ViTs) have achieved impressive results in computer\nvision by leveraging self-attention to model long-range dependencies. However,\ntheir emphasis on global context often comes at the expense of local feature\nextraction in small datasets, particularly due to the lack of key inductive\nbiases such as locality and translation equivariance. To mitigate this, we\npropose CoSwin, a novel feature-fusion architecture that augments the\nhierarchical shifted window attention with localized convolutional feature\nlearning. Specifically, CoSwin integrates a learnable local feature enhancement\nmodule into each attention block, enabling the model to simultaneously capture\nfine-grained spatial details and global semantic structure. We evaluate CoSwin\non multiple image classification benchmarks including CIFAR-10, CIFAR-100,\nMNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent\nperformance gains over state-of-the-art convolutional and transformer-based\nmodels. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on\nCIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the\nbaseline Swin Transformer. These improvements underscore the effectiveness of\nlocal-global feature fusion in enhancing the generalization and robustness of\ntransformers for small-scale vision. Code and pretrained weights available at\nhttps://github.com/puskal-khadka/coswin",
    "published": "2025-09-10T19:43:16Z",
    "updated": "2025-09-10T19:43:16Z",
    "authors": [
      "Puskal Khadka",
      "Rodrigue Rizk",
      "Longwei Wang",
      "KC Santosh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14954v1",
    "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked\n  Autoregression",
    "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.",
    "published": "2025-10-16T17:57:53Z",
    "updated": "2025-10-16T17:57:53Z",
    "authors": [
      "Zhe Li",
      "Weihao Yuan",
      "Weichao Shen",
      "Siyu Zhu",
      "Zilong Dong",
      "Chang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.14052v3",
    "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
    "summary": "State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.",
    "published": "2022-12-28T17:56:03Z",
    "updated": "2023-04-29T03:18:40Z",
    "authors": [
      "Daniel Y. Fu",
      "Tri Dao",
      "Khaled K. Saab",
      "Armin W. Thomas",
      "Atri Rudra",
      "Christopher RÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.16065v1",
    "title": "The Hyperdimensional Transform: a Holographic Representation of\n  Functions",
    "summary": "Integral transforms are invaluable mathematical tools to map functions into\nspaces where they are easier to characterize. We introduce the hyperdimensional\ntransform as a new kind of integral transform. It converts square-integrable\nfunctions into noise-robust, holographic, high-dimensional representations\ncalled hyperdimensional vectors. The central idea is to approximate a function\nby a linear combination of random functions. We formally introduce a set of\nstochastic, orthogonal basis functions and define the hyperdimensional\ntransform and its inverse. We discuss general transform-related properties such\nas its uniqueness, approximation properties of the inverse transform, and the\nrepresentation of integrals and derivatives. The hyperdimensional transform\noffers a powerful, flexible framework that connects closely with other integral\ntransforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it\nprovides theoretical foundations and new insights for the field of\nhyperdimensional computing, a computing paradigm that is rapidly gaining\nattention for efficient and explainable machine learning algorithms, with\npotential applications in statistical modelling and machine learning. In\naddition, we provide straightforward and easily understandable code, which can\nfunction as a tutorial and allows for the reproduction of the demonstrated\nexamples, from computing the transform to solving differential equations.",
    "published": "2023-10-24T11:33:39Z",
    "updated": "2023-10-24T11:33:39Z",
    "authors": [
      "Pieter Dewulf",
      "Michiel Stock",
      "Bernard De Baets"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.09777v1",
    "title": "Graph Transformers: A Survey",
    "summary": "Graph transformers are a recent advancement in machine learning, offering a\nnew class of neural network models for graph-structured data. The synergy\nbetween transformers and graph learning demonstrates strong performance and\nversatility across various graph-related tasks. This survey provides an\nin-depth review of recent progress and challenges in graph transformer\nresearch. We begin with foundational concepts of graphs and transformers. We\nthen explore design perspectives of graph transformers, focusing on how they\nintegrate graph inductive biases and graph attention mechanisms into the\ntransformer architecture. Furthermore, we propose a taxonomy classifying graph\ntransformers based on depth, scalability, and pre-training strategies,\nsummarizing key principles for effective development of graph transformer\nmodels. Beyond technical analysis, we discuss the applications of graph\ntransformer models for node-level, edge-level, and graph-level tasks, exploring\ntheir potential in other application scenarios as well. Finally, we identify\nremaining challenges in the field, such as scalability and efficiency,\ngeneralization and robustness, interpretability and explainability, dynamic and\ncomplex graphs, as well as data quality and diversity, charting future\ndirections for graph transformer research.",
    "published": "2024-07-13T05:15:24Z",
    "updated": "2024-07-13T05:15:24Z",
    "authors": [
      "Ahsan Shehzad",
      "Feng Xia",
      "Shagufta Abid",
      "Ciyuan Peng",
      "Shuo Yu",
      "Dongyu Zhang",
      "Karin Verspoor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.06296v2",
    "title": "Stabilizing Transformer Training by Preventing Attention Entropy\n  Collapse",
    "summary": "Training stability is of great importance to Transformers. In this work, we\ninvestigate the training dynamics of Transformers by examining the evolution of\nthe attention layers. In particular, we track the attention entropy for each\nattention head during the course of training, which is a proxy for model\nsharpness. We identify a common pattern across different architectures and\ntasks, where low attention entropy is accompanied by high training instability,\nwhich can take the form of oscillating loss or divergence. We denote the\npathologically low attention entropy, corresponding to highly concentrated\nattention scores, as $\\textit{entropy collapse}$. As a remedy, we propose\n$\\sigma$Reparam, a simple and efficient solution where we reparametrize all\nlinear layers with spectral normalization and an additional learned scalar. We\ndemonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the\nattention layers, promoting more stable training. Additionally, we prove a\ntight lower bound of the attention entropy, which decreases exponentially fast\nwith the spectral norm of the attention logits, providing additional motivation\nfor our approach. We conduct experiments with $\\sigma$Reparam on image\nclassification, image self-supervised learning, machine translation, speech\nrecognition, and language modeling tasks. We show that $\\sigma$Reparam provides\nstability and robustness with respect to the choice of hyperparameters, going\nso far as enabling training (a) a Vision Transformer {to competitive\nperformance} without warmup, weight decay, layer normalization or adaptive\noptimizers; (b) deep architectures in machine translation and (c) speech\nrecognition to competitive performance without warmup and adaptive optimizers.\nCode is available at \\url{https://github.com/apple/ml-sigma-reparam}.",
    "published": "2023-03-11T03:30:47Z",
    "updated": "2023-07-25T17:42:37Z",
    "authors": [
      "Shuangfei Zhai",
      "Tatiana Likhomanenko",
      "Etai Littwin",
      "Dan Busbridge",
      "Jason Ramapuram",
      "Yizhe Zhang",
      "Jiatao Gu",
      "Josh Susskind"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2009.14794v4",
    "title": "Rethinking Attention with Performers",
    "summary": "We introduce Performers, Transformer architectures which can estimate regular\n(softmax) full-rank-attention Transformers with provable accuracy, but using\nonly linear (as opposed to quadratic) space and time complexity, without\nrelying on any priors such as sparsity or low-rankness. To approximate softmax\nattention-kernels, Performers use a novel Fast Attention Via positive\nOrthogonal Random features approach (FAVOR+), which may be of independent\ninterest for scalable kernel methods. FAVOR+ can be also used to efficiently\nmodel kernelizable attention mechanisms beyond softmax. This representational\npower is crucial to accurately compare softmax with other kernels for the first\ntime on large-scale tasks, beyond the reach of regular Transformers, and\ninvestigate optimal attention-kernels. Performers are linear architectures\nfully compatible with regular Transformers and with strong theoretical\nguarantees: unbiased or nearly-unbiased estimation of the attention matrix,\nuniform convergence and low estimation variance. We tested Performers on a rich\nset of tasks stretching from pixel-prediction through text models to protein\nsequence modeling. We demonstrate competitive results with other examined\nefficient sparse and dense attention methods, showcasing effectiveness of the\nnovel attention-learning paradigm leveraged by Performers.",
    "published": "2020-09-30T17:09:09Z",
    "updated": "2022-11-19T12:45:21Z",
    "authors": [
      "Krzysztof Choromanski",
      "Valerii Likhosherstov",
      "David Dohan",
      "Xingyou Song",
      "Andreea Gane",
      "Tamas Sarlos",
      "Peter Hawkins",
      "Jared Davis",
      "Afroz Mohiuddin",
      "Lukasz Kaiser",
      "David Belanger",
      "Lucy Colwell",
      "Adrian Weller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1811.04716v1",
    "title": "Input Combination Strategies for Multi-Source Transformer Decoder",
    "summary": "In multi-source sequence-to-sequence tasks, the attention mechanism can be\nmodeled in several ways. This topic has been thoroughly studied on recurrent\narchitectures. In this paper, we extend the previous work to the\nencoder-decoder attention in the Transformer architecture. We propose four\ndifferent input combination strategies for the encoder-decoder attention:\nserial, parallel, flat, and hierarchical. We evaluate our methods on tasks of\nmultimodal translation and translation with multiple source languages. The\nexperiments show that the models are able to use multiple sources and improve\nover single source baselines.",
    "published": "2018-11-12T13:33:35Z",
    "updated": "2018-11-12T13:33:35Z",
    "authors": [
      "JindÅich LibovickÃ½",
      "JindÅich Helcl",
      "David MareÄek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.09233v1",
    "title": "Unified and Multilingual Author Profiling for Detecting Haters",
    "summary": "This paper presents a unified user profiling framework to identify hate\nspeech spreaders by processing their tweets regardless of the language. The\nframework encodes the tweets with sentence transformers and applies an\nattention mechanism to select important tweets for learning user profiles.\nFurthermore, the attention layer helps to explain why a user is a hate speech\nspreader by producing attention weights at both token and post level. Our\nproposed model outperformed the state-of-the-art multilingual transformer\nmodels.",
    "published": "2021-09-19T21:53:23Z",
    "updated": "2021-09-19T21:53:23Z",
    "authors": [
      "Ipek Baris Schlicht",
      "Angel Felipe MagnossÃ£o de Paula"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.11073v1",
    "title": "Grad-SAM: Explaining Transformers via Gradient Self-Attention Maps",
    "summary": "Transformer-based language models significantly advanced the state-of-the-art\nin many linguistic tasks. As this revolution continues, the ability to explain\nmodel predictions has become a major area of interest for the NLP community. In\nthis work, we present Gradient Self-Attention Maps (Grad-SAM) - a novel\ngradient-based method that analyzes self-attention units and identifies the\ninput elements that explain the model's prediction the best. Extensive\nevaluations on various benchmarks show that Grad-SAM obtains significant\nimprovements over state-of-the-art alternatives.",
    "published": "2022-04-23T13:56:55Z",
    "updated": "2022-04-23T13:56:55Z",
    "authors": [
      "Oren Barkan",
      "Edan Hauon",
      "Avi Caciularu",
      "Ori Katz",
      "Itzik Malkiel",
      "Omri Armstrong",
      "Noam Koenigstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.11247v2",
    "title": "Attention-Based Multimodal Image Matching",
    "summary": "We propose an attention-based approach for multimodal image patch matching\nusing a Transformer encoder attending to the feature maps of a multiscale\nSiamese CNN. Our encoder is shown to efficiently aggregate multiscale image\nembeddings while emphasizing task-specific appearance-invariant image cues. We\nalso introduce an attention-residual architecture, using a residual connection\nbypassing the encoder. This additional learning signal facilitates end-to-end\ntraining from scratch. Our approach is experimentally shown to achieve new\nstate-of-the-art accuracy on both multimodal and single modality benchmarks,\nillustrating its general applicability. To the best of our knowledge, this is\nthe first successful implementation of the Transformer encoder architecture to\nthe multimodal image patch matching task.",
    "published": "2021-03-20T21:14:24Z",
    "updated": "2023-09-24T11:58:32Z",
    "authors": [
      "Aviad Moreshet",
      "Yosi Keller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1905.07799v2",
    "title": "Adaptive Attention Span in Transformers",
    "summary": "We propose a novel self-attention mechanism that can learn its optimal\nattention span. This allows us to extend significantly the maximum context size\nused in Transformer, while maintaining control over their memory footprint and\ncomputational time. We show the effectiveness of our approach on the task of\ncharacter level language modeling, where we achieve state-of-the-art\nperformances on text8 and enwiki8 by using a maximum context of 8k characters.",
    "published": "2019-05-19T19:43:14Z",
    "updated": "2019-08-08T06:58:31Z",
    "authors": [
      "Sainbayar Sukhbaatar",
      "Edouard Grave",
      "Piotr Bojanowski",
      "Armand Joulin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.14318v1",
    "title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language\n  Understanding",
    "summary": "End-to-end spoken language understanding (SLU) systems benefit from\npretraining on large corpora, followed by fine-tuning on application-specific\ndata. The resulting models are too large for on-edge applications. For\ninstance, BERT-based systems contain over 110M parameters. Observing the model\nis overparameterized, we propose lean transformer structure where the dimension\nof the attention mechanism is automatically reduced using group sparsity. We\npropose a variant where the learned attention subspace is transferred to an\nattention bottleneck layer. In a low-resource setting and without pre-training,\nthe resulting compact SLU model achieves accuracies competitive with\npre-trained large models.",
    "published": "2022-06-28T23:08:32Z",
    "updated": "2022-06-28T23:08:32Z",
    "authors": [
      "Pu Wang",
      "Hugo Van hamme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.09400v1",
    "title": "Probing for Bridging Inference in Transformer Language Models",
    "summary": "We probe pre-trained transformer language models for bridging inference. We\nfirst investigate individual attention heads in BERT and observe that attention\nheads at higher layers prominently focus on bridging relations in-comparison\nwith the lower and middle layers, also, few specific attention heads\nconcentrate consistently on bridging. More importantly, we consider language\nmodels as a whole in our second approach where bridging anaphora resolution is\nformulated as a masked token prediction task (Of-Cloze test). Our formulation\nproduces optimistic results without any fine-tuning, which indicates that\npre-trained language models substantially capture bridging inference. Our\nfurther investigation shows that the distance between anaphor-antecedent and\nthe context provided to language models play an important role in the\ninference.",
    "published": "2021-04-19T15:42:24Z",
    "updated": "2021-04-19T15:42:24Z",
    "authors": [
      "Onkar Pandit",
      "Yufang Hou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08247v1",
    "title": "PAT: Parallel Attention Transformer for Visual Question Answering in\n  Vietnamese",
    "summary": "We present in this paper a novel scheme for multimodal learning named the\nParallel Attention mechanism. In addition, to take into account the advantages\nof grammar and context in Vietnamese, we propose the Hierarchical Linguistic\nFeatures Extractor instead of using an LSTM network to extract linguistic\nfeatures. Based on these two novel modules, we introduce the Parallel Attention\nTransformer (PAT), achieving the best accuracy compared to all baselines on the\nbenchmark ViVQA dataset and other SOTA methods including SAAA and MCAN.",
    "published": "2023-07-17T05:05:15Z",
    "updated": "2023-07-17T05:05:15Z",
    "authors": [
      "Nghia Hieu Nguyen",
      "Kiet Van Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.14850v1",
    "title": "Attention Visualizer Package: Revealing Word Importance for Deeper\n  Insight into Encoder-Only Transformer Models",
    "summary": "This report introduces the Attention Visualizer package, which is crafted to\nvisually illustrate the significance of individual words in encoder-only\ntransformer-based models. In contrast to other methods that center on tokens\nand self-attention scores, our approach will examine the words and their impact\non the final embedding representation. Libraries like this play a crucial role\nin enhancing the interpretability and explainability of neural networks. They\noffer the opportunity to illuminate their internal mechanisms, providing a\nbetter understanding of how they operate and can be enhanced. You can access\nthe code and review examples on the following GitHub repository:\nhttps://github.com/AlaFalaki/AttentionVisualizer.",
    "published": "2023-08-28T19:11:52Z",
    "updated": "2023-08-28T19:11:52Z",
    "authors": [
      "Ala Alam Falaki",
      "Robin Gras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.10115v2",
    "title": "Memorization in Attention-only Transformers",
    "summary": "Recent research has explored the memorization capacity of multi-head\nattention, but these findings are constrained by unrealistic limitations on the\ncontext size. We present a novel proof for language-based Transformers that\nextends the current hypothesis to any context size. Our approach improves upon\nthe state-of-the-art by achieving more effective exact memorization with an\nattention layer, while also introducing the concept of approximate memorization\nof distributions. Through experimental validation, we demonstrate that our\nproposed bounds more accurately reflect the true memorization capacity of\nlanguage models, and provide a precise comparison with prior work.",
    "published": "2024-11-15T11:29:31Z",
    "updated": "2025-03-10T08:40:41Z",
    "authors": [
      "LÃ©o Dana",
      "Muni Sreenivas Pydi",
      "Yann Chevaleyre"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03370v1",
    "title": "Comparison of different Unique hard attention transformer models by the\n  formal languages they can recognize",
    "summary": "This note is a survey of various results on the capabilities of unique hard\nattention transformers encoders (UHATs) to recognize formal languages. We\ndistinguish between masked vs. non-masked, finite vs. infinite image and\ngeneral vs. bilinear attention score functions. We recall some relations\nbetween these models, as well as a lower bound in terms of first-order logic\nand an upper bound in terms of circuit complexity.",
    "published": "2025-06-03T20:28:51Z",
    "updated": "2025-06-03T20:28:51Z",
    "authors": [
      "Leonid Ryvkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04653v2",
    "title": "Deriving Transformer Architectures as Implicit Multinomial Regression",
    "summary": "While attention has been empirically shown to improve model performance, it\nlacks a rigorous mathematical justification. This short paper establishes a\nnovel connection between attention mechanisms and multinomial regression.\nSpecifically, we show that in a fixed multinomial regression setting,\noptimizing over latent features yields solutions that align with the dynamics\ninduced on features by attention blocks. In other words, the evolution of\nrepresentations through a transformer can be interpreted as a trajectory that\nrecovers the optimal features for classification.",
    "published": "2025-09-04T20:40:37Z",
    "updated": "2025-10-27T16:26:55Z",
    "authors": [
      "Jonas A. Actor",
      "Anthony Gruber",
      "Eric C. Cyr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.14319v1",
    "title": "Explicitly Increasing Input Information Density for Vision Transformers\n  on Small Datasets",
    "summary": "Vision Transformers have attracted a lot of attention recently since the\nsuccessful implementation of Vision Transformer (ViT) on vision tasks. With\nvision Transformers, specifically the multi-head self-attention modules,\nnetworks can capture long-term dependencies inherently. However, these\nattention modules normally need to be trained on large datasets, and vision\nTransformers show inferior performance on small datasets when training from\nscratch compared with widely dominant backbones like ResNets. Note that the\nTransformer model was first proposed for natural language processing, which\ncarries denser information than natural images. To boost the performance of\nvision Transformers on small datasets, this paper proposes to explicitly\nincrease the input information density in the frequency domain. Specifically,\nwe introduce selecting channels by calculating the channel-wise heatmaps in the\nfrequency domain using Discrete Cosine Transform (DCT), reducing the size of\ninput while keeping most information and hence increasing the information\ndensity. As a result, 25% fewer channels are kept while better performance is\nachieved compared with previous work. Extensive experiments demonstrate the\neffectiveness of the proposed approach on five small-scale datasets, including\nCIFAR-10/100, SVHN, Flowers-102, and Tiny ImageNet. The accuracy has been\nboosted up to 17.05% with Swin and Focal Transformers. Codes are available at\nhttps://github.com/xiangyu8/DenseVT.",
    "published": "2022-10-25T20:24:53Z",
    "updated": "2022-10-25T20:24:53Z",
    "authors": [
      "Xiangyu Chen",
      "Ying Qin",
      "Wenju Xu",
      "AndrÃ©s M. Bur",
      "Cuncong Zhong",
      "Guanghui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.03107v2",
    "title": "Attention and Encoder-Decoder based models for transforming articulatory\n  movements at different speaking rates",
    "summary": "While speaking at different rates, articulators (like tongue, lips) tend to\nmove differently and the enunciations are also of different durations. In the\npast, affine transformation and DNN have been used to transform articulatory\nmovements from neutral to fast(N2F) and neutral to slow(N2S) speaking rates\n[1]. In this work, we improve over the existing transformation techniques by\nmodeling rate specific durations and their transformation using AstNet, an\nencoder-decoder framework with attention. In the current work, we propose an\nencoder-decoder architecture using LSTMs which generates smoother predicted\narticulatory trajectories. For modeling duration variations across speaking\nrates, we deploy attention network, which eliminates the needto align\ntrajectories in different rates using DTW. We performa phoneme specific\nduration analysis to examine how well duration is transformed using the\nproposed AstNet. As the range of articulatory motions is correlated with\nspeaking rate, we also analyze amplitude of the transformed articulatory\nmovements at different rates compared to their original counterparts, to\nexamine how well the proposed AstNet predicts the extent of articulatory\nmovements in N2F and N2S. We observe that AstNet could model both duration and\nextent of articulatory movements better than the existing transformation\ntechniques resulting in more accurate transformed articulatory trajectories.",
    "published": "2020-06-04T19:33:26Z",
    "updated": "2020-08-20T05:00:07Z",
    "authors": [
      "Abhayjeet Singh",
      "Aravind Illa",
      "Prasanta Kumar Ghosh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.13094v1",
    "title": "Gophormer: Ego-Graph Transformer for Node Classification",
    "summary": "Transformers have achieved remarkable performance in a myriad of fields\nincluding natural language processing and computer vision. However, when it\ncomes to the graph mining area, where graph neural network (GNN) has been the\ndominant paradigm, transformers haven't achieved competitive performance,\nespecially on the node classification task. Existing graph transformer models\ntypically adopt fully-connected attention mechanism on the whole input graph\nand thus suffer from severe scalability issues and are intractable to train in\ndata insufficient cases. To alleviate these issues, we propose a novel\nGophormer model which applies transformers on ego-graphs instead of\nfull-graphs. Specifically, Node2Seq module is proposed to sample ego-graphs as\nthe input of transformers, which alleviates the challenge of scalability and\nserves as an effective data augmentation technique to boost model performance.\nMoreover, different from the feature-based attention strategy in vanilla\ntransformers, we propose a proximity-enhanced attention mechanism to capture\nthe fine-grained structural bias. In order to handle the uncertainty introduced\nby the ego-graph sampling, we further propose a consistency regularization and\na multi-sample inference strategy for stabilized training and testing,\nrespectively. Extensive experiments on six benchmark datasets are conducted to\ndemonstrate the superiority of Gophormer over existing graph transformers and\npopular GNNs, revealing the promising future of graph transformers.",
    "published": "2021-10-25T16:43:32Z",
    "updated": "2021-10-25T16:43:32Z",
    "authors": [
      "Jianan Zhao",
      "Chaozhuo Li",
      "Qianlong Wen",
      "Yiqi Wang",
      "Yuming Liu",
      "Hao Sun",
      "Xing Xie",
      "Yanfang Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.01745v1",
    "title": "SEFormer: Structure Embedding Transformer for 3D Object Detection",
    "summary": "Effectively preserving and encoding structure features from objects in\nirregular and sparse LiDAR points is a key challenge to 3D object detection on\npoint cloud. Recently, Transformer has demonstrated promising performance on\nmany 2D and even 3D vision tasks. Compared with the fixed and rigid convolution\nkernels, the self-attention mechanism in Transformer can adaptively exclude the\nunrelated or noisy points and thus suitable for preserving the local spatial\nstructure in irregular LiDAR point cloud. However, Transformer only performs a\nsimple sum on the point features, based on the self-attention mechanism, and\nall the points share the same transformation for value. Such isotropic\noperation lacks the ability to capture the direction-distance-oriented local\nstructure which is important for 3D object detection. In this work, we propose\na Structure-Embedding transFormer (SEFormer), which can not only preserve local\nstructure as traditional Transformer but also have the ability to encode the\nlocal structure. Compared to the self-attention mechanism in traditional\nTransformer, SEFormer learns different feature transformations for value points\nbased on the relative directions and distances to the query point. Then we\npropose a SEFormer based network for high-performance 3D object detection.\nExtensive experiments show that the proposed architecture can achieve SOTA\nresults on Waymo Open Dataset, the largest 3D detection benchmark for\nautonomous driving. Specifically, SEFormer achieves 79.02% mAP, which is 1.2%\nhigher than existing works. We will release the codes.",
    "published": "2022-09-05T03:38:12Z",
    "updated": "2022-09-05T03:38:12Z",
    "authors": [
      "Xiaoyu Feng",
      "Heming Du",
      "Yueqi Duan",
      "Yongpan Liu",
      "Hehe Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.01694v1",
    "title": "Spike-driven Transformer",
    "summary": "Spiking Neural Networks (SNNs) provide an energy-efficient deep learning\noption due to their unique spike-based event-driven (i.e., spike-driven)\nparadigm. In this paper, we incorporate the spike-driven paradigm into\nTransformer by the proposed Spike-driven Transformer with four unique\nproperties: 1) Event-driven, no calculation is triggered when the input of\nTransformer is zero; 2) Binary spike communication, all matrix multiplications\nassociated with the spike matrix can be transformed into sparse additions; 3)\nSelf-attention with linear complexity at both token and channel dimensions; 4)\nThe operations between spike-form Query, Key, and Value are mask and addition.\nTogether, there are only sparse addition operations in the Spike-driven\nTransformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA),\nwhich exploits only mask and addition operations without any multiplication,\nand thus having up to $87.2\\times$ lower computation energy than vanilla\nself-attention. Especially in SDSA, the matrix multiplication between Query,\nKey, and Value is designed as the mask operation. In addition, we rearrange all\nresidual connections in the vanilla Transformer before the activation functions\nto ensure that all neurons transmit binary spike signals. It is shown that the\nSpike-driven Transformer can achieve 77.1\\% top-1 accuracy on ImageNet-1K,\nwhich is the state-of-the-art result in the SNN field. The source code is\navailable at https://github.com/BICLab/Spike-Driven-Transformer.",
    "published": "2023-07-04T13:00:18Z",
    "updated": "2023-07-04T13:00:18Z",
    "authors": [
      "Man Yao",
      "Jiakui Hu",
      "Zhaokun Zhou",
      "Li Yuan",
      "Yonghong Tian",
      "Bo Xu",
      "Guoqi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.18199v1",
    "title": "Rethinking Attention Gated with Hybrid Dual Pyramid Transformer-CNN for\n  Generalized Segmentation in Medical Imaging",
    "summary": "Inspired by the success of Transformers in Computer vision, Transformers have\nbeen widely investigated for medical imaging segmentation. However, most of\nTransformer architecture are using the recent transformer architectures as\nencoder or as parallel encoder with the CNN encoder. In this paper, we\nintroduce a novel hybrid CNN-Transformer segmentation architecture\n(PAG-TransYnet) designed for efficiently building a strong CNN-Transformer\nencoder. Our approach exploits attention gates within a Dual Pyramid hybrid\nencoder. The contributions of this methodology can be summarized into three key\naspects: (i) the utilization of Pyramid input for highlighting the prominent\nfeatures at different scales, (ii) the incorporation of a PVT transformer to\ncapture long-range dependencies across various resolutions, and (iii) the\nimplementation of a Dual-Attention Gate mechanism for effectively fusing\nprominent features from both CNN and Transformer branches. Through\ncomprehensive evaluation across different segmentation tasks including:\nabdominal multi-organs segmentation, infection segmentation (Covid-19 and Bone\nMetastasis), microscopic tissues segmentation (Gland and Nucleus). The proposed\napproach demonstrates state-of-the-art performance and exhibits remarkable\ngeneralization capabilities. This research represents a significant advancement\ntowards addressing the pressing need for efficient and adaptable segmentation\nsolutions in medical imaging applications.",
    "published": "2024-04-28T14:37:10Z",
    "updated": "2024-04-28T14:37:10Z",
    "authors": [
      "Fares Bougourzi",
      "Fadi Dornaika",
      "Abdelmalik Taleb-Ahmed",
      "Vinh Truong Hoang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.06443v2",
    "title": "Residual-based Attention Physics-informed Neural Networks for\n  Spatio-Temporal Ageing Assessment of Transformers Operated in Renewable Power\n  Plants",
    "summary": "Transformers are crucial for reliable and efficient power system operations,\nparticularly in supporting the integration of renewable energy. Effective\nmonitoring of transformer health is critical to maintain grid stability and\nperformance. Thermal insulation ageing is a key transformer failure mode, which\nis generally tracked by monitoring the hotspot temperature (HST). However, HST\nmeasurement is complex, costly, and often estimated from indirect measurements.\nExisting HST models focus on space-agnostic thermal models, providing\nworst-case HST estimates. This article introduces a spatio-temporal model for\ntransformer winding temperature and ageing estimation, which leverages\nphysics-based partial differential equations (PDEs) with data-driven Neural\nNetworks (NN) in a Physics Informed Neural Networks (PINNs) configuration to\nimprove prediction accuracy and acquire spatio-temporal resolution. The\ncomputational accuracy of the PINN model is improved through the implementation\nof the Residual-Based Attention (PINN-RBA) scheme that accelerates the PINN\nmodel convergence. The PINN-RBA model is benchmarked against self-adaptive\nattention schemes and classical vanilla PINN configurations. For the first\ntime, PINN based oil temperature predictions are used to estimate\nspatio-temporal transformer winding temperature values, validated through PDE\nnumerical solution and fiber optic sensor measurements. Furthermore, the\nspatio-temporal transformer ageing model is inferred, which supports\ntransformer health management decision-making. Results are validated with a\ndistribution transformer operating on a floating photovoltaic power plant.",
    "published": "2024-05-10T12:48:57Z",
    "updated": "2024-10-03T15:34:02Z",
    "authors": [
      "Ibai Ramirez",
      "Joel Pino",
      "David Pardo",
      "Mikel Sanz",
      "Luis del Rio",
      "Alvaro Ortiz",
      "Kateryna Morozovska",
      "Jose I. Aizpurua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.01290v2",
    "title": "Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space",
    "summary": "Hyperbolic geometry have shown significant potential in modeling complex\nstructured data, particularly those with underlying tree-like and hierarchical\nstructures. Despite the impressive performance of various hyperbolic neural\nnetworks across numerous domains, research on adapting the Transformer to\nhyperbolic space remains limited. Previous attempts have mainly focused on\nmodifying self-attention modules in the Transformer. However, these efforts\nhave fallen short of developing a complete hyperbolic Transformer. This stems\nprimarily from: (i) the absence of well-defined modules in hyperbolic space,\nincluding linear transformation layers, LayerNorm layers, activation functions,\ndropout operations, etc. (ii) the quadratic time complexity of the existing\nhyperbolic self-attention module w.r.t the number of input tokens, which\nhinders its scalability. To address these challenges, we propose, Hypformer, a\nnovel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry.\nIn Hypformer, we introduce two foundational blocks that define the essential\nmodules of the Transformer in hyperbolic space. Furthermore, we develop a\nlinear self-attention mechanism in hyperbolic space, enabling hyperbolic\nTransformer to process billion-scale graph data and long-sequence inputs for\nthe first time. Our experimental results confirm the effectiveness and\nefficiency of Hypformer across various datasets, demonstrating its potential as\nan effective and scalable solution for large-scale data representation and\nlarge models.",
    "published": "2024-07-01T13:44:38Z",
    "updated": "2025-08-24T12:17:27Z",
    "authors": [
      "Menglin Yang",
      "Harshit Verma",
      "Delvin Ce Zhang",
      "Jiahong Liu",
      "Irwin King",
      "Rex Ying"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.21698v1",
    "title": "On the Role of Depth and Looping for In-Context Learning with Task\n  Diversity",
    "summary": "The intriguing in-context learning (ICL) abilities of deep Transformer models\nhave lately garnered significant attention. By studying in-context linear\nregression on unimodal Gaussian data, recent empirical and theoretical works\nhave argued that ICL emerges from Transformers' abilities to simulate learning\nalgorithms like gradient descent. However, these works fail to capture the\nremarkable ability of Transformers to learn multiple tasks in context. To this\nend, we study in-context learning for linear regression with diverse tasks,\ncharacterized by data covariance matrices with condition numbers ranging from\n$[1, \\kappa]$, and highlight the importance of depth in this setting. More\nspecifically, (a) we show theoretical lower bounds of $\\log(\\kappa)$ (or\n$\\sqrt{\\kappa}$) linear attention layers in the unrestricted (or restricted)\nattention setting and, (b) we show that multilayer Transformers can indeed\nsolve such tasks with a number of layers that matches the lower bounds.\nHowever, we show that this expressivity of multilayer Transformer comes at the\nprice of robustness. In particular, multilayer Transformers are not robust to\neven distributional shifts as small as $O(e^{-L})$ in Wasserstein distance,\nwhere $L$ is the depth of the network. We then demonstrate that Looped\nTransformers -- a special class of multilayer Transformers with weight-sharing\n-- not only exhibit similar expressive power but are also provably robust under\nmild assumptions. Besides out-of-distribution generalization, we also show that\nLooped Transformers are the only models that exhibit a monotonic behavior of\nloss with respect to depth.",
    "published": "2024-10-29T03:27:56Z",
    "updated": "2024-10-29T03:27:56Z",
    "authors": [
      "Khashayar Gatmiry",
      "Nikunj Saunshi",
      "Sashank J. Reddi",
      "Stefanie Jegelka",
      "Sanjiv Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.03465v2",
    "title": "DTU-Net: A Multi-Scale Dilated Transformer Network for Nonlinear\n  Hyperspectral Unmixing",
    "summary": "Transformers have shown significant success in hyperspectral unmixing (HU).\nHowever, challenges remain. While multi-scale and long-range spatial\ncorrelations are essential in unmixing tasks, current Transformer-based\nunmixing networks, built on Vision Transformer (ViT) or Swin-Transformer,\nstruggle to capture them effectively. Additionally, current Transformer-based\nunmixing networks rely on the linear mixing model, which lacks the flexibility\nto accommodate scenarios where nonlinear effects are significant. To address\nthese limitations, we propose a multi-scale Dilated Transformer-based unmixing\nnetwork for nonlinear HU (DTU-Net). The encoder employs two branches. The first\none performs multi-scale spatial feature extraction using Multi-Scale Dilated\nAttention (MSDA) in the Dilated Transformer, which varies dilation rates across\nattention heads to capture long-range and multi-scale spatial correlations. The\nsecond one performs spectral feature extraction utilizing 3D-CNNs with channel\nattention. The outputs from both branches are then fused to integrate\nmulti-scale spatial and spectral information, which is subsequently transformed\nto estimate the abundances. The decoder is designed to accommodate both linear\nand nonlinear mixing scenarios. Its interpretability is enhanced by explicitly\nmodeling the relationships between endmembers, abundances, and nonlinear\ncoefficients in accordance with the polynomial post-nonlinear mixing model\n(PPNMM). Experiments on synthetic and real datasets validate the effectiveness\nof the proposed DTU-Net compared to PPNMM-derived methods and several advanced\nunmixing networks.",
    "published": "2025-03-05T12:56:33Z",
    "updated": "2025-03-06T02:55:33Z",
    "authors": [
      "ChenTong Wang",
      "Jincheng Gao",
      "Fei Zhu",
      "Abderrahim Halimi",
      "CÃ©dric Richard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.12588v2",
    "title": "Plain Transformers Can be Powerful Graph Learners",
    "summary": "Transformers have attained outstanding performance across various modalities,\nowing to their simple but powerful scaled-dot-product (SDP) attention\nmechanisms. Researchers have attempted to migrate Transformers to graph\nlearning, but most advanced Graph Transformers (GTs) have strayed far from\nplain Transformers, exhibiting major architectural differences either by\nintegrating message-passing or incorporating sophisticated attention\nmechanisms. These divergences hinder the easy adoption of training advances for\nTransformers developed in other domains. Contrary to previous GTs, this work\ndemonstrates that the plain Transformer architecture can be a powerful graph\nlearner. To achieve this, we propose to incorporate three simple, minimal, and\neasy-to-implement modifications to the plain Transformer architecture to\nconstruct our Powerful Plain Graph Transformers (PPGT): (1) simplified $L_2$\nattention for measuring the magnitude closeness among tokens; (2) adaptive\nroot-mean-square normalization to preserve token magnitude information; and (3)\na simple MLP-based stem for graph positional encoding. Consistent with its\ntheoretical expressivity, PPGT demonstrates noteworthy realized expressivity on\nthe empirical graph expressivity benchmark, comparing favorably to more\ncomplicated competitors such as subgraph GNNs and higher-order GNNs. Its\noutstanding empirical performance across various graph datasets also justifies\nthe practical effectiveness of PPGT.",
    "published": "2025-04-17T02:06:50Z",
    "updated": "2025-05-20T20:36:09Z",
    "authors": [
      "Liheng Ma",
      "Soumyasundar Pal",
      "Yingxue Zhang",
      "Philip H. S. Torr",
      "Mark Coates"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.13220v1",
    "title": "SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor\n  Imagery Classification",
    "summary": "Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor\nimagery classification offer promising solutions in neurorehabilitation and\nassistive technologies by enabling communication between the brain and external\ndevices. However, the non-stationary nature of EEG signals and significant\ninter-subject variability cause substantial challenges for developing robust\ncross-subject classification models. This paper introduces a novel\nSpatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically\ndesigned for upper-limb motor imagery classification. Our architecture consists\nof a spectral transformer and a spatial transformer, followed by a transformer\nblock and a classifier network. Each module is integrated with attention\nmechanisms that dynamically attend to the most discriminative patterns across\nmultiple domains, such as spectral frequencies, spatial electrode locations,\nand temporal dynamics. The short-time Fourier transform is incorporated to\nextract features in the time-frequency domain to make it easier for the model\nto obtain a better feature distinction. We evaluated our SSTAF Transformer\nmodel on two publicly available datasets, the EEGMMIDB dataset, and BCI\nCompetition IV-2a. SSTAF Transformer achieves an accuracy of 76.83% and 68.30%\nin the data sets, respectively, outperforms traditional CNN-based architectures\nand a few existing transformer-based approaches.",
    "published": "2025-04-17T07:45:14Z",
    "updated": "2025-04-17T07:45:14Z",
    "authors": [
      "Ummay Maria Muna",
      "Md. Mehedi Hasan Shawon",
      "Md Jobayer",
      "Sumaiya Akter",
      "Saifur Rahman Sabuj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.03201v6",
    "title": "nnFormer: Interleaved Transformer for Volumetric Segmentation",
    "summary": "Transformer, the model of choice for natural language processing, has drawn\nscant attention from the medical imaging community. Given the ability to\nexploit long-term dependencies, transformers are promising to help atypical\nconvolutional neural networks to overcome their inherent shortcomings of\nspatial inductive bias. However, most of recently proposed transformer-based\nsegmentation approaches simply treated transformers as assisted modules to help\nencode global context into convolutional representations. To address this\nissue, we introduce nnFormer, a 3D transformer for volumetric medical image\nsegmentation. nnFormer not only exploits the combination of interleaved\nconvolution and self-attention operations, but also introduces local and global\nvolume-based self-attention mechanism to learn volume representations.\nMoreover, nnFormer proposes to use skip attention to replace the traditional\nconcatenation/summation operations in skip connections in U-Net like\narchitecture. Experiments show that nnFormer significantly outperforms previous\ntransformer-based counterparts by large margins on three public datasets.\nCompared to nnUNet, nnFormer produces significantly lower HD95 and comparable\nDSC results. Furthermore, we show that nnFormer and nnUNet are highly\ncomplementary to each other in model ensembling.",
    "published": "2021-09-07T17:08:24Z",
    "updated": "2022-02-04T06:53:37Z",
    "authors": [
      "Hong-Yu Zhou",
      "Jiansen Guo",
      "Yinghao Zhang",
      "Lequan Yu",
      "Liansheng Wang",
      "Yizhou Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.01615v4",
    "title": "Lawin Transformer: Improving Semantic Segmentation Transformer with\n  Multi-Scale Representations via Large Window Attention",
    "summary": "Multi-scale representations are crucial for semantic segmentation. The\ncommunity has witnessed the flourish of semantic segmentation convolutional\nneural networks (CNN) exploiting multi-scale contextual information. Motivated\nby that the vision transformer (ViT) is powerful in image classification, some\nsemantic segmentation ViTs are recently proposed, most of them attaining\nimpressive results but at a cost of computational economy. In this paper, we\nsucceed in introducing multi-scale representations into semantic segmentation\nViT via window attention mechanism and further improves the performance and\nefficiency. To this end, we introduce large window attention which allows the\nlocal window to query a larger area of context window at only a little\ncomputation overhead. By regulating the ratio of the context area to the query\narea, we enable the $\\textit{large window attention}$ to capture the contextual\ninformation at multiple scales. Moreover, the framework of spatial pyramid\npooling is adopted to collaborate with $\\textit{the large window attention}$,\nwhich presents a novel decoder named $\\textbf{la}$rge $\\textbf{win}$dow\nattention spatial pyramid pooling (LawinASPP) for semantic segmentation ViT.\nOur resulting ViT, Lawin Transformer, is composed of an efficient hierachical\nvision transformer (HVT) as encoder and a LawinASPP as decoder. The empirical\nresults demonstrate that Lawin Transformer offers an improved efficiency\ncompared to the existing method. Lawin Transformer further sets new\nstate-of-the-art performance on Cityscapes (84.4% mIoU), ADE20K (56.2% mIoU)\nand COCO-Stuff datasets. The code will be released at\nhttps://github.com/yan-hao-tian/lawin",
    "published": "2022-01-05T13:51:20Z",
    "updated": "2023-08-09T14:15:32Z",
    "authors": [
      "Haotian Yan",
      "Chuang Zhang",
      "Ming Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.14415v4",
    "title": "Non-stationary Transformers: Exploring the Stationarity in Time Series\n  Forecasting",
    "summary": "Transformers have shown great power in time series forecasting due to their\nglobal-range modeling ability. However, their performance can degenerate\nterribly on non-stationary real-world data in which the joint distribution\nchanges over time. Previous studies primarily adopt stationarization to\nattenuate the non-stationarity of original series for better predictability.\nBut the stationarized series deprived of inherent non-stationarity can be less\ninstructive for real-world bursty events forecasting. This problem, termed\nover-stationarization in this paper, leads Transformers to generate\nindistinguishable temporal attentions for different series and impedes the\npredictive capability of deep models. To tackle the dilemma between series\npredictability and model capability, we propose Non-stationary Transformers as\na generic framework with two interdependent modules: Series Stationarization\nand De-stationary Attention. Concretely, Series Stationarization unifies the\nstatistics of each input and converts the output with restored statistics for\nbetter predictability. To address the over-stationarization problem,\nDe-stationary Attention is devised to recover the intrinsic non-stationary\ninformation into temporal dependencies by approximating distinguishable\nattentions learned from raw series. Our Non-stationary Transformers framework\nconsistently boosts mainstream Transformers by a large margin, which reduces\nMSE by 49.43% on Transformer, 47.34% on Informer, and 46.89% on Reformer,\nmaking them the state-of-the-art in time series forecasting. Code is available\nat this repository: https://github.com/thuml/Nonstationary_Transformers.",
    "published": "2022-05-28T12:27:27Z",
    "updated": "2023-11-24T09:01:12Z",
    "authors": [
      "Yong Liu",
      "Haixu Wu",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.16961v5",
    "title": "Pattern Attention Transformer with Doughnut Kernel",
    "summary": "We present in this paper a new architecture, the Pattern Attention\nTransformer (PAT), that is composed of the new doughnut kernel. Compared with\ntokens in the NLP field, Transformer in computer vision has the problem of\nhandling the high resolution of pixels in images. In ViT, an image is cut into\nsquare-shaped patches. As the follow-up of ViT, Swin Transformer proposes an\nadditional step of shifting to decrease the existence of fixed boundaries,\nwhich also incurs 'two connected Swin Transformer blocks' as the minimum unit\nof the model. Inheriting the patch/window idea, our doughnut kernel enhances\nthe design of patches further. It replaces the line-cut boundaries with two\ntypes of areas: sensor and updating, which is based on the comprehension of\nself-attention (named QKVA grid). The doughnut kernel also brings a new topic\nabout the shape of kernels beyond square. To verify its performance on image\nclassification, PAT is designed with Transformer blocks of regular octagon\nshape doughnut kernels. Its architecture is lighter: the minimum pattern\nattention layer is only one for each stage. Under similar complexity of\ncomputation, its performances on ImageNet 1K reach higher throughput (+10%) and\nsurpass Swin Transformer (+0.8 acc1).",
    "published": "2022-11-30T13:11:46Z",
    "updated": "2023-09-17T13:43:53Z",
    "authors": [
      "WenYuan Sheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.14735v2",
    "title": "How Transformers Learn Causal Structure with Gradient Descent",
    "summary": "The incredible success of transformers on sequence modeling tasks can be\nlargely attributed to the self-attention mechanism, which allows information to\nbe transferred between different parts of a sequence. Self-attention allows\ntransformers to encode causal structure which makes them particularly suitable\nfor sequence modeling. However, the process by which transformers learn such\ncausal structure via gradient-based training algorithms remains poorly\nunderstood. To better understand this process, we introduce an in-context\nlearning task that requires learning latent causal structure. We prove that\ngradient descent on a simplified two-layer transformer learns to solve this\ntask by encoding the latent causal graph in the first attention layer. The key\ninsight of our proof is that the gradient of the attention matrix encodes the\nmutual information between tokens. As a consequence of the data processing\ninequality, the largest entries of this gradient correspond to edges in the\nlatent causal graph. As a special case, when the sequences are generated from\nin-context Markov chains, we prove that transformers learn an induction head\n(Olsson et al., 2022). We confirm our theoretical findings by showing that\ntransformers trained on our in-context learning task are able to recover a wide\nvariety of causal structures.",
    "published": "2024-02-22T17:47:03Z",
    "updated": "2024-08-13T15:45:37Z",
    "authors": [
      "Eshaan Nichani",
      "Alex Damian",
      "Jason D. Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.12982v1",
    "title": "DuoFormer: Leveraging Hierarchical Representations by Local and Global\n  Attention Vision Transformer",
    "summary": "Despite the widespread adoption of transformers in medical applications, the\nexploration of multi-scale learning through transformers remains limited, while\nhierarchical representations are considered advantageous for computer-aided\nmedical diagnosis. We propose a novel hierarchical transformer model that\nadeptly integrates the feature extraction capabilities of Convolutional Neural\nNetworks (CNNs) with the advanced representational potential of Vision\nTransformers (ViTs). Addressing the lack of inductive biases and dependence on\nextensive training datasets in ViTs, our model employs a CNN backbone to\ngenerate hierarchical visual representations. These representations are adapted\nfor transformer input through an innovative patch tokenization process,\npreserving the inherited multi-scale inductive biases. We also introduce a\nscale-wise attention mechanism that directly captures intra-scale and\ninter-scale associations. This mechanism complements patch-wise attention by\nenhancing spatial understanding and preserving global perception, which we\nrefer to as local and global attention, respectively. Our model significantly\noutperforms baseline models in terms of classification accuracy, demonstrating\nits efficiency in bridging the gap between Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). The components are designed as plug-and-play\nfor different CNN architectures and can be adapted for multiple applications.\nThe code is available at https://github.com/xiaoyatang/DuoFormer.git.",
    "published": "2025-06-15T22:42:57Z",
    "updated": "2025-06-15T22:42:57Z",
    "authors": [
      "Xiaoya Tang",
      "Bodong Zhang",
      "Man Minh Ho",
      "Beatrice S. Knudsen",
      "Tolga Tasdizen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.17671v2",
    "title": "TPTT: Transforming Pretrained Transformers into Titans",
    "summary": "Transformer-based large language models (LLMs) have achieved strong\nperformance across many natural language processing tasks. Nonetheless, their\nquadratic computational and memory requirements, particularly in self-attention\nlayers, pose challenges for efficient inference on long contexts and for\ndeployment in resource-limited environments. We present TPTT (Transforming\nPretrained Transformers into Titans), a framework designed to augment\npretrained Transformers with linearized attention (LiZA) and internal memory\ngating via Memory as Gate (MaG), applied without full retraining. TPTT supports\nparameter-efficient fine-tuning (LoRA) and integrates with standard toolkits\nsuch as Hugging Face Transformers. We evaluated TPTT on several pretrained\nmodels, including Llama-1B, OlMoE-1B-7B, Qwen2.5-1.5B, Gemma3-270m,\nOpenELM-1.3B, and Mistral-7B, in order to assess applicability across\narchitectures of different scales. Experiments on models with approximately 1\nbillion parameters, evaluated primarily on the MMLU benchmark, suggest\npotential improvements in both efficiency and accuracy compared to baseline\nmodels. For example, Titans-Llama-1B exhibited up to a 20\\% relative increase\nin Exact Match scores in one-shot evaluation. An additional finding is that it\nis possible to convert a quadratic-attention model into a purely\nlinear-attention model using the DeltaProduct mechanism. All training runs were\ncarried out with modest computational resources. These preliminary findings\nindicate that TPTT may help adapt pretrained LLMs for long-context tasks with\nlimited overhead. Further studies on larger models and a broader set of\nbenchmarks will be necessary to evaluate the generality and robustness of the\nframework. Code is available at https://github.com/fabienfrfr/tptt . Python\npackage at https://pypi.org/project/tptt/ .",
    "published": "2025-06-21T10:06:07Z",
    "updated": "2025-08-31T14:32:19Z",
    "authors": [
      "Fabien Furfaro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.06931v1",
    "title": "Stand-Alone Inter-Frame Attention in Video Models",
    "summary": "Motion, as the uniqueness of a video, has been critical to the development of\nvideo understanding models. Modern deep learning models leverage motion by\neither executing spatio-temporal 3D convolutions, factorizing 3D convolutions\ninto spatial and temporal convolutions separately, or computing self-attention\nalong temporal dimension. The implicit assumption behind such successes is that\nthe feature maps across consecutive frames can be nicely aggregated.\nNevertheless, the assumption may not always hold especially for the regions\nwith large deformation. In this paper, we present a new recipe of inter-frame\nattention block, namely Stand-alone Inter-Frame Attention (SIFA), that novelly\ndelves into the deformation across frames to estimate local self-attention on\neach spatial location. Technically, SIFA remoulds the deformable design via\nre-scaling the offset predictions by the difference between two frames. Taking\neach spatial location in the current frame as the query, the locally deformable\nneighbors in the next frame are regarded as the keys/values. Then, SIFA\nmeasures the similarity between query and keys as stand-alone attention to\nweighted average the values for temporal aggregation. We further plug SIFA\nblock into ConvNets and Vision Transformer, respectively, to devise SIFA-Net\nand SIFA-Transformer. Extensive experiments conducted on four video datasets\ndemonstrate the superiority of SIFA-Net and SIFA-Transformer as stronger\nbackbones. More remarkably, SIFA-Transformer achieves an accuracy of 83.1% on\nKinetics-400 dataset. Source code is available at\n\\url{https://github.com/FuchenUSTC/SIFA}.",
    "published": "2022-06-14T15:51:28Z",
    "updated": "2022-06-14T15:51:28Z",
    "authors": [
      "Fuchen Long",
      "Zhaofan Qiu",
      "Yingwei Pan",
      "Ting Yao",
      "Jiebo Luo",
      "Tao Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14995v4",
    "title": "Choose a Transformer: Fourier or Galerkin",
    "summary": "In this paper, we apply the self-attention from the state-of-the-art\nTransformer in Attention Is All You Need for the first time to a data-driven\noperator learning problem related to partial differential equations. An effort\nis put together to explain the heuristics of, and to improve the efficacy of\nthe attention mechanism. By employing the operator approximation theory in\nHilbert spaces, it is demonstrated for the first time that the softmax\nnormalization in the scaled dot-product attention is sufficient but not\nnecessary. Without softmax, the approximation capacity of a linearized\nTransformer variant can be proved to be comparable to a Petrov-Galerkin\nprojection layer-wise, and the estimate is independent with respect to the\nsequence length. A new layer normalization scheme mimicking the Petrov-Galerkin\nprojection is proposed to allow a scaling to propagate through attention\nlayers, which helps the model achieve remarkable accuracy in operator learning\ntasks with unnormalized data. Finally, we present three operator learning\nexperiments, including the viscid Burgers' equation, an interface Darcy flow,\nand an inverse interface coefficient identification problem. The newly proposed\nsimple attention-based operator learner, Galerkin Transformer, shows\nsignificant improvements in both training cost and evaluation accuracy over its\nsoftmax-normalized counterparts.",
    "published": "2021-05-31T14:30:53Z",
    "updated": "2021-11-01T15:43:32Z",
    "authors": [
      "Shuhao Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.03180v5",
    "title": "Vision Transformers with Hierarchical Attention",
    "summary": "This paper tackles the high computational/space complexity associated with\nMulti-Head Self-Attention (MHSA) in vanilla vision transformers. To this end,\nwe propose Hierarchical MHSA (H-MHSA), a novel approach that computes\nself-attention in a hierarchical fashion. Specifically, we first divide the\ninput image into patches as commonly done, and each patch is viewed as a token.\nThen, the proposed H-MHSA learns token relationships within local patches,\nserving as local relationship modeling. Then, the small patches are merged into\nlarger ones, and H-MHSA models the global dependencies for the small number of\nthe merged tokens. At last, the local and global attentive features are\naggregated to obtain features with powerful representation capacity. Since we\nonly calculate attention for a limited number of tokens at each step, the\ncomputational load is reduced dramatically. Hence, H-MHSA can efficiently model\nglobal relationships among tokens without sacrificing fine-grained information.\nWith the H-MHSA module incorporated, we build a family of\nHierarchical-Attention-based Transformer Networks, namely HAT-Net. To\ndemonstrate the superiority of HAT-Net in scene understanding, we conduct\nextensive experiments on fundamental vision tasks, including image\nclassification, semantic segmentation, object detection, and instance\nsegmentation. Therefore, HAT-Net provides a new perspective for vision\ntransformers. Code and pretrained models are available at\nhttps://github.com/yun-liu/HAT-Net.",
    "published": "2021-06-06T17:01:13Z",
    "updated": "2024-03-26T07:44:45Z",
    "authors": [
      "Yun Liu",
      "Yu-Huan Wu",
      "Guolei Sun",
      "Le Zhang",
      "Ajad Chhatkuli",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.05274v2",
    "title": "TransAttUnet: Multi-level Attention-guided U-Net with Transformer for\n  Medical Image Segmentation",
    "summary": "Accurate segmentation of organs or lesions from medical images is crucial for\nreliable diagnosis of diseases and organ morphometry. In recent years,\nconvolutional encoder-decoder solutions have achieved substantial progress in\nthe field of automatic medical image segmentation. Due to the inherent bias in\nthe convolution operations, prior models mainly focus on local visual cues\nformed by the neighboring pixels, but fail to fully model the long-range\ncontextual dependencies. In this paper, we propose a novel Transformer-based\nAttention Guided Network called TransAttUnet, in which the multi-level guided\nattention and multi-scale skip connection are designed to jointly enhance the\nperformance of the semantical segmentation architecture. Inspired by\nTransformer, the self-aware attention (SAA) module with Transformer Self\nAttention (TSA) and Global Spatial Attention (GSA) is incorporated into\nTransAttUnet to effectively learn the non-local interactions among encoder\nfeatures. Moreover, we also use additional multi-scale skip connections between\ndecoder blocks to aggregate the upsampled features with different semantic\nscales. In this way, the representation ability of multi-scale context\ninformation is strengthened to generate discriminative features. Benefitting\nfrom these complementary components, the proposed TransAttUnet can effectively\nalleviate the loss of fine details caused by the stacking of convolution layers\nand the consecutive sampling operations, finally improving the segmentation\nquality of medical images. Extensive experiments on multiple medical image\nsegmentation datasets from different imaging modalities demonstrate that the\nproposed method consistently outperforms the state-of-the-art baselines. Our\ncode and pre-trained models are available at:\nhttps://github.com/YishuLiu/TransAttUnet.",
    "published": "2021-07-12T09:17:06Z",
    "updated": "2022-07-09T03:28:13Z",
    "authors": [
      "Bingzhi Chen",
      "Yishu Liu",
      "Zheng Zhang",
      "Guangming Lu",
      "Adams Wai Kin Kong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.09310v2",
    "title": "Energon: Towards Efficient Acceleration of Transformers Using Dynamic\n  Sparse Attention",
    "summary": "In recent years, transformer models have revolutionized Natural Language\nProcessing (NLP) and shown promising performance on Computer Vision (CV) tasks.\nDespite their effectiveness, transformers' attention operations are hard to\naccelerate due to the complicated data movement and quadratic computational\ncomplexity, prohibiting the real-time inference on resource-constrained\nedge-computing platforms.\n  To tackle this challenge, we propose Energon, an algorithm-architecture\nco-design approach that accelerates various transformers using dynamic sparse\nattention. With the observation that attention results only depend on a few\nimportant query-key pairs, we propose a Mix-Precision Multi-Round Filtering\n(MP-MRF) algorithm to dynamically identify such pairs at runtime. We adopt low\nbitwidth in each filtering round and only use high-precision tensors in the\nattention stage to reduce overall complexity. By this means, we significantly\nmitigate the computational cost with negligible accuracy loss. To enable such\nan algorithm with lower latency and better energy efficiency, we also propose\nan Energon co-processor architecture. Elaborated pipelines and specialized\noptimizations jointly boost the performance and reduce power consumption.\nExtensive experiments on both NLP and CV benchmarks demonstrate that Energon\nachieves $168\\times$ and $8.7\\times$ geo-mean speedup and up to $10^4\\times$\nand $10^3\\times$ energy reduction compared with Intel Xeon 5220 CPU and NVIDIA\nV100 GPU. Compared to state-of-the-art attention accelerators SpAtten and\n$A^3$, Energon also achieves $1.7\\times, 1.25\\times$ speedup and $1.6 \\times,\n1.5\\times $ higher energy efficiency.",
    "published": "2021-10-18T13:42:43Z",
    "updated": "2022-04-25T12:29:17Z",
    "authors": [
      "Zhe Zhou",
      "Junlin Liu",
      "Zhenyu Gu",
      "Guangyu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.13087v2",
    "title": "BoxeR: Box-Attention for 2D and 3D Transformers",
    "summary": "In this paper, we propose a simple attention mechanism, we call\nbox-attention. It enables spatial interaction between grid features, as sampled\nfrom boxes of interest, and improves the learning capability of transformers\nfor several vision tasks. Specifically, we present BoxeR, short for Box\nTransformer, which attends to a set of boxes by predicting their transformation\nfrom a reference window on an input feature map. The BoxeR computes attention\nweights on these boxes by considering its grid structure. Notably, BoxeR-2D\nnaturally reasons about box information within its attention module, making it\nsuitable for end-to-end instance detection and segmentation tasks. By learning\ninvariance to rotation in the box-attention module, BoxeR-3D is capable of\ngenerating discriminative information from a bird's-eye view plane for 3D\nend-to-end object detection. Our experiments demonstrate that the proposed\nBoxeR-2D achieves state-of-the-art results on COCO detection and instance\nsegmentation. Besides, BoxeR-3D improves over the end-to-end 3D object\ndetection baseline and already obtains a compelling performance for the vehicle\ncategory of Waymo Open, without any class-specific optimization. Code is\navailable at https://github.com/kienduynguyen/BoxeR.",
    "published": "2021-11-25T13:54:25Z",
    "updated": "2022-03-25T09:42:32Z",
    "authors": [
      "Duy-Kien Nguyen",
      "Jihong Ju",
      "Olaf Booij",
      "Martin R. Oswald",
      "Cees G. M. Snoek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.13515v2",
    "title": "Green Hierarchical Vision Transformer for Masked Image Modeling",
    "summary": "We present an efficient approach for Masked Image Modeling (MIM) with\nhierarchical Vision Transformers (ViTs), allowing the hierarchical ViTs to\ndiscard masked patches and operate only on the visible ones. Our approach\nconsists of three key designs. First, for window attention, we propose a Group\nWindow Attention scheme following the Divide-and-Conquer strategy. To mitigate\nthe quadratic complexity of the self-attention w.r.t. the number of patches,\ngroup attention encourages a uniform partition that visible patches within each\nlocal window of arbitrary size can be grouped with equal size, where masked\nself-attention is then performed within each group. Second, we further improve\nthe grouping strategy via the Dynamic Programming algorithm to minimize the\noverall computation cost of the attention on the grouped patches. Third, as for\nthe convolution layers, we convert them to the Sparse Convolution that works\nseamlessly with the sparse data, i.e., the visible patches in MIM. As a result,\nMIM can now work on most, if not all, hierarchical ViTs in a green and\nefficient way. For example, we can train the hierarchical ViTs, e.g., Swin\nTransformer and Twins Transformer, about 2.7$\\times$ faster and reduce the GPU\nmemory usage by 70%, while still enjoying competitive performance on ImageNet\nclassification and the superiority on downstream COCO object detection\nbenchmarks. Code and pre-trained models have been made publicly available at\nhttps://github.com/LayneH/GreenMIM.",
    "published": "2022-05-26T17:34:42Z",
    "updated": "2022-10-14T06:40:23Z",
    "authors": [
      "Lang Huang",
      "Shan You",
      "Mingkai Zheng",
      "Fei Wang",
      "Chen Qian",
      "Toshihiko Yamasaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.11167v2",
    "title": "Vision Transformer with Super Token Sampling",
    "summary": "Vision transformer has achieved impressive performance for many vision tasks.\nHowever, it may suffer from high redundancy in capturing local features for\nshallow layers. Local self-attention or early-stage convolutions are thus\nutilized, which sacrifice the capacity to capture long-range dependency. A\nchallenge then arises: can we access efficient and effective global context\nmodeling at the early stages of a neural network? To address this issue, we\ndraw inspiration from the design of superpixels, which reduces the number of\nimage primitives in subsequent processing, and introduce super tokens into\nvision transformer. Super tokens attempt to provide a semantically meaningful\ntessellation of visual content, thus reducing the token number in\nself-attention as well as preserving global modeling. Specifically, we propose\na simple yet strong super token attention (STA) mechanism with three steps: the\nfirst samples super tokens from visual tokens via sparse association learning,\nthe second performs self-attention on super tokens, and the last maps them back\nto the original token space. STA decomposes vanilla global attention into\nmultiplications of a sparse association map and a low-dimensional attention,\nleading to high efficiency in capturing global dependencies. Based on STA, we\ndevelop a hierarchical vision transformer. Extensive experiments demonstrate\nits strong performance on various vision tasks. In particular, without any\nextra training data or label, it achieves 86.4% top-1 accuracy on ImageNet-1K\nwith less than 100M parameters. It also achieves 53.9 box AP and 46.8 mask AP\non the COCO detection task, and 51.9 mIOU on the ADE20K semantic segmentation\ntask. Code is released at https://github.com/hhb072/STViT.",
    "published": "2022-11-21T03:48:13Z",
    "updated": "2024-01-25T08:23:42Z",
    "authors": [
      "Huaibo Huang",
      "Xiaoqiang Zhou",
      "Jie Cao",
      "Ran He",
      "Tieniu Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.10866v3",
    "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
    "summary": "Recent advances in deep learning have relied heavily on the use of large\nTransformers due to their ability to learn at scale. However, the core building\nblock of Transformers, the attention operator, exhibits quadratic cost in\nsequence length, limiting the amount of context accessible. Existing\nsubquadratic methods based on low-rank and sparse approximations need to be\ncombined with dense attention layers to match Transformers, indicating a gap in\ncapability. In this work, we propose Hyena, a subquadratic drop-in replacement\nfor attention constructed by interleaving implicitly parametrized long\nconvolutions and data-controlled gating. In recall and reasoning tasks on\nsequences of thousands to hundreds of thousands of tokens, Hyena improves\naccuracy by more than 50 points over operators relying on state-spaces and\nother implicit and explicit methods, matching attention-based models. We set a\nnew state-of-the-art for dense-attention-free architectures on language\nmodeling in standard datasets (WikiText103 and The Pile), reaching Transformer\nquality with a 20% reduction in training compute required at sequence length\n2K. Hyena operators are twice as fast as highly optimized attention at sequence\nlength 8K, and 100x faster at sequence length 64K.",
    "published": "2023-02-21T18:29:25Z",
    "updated": "2023-04-19T20:08:39Z",
    "authors": [
      "Michael Poli",
      "Stefano Massaroli",
      "Eric Nguyen",
      "Daniel Y. Fu",
      "Tri Dao",
      "Stephen Baccus",
      "Yoshua Bengio",
      "Stefano Ermon",
      "Christopher RÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.11474v4",
    "title": "Reciprocal Attention Mixing Transformer for Lightweight Image\n  Restoration",
    "summary": "Although many recent works have made advancements in the image restoration\n(IR) field, they often suffer from an excessive number of parameters. Another\nissue is that most Transformer-based IR methods focus only on either local or\nglobal features, leading to limited receptive fields or deficient parameter\nissues. To address these problems, we propose a lightweight IR network,\nReciprocal Attention Mixing Transformer (RAMiT). It employs our proposed\ndimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which\ncompute bi-dimensional (spatial and channel) self-attentions in parallel with\ndifferent numbers of multi-heads. The bi-dimensional attentions help each other\nto complement their counterpart's drawbacks and are then mixed. Additionally,\nwe introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that\ncompensates for pixel-level information losses and utilizes semantic\ninformation while maintaining an efficient hierarchical structure. Furthermore,\nwe revisit and modify MobileNet V1 and V2 to attach efficient convolutions to\nour proposed components. The experimental results demonstrate that RAMiT\nachieves state-of-the-art performance on multiple lightweight IR tasks,\nincluding super-resolution, color denoising, grayscale denoising, low-light\nenhancement, and deraining. Codes are available at\nhttps://github.com/rami0205/RAMiT.",
    "published": "2023-05-19T06:55:04Z",
    "updated": "2024-04-18T15:10:47Z",
    "authors": [
      "Haram Choi",
      "Cheolwoong Na",
      "Jihyeon Oh",
      "Seungjae Lee",
      "Jinseop Kim",
      "Subeen Choe",
      "Jeongmin Lee",
      "Taehoon Kim",
      "Jihoon Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.12775v1",
    "title": "Is attention all you need in medical image analysis? A review",
    "summary": "Medical imaging is a key component in clinical diagnosis, treatment planning\nand clinical trial design, accounting for almost 90% of all healthcare data.\nCNNs achieved performance gains in medical image analysis (MIA) over the last\nyears. CNNs can efficiently model local pixel interactions and be trained on\nsmall-scale MI data. The main disadvantage of typical CNN models is that they\nignore global pixel relationships within images, which limits their\ngeneralisation ability to understand out-of-distribution data with different\n'global' information. The recent progress of Artificial Intelligence gave rise\nto Transformers, which can learn global relationships from data. However, full\nTransformer models need to be trained on large-scale data and involve\ntremendous computational complexity. Attention and Transformer compartments\n(Transf/Attention) which can well maintain properties for modelling global\nrelationships, have been proposed as lighter alternatives of full Transformers.\nRecently, there is an increasing trend to co-pollinate complementary\nlocal-global properties from CNN and Transf/Attention architectures, which led\nto a new era of hybrid models. The past years have witnessed substantial growth\nin hybrid CNN-Transf/Attention models across diverse MIA problems. In this\nsystematic review, we survey existing hybrid CNN-Transf/Attention models,\nreview and unravel key architectural designs, analyse breakthroughs, and\nevaluate current and future opportunities as well as challenges. We also\nintroduced a comprehensive analysis framework on generalisation opportunities\nof scientific and clinical impact, based on which new data-driven domain\ngeneralisation and adaptation methods can be stimulated.",
    "published": "2023-07-24T13:24:56Z",
    "updated": "2023-07-24T13:24:56Z",
    "authors": [
      "Giorgos Papanastasiou",
      "Nikolaos Dikaios",
      "Jiahao Huang",
      "Chengjia Wang",
      "Guang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.13657v1",
    "title": "Efficient Transformer Knowledge Distillation: A Performance Review",
    "summary": "As pretrained transformer language models continue to achieve\nstate-of-the-art performance, the Natural Language Processing community has\npushed for advances in model compression and efficient attention mechanisms to\naddress high computational requirements and limited input sequence length.\nDespite these separate efforts, no investigation has been done into the\nintersection of these two fields. In this work, we provide an evaluation of\nmodel compression via knowledge distillation on efficient attention\ntransformers. We provide cost-performance trade-offs for the compression of\nstate-of-the-art efficient attention architectures and the gains made in\nperformance in comparison to their full attention counterparts. Furthermore, we\nintroduce a new long-context Named Entity Recognition dataset, GONERD, to train\nand test the performance of NER models on long sequences. We find that\ndistilled efficient attention transformers can preserve a significant amount of\noriginal model performance, preserving up to 98.6% across short-context tasks\n(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context\nQuestion-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on\nlong-context Named Entity Recognition (GONERD), while decreasing inference\ntimes by up to 57.8%. We find that, for most models on most tasks, performing\nknowledge distillation is an effective method to yield high-performing\nefficient attention models with low costs.",
    "published": "2023-11-22T19:19:37Z",
    "updated": "2023-11-22T19:19:37Z",
    "authors": [
      "Nathan Brown",
      "Ashton Williamson",
      "Tahj Anderson",
      "Logan Lawrence"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.03870v1",
    "title": "Gramformer: Learning Crowd Counting via Graph-Modulated Transformer",
    "summary": "Transformer has been popular in recent crowd counting work since it breaks\nthe limited receptive field of traditional CNNs. However, since crowd images\nalways contain a large number of similar patches, the self-attention mechanism\nin Transformer tends to find a homogenized solution where the attention maps of\nalmost all patches are identical. In this paper, we address this problem by\nproposing Gramformer: a graph-modulated transformer to enhance the network by\nadjusting the attention and input node features respectively on the basis of\ntwo different types of graphs. Firstly, an attention graph is proposed to\ndiverse attention maps to attend to complementary information. The graph is\nbuilding upon the dissimilarities between patches, modulating the attention in\nan anti-similarity fashion. Secondly, a feature-based centrality encoding is\nproposed to discover the centrality positions or importance of nodes. We encode\nthem with a proposed centrality indices scheme to modulate the node features\nand similarity relationships. Extensive experiments on four challenging crowd\ncounting datasets have validated the competitiveness of the proposed method.\nCode is available at {https://github.com/LoraLinH/Gramformer}.",
    "published": "2024-01-08T13:01:54Z",
    "updated": "2024-01-08T13:01:54Z",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Xiaopeng Hong",
      "Qinnan Shangguan",
      "Deyu Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.04924v2",
    "title": "GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing\n  Sparsity, Trained from Scratch on Small Datasets",
    "summary": "Vision Transformers (ViTs) have achieved impressive results in large-scale\nimage classification. However, when training from scratch on small datasets,\nthere is still a significant performance gap between ViTs and Convolutional\nNeural Networks (CNNs), which is attributed to the lack of inductive bias. To\naddress this issue, we propose a Graph-based Vision Transformer (GvT) that\nutilizes graph convolutional projection and graph-pooling. In each block,\nqueries and keys are calculated through graph convolutional projection based on\nthe spatial adjacency matrix, while dot-product attention is used in another\ngraph convolution to generate values. When using more attention heads, the\nqueries and keys become lower-dimensional, making their dot product an\nuninformative matching function. To overcome this low-rank bottleneck in\nattention heads, we employ talking-heads technology based on bilinear pooled\nfeatures and sparse selection of attention tensors. This allows interaction\namong filtered attention scores and enables each attention mechanism to depend\non all queries and keys. Additionally, we apply graph-pooling between two\nintermediate blocks to reduce the number of tokens and aggregate semantic\ninformation more effectively. Our experimental results show that GvT produces\ncomparable or superior outcomes to deep convolutional networks and surpasses\nvision transformers without pre-training on large datasets. The code for our\nproposed model is publicly available on the website.",
    "published": "2024-04-07T11:48:07Z",
    "updated": "2025-06-03T01:00:29Z",
    "authors": [
      "Dongjing Shan",
      "guiqiang chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.01375v2",
    "title": "Transferable-guided Attention Is All You Need for Video Domain\n  Adaptation",
    "summary": "Unsupervised domain adaptation (UDA) in videos is a challenging task that\nremains not well explored compared to image-based UDA techniques. Although\nvision transformers (ViT) achieve state-of-the-art performance in many computer\nvision tasks, their use in video UDA has been little explored. Our key idea is\nto use transformer layers as a feature encoder and incorporate spatial and\ntemporal transferability relationships into the attention mechanism. A\nTransferable-guided Attention (TransferAttn) framework is then developed to\nexploit the capacity of the transformer to adapt cross-domain knowledge across\ndifferent backbones. To improve the transferability of ViT, we introduce a\nnovel and effective module, named Domain Transferable-guided Attention Block\n(DTAB). DTAB compels ViT to focus on the spatio-temporal transferability\nrelationship among video frames by changing the self-attention mechanism to a\ntransferability attention mechanism. Extensive experiments were conducted on\nUCF-HMDB, Kinetics-Gameplay, and Kinetics-NEC Drone datasets, with different\nbackbones, like ResNet101, I3D, and STAM, to verify the effectiveness of\nTransferAttn compared with state-of-the-art approaches. Also, we demonstrate\nthat DTAB yields performance gains when applied to other state-of-the-art\ntransformer-based UDA methods from both video and image domains. Our code is\navailable at https://github.com/Andre-Sacilotti/transferattn-project-code.",
    "published": "2024-07-01T15:29:27Z",
    "updated": "2024-09-17T10:35:46Z",
    "authors": [
      "AndrÃ© Sacilotti",
      "Samuel Felipe dos Santos",
      "Nicu Sebe",
      "Jurandy Almeida"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.21273v1",
    "title": "On Inductive Biases That Enable Generalization of Diffusion Transformers",
    "summary": "Recent work studying the generalization of diffusion models with UNet-based\ndenoisers reveals inductive biases that can be expressed via geometry-adaptive\nharmonic bases. However, in practice, more recent denoising networks are often\nbased on transformers, e.g., the diffusion transformer (DiT). This raises the\nquestion: do transformer-based denoising networks exhibit inductive biases that\ncan also be expressed via geometry-adaptive harmonic bases? To our surprise, we\nfind that this is not the case. This discrepancy motivates our search for the\ninductive bias that can lead to good generalization in DiT models.\nInvestigating the pivotal attention modules of a DiT, we find that locality of\nattention maps are closely associated with generalization. To verify this\nfinding, we modify the generalization of a DiT by restricting its attention\nwindows. We inject local attention windows to a DiT and observe an improvement\nin generalization. Furthermore, we empirically find that both the placement and\nthe effective attention size of these local attention windows are crucial\nfactors. Experimental results on the CelebA, ImageNet, and LSUN datasets show\nthat strengthening the inductive bias of a DiT can improve both generalization\nand generation quality when less training data is available. Source code will\nbe released publicly upon paper publication. Project page:\ndit-generalization.github.io/.",
    "published": "2024-10-28T17:59:13Z",
    "updated": "2024-10-28T17:59:13Z",
    "authors": [
      "Jie An",
      "De Wang",
      "Pengsheng Guo",
      "Jiebo Luo",
      "Alexander Schwing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.04776v1",
    "title": "Megatron: Evasive Clean-Label Backdoor Attacks against Vision\n  Transformer",
    "summary": "Vision transformers have achieved impressive performance in various\nvision-related tasks, but their vulnerability to backdoor attacks is\nunder-explored. A handful of existing works focus on dirty-label attacks with\nwrongly-labeled poisoned training samples, which may fail if a benign model\ntrainer corrects the labels. In this paper, we propose Megatron, an evasive\nclean-label backdoor attack against vision transformers, where the attacker\ninjects the backdoor without manipulating the data-labeling process. To\ngenerate an effective trigger, we customize two loss terms based on the\nattention mechanism used in transformer networks, i.e., latent loss and\nattention diffusion loss. The latent loss aligns the last attention layer\nbetween triggered samples and clean samples of the target label. The attention\ndiffusion loss emphasizes the attention diffusion area that encompasses the\ntrigger. A theoretical analysis is provided to underpin the rationale behind\nthe attention diffusion loss. Extensive experiments on CIFAR-10, GTSRB,\nCIFAR-100, and Tiny ImageNet demonstrate the effectiveness of Megatron.\nMegatron can achieve attack success rates of over 90% even when the position of\nthe trigger is slightly shifted during testing. Furthermore, Megatron achieves\nbetter evasiveness than baselines regarding both human visual inspection and\ndefense strategies (i.e., DBAVT, BAVT, Beatrix, TeCo, and SAGE).",
    "published": "2024-12-06T04:39:41Z",
    "updated": "2024-12-06T04:39:41Z",
    "authors": [
      "Xueluan Gong",
      "Bowei Tian",
      "Meng Xue",
      "Shuike Li",
      "Yanjiao Chen",
      "Qian Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.13989v1",
    "title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series\n  Forecasting",
    "summary": "This paper presents \\textbf{FreEformer}, a simple yet effective model that\nleverages a \\textbf{Fre}quency \\textbf{E}nhanced Trans\\textbf{former} for\nmultivariate time series forecasting. Our work is based on the assumption that\nthe frequency spectrum provides a global perspective on the composition of\nseries across various frequencies and is highly suitable for robust\nrepresentation learning. Specifically, we first convert time series into the\ncomplex frequency domain using the Discrete Fourier Transform (DFT). The\nTransformer architecture is then applied to the frequency spectra to capture\ncross-variate dependencies, with the real and imaginary parts processed\nindependently. However, we observe that the vanilla attention matrix exhibits a\nlow-rank characteristic, thus limiting representation diversity. This could be\nattributed to the inherent sparsity of the frequency domain and the\nstrong-value-focused nature of Softmax in vanilla attention. To address this,\nwe enhance the vanilla attention mechanism by introducing an additional\nlearnable matrix to the original attention matrix, followed by row-wise L1\nnormalization. Theoretical analysis~demonstrates that this enhanced attention\nmechanism improves both feature diversity and gradient flow. Extensive\nexperiments demonstrate that FreEformer consistently outperforms\nstate-of-the-art models on eighteen real-world benchmarks covering electricity,\ntraffic, weather, healthcare and finance. Notably, the enhanced attention\nmechanism also consistently improves the performance of state-of-the-art\nTransformer-based forecasters.",
    "published": "2025-01-23T08:53:45Z",
    "updated": "2025-01-23T08:53:45Z",
    "authors": [
      "Wenzhen Yue",
      "Yong Liu",
      "Xianghua Ying",
      "Bowei Xing",
      "Ruohao Guo",
      "Ji Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01770v1",
    "title": "Hamming Attention Distillation: Binarizing Keys and Queries for\n  Efficient Long-Context Transformers",
    "summary": "Pre-trained transformer models with extended context windows are notoriously\nexpensive to run at scale, often limiting real-world deployment due to their\nhigh computational and memory requirements. In this paper, we introduce Hamming\nAttention Distillation (HAD), a novel framework that binarizes keys and queries\nin the attention mechanism to achieve significant efficiency gains. By\nconverting keys and queries into {-1, +1} vectors and replacing dot-product\noperations with efficient Hamming distance computations, our method drastically\nreduces computational overhead. Additionally, we incorporate attention matrix\nsparsification to prune low-impact activations, which further reduces the cost\nof processing long-context sequences. \\par Despite these aggressive compression\nstrategies, our distilled approach preserves a high degree of representational\npower, leading to substantially improved accuracy compared to prior transformer\nbinarization methods. We evaluate HAD on a range of tasks and models, including\nthe GLUE benchmark, ImageNet, and QuALITY, demonstrating state-of-the-art\nperformance among binarized Transformers while drastically reducing the\ncomputational costs of long-context inference. \\par We implement HAD in custom\nhardware simulations, demonstrating superior performance characteristics\ncompared to a custom hardware implementation of standard attention. HAD\nachieves just $\\mathbf{1.78}\\%$ performance losses on GLUE compared to $9.08\\%$\nin state-of-the-art binarization work, and $\\mathbf{2.5}\\%$ performance losses\non ImageNet compared to $12.14\\%$, all while targeting custom hardware with a\n$\\mathbf{79}\\%$ area reduction and $\\mathbf{87}\\%$ power reduction compared to\nits standard attention counterpart.",
    "published": "2025-02-03T19:24:01Z",
    "updated": "2025-02-03T19:24:01Z",
    "authors": [
      "Mark Horton",
      "Tergel Molom-Ochir",
      "Peter Liu",
      "Bhavna Gopal",
      "Chiyue Wei",
      "Cong Guo",
      "Brady Taylor",
      "Deliang Fan",
      "Shan X. Wang",
      "Hai Li",
      "Yiran Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18337v1",
    "title": "Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based\n  Large Models",
    "summary": "Transformer-based large pre-trained models have shown remarkable\ngeneralization ability, and various parameter-efficient fine-tuning (PEFT)\nmethods have been proposed to customize these models on downstream tasks with\nminimal computational and memory budgets. Previous PEFT methods are primarily\ndesigned from a tensor-decomposition perspective that tries to effectively tune\nthe linear transformation by finding the smallest subset of parameters to\ntrain. Our study adopts an orthogonal view by representing the attention\noperation as a graph convolution and formulating the multi-head attention maps\nas a convolutional filter subspace, with each attention map as a subspace\nelement. In this paper, we propose to tune the large pre-trained transformers\nby learning a small set of combination coefficients that construct a more\nexpressive filter subspace from the original multi-head attention maps. We show\nanalytically and experimentally that the tuned filter subspace can effectively\nexpand the feature space of the multi-head attention and further enhance the\ncapacity of transformers. We further stabilize the fine-tuning with a residual\nparameterization of the tunable subspace coefficients, and enhance the\ngeneralization with a regularization design by directly applying dropout on the\ntunable coefficient during training. The tunable coefficients take a tiny\nnumber of parameters and can be combined with previous PEFT methods in a\nplug-and-play manner. Extensive experiments show that our approach achieves\nsuperior performances than PEFT baselines with neglectable additional\nparameters.",
    "published": "2025-03-24T04:42:40Z",
    "updated": "2025-03-24T04:42:40Z",
    "authors": [
      "Zichen Miao",
      "Wei Chen",
      "Qiang Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1110.5989v1",
    "title": "A Heuristic Description of Fast Fourier Transform",
    "summary": "Fast Fourier Transform (FFT) is an efficient algorithm to compute the\nDiscrete Fourier Transform (DFT) and its inverse. In this paper, we pay special\nattention to the description of complex-data FFT. We analyze two common\ndescriptions of FFT and propose a new presentation. Our heuristic description\nis helpful for students and programmers to grasp the algorithm entirely and\ndeeply.",
    "published": "2011-10-27T05:51:23Z",
    "updated": "2011-10-27T05:51:23Z",
    "authors": [
      "Zhengjun Cao",
      "Xiao Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1905.10308v1",
    "title": "SCRAM: Spatially Coherent Randomized Attention Maps",
    "summary": "Attention mechanisms and non-local mean operations in general are key\ningredients in many state-of-the-art deep learning techniques. In particular,\nthe Transformer model based on multi-head self-attention has recently achieved\ngreat success in natural language processing and computer vision. However, the\nvanilla algorithm computing the Transformer of an image with n pixels has\nO(n^2) complexity, which is often painfully slow and sometimes prohibitively\nexpensive for large-scale image data. In this paper, we propose a fast\nrandomized algorithm --- SCRAM --- that only requires O(n log(n)) time to\nproduce an image attention map. Such a dramatic acceleration is attributed to\nour insight that attention maps on real-world images usually exhibit (1)\nspatial coherence and (2) sparse structure. The central idea of SCRAM is to\nemploy PatchMatch, a randomized correspondence algorithm, to quickly pinpoint\nthe most compatible key (argmax) for each query first, and then exploit that\nknowledge to design a sparse approximation to non-local mean operations. Using\nthe argmax (mode) to dynamically construct the sparse approximation\ndistinguishes our algorithm from all of the existing sparse approximate methods\nand makes it very efficient. Moreover, SCRAM is a broadly applicable\napproximation to any non-local mean layer in contrast to some other sparse\napproximations that can only approximate self-attention. Our preliminary\nexperimental results suggest that SCRAM is indeed promising for speeding up or\nscaling up the computation of attention maps in the Transformer.",
    "published": "2019-05-24T16:03:44Z",
    "updated": "2019-05-24T16:03:44Z",
    "authors": [
      "Dan A. Calian",
      "Peter Roelants",
      "Jacques Cali",
      "Ben Carr",
      "Krishna Dubba",
      "John E. Reid",
      "Dell Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.10801v1",
    "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple\n  Alternative to Attention Mechanism",
    "summary": "Attention mechanism has been widely believed as the key to success of vision\ntransformers (ViTs), since it provides a flexible and powerful way to model\nspatial relationships. However, is the attention mechanism truly an\nindispensable part of ViT? Can it be replaced by some other alternatives? To\ndemystify the role of attention mechanism, we simplify it into an extremely\nsimple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift\noperation. It does not contain any parameter or arithmetic calculation. The\nonly operation is to exchange a small portion of the channels between\nneighboring features. Based on this simple operation, we construct a new\nbackbone network, namely ShiftViT, where the attention layers in ViT are\nsubstituted by shift operations. Surprisingly, ShiftViT works quite well in\nseveral mainstream tasks, e.g., classification, detection, and segmentation.\nThe performance is on par with or even better than the strong baseline Swin\nTransformer. These results suggest that the attention mechanism might not be\nthe vital factor that makes ViT successful. It can be even replaced by a\nzero-parameter operation. We should pay more attentions to the remaining parts\nof ViT in the future work. Code is available at github.com/microsoft/SPACH.",
    "published": "2022-01-26T08:17:06Z",
    "updated": "2022-01-26T08:17:06Z",
    "authors": [
      "Guangting Wang",
      "Yucheng Zhao",
      "Chuanxin Tang",
      "Chong Luo",
      "Wenjun Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.08791v1",
    "title": "cosFormer: Rethinking Softmax in Attention",
    "summary": "Transformer has shown great successes in natural language processing,\ncomputer vision, and audio processing. As one of its core components, the\nsoftmax attention helps to capture long-range dependencies yet prohibits its\nscale-up due to the quadratic space and time complexity to the sequence length.\nKernel methods are often adopted to reduce the complexity by approximating the\nsoftmax operator. Nevertheless, due to the approximation errors, their\nperformances vary in different tasks/corpus and suffer crucial performance\ndrops when compared with the vanilla softmax attention. In this paper, we\npropose a linear transformer called cosFormer that can achieve comparable or\nbetter accuracy to the vanilla transformer in both casual and cross attentions.\ncosFormer is based on two key properties of softmax attention: i).\nnon-negativeness of the attention matrix; ii). a non-linear re-weighting scheme\nthat can concentrate the distribution of the attention matrix. As its linear\nsubstitute, cosFormer fulfills these properties with a linear operator and a\ncosine-based distance re-weighting mechanism. Extensive experiments on language\nmodeling and text understanding tasks demonstrate the effectiveness of our\nmethod. We further examine our method on long sequences and achieve\nstate-of-the-art performance on the Long-Range Arena benchmark. The source code\nis available at https://github.com/OpenNLPLab/cosFormer.",
    "published": "2022-02-17T17:53:48Z",
    "updated": "2022-02-17T17:53:48Z",
    "authors": [
      "Zhen Qin",
      "Weixuan Sun",
      "Hui Deng",
      "Dongxu Li",
      "Yunshen Wei",
      "Baohong Lv",
      "Junjie Yan",
      "Lingpeng Kong",
      "Yiran Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.11126v3",
    "title": "Robustifying Token Attention for Vision Transformers",
    "summary": "Despite the success of vision transformers (ViTs), they still suffer from\nsignificant drops in accuracy in the presence of common corruptions, such as\nnoise or blur. Interestingly, we observe that the attention mechanism of ViTs\ntends to rely on few important tokens, a phenomenon we call token overfocusing.\nMore critically, these tokens are not robust to corruptions, often leading to\nhighly diverging attention patterns. In this paper, we intend to alleviate this\noverfocusing issue and make attention more stable through two general\ntechniques: First, our Token-aware Average Pooling (TAP) module encourages the\nlocal neighborhood of each token to take part in the attention mechanism.\nSpecifically, TAP learns average pooling schemes for each token such that the\ninformation of potentially important tokens in the neighborhood can adaptively\nbe taken into account. Second, we force the output tokens to aggregate\ninformation from a diverse set of input tokens rather than focusing on just a\nfew by using our Attention Diversification Loss (ADL). We achieve this by\npenalizing high cosine similarity between the attention vectors of different\ntokens. In experiments, we apply our methods to a wide range of transformer\narchitectures and improve robustness significantly. For example, we improve\ncorruption robustness on ImageNet-C by 2.4% while improving accuracy by 0.4%\nbased on state-of-the-art robust architecture FAN. Also, when fine-tuning on\nsemantic segmentation tasks, we improve robustness on CityScapes-C by 2.4% and\nACDC by 3.0%. Our code is available at https://github.com/guoyongcs/TAPADL.",
    "published": "2023-03-20T14:04:40Z",
    "updated": "2023-09-06T11:09:26Z",
    "authors": [
      "Yong Guo",
      "David Stutz",
      "Bernt Schiele"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.11423v2",
    "title": "Single Headed Attention RNN: Stop Thinking With Your Head",
    "summary": "The leading approaches in language modeling are all obsessed with TV shows of\nmy youth - namely Transformers and Sesame Street. Transformers this,\nTransformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer\nscale silicon. We opt for the lazy path of old and proven techniques with a\nfancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The\nauthor's lone goal is to show that the entire field might have evolved a\ndifferent direction if we had instead been obsessed with a slightly different\nacronym and slightly different result. We take a previously strong language\nmodel based only on boring LSTMs and get it to within a stone's throw of a\nstone's throw of state-of-the-art byte level language model results on enwik8.\nThis work has undergone no intensive hyperparameter optimization and lived\nentirely on a commodity desktop machine that made the author's small studio\napartment far too warm in the midst of a San Franciscan summer. The final\nresults are achievable in plus or minus 24 hours on a single GPU as the author\nis impatient. The attention mechanism is also readily extended to large\ncontexts with minimal computation. Take that Sesame Street.",
    "published": "2019-11-26T09:45:33Z",
    "updated": "2019-11-27T12:00:15Z",
    "authors": [
      "Stephen Merity"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1912.10435v1",
    "title": "BERTQA -- Attention on Steroids",
    "summary": "In this work, we extend the Bidirectional Encoder Representations from\nTransformers (BERT) with an emphasis on directed coattention to obtain an\nimproved F1 performance on the SQUAD2.0 dataset. The Transformer architecture\non which BERT is based places hierarchical global attention on the\nconcatenation of the context and query. Our additions to the BERT architecture\naugment this attention with a more focused context to query (C2Q) and query to\ncontext (Q2C) attention via a set of modified Transformer encoder units. In\naddition, we explore adding convolution-based feature extraction within the\ncoattention architecture to add localized information to self-attention. We\nfound that coattention significantly improves the no answer F1 by 4 points in\nthe base and 1 point in the large architecture. After adding skip connections\nthe no answer F1 improved further without causing an additional loss in has\nanswer F1. The addition of localized feature extraction added to attention\nproduced an overall dev F1 of 77.03 in the base architecture. We applied our\nfindings to the large BERT model which contains twice as many layers and\nfurther used our own augmented version of the SQUAD 2.0 dataset created by back\ntranslation, which we have named SQUAD 2.Q. Finally, we performed\nhyperparameter tuning and ensembled our best models for a final F1/EM of\n82.317/79.442 (Attention on Steroids, PCE Test Leaderboard).",
    "published": "2019-12-14T06:44:12Z",
    "updated": "2019-12-14T06:44:12Z",
    "authors": [
      "Ankit Chadha",
      "Rewa Sood"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.07631v3",
    "title": "Improved Transformer for High-Resolution GANs",
    "summary": "Attention-based models, exemplified by the Transformer, can effectively model\nlong range dependency, but suffer from the quadratic complexity of\nself-attention operation, making them difficult to be adopted for\nhigh-resolution image generation based on Generative Adversarial Networks\n(GANs). In this paper, we introduce two key ingredients to Transformer to\naddress this challenge. First, in low-resolution stages of the generative\nprocess, standard global self-attention is replaced with the proposed\nmulti-axis blocked self-attention which allows efficient mixing of local and\nglobal attention. Second, in high-resolution stages, we drop self-attention\nwhile only keeping multi-layer perceptrons reminiscent of the implicit neural\nfunction. To further improve the performance, we introduce an additional\nself-modulation component based on cross-attention. The resulting model,\ndenoted as HiT, has a nearly linear computational complexity with respect to\nthe image size and thus directly scales to synthesizing high definition images.\nWe show in the experiments that the proposed HiT achieves state-of-the-art FID\nscores of 30.83 and 2.95 on unconditional ImageNet $128 \\times 128$ and FFHQ\n$256 \\times 256$, respectively, with a reasonable throughput. We believe the\nproposed HiT is an important milestone for generators in GANs which are\ncompletely free of convolutions. Our code is made publicly available at\nhttps://github.com/google-research/hit-gan",
    "published": "2021-06-14T17:39:49Z",
    "updated": "2021-12-24T01:11:20Z",
    "authors": [
      "Long Zhao",
      "Zizhao Zhang",
      "Ting Chen",
      "Dimitris N. Metaxas",
      "Han Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.11945v3",
    "title": "SOFT: Softmax-free Transformer with Linear Complexity",
    "summary": "Vision transformers (ViTs) have pushed the state-of-the-art for various\nvisual recognition tasks by patch-wise image tokenization followed by\nself-attention. However, the employment of self-attention modules results in a\nquadratic complexity in both computation and memory usage. Various attempts on\napproximating the self-attention computation with linear complexity have been\nmade in Natural Language Processing. However, an in-depth analysis in this work\nshows that they are either theoretically flawed or empirically ineffective for\nvisual recognition. We further identify that their limitations are rooted in\nkeeping the softmax self-attention during approximations. Specifically,\nconventional self-attention is computed by normalizing the scaled dot-product\nbetween token feature vectors. Keeping this softmax operation challenges any\nsubsequent linearization efforts. Based on this insight, for the first time, a\nsoftmax-free transformer or SOFT is proposed. To remove softmax in\nself-attention, Gaussian kernel function is used to replace the dot-product\nsimilarity without further normalization. This enables a full self-attention\nmatrix to be approximated via a low-rank matrix decomposition. The robustness\nof the approximation is achieved by calculating its Moore-Penrose inverse using\na Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT\nsignificantly improves the computational efficiency of existing ViT variants.\nCrucially, with a linear complexity, much longer token sequences are permitted\nin SOFT, resulting in superior trade-off between accuracy and complexity.",
    "published": "2021-10-22T17:57:29Z",
    "updated": "2022-04-30T11:34:05Z",
    "authors": [
      "Jiachen Lu",
      "Jinghan Yao",
      "Junge Zhang",
      "Xiatian Zhu",
      "Hang Xu",
      "Weiguo Gao",
      "Chunjing Xu",
      "Tao Xiang",
      "Li Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.09714v1",
    "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli\n  Sampling",
    "summary": "Transformer-based models are widely used in natural language processing\n(NLP). Central to the transformer model is the self-attention mechanism, which\ncaptures the interactions of token pairs in the input sequences and depends\nquadratically on the sequence length. Training such models on longer sequences\nis expensive. In this paper, we show that a Bernoulli sampling attention\nmechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic\ncomplexity of such models to linear. We bypass the quadratic cost by\nconsidering self-attention as a sum of individual tokens associated with\nBernoulli random variables that can, in principle, be sampled at once by a\nsingle hash (although in practice, this number may be a small constant). This\nleads to an efficient sampling scheme to estimate self-attention which relies\non specific modifications of LSH (to enable deployment on GPU architectures).\nWe evaluate our algorithm on the GLUE benchmark with standard 512 sequence\nlength where we see favorable performance relative to a standard pretrained\nTransformer. On the Long Range Arena (LRA) benchmark, for evaluating\nperformance on long sequences, our method achieves results consistent with\nsoftmax self-attention but with sizable speed-ups and memory savings and often\noutperforms other efficient self-attention methods. Our code is available at\nhttps://github.com/mlpen/YOSO",
    "published": "2021-11-18T14:24:34Z",
    "updated": "2021-11-18T14:24:34Z",
    "authors": [
      "Zhanpeng Zeng",
      "Yunyang Xiong",
      "Sathya N. Ravi",
      "Shailesh Acharya",
      "Glenn Fung",
      "Vikas Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.03254v3",
    "title": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External\n  Attention",
    "summary": "Most of today's AI systems focus on using self-attention mechanisms and\ntransformer architectures on large amounts of diverse data to achieve\nimpressive performance gains. In this paper, we propose to augment the\ntransformer architecture with an external attention mechanism to bring external\nknowledge and context to bear. By integrating external information into the\nprediction process, we hope to reduce the need for ever-larger models and\nincrease the democratization of AI systems. We find that the proposed external\nattention mechanism can significantly improve the performance of existing AI\nsystems, allowing practitioners to easily customize foundation AI models to\nmany diverse downstream applications. In particular, we focus on the task of\nCommonsense Reasoning, demonstrating that the proposed external attention\nmechanism can augment existing transformer models and significantly improve the\nmodel's reasoning capabilities. The proposed system, Knowledgeable External\nAttention for commonsense Reasoning (KEAR), reaches human parity on the open\nCommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to\nthe human accuracy of 88.9\\%.",
    "published": "2021-12-06T18:59:02Z",
    "updated": "2022-05-04T22:56:45Z",
    "authors": [
      "Yichong Xu",
      "Chenguang Zhu",
      "Shuohang Wang",
      "Siqi Sun",
      "Hao Cheng",
      "Xiaodong Liu",
      "Jianfeng Gao",
      "Pengcheng He",
      "Michael Zeng",
      "Xuedong Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.14046v1",
    "title": "Learning Spatiotemporal Frequency-Transformer for Low-Quality Video\n  Super-Resolution",
    "summary": "Video Super-Resolution (VSR) aims to restore high-resolution (HR) videos from\nlow-resolution (LR) videos. Existing VSR techniques usually recover HR frames\nby extracting pertinent textures from nearby frames with known degradation\nprocesses. Despite significant progress, grand challenges are remained to\neffectively extract and transmit high-quality textures from high-degraded\nlow-quality sequences, such as blur, additive noises, and compression\nartifacts. In this work, a novel Frequency-Transformer (FTVSR) is proposed for\nhandling low-quality videos that carry out self-attention in a combined\nspace-time-frequency domain. First, video frames are split into patches and\neach patch is transformed into spectral maps in which each channel represents a\nfrequency band. It permits a fine-grained self-attention on each frequency\nband, so that real visual texture can be distinguished from artifacts. Second,\na novel dual frequency attention (DFA) mechanism is proposed to capture the\nglobal frequency relations and local frequency relations, which can handle\ndifferent complicated degradation processes in real-world scenarios. Third, we\nexplore different self-attention schemes for video processing in the frequency\ndomain and discover that a ``divided attention'' which conducts a joint\nspace-frequency attention before applying temporal-frequency attention, leads\nto the best video enhancement quality. Extensive experiments on three\nwidely-used VSR datasets show that FTVSR outperforms state-of-the-art methods\non different low-quality videos with clear visual margins. Code and pre-trained\nmodels are available at https://github.com/researchmm/FTVSR.",
    "published": "2022-12-27T16:26:15Z",
    "updated": "2022-12-27T16:26:15Z",
    "authors": [
      "Zhongwei Qiu",
      "Huan Yang",
      "Jianlong Fu",
      "Daochang Liu",
      "Chang Xu",
      "Dongmei Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.08646v3",
    "title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in\n  Transformers for Long Context Window Extending",
    "summary": "Self-attention and position embedding are two key modules in\ntransformer-based Large Language Models (LLMs). However, the potential\nrelationship between them is far from well studied, especially for long context\nwindow extending. In fact, anomalous behaviors harming long context\nextrapolation exist between Rotary Position Embedding (RoPE) and vanilla\nself-attention unveiled by our work. To address this issue, we propose a novel\nattention mechanism, CoCA (Collinear Constrained Attention). Specifically, we\nenforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE\nand self-attention. While only adding minimal computational and spatial\ncomplexity, this integration significantly enhances long context window\nextrapolation ability. We provide an optimized implementation, making it a\ndrop-in replacement for any existing transformer-based models. Extensive\nexperiments show that CoCA performs extraordinarily well in extending context\nwindows. A CoCA-based GPT model, trained with a context length of 512, can\nseamlessly extend the context window up to 32K (60$\\times$), without any\nfine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve\nextrapolation up to 32K within only 2K training length. Our code is publicly\navailable at: https://github.com/codefuse-ai/Collinear-Constrained-Attention",
    "published": "2023-09-15T09:36:51Z",
    "updated": "2024-02-28T05:56:43Z",
    "authors": [
      "Shiyi Zhu",
      "Jing Ye",
      "Wei Jiang",
      "Siqiao Xue",
      "Qi Zhang",
      "Yifan Wu",
      "Jianguo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.01209v2",
    "title": "Self-distilled Masked Attention guided masked image modeling with noise\n  Regularized Teacher (SMART) for medical image analysis",
    "summary": "Pretraining vision transformers (ViT) with attention guided masked image\nmodeling (MIM) has shown to increase downstream accuracy for natural image\nanalysis. Hierarchical shifted window (Swin) transformer, often used in medical\nimage analysis cannot use attention guided masking as it lacks an explicit\n[CLS] token, needed for computing attention maps for selective masking. We thus\nenhanced Swin with semantic class attention. We developed a co-distilled Swin\ntransformer that combines a noisy momentum updated teacher to guide selective\nmasking for MIM. Our approach called \\textsc{s}e\\textsc{m}antic\n\\textsc{a}ttention guided co-distillation with noisy teacher\n\\textsc{r}egularized Swin \\textsc{T}rans\\textsc{F}ormer (SMARTFormer) was\napplied for analyzing 3D computed tomography datasets with lung nodules and\nmalignant lung cancers (LC). We also analyzed the impact of semantic attention\nand noisy teacher on pretraining and downstream accuracy. SMARTFormer\nclassified lesions (malignant from benign) with a high accuracy of 0.895 of\n1000 nodules, predicted LC treatment response with accuracy of 0.74, and\nachieved high accuracies even in limited data regimes. Pretraining with\nsemantic attention and noisy teacher improved ability to distinguish\nsemantically meaningful structures such as organs in a unsupervised clustering\ntask and localize abnormal structures like tumors. Code, models will be made\navailable through GitHub upon paper acceptance.",
    "published": "2023-10-02T13:53:55Z",
    "updated": "2024-07-03T11:49:33Z",
    "authors": [
      "Jue Jiang",
      "Aneesh Rangnekar",
      "Chloe Min Seo Choi",
      "Harini Veeraraghavan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.11960v4",
    "title": "Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for\n  Text and Images",
    "summary": "While Transformer networks benefit from a global receptive field, their\nquadratic cost relative to sequence length restricts their application to long\nsequences and high-resolution inputs. We introduce Fast Multipole Attention\n(FMA), a divide-and-conquer mechanism for self-attention inspired by the Fast\nMultipole Method from n-body physics. FMA reduces the time and memory\ncomplexity of self-attention from $\\mathcal{O}\\left(n^2\\right)$ to\n$\\mathcal{O}(n \\log n)$ and $\\mathcal{O}(n)$ while preserving full-context\ninteractions.\n  FMA contains a learned hierarchy with $\\mathcal{O}(\\log n)$ levels of\nresolution. In this hierarchy, nearby tokens interact at full resolution, while\ndistant tokens engage through progressively coarser, learned basis functions.\nWe have developed both 1D and 2D implementations of FMA for language and vision\ntasks, respectively. On autoregressive and bidirectional language modeling\nbenchmarks, the 1D variant either matches or outperforms leading efficient\nattention baselines with substantially lower memory use. With linear\ncomplexity, the 2D variant demonstrates superior performance over strong vision\ntransformer baselines in classification and semantic segmentation tasks.\n  Our results confirm that the multilevel attention implemented by FMA allows\nTransformer-based models to scale to much longer sequences and\nhigher-resolution inputs without loss in accuracy. This provides a principled,\nphysics-inspired approach for developing scalable neural networks suitable for\nlanguage, vision, and multimodal tasks. Our code will be available at\nhttps://github.com/epoch98/FMA.",
    "published": "2023-10-18T13:40:41Z",
    "updated": "2025-09-18T07:14:08Z",
    "authors": [
      "Yanming Kang",
      "Giang Tran",
      "Hans De Sterck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.04563v1",
    "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided\n  by Self-Attention",
    "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer\nvision field with its great performance on various tasks. In order to fully\nutilize the ViT-based architecture in various applications, proper\nvisualization methods with a decent localization performance are necessary, but\nthese methods employed in CNN-based models are still not available in ViT due\nto its unique structure. In this work, we propose an attention-guided\nvisualization method applied to ViT that provides a high-level semantic\nexplanation for its decision. Our method selectively aggregates the gradients\ndirectly propagated from the classification output to each self-attention,\ncollecting the contribution of image features extracted from each location of\nthe input image. These gradients are additionally guided by the normalized\nself-attention scores, which are the pairwise patch correlation scores. They\nare used to supplement the gradients on the patch-level context information\nefficiently detected by the self-attention mechanism. This approach of our\nmethod provides elaborate high-level semantic explanations with great\nlocalization performance only with the class labels. As a result, our method\noutperforms the previous leading explainability methods of ViT in the\nweakly-supervised localization task and presents great capability in capturing\nthe full instances of the target class object. Meanwhile, our method provides a\nvisualization that faithfully explains the model, which is demonstrated in the\nperturbation comparison test.",
    "published": "2024-02-07T03:43:56Z",
    "updated": "2024-02-07T03:43:56Z",
    "authors": [
      "Saebom Leem",
      "Hyunseok Seo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.12788v3",
    "title": "RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse\n  Attention",
    "summary": "Remote photoplethysmography (rPPG) is a non-contact method for detecting\nphysiological signals based on facial videos, holding high potential in various\napplications. Due to the periodicity nature of rPPG signals, the long-range\ndependency capturing capacity of the transformer was assumed to be advantageous\nfor such signals. However, existing methods have not conclusively demonstrated\nthe superior performance of transformers over traditional convolutional neural\nnetworks. This may be attributed to the quadratic scaling exhibited by\ntransformer with sequence length, resulting in coarse-grained feature\nextraction, which in turn affects robustness and generalization. To address\nthat, this paper proposes a periodic sparse attention mechanism based on\ntemporal attention sparsity induced by periodicity. A pre-attention stage is\nintroduced before the conventional attention mechanism. This stage learns\nperiodic patterns to filter out a large number of irrelevant attention\ncomputations, thus enabling fine-grained feature extraction. Moreover, to\naddress the issue of fine-grained features being more susceptible to noise\ninterference, a fusion stem is proposed to effectively guide self-attention\ntowards rPPG features. It can be easily integrated into existing methods to\nenhance their performance. Extensive experiments show that the proposed method\nachieves state-of-the-art performance in both intra-dataset and cross-dataset\nevaluations. The codes are available at\nhttps://github.com/zizheng-guo/RhythmFormer.",
    "published": "2024-02-20T07:56:02Z",
    "updated": "2025-02-20T12:02:11Z",
    "authors": [
      "Bochao Zou",
      "Zizheng Guo",
      "Jiansheng Chen",
      "Junbao Zhuo",
      "Weiran Huang",
      "Huimin Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.12981v1",
    "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
    "summary": "Key-value (KV) caching plays an essential role in accelerating decoding for\ntransformer-based autoregressive large language models (LLMs). However, the\namount of memory required to store the KV cache can become prohibitive at long\nsequence lengths and large batch sizes. Since the invention of the transformer,\ntwo of the most effective interventions discovered for reducing the size of the\nKV cache have been Multi-Query Attention (MQA) and its generalization,\nGrouped-Query Attention (GQA). MQA and GQA both modify the design of the\nattention block so that multiple query heads can share a single key/value head,\nreducing the number of distinct key/value heads by a large factor while only\nminimally degrading accuracy. In this paper, we show that it is possible to\ntake Multi-Query Attention a step further by also sharing key and value heads\nbetween adjacent layers, yielding a new attention design we call Cross-Layer\nAttention (CLA). With CLA, we find that it is possible to reduce the size of\nthe KV cache by another 2x while maintaining nearly the same accuracy as\nunmodified MQA. In experiments training 1B- and 3B-parameter models from\nscratch, we demonstrate that CLA provides a Pareto improvement over the\nmemory/accuracy tradeoffs which are possible with traditional MQA, enabling\ninference with longer sequence lengths and larger batch sizes than would\notherwise be possible",
    "published": "2024-05-21T17:59:29Z",
    "updated": "2024-05-21T17:59:29Z",
    "authors": [
      "William Brandon",
      "Mayank Mishra",
      "Aniruddha Nrusimha",
      "Rameswar Panda",
      "Jonathan Ragan Kelly"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.18874v2",
    "title": "Are queries and keys always relevant? A case study on Transformer wave\n  functions",
    "summary": "The dot product attention mechanism, originally designed for natural language\nprocessing tasks, is a cornerstone of modern Transformers. It adeptly captures\nsemantic relationships between word pairs in sentences by computing a\nsimilarity overlap between queries and keys. In this work, we explore the\nsuitability of Transformers, focusing on their attention mechanisms, in the\nspecific domain of the parametrization of variational wave functions to\napproximate ground states of quantum many-body spin Hamiltonians. Specifically,\nwe perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg\nmodel, a common benchmark in the field of quantum many-body systems on lattice.\nBy comparing the performance of standard attention mechanisms with a simplified\nversion that excludes queries and keys, relying solely on positions, we achieve\ncompetitive results while reducing computational cost and parameter usage.\nFurthermore, through the analysis of the attention maps generated by standard\nattention mechanisms, we show that the attention weights become effectively\ninput-independent at the end of the optimization. We support the numerical\nresults with analytical calculations, providing physical insights of why\nqueries and keys should be, in principle, omitted from the attention mechanism\nwhen studying large systems.",
    "published": "2024-05-29T08:32:37Z",
    "updated": "2025-01-13T15:23:47Z",
    "authors": [
      "Riccardo Rende",
      "Luciano Loris Viteritti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.12502v3",
    "title": "Transformer Neural Processes - Kernel Regression",
    "summary": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nOriginally developed as a scalable alternative to Gaussian Processes (GPs),\nwhich are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs\ncan often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their\nattention mechanism. We introduce the Transformer Neural Process - Kernel\nRegression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block\n(KRBlock), a simple, extensible, and parameter efficient transformer block with\ncomplexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of\ncontext and test points, respectively; (2) a kernel-based attention bias; and\n(3) two novel attention mechanisms: scan attention (SA), a memory-efficient\nscan-based attention that when paired with a kernel-based bias can make TNP-KR\ntranslation invariant, and deep kernel attention (DKA), a Performer-style\nattention that implicitly incoporates a distance bias and further reduces\ncomplexity to $O(n_c)$. These enhancements enable both TNP-KR variants to\nperform inference with 100K context points on over 1M test points in under a\nminute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian\noptimization, image completion, and epidemiology, TNP-KR with DKA outperforms\nits Performer counterpart on nearly every benchmark, while TNP-KR with SA\nachieves state-of-the-art results.",
    "published": "2024-11-19T13:40:49Z",
    "updated": "2025-02-11T11:03:24Z",
    "authors": [
      "Daniel Jenson",
      "Jhonathan Navott",
      "Mengyan Zhang",
      "Makkunda Sharma",
      "Elizaveta Semenova",
      "Seth Flaxman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.17214v3",
    "title": "MAT: Multi-Range Attention Transformer for Efficient Image\n  Super-Resolution",
    "summary": "Image super-resolution (SR) has significantly advanced through the adoption\nof Transformer architectures. However, conventional techniques aimed at\nenlarging the self-attention window to capture broader contexts come with\ninherent drawbacks, especially the significantly increased computational\ndemands. Moreover, the feature perception within a fixed-size window of\nexisting models restricts the effective receptive field (ERF) and the\nintermediate feature diversity. We demonstrate that a flexible integration of\nattention across diverse spatial extents can yield significant performance\nenhancements. In line with this insight, we introduce Multi-Range Attention\nTransformer (MAT) for SR tasks. MAT leverages the computational advantages\ninherent in dilation operation, in conjunction with self-attention mechanism,\nto facilitate both multi-range attention (MA) and sparse multi-range attention\n(SMA), enabling efficient capture of both regional and sparse global features.\nCombined with local feature extraction, MAT adeptly capture dependencies across\nvarious spatial ranges, improving the diversity and efficacy of its feature\nrepresentations. We also introduce the MSConvStar module, which augments the\nmodel's ability for multi-range representation learning. Comprehensive\nexperiments show that our MAT exhibits superior performance to existing\nstate-of-the-art SR models with remarkable efficiency (~3.3 faster than\nSRFormer-light).",
    "published": "2024-11-26T08:30:31Z",
    "updated": "2025-03-19T03:09:55Z",
    "authors": [
      "Chengxing Xie",
      "Xiaoming Zhang",
      "Linze Li",
      "Yuqian Fu",
      "Biao Gong",
      "Tianrui Li",
      "Kai Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.18778v1",
    "title": "Unified Local and Global Attention Interaction Modeling for Vision\n  Transformers",
    "summary": "We present a novel method that extends the self-attention mechanism of a\nvision transformer (ViT) for more accurate object detection across diverse\ndatasets. ViTs show strong capability for image understanding tasks such as\nobject detection, segmentation, and classification. This is due in part to\ntheir ability to leverage global information from interactions among visual\ntokens. However, the self-attention mechanism in ViTs are limited because they\ndo not allow visual tokens to exchange local or global information with\nneighboring features before computing global attention. This is problematic\nbecause tokens are treated in isolation when attending (matching) to other\ntokens, and valuable spatial relationships are overlooked. This isolation is\nfurther compounded by dot-product similarity operations that make tokens from\ndifferent semantic classes appear visually similar. To address these\nlimitations, we introduce two modifications to the traditional self-attention\nframework; a novel aggressive convolution pooling strategy for local feature\nmixing, and a new conceptual attention transformation to facilitate interaction\nand feature exchange between semantic concepts. Experimental results\ndemonstrate that local and global information exchange among visual features\nbefore self-attention significantly improves performance on challenging object\ndetection tasks and generalizes across multiple benchmark datasets and\nchallenging medical datasets. We publish source code and a novel dataset of\ncancerous tumors (chimeric cell clusters).",
    "published": "2024-12-25T04:53:19Z",
    "updated": "2024-12-25T04:53:19Z",
    "authors": [
      "Tan Nguyen",
      "Coy D. Heldermon",
      "Corey Toler-Franklin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.14376v2",
    "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM\n  Kernels",
    "summary": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels\n(Dao, 2024). Leveraging the chunkwise-parallel formulation of linear RNNs,\nFlash Linear Attention (FLA) (Yang & Zhang, 2024) shows that linear RNN kernels\nare faster than Flash Attention, by parallelizing over chunks of the input\nsequence. However, since the chunk size of FLA is limited, many intermediate\nstates must be materialized in GPU memory. This leads to low arithmetic\nintensity and causes high memory consumption and IO cost, especially for\nlong-context pre-training. In this work, we present Tiled Flash Linear\nAttention (TFLA), a novel kernel algorithm for linear RNNs, that enables\narbitrary large chunk sizes and high arithmetic intensity by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM (Beck et al., 2024). Second, we\npropose an mLSTM variant with sigmoid input gate and reduced computation for\neven faster kernel runtimes at equal language modeling performance. In our\nspeed benchmarks, we show that our new mLSTM kernels based on TFLA outperform\nhighly optimized Flash Attention, Linear Attention and Mamba kernels, setting a\nnew state of the art for efficient long-context sequence modeling primitives.",
    "published": "2025-03-18T16:09:47Z",
    "updated": "2025-05-10T08:07:13Z",
    "authors": [
      "Maximilian Beck",
      "Korbinian PÃ¶ppel",
      "Phillip Lippe",
      "Sepp Hochreiter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.09318v1",
    "title": "Efficient Transformer-Based Piano Transcription With Sparse Attention\n  Mechanisms",
    "summary": "This paper investigates automatic piano transcription based on\ncomputationally-efficient yet high-performant variants of the Transformer that\ncan capture longer-term dependency over the whole musical piece. Recently,\ntransformer-based sequence-to-sequence models have demonstrated excellent\nperformance in piano transcription. These models, however, fail to deal with\nthe whole piece at once due to the quadratic complexity of the self-attention\nmechanism, and music signals are thus typically processed in a sliding-window\nmanner in practice. To overcome this limitation, we propose an efficient\narchitecture with sparse attention mechanisms. Specifically, we introduce\nsliding-window self-attention mechanisms for both the encoder and decoder, and\na hybrid global-local cross-attention mechanism that attends to various spans\naccording to the MIDI token types. We also use a hierarchical pooling strategy\nbetween the encoder and decoder to further reduce computational load. Our\nexperiments on the MAESTRO dataset showed that the proposed model achieved a\nsignificant reduction in computational cost and memory usage, accelerating\ninference speed, while maintaining transcription performance comparable to the\nfull-attention baseline. This allows for training with longer audio contexts on\nthe same hardware, demonstrating the viability of sparse attention for building\nefficient and high-performance piano transcription systems. The code is\navailable at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.",
    "published": "2025-09-11T10:02:11Z",
    "updated": "2025-09-11T10:02:11Z",
    "authors": [
      "Weixing Wei",
      "Kazuyoshi Yoshii"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25231v1",
    "title": "WDformer: A Wavelet-based Differential Transformer Model for Time Series\n  Forecasting",
    "summary": "Time series forecasting has various applications, such as meteorological\nrainfall prediction, traffic flow analysis, financial forecasting, and\noperational load monitoring for various systems. Due to the sparsity of time\nseries data, relying solely on time-domain or frequency-domain modeling limits\nthe model's ability to fully leverage multi-domain information. Moreover, when\napplied to time series forecasting tasks, traditional attention mechanisms tend\nto over-focus on irrelevant historical information, which may introduce noise\ninto the prediction process, leading to biased results. We proposed WDformer, a\nwavelet-based differential Transformer model. This study employs the wavelet\ntransform to conduct a multi-resolution analysis of time series data. By\nleveraging the advantages of joint representation in the time-frequency domain,\nit accurately extracts the key information components that reflect the\nessential characteristics of the data. Furthermore, we apply attention\nmechanisms on inverted dimensions, allowing the attention mechanism to capture\nrelationships between multiple variables. When performing attention\ncalculations, we introduced the differential attention mechanism, which\ncomputes the attention score by taking the difference between two separate\nsoftmax attention matrices. This approach enables the model to focus more on\nimportant information and reduce noise. WDformer has achieved state-of-the-art\n(SOTA) results on multiple challenging real-world datasets, demonstrating its\naccuracy and effectiveness. Code is available at\nhttps://github.com/xiaowangbc/WDformer.",
    "published": "2025-09-25T02:43:51Z",
    "updated": "2025-09-25T02:43:51Z",
    "authors": [
      "Xiaojian Wang",
      "Chaoli Zhang",
      "Zhonglong Zheng",
      "Yunliang Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.00907v1",
    "title": "Transformers as Intrinsic Optimizers: Forward Inference through the\n  Energy Principle",
    "summary": "Transformers have demonstrated strong adaptability across a wide range of\ntasks and have become the backbone of modern Large Language Models (LLMs).\nHowever, their underlying mechanisms remain open for further exploration. The\nenergy-based perspective has long provided a valuable principle for\nunderstanding neural computation. In this paper, we revisit the principle of\nenergy as a lens to understand attention-based Transformer models. We present a\nunified energy-based framework which is composed of three key components: the\nglobal energy $F^*$, the energy function $E_i$ and the employed gradient\ndescent (GD) form. Within this framework, standard softmax attention can be\nviewed as a special case of minimizing the Helmholtz free energy as $F^*$ using\nstandard GD when $E_i$ takes the form of elastic potential energy, with\nresidual connections ensuring that this optimization proceeds in an incremental\nmanner. In addition, linear attentions can also be naturally incorporated into\nthis framework by adjusting the corresponding energy forms. We also extend the\nabove analysis to the multi-head setting, where the energy is defined across\nmultiple low-dimensional subspaces. Building on this framework, we propose\nenergy-based modifications of attention structures. Inspired by classical GD\nalgorithms, we extend the original attention formulation based on standard GD\nto the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's\nmethod variants, each inducing a corresponding new attention structure. Our\nexperiments provide preliminary support for the potential of the energy-based\nframework for designing attention mechanisms.",
    "published": "2025-11-02T11:58:50Z",
    "updated": "2025-11-02T11:58:50Z",
    "authors": [
      "Ruifeng Ren",
      "Sheng Ouyang",
      "Huayi Tang",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.06587v1",
    "title": "Sum-Product-Attention Networks: Leveraging Self-Attention in\n  Probabilistic Circuits",
    "summary": "Probabilistic circuits (PCs) have become the de-facto standard for learning\nand inference in probabilistic modeling. We introduce Sum-Product-Attention\nNetworks (SPAN), a new generative model that integrates probabilistic circuits\nwith Transformers. SPAN uses self-attention to select the most relevant parts\nof a probabilistic circuit, here sum-product networks, to improve the modeling\ncapability of the underlying sum-product network. We show that while modeling,\nSPAN focuses on a specific set of independent assumptions in every product\nlayer of the sum-product network. Our empirical evaluations show that SPAN\noutperforms state-of-the-art probabilistic generative models on various\nbenchmark data sets as well is an efficient generative image model.",
    "published": "2021-09-14T11:20:54Z",
    "updated": "2021-09-14T11:20:54Z",
    "authors": [
      "Zhongjie Yu",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.13465v1",
    "title": "Attention Based Neural Networks for Wireless Channel Estimation",
    "summary": "In this paper, we deploy the self-attention mechanism to achieve improved\nchannel estimation for orthogonal frequency-division multiplexing waveforms in\nthe downlink. Specifically, we propose a new hybrid encoder-decoder structure\n(called HA02) for the first time which exploits the attention mechanism to\nfocus on the most important input information. In particular, we implement a\ntransformer encoder block as the encoder to achieve the sparsity in the input\nfeatures and a residual neural network as the decoder respectively, inspired by\nthe success of the attention mechanism. Using 3GPP channel models, our\nsimulations show superior estimation performance compared with other candidate\nneural network methods for channel estimation.",
    "published": "2022-04-28T12:54:19Z",
    "updated": "2022-04-28T12:54:19Z",
    "authors": [
      "Dianxin Luan",
      "John Thompson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.10247v1",
    "title": "Generalized Attention Mechanism and Relative Position for Transformer",
    "summary": "In this paper, we propose generalized attention mechanism (GAM) by first\nsuggesting a new interpretation for self-attention mechanism of Vaswani et al.\n. Following the interpretation, we provide description for different variants\nof attention mechanism which together form GAM. Further, we propose a new\nrelative position representation within the framework of GAM. This\nrepresentation can be easily utilized for cases in which elements next to each\nother in input sequence can be at random locations in actual dataset/corpus.",
    "published": "2022-07-24T00:57:06Z",
    "updated": "2022-07-24T00:57:06Z",
    "authors": [
      "R. V. R. Pandya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1909.00188v1",
    "title": "Improving Multi-Head Attention with Capsule Networks",
    "summary": "Multi-head attention advances neural machine translation by working out\nmultiple versions of attention in different subspaces, but the neglect of\nsemantic overlapping between subspaces increases the difficulty of translation\nand consequently hinders the further improvement of translation performance. In\nthis paper, we employ capsule networks to comb the information from the\nmultiple heads of the attention so that similar information can be clustered\nand unique information can be reserved. To this end, we adopt two routing\nmechanisms of Dynamic Routing and EM Routing, to fulfill the clustering and\nseparating. We conducted experiments on Chinese-to-English and\nEnglish-to-German translation tasks and got consistent improvements over the\nstrong Transformer baseline.",
    "published": "2019-08-31T10:43:06Z",
    "updated": "2019-08-31T10:43:06Z",
    "authors": [
      "Shuhao Gu",
      "Yang Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.12300v2",
    "title": "Tree Decomposition Attention for AMR-to-Text Generation",
    "summary": "Text generation from AMR requires mapping a semantic graph to a string that\nit annotates. Transformer-based graph encoders, however, poorly capture vertex\ndependencies that may benefit sequence prediction. To impose order on an\nencoder, we locally constrain vertex self-attention using a graph's tree\ndecomposition. Instead of forming a full query-key bipartite graph, we restrict\nattention to vertices in parent, subtree, and same-depth bags of a vertex. This\nhierarchical context lends both sparsity and structure to vertex state updates.\nWe apply dynamic programming to derive a forest of tree decompositions,\nchoosing the most structurally similar tree to the AMR. Our system outperforms\na self-attentive baseline by 1.6 BLEU and 1.8 chrF++.",
    "published": "2021-08-27T14:24:25Z",
    "updated": "2021-09-01T19:54:57Z",
    "authors": [
      "Lisa Jin",
      "Daniel Gildea"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.11829v1",
    "title": "System 2 Attention (is something you might need too)",
    "summary": "Soft attention in Transformer-based Large Language Models (LLMs) is\nsusceptible to incorporating irrelevant information from the context into its\nlatent representations, which adversely affects next token generations. To help\nrectify these issues, we introduce System 2 Attention (S2A), which leverages\nthe ability of LLMs to reason in natural language and follow instructions in\norder to decide what to attend to. S2A regenerates the input context to only\ninclude the relevant portions, before attending to the regenerated context to\nelicit the final response. In experiments, S2A outperforms standard\nattention-based LLMs on three tasks containing opinion or irrelevant\ninformation, QA, math word problems and longform generation, where S2A\nincreases factuality and objectivity, and decreases sycophancy.",
    "published": "2023-11-20T15:04:50Z",
    "updated": "2023-11-20T15:04:50Z",
    "authors": [
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.11793v3",
    "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency\n  for Clinical Text Classification",
    "summary": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation.",
    "published": "2025-04-16T05:59:29Z",
    "updated": "2025-04-18T20:58:03Z",
    "authors": [
      "Yue Li",
      "Lihong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.02192v3",
    "title": "Long-Short Transformer: Efficient Transformers for Language and Vision",
    "summary": "Transformers have achieved success in both language and vision domains.\nHowever, it is prohibitively expensive to scale them to long sequences such as\nlong documents or high-resolution images, because self-attention mechanism has\nquadratic time and memory complexities with respect to the input sequence\nlength. In this paper, we propose Long-Short Transformer (Transformer-LS), an\nefficient self-attention mechanism for modeling long sequences with linear\ncomplexity for both language and vision tasks. It aggregates a novel long-range\nattention with dynamic projection to model distant correlations and a\nshort-term attention to capture fine-grained local correlations. We propose a\ndual normalization strategy to account for the scale mismatch between the two\nattention mechanisms. Transformer-LS can be applied to both autoregressive and\nbidirectional models without additional complexity. Our method outperforms the\nstate-of-the-art models on multiple tasks in language and vision domains,\nincluding the Long Range Arena benchmark, autoregressive language modeling, and\nImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on\nenwik8 using half the number of parameters than previous method, while being\nfaster and is able to handle 3x as long sequences compared to its\nfull-attention version on the same hardware. On ImageNet, it can obtain the\nstate-of-the-art results (e.g., a moderate size of 55.8M model solely trained\non 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more\nscalable on high-resolution images. The source code and models are released at\nhttps://github.com/NVIDIA/transformer-ls .",
    "published": "2021-07-05T18:00:14Z",
    "updated": "2021-12-07T07:23:24Z",
    "authors": [
      "Chen Zhu",
      "Wei Ping",
      "Chaowei Xiao",
      "Mohammad Shoeybi",
      "Tom Goldstein",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.06446v2",
    "title": "SpectFormer: Frequency and Attention is what you need in a Vision\n  Transformer",
    "summary": "Vision transformers have been applied successfully for image recognition\ntasks. There have been either multi-headed self-attention based (ViT\n\\cite{dosovitskiy2020image}, DeIT, \\cite{touvron2021training}) similar to the\noriginal work in textual models or more recently based on spectral layers\n(Fnet\\cite{lee2021fnet}, GFNet\\cite{rao2021global},\nAFNO\\cite{guibas2021efficient}). We hypothesize that both spectral and\nmulti-headed attention plays a major role. We investigate this hypothesis\nthrough this work and observe that indeed combining spectral and multi-headed\nattention layers provides a better transformer architecture. We thus propose\nthe novel Spectformer architecture for transformers that combines spectral and\nmulti-headed attention layers. We believe that the resulting representation\nallows the transformer to capture the feature representation appropriately and\nit yields improved performance over other transformer representations. For\ninstance, it improves the top-1 accuracy by 2\\% on ImageNet compared to both\nGFNet-H and LiT. SpectFormer-S reaches 84.25\\% top-1 accuracy on ImageNet-1K\n(state of the art for small version). Further, Spectformer-L achieves 85.7\\%\nthat is the state of the art for the comparable base version of the\ntransformers. We further ensure that we obtain reasonable results in other\nscenarios such as transfer learning on standard datasets such as CIFAR-10,\nCIFAR-100, Oxford-IIIT-flower, and Standford Car datasets. We then investigate\nits use in downstream tasks such of object detection and instance segmentation\non the MS-COCO dataset and observe that Spectformer shows consistent\nperformance that is comparable to the best backbones and can be further\noptimized and improved. Hence, we believe that combined spectral and attention\nlayers are what are needed for vision transformers.",
    "published": "2023-04-13T12:27:17Z",
    "updated": "2023-04-14T22:20:46Z",
    "authors": [
      "Badri N. Patro",
      "Vinay P. Namboodiri",
      "Vijay Srinivas Agneeswaran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.09244v1",
    "title": "Investigation of Hierarchical Spectral Vision Transformer Architecture\n  for Classification of Hyperspectral Imagery",
    "summary": "In the past three years, there has been significant interest in hyperspectral\nimagery (HSI) classification using vision Transformers for analysis of remotely\nsensed data. Previous research predominantly focused on the empirical\nintegration of convolutional neural networks (CNNs) to augment the network's\ncapability to extract local feature information. Yet, the theoretical\njustification for vision Transformers out-performing CNN architectures in HSI\nclassification remains a question. To address this issue, a unified\nhierarchical spectral vision Transformer architecture, specifically tailored\nfor HSI classification, is investigated. In this streamlined yet effective\nvision Transformer architecture, multiple mixer modules are strategically\nintegrated separately. These include the CNN-mixer, which executes convolution\noperations; the spatial self-attention (SSA)-mixer and channel self-attention\n(CSA)-mixer, both of which are adaptations of classical self-attention blocks;\nand hybrid models such as the SSA+CNN-mixer and CSA+CNN-mixer, which merge\nconvolution with self-attention operations. This integration facilitates the\ndevelopment of a broad spectrum of vision Transformer-based models tailored for\nHSI classification. In terms of the training process, a comprehensive analysis\nis performed, contrasting classical CNN models and vision Transformer-based\ncounterparts, with particular attention to disturbance robustness and the\ndistribution of the largest eigenvalue of the Hessian. From the evaluations\nconducted on various mixer models rooted in the unified architecture, it is\nconcluded that the unique strength of vision Transformers can be attributed to\ntheir overarching architecture, rather than being exclusively reliant on\nindividual multi-head self-attention (MSA) components.",
    "published": "2024-09-14T00:53:13Z",
    "updated": "2024-09-14T00:53:13Z",
    "authors": [
      "Wei Liu",
      "Saurabh Prasad",
      "Melba Crawford"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.14255v1",
    "title": "Degenerate Swin to Win: Plain Window-based Transformer without\n  Sophisticated Operations",
    "summary": "The formidable accomplishment of Transformers in natural language processing\nhas motivated the researchers in the computer vision community to build Vision\nTransformers. Compared with the Convolution Neural Networks (CNN), a Vision\nTransformer has a larger receptive field which is capable of characterizing the\nlong-range dependencies. Nevertheless, the large receptive field of Vision\nTransformer is accompanied by the huge computational cost. To boost efficiency,\nthe window-based Vision Transformers emerge. They crop an image into several\nlocal windows, and the self-attention is conducted within each window. To bring\nback the global receptive field, window-based Vision Transformers have devoted\na lot of efforts to achieving cross-window communications by developing several\nsophisticated operations. In this work, we check the necessity of the key\ndesign element of Swin Transformer, the shifted window partitioning. We\ndiscover that a simple depthwise convolution is sufficient for achieving\neffective cross-window communications. Specifically, with the existence of the\ndepthwise convolution, the shifted window configuration in Swin Transformer\ncannot lead to an additional performance improvement. Thus, we degenerate the\nSwin Transformer to a plain Window-based (Win) Transformer by discarding\nsophisticated shifted window partitioning. The proposed Win Transformer is\nconceptually simpler and easier for implementation than Swin Transformer.\nMeanwhile, our Win Transformer achieves consistently superior performance than\nSwin Transformer on multiple computer vision tasks, including image\nrecognition, semantic segmentation, and object detection.",
    "published": "2022-11-25T17:36:20Z",
    "updated": "2022-11-25T17:36:20Z",
    "authors": [
      "Tan Yu",
      "Ping Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.02143v2",
    "title": "Random Feature Attention",
    "summary": "Transformers are state-of-the-art models for a variety of sequence modeling\ntasks. At their core is an attention function which models pairwise\ninteractions between the inputs at every timestep. While attention is powerful,\nit does not scale efficiently to long sequences due to its quadratic time and\nspace complexity in the sequence length. We propose RFA, a linear time and\nspace attention that uses random feature methods to approximate the softmax\nfunction, and explore its application in transformers. RFA can be used as a\ndrop-in replacement for conventional softmax attention and offers a\nstraightforward way of learning with recency bias through an optional gating\nmechanism. Experiments on language modeling and machine translation demonstrate\nthat RFA achieves similar or better performance compared to strong transformer\nbaselines. In the machine translation experiment, RFA decodes twice as fast as\na vanilla transformer. Compared to existing efficient transformer variants, RFA\nis competitive in terms of both accuracy and efficiency on three long text\nclassification datasets. Our analysis shows that RFA's efficiency gains are\nespecially notable on long sequences, suggesting that RFA will be particularly\nuseful in tasks that require working with large inputs, fast decoding speed, or\nlow memory footprints.",
    "published": "2021-03-03T02:48:56Z",
    "updated": "2021-03-19T21:24:06Z",
    "authors": [
      "Hao Peng",
      "Nikolaos Pappas",
      "Dani Yogatama",
      "Roy Schwartz",
      "Noah A. Smith",
      "Lingpeng Kong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.11328v2",
    "title": "K-Order Graph-oriented Transformer with GraAttention for 3D Pose and\n  Shape Estimation",
    "summary": "We propose a novel attention-based 2D-to-3D pose estimation network for\ngraph-structured data, named KOG-Transformer, and a 3D pose-to-shape estimation\nnetwork for hand data, named GASE-Net. Previous 3D pose estimation methods have\nfocused on various modifications to the graph convolution kernel, such as\nabandoning weight sharing or increasing the receptive field. Some of these\nmethods employ attention-based non-local modules as auxiliary modules. In order\nto better model the relationship between nodes in graph-structured data and\nfuse the information of different neighbor nodes in a differentiated way, we\nmake targeted modifications to the attention module and propose two modules\ndesigned for graph-structured data, graph relative positional encoding\nmulti-head self-attention (GR-MSA) and K-order graph-oriented multi-head\nself-attention (KOG-MSA). By stacking GR-MSA and KOG-MSA, we propose a novel\nnetwork KOG-Transformer for 2D-to-3D pose estimation. Furthermore, we propose a\nnetwork for shape estimation on hand data, called GraAttention shape estimation\nnetwork (GASE-Net), which takes a 3D pose as input and gradually models the\nshape of the hand from sparse to dense. We have empirically shown the\nsuperiority of KOG-Transformer through extensive experiments. Experimental\nresults show that KOG-Transformer significantly outperforms the previous\nstate-of-the-art methods on the benchmark dataset Human3.6M. We evaluate the\neffect of GASE-Net on two public available hand datasets, ObMan and\nInterHand2.6M. GASE-Net can predict the corresponding shape for input pose with\nstrong generalization ability.",
    "published": "2022-08-24T06:54:03Z",
    "updated": "2022-09-24T02:20:27Z",
    "authors": [
      "Weixi Zhao",
      "Weiqiang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.06908v2",
    "title": "CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale\n  Attention",
    "summary": "While features of different scales are perceptually important to visual\ninputs, existing vision transformers do not yet take advantage of them\nexplicitly. To this end, we first propose a cross-scale vision transformer,\nCrossFormer. It introduces a cross-scale embedding layer (CEL) and a long-short\ndistance attention (LSDA). On the one hand, CEL blends each token with multiple\npatches of different scales, providing the self-attention module itself with\ncross-scale features. On the other hand, LSDA splits the self-attention module\ninto a short-distance one and a long-distance counterpart, which not only\nreduces the computational burden but also keeps both small-scale and\nlarge-scale features in the tokens. Moreover, through experiments on\nCrossFormer, we observe another two issues that affect vision transformers'\nperformance, i.e., the enlarging self-attention maps and amplitude explosion.\nThus, we further propose a progressive group size (PGS) paradigm and an\namplitude cooling layer (ACL) to alleviate the two issues, respectively. The\nCrossFormer incorporating with PGS and ACL is called CrossFormer++. Extensive\nexperiments show that CrossFormer++ outperforms the other vision transformers\non image classification, object detection, instance segmentation, and semantic\nsegmentation tasks. The code will be available at:\nhttps://github.com/cheerss/CrossFormer.",
    "published": "2023-03-13T07:54:29Z",
    "updated": "2023-12-01T02:13:22Z",
    "authors": [
      "Wenxiao Wang",
      "Wei Chen",
      "Qibo Qiu",
      "Long Chen",
      "Boxi Wu",
      "Binbin Lin",
      "Xiaofei He",
      "Wei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.00781v2",
    "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation",
    "summary": "Transformer architecture has emerged to be successful in a number of natural\nlanguage processing tasks. However, its applications to medical vision remain\nlargely unexplored. In this study, we present UTNet, a simple yet powerful\nhybrid Transformer architecture that integrates self-attention into a\nconvolutional neural network for enhancing medical image segmentation. UTNet\napplies self-attention modules in both encoder and decoder for capturing\nlong-range dependency at different scales with minimal overhead. To this end,\nwe propose an efficient self-attention mechanism along with relative position\nencoding that reduces the complexity of self-attention operation significantly\nfrom $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also\nproposed to recover fine-grained details from the skipped connections in the\nencoder. Our approach addresses the dilemma that Transformer requires huge\namounts of data to learn vision inductive bias. Our hybrid layer design allows\nthe initialization of Transformer into convolutional networks without a need of\npre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac\nmagnetic resonance imaging cohort. UTNet demonstrates superior segmentation\nperformance and robustness against the state-of-the-art approaches, holding the\npromise to generalize well on other medical image segmentations.",
    "published": "2021-07-02T00:56:27Z",
    "updated": "2021-09-28T00:52:51Z",
    "authors": [
      "Yunhe Gao",
      "Mu Zhou",
      "Dimitris Metaxas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.00842v2",
    "title": "Cross-view Geo-localization with Evolving Transformer",
    "summary": "In this work, we address the problem of cross-view geo-localization, which\nestimates the geospatial location of a street view image by matching it with a\ndatabase of geo-tagged aerial images. The cross-view matching task is extremely\nchallenging due to drastic appearance and geometry differences across views.\nUnlike existing methods that predominantly fall back on CNN, here we devise a\nnovel evolving geo-localization Transformer (EgoTR) that utilizes the\nproperties of self-attention in Transformer to model global dependencies, thus\nsignificantly decreasing visual ambiguities in cross-view geo-localization. We\nalso exploit the positional encoding of Transformer to help the EgoTR\nunderstand and correspond geometric configurations between ground and aerial\nimages. Compared to state-of-the-art methods that impose strong assumption on\ngeometry knowledge, the EgoTR flexibly learns the positional embeddings through\nthe training objective and hence becomes more practical in many real-world\nscenarios. Although Transformer is well suited to our task, its vanilla\nself-attention mechanism independently interacts within image patches in each\nlayer, which overlooks correlations between layers. Instead, this paper propose\na simple yet effective self-cross attention mechanism to improve the quality of\nlearned representations. The self-cross attention models global dependencies\nbetween adjacent layers, which relates between image patches while modeling how\nfeatures evolve in the previous layer. As a result, the proposed self-cross\nattention leads to more stable training, improves the generalization ability\nand encourages representations to keep evolving as the network goes deeper.\nExtensive experiments demonstrate that our EgoTR performs favorably against\nstate-of-the-art methods on standard, fine-grained and cross-dataset cross-view\ngeo-localization tasks.",
    "published": "2021-07-02T05:33:14Z",
    "updated": "2021-07-05T02:23:48Z",
    "authors": [
      "Hongji Yang",
      "Xiufan Lu",
      "Yingying Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.12364v1",
    "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot\n  MultiBox Detector",
    "summary": "Due to the success of Bidirectional Encoder Representations from Transformers\n(BERT) in natural language process (NLP), the multi-head attention transformer\nhas been more and more prevalent in computer-vision researches (CV). However,\nit still remains a challenge for researchers to put forward complex tasks such\nas vision detection and semantic segmentation. Although multiple\nTransformer-Based architectures like DETR and ViT-FRCNN have been proposed to\ncomplete object detection task, they inevitably decreases discrimination\naccuracy and brings down computational efficiency caused by the enormous\nlearning parameters and heavy computational complexity incurred by the\ntraditional self-attention operation. In order to alleviate these issues, we\npresent a novel object detection architecture, named Convolutional vision\nTransformer Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that\nbuilt on the top of Convolutional vision Transormer (CvT) with the efficient\nAttentive Single Shot MultiBox Detector (ASSD). We provide comprehensive\nempirical evidence showing that our model CvT-ASSD can leads to good system\nefficiency and performance while being pretrained on large-scale detection\ndatasets such as PASCAL VOC and MS COCO. Code has been released on public\ngithub repository at https://github.com/albert-jin/CvT-ASSD.",
    "published": "2021-10-24T06:45:33Z",
    "updated": "2021-10-24T06:45:33Z",
    "authors": [
      "Weiqiang Jin",
      "Hang Yu",
      "Hang Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.01562v2",
    "title": "ViTransPAD: Video Transformer using convolution and self-attention for\n  Face Presentation Attack Detection",
    "summary": "Face Presentation Attack Detection (PAD) is an important measure to prevent\nspoof attacks for face biometric systems. Many works based on Convolution\nNeural Networks (CNNs) for face PAD formulate the problem as an image-level\nbinary classification task without considering the context. Alternatively,\nVision Transformers (ViT) using self-attention to attend the context of an\nimage become the mainstreams in face PAD. Inspired by ViT, we propose a\nVideo-based Transformer for face PAD (ViTransPAD) with short/long-range\nspatio-temporal attention which can not only focus on local details with short\nattention within a frame but also capture long-range dependencies over frames.\nInstead of using coarse image patches with single-scale as in ViT, we propose\nthe Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate\nmulti-scale patch partitions of Q, K, V feature maps to the heads of\ntransformer in a coarse-to-fine manner, which enables to learn a fine-grained\nrepresentation to perform pixel-level discrimination for face PAD. Due to lack\ninductive biases of convolutions in pure transformers, we also introduce\nconvolutions to the proposed ViTransPAD to integrate the desirable properties\nof CNNs by using convolution patch embedding and convolution projection. The\nextensive experiments show the effectiveness of our proposed ViTransPAD with a\npreferable accuracy-computation balance, which can serve as a new backbone for\nface PAD.",
    "published": "2022-03-03T08:23:20Z",
    "updated": "2022-03-14T10:44:06Z",
    "authors": [
      "Zuheng Ming",
      "Zitong Yu",
      "Musab Al-Ghadi",
      "Muriel Visani",
      "Muhammad MuzzamilLuqman",
      "Jean-Christophe Burie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.08450v1",
    "title": "The Devil Is in the Details: Window-based Attention for Image\n  Compression",
    "summary": "Learned image compression methods have exhibited superior rate-distortion\nperformance than classical image compression standards. Most existing learned\nimage compression models are based on Convolutional Neural Networks (CNNs).\nDespite great contributions, a main drawback of CNN based model is that its\nstructure is not designed for capturing local redundancy, especially the\nnon-repetitive textures, which severely affects the reconstruction quality.\nTherefore, how to make full use of both global structure and local texture\nbecomes the core problem for learning-based image compression. Inspired by\nrecent progresses of Vision Transformer (ViT) and Swin Transformer, we found\nthat combining the local-aware attention mechanism with the global-related\nfeature learning could meet the expectation in image compression. In this\npaper, we first extensively study the effects of multiple kinds of attention\nmechanisms for local features learning, then introduce a more straightforward\nyet effective window-based local attention block. The proposed window-based\nattention is very flexible which could work as a plug-and-play component to\nenhance CNN and Transformer models. Moreover, we propose a novel Symmetrical\nTransFormer (STF) framework with absolute transformer blocks in the\ndown-sampling encoder and up-sampling decoder. Extensive experimental\nevaluations have shown that the proposed method is effective and outperforms\nthe state-of-the-art methods. The code is publicly available at\nhttps://github.com/Googolxx/STF.",
    "published": "2022-03-16T07:55:49Z",
    "updated": "2022-03-16T07:55:49Z",
    "authors": [
      "Renjie Zou",
      "Chunfeng Song",
      "Zhaoxiang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.01620v1",
    "title": "MAFormer: A Transformer Network with Multi-scale Attention Fusion for\n  Visual Recognition",
    "summary": "Vision Transformer and its variants have demonstrated great potential in\nvarious computer vision tasks. But conventional vision transformers often focus\non global dependency at a coarse level, which suffer from a learning challenge\non global relationships and fine-grained representation at a token level. In\nthis paper, we introduce Multi-scale Attention Fusion into transformer\n(MAFormer), which explores local aggregation and global feature extraction in a\ndual-stream framework for visual recognition. We develop a simple but effective\nmodule to explore the full potential of transformers for visual representation\nby learning fine-grained and coarse-grained features at a token level and\ndynamically fusing them. Our Multi-scale Attention Fusion (MAF) block consists\nof: i) a local window attention branch that learns short-range interactions\nwithin windows, aggregating fine-grained local features; ii) global feature\nextraction through a novel Global Learning with Down-sampling (GLD) operation\nto efficiently capture long-range context information within the whole image;\niii) a fusion module that self-explores the integration of both features via\nattention. Our MAFormer achieves state-of-the-art performance on common vision\ntasks. In particular, MAFormer-L achieves 85.9$\\%$ Top-1 accuracy on ImageNet,\nsurpassing CSWin-B and LV-ViT-L by 1.7$\\%$ and 0.6$\\%$ respectively. On MSCOCO,\nMAFormer outperforms the prior art CSWin by 1.7$\\%$ mAPs on object detection\nand 1.4$\\%$ on instance segmentation with similar-sized parameters,\ndemonstrating the potential to be a general backbone network.",
    "published": "2022-08-31T06:29:27Z",
    "updated": "2022-08-31T06:29:27Z",
    "authors": [
      "Yunhao Wang",
      "Huixin Sun",
      "Xiaodi Wang",
      "Bin Zhang",
      "Chao Li",
      "Ying Xin",
      "Baochang Zhang",
      "Errui Ding",
      "Shumin Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.07157v3",
    "title": "ParCNetV2: Oversized Kernel with Enhanced Attention",
    "summary": "Transformers have shown great potential in various computer vision tasks. By\nborrowing design concepts from transformers, many studies revolutionized CNNs\nand showed remarkable results. This paper falls in this line of studies.\nSpecifically, we propose a new convolutional neural network, ParCNetV2, that\nextends position-aware circular convolution (ParCNet) with oversized\nconvolutions and bifurcate gate units to enhance attention. The oversized\nconvolution employs a kernel with twice the input size to model long-range\ndependencies through a global receptive field. Simultaneously, it achieves\nimplicit positional encoding by removing the shift-invariant property from\nconvolution kernels, i.e., the effective kernels at different spatial locations\nare different when the kernel size is twice as large as the input size. The\nbifurcate gate unit implements an attention mechanism similar to self-attention\nin transformers. It is applied through element-wise multiplication of the two\nbranches, one serves as feature transformation while the other serves as\nattention weights. Additionally, we introduce a uniform local-global\nconvolution block to unify the design of the early and late stage convolution\nblocks. Extensive experiments demonstrate the superiority of our method over\nother convolutional neural networks and hybrid models that combine CNNs and\ntransformers. Code will be released.",
    "published": "2022-11-14T07:22:55Z",
    "updated": "2023-03-16T02:38:06Z",
    "authors": [
      "Ruihan Xu",
      "Haokui Zhang",
      "Wenze Hu",
      "Shiliang Zhang",
      "Xiaoyu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.14508v1",
    "title": "3D Brainformer: 3D Fusion Transformer for Brain Tumor Segmentation",
    "summary": "Magnetic resonance imaging (MRI) is critically important for brain mapping in\nboth scientific research and clinical studies. Precise segmentation of brain\ntumors facilitates clinical diagnosis, evaluations, and surgical planning. Deep\nlearning has recently emerged to improve brain tumor segmentation and achieved\nimpressive results. Convolutional architectures are widely used to implement\nthose neural networks. By the nature of limited receptive fields, however,\nthose architectures are subject to representing long-range spatial dependencies\nof the voxel intensities in MRI images. Transformers have been leveraged\nrecently to address the above limitations of convolutional networks.\nUnfortunately, the majority of current Transformers-based methods in\nsegmentation are performed with 2D MRI slices, instead of 3D volumes. Moreover,\nit is difficult to incorporate the structures between layers because each head\nis calculated independently in the Multi-Head Self-Attention mechanism (MHSA).\nIn this work, we proposed a 3D Transformer-based segmentation approach. We\ndeveloped a Fusion-Head Self-Attention mechanism (FHSA) to combine each\nattention head through attention logic and weight mapping, for the exploration\nof the long-range spatial dependencies in 3D MRI images. We implemented a\nplug-and-play self-attention module, named the Infinite Deformable Fusion\nTransformer Module (IDFTM), to extract features on any deformable feature maps.\nWe applied our approach to the task of brain tumor segmentation, and assessed\nit on the public BRATS datasets. The experimental results demonstrated that our\nproposed approach achieved superior performance, in comparison to several\nstate-of-the-art segmentation methods.",
    "published": "2023-04-28T02:11:29Z",
    "updated": "2023-04-28T02:11:29Z",
    "authors": [
      "Rui Nian",
      "Guoyao Zhang",
      "Yao Sui",
      "Yuqi Qian",
      "Qiuying Li",
      "Mingzhang Zhao",
      "Jianhui Li",
      "Ali Gholipour",
      "Simon K. Warfield"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.12929v2",
    "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads\n  Do Nothing",
    "summary": "Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.",
    "published": "2023-06-22T14:39:04Z",
    "updated": "2023-11-09T14:05:51Z",
    "authors": [
      "Yelysei Bondarenko",
      "Markus Nagel",
      "Tijmen Blankevoort"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.02974v1",
    "title": "Cross-Spatial Pixel Integration and Cross-Stage Feature Fusion Based\n  Transformer Network for Remote Sensing Image Super-Resolution",
    "summary": "Remote sensing image super-resolution (RSISR) plays a vital role in enhancing\nspatial detials and improving the quality of satellite imagery. Recently,\nTransformer-based models have shown competitive performance in RSISR. To\nmitigate the quadratic computational complexity resulting from global\nself-attention, various methods constrain attention to a local window,\nenhancing its efficiency. Consequently, the receptive fields in a single\nattention layer are inadequate, leading to insufficient context modeling.\nFurthermore, while most transform-based approaches reuse shallow features\nthrough skip connections, relying solely on these connections treats shallow\nand deep features equally, impeding the model's ability to characterize them.\nTo address these issues, we propose a novel transformer architecture called\nCross-Spatial Pixel Integration and Cross-Stage Feature Fusion Based\nTransformer Network (SPIFFNet) for RSISR. Our proposed model effectively\nenhances global cognition and understanding of the entire image, facilitating\nefficient integration of features cross-stages. The model incorporates\ncross-spatial pixel integration attention (CSPIA) to introduce contextual\ninformation into a local window, while cross-stage feature fusion attention\n(CSFFA) adaptively fuses features from the previous stage to improve feature\nexpression in line with the requirements of the current stage. We conducted\ncomprehensive experiments on multiple benchmark datasets, demonstrating the\nsuperior performance of our proposed SPIFFNet in terms of both quantitative\nmetrics and visual quality when compared to state-of-the-art methods.",
    "published": "2023-07-06T13:19:06Z",
    "updated": "2023-07-06T13:19:06Z",
    "authors": [
      "Yuting Lu",
      "Lingtong Min",
      "Binglu Wang",
      "Le Zheng",
      "Xiaoxu Wang",
      "Yongqiang Zhao",
      "Teng Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.10365v1",
    "title": "SPT: Fine-Tuning Transformer-based Language Models Efficiently with\n  Sparsification",
    "summary": "Transformer-based large language models (e.g., BERT and GPT) achieve great\nsuccess, and fine-tuning, which tunes a pre-trained model on a task-specific\ndataset, is the standard practice to utilize these models for downstream tasks.\nHowever, Transformer fine-tuning has long running time and high memory\nconsumption due to the large size of the models. We propose the SPT system to\nfine-tune Transformer-based models efficiently by introducing sparsity. We\nobserve that the memory consumption of Transformer mainly comes from storing\nattention weights for multi-head attention (MHA), and the majority of running\ntime is spent on feed-forward network (FFN). Thus, we design the sparse MHA\nmodule, which computes and stores only large attention weights to reduce memory\nconsumption, and the routed FFN module, which dynamically activates a subset of\nmodel parameters for each token to reduce computation cost. We implement SPT on\nPyTorch and customize CUDA kernels to run sparse MHA and routed FFN\nefficiently. Specifically, we use product quantization to identify the large\nattention weights and compute attention via sparse matrix multiplication for\nsparse MHA. For routed FFN, we batch the tokens according to their activated\nmodel parameters for efficient computation. We conduct extensive experiments to\nevaluate SPT on various model configurations. The results show that SPT\nconsistently outperforms well-optimized baselines, reducing the peak memory\nconsumption by up to 50% and accelerating fine-tuning by up to 2.2x.",
    "published": "2023-12-16T07:44:52Z",
    "updated": "2023-12-16T07:44:52Z",
    "authors": [
      "Yuntao Gui",
      "Xiao Yan",
      "Peiqi Yin",
      "Han Yang",
      "James Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.10559v1",
    "title": "Unveiling Induction Heads: Provable Training Dynamics and Feature\n  Learning in Transformers",
    "summary": "In-context learning (ICL) is a cornerstone of large language model (LLM)\nfunctionality, yet its theoretical foundations remain elusive due to the\ncomplexity of transformer architectures. In particular, most existing work only\ntheoretically explains how the attention mechanism facilitates ICL under\ncertain data models. It remains unclear how the other building blocks of the\ntransformer contribute to ICL. To address this question, we study how a\ntwo-attention-layer transformer is trained to perform ICL on $n$-gram Markov\nchain data, where each token in the Markov chain statistically depends on the\nprevious $n$ tokens. We analyze a sophisticated transformer model featuring\nrelative positional embedding, multi-head softmax attention, and a feed-forward\nlayer with normalization. We prove that the gradient flow with respect to a\ncross-entropy ICL loss converges to a limiting model that performs a\ngeneralized version of the induction head mechanism with a learned feature,\nresulting from the congruous contribution of all the building blocks. In the\nlimiting model, the first attention layer acts as a $\\mathit{copier}$, copying\npast tokens within a given window to each position, and the feed-forward\nnetwork with normalization acts as a $\\mathit{selector}$ that generates a\nfeature vector by only looking at informationally relevant parents from the\nwindow. Finally, the second attention layer is a $\\mathit{classifier}$ that\ncompares these features with the feature at the output position, and uses the\nresulting similarity scores to generate the desired output. Our theory is\nfurther validated by experiments.",
    "published": "2024-09-09T18:10:26Z",
    "updated": "2024-09-09T18:10:26Z",
    "authors": [
      "Siyu Chen",
      "Heejune Sheen",
      "Tianhao Wang",
      "Zhuoran Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.00687v3",
    "title": "Transformer Meets Twicing: Harnessing Unattended Residual Information",
    "summary": "Transformer-based deep learning models have achieved state-of-the-art\nperformance across numerous language and vision tasks. While the self-attention\nmechanism, a core component of transformers, has proven capable of handling\ncomplex data patterns, it has been observed that the representational capacity\nof the attention matrix degrades significantly across transformer layers,\nthereby hurting its overall performance. In this work, we leverage the\nconnection between self-attention computations and low-pass non-local means\n(NLM) smoothing filters and propose the Twicing Attention, a novel attention\nmechanism that uses kernel twicing procedure in nonparametric regression to\nalleviate the low-pass behavior of associated NLM smoothing with compelling\ntheoretical guarantees and enhanced adversarial robustness. This approach\nenables the extraction and reuse of meaningful information retained in the\nresiduals following the imperfect smoothing operation at each layer. Our\nproposed method offers two key advantages over standard self-attention: 1) a\nprovably slower decay of representational capacity and 2) improved robustness\nand accuracy across various data modalities and tasks. We empirically\ndemonstrate the performance gains of our model over baseline transformers on\nmultiple tasks and benchmarks, including image classification and language\nmodeling, on both clean and corrupted data.",
    "published": "2025-03-02T01:56:35Z",
    "updated": "2025-08-04T12:28:30Z",
    "authors": [
      "Laziz Abdullaev",
      "Tan M. Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.05271v1",
    "title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet\n  Extraction",
    "summary": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs.",
    "published": "2025-05-08T14:17:27Z",
    "updated": "2025-05-08T14:17:27Z",
    "authors": [
      "Kun Peng",
      "Chaodong Tong",
      "Cong Cao",
      "Hao Peng",
      "Qian Li",
      "Guanlin Wu",
      "Lei Jiang",
      "Yanbing Liu",
      "Philip S. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12771v1",
    "title": "FireFly-T: High-Throughput Sparsity Exploitation for Spiking Transformer\n  Acceleration with Dual-Engine Overlay Architecture",
    "summary": "Spiking transformers are emerging as a promising architecture that combines\nthe energy efficiency of Spiking Neural Networks (SNNs) with the powerful\nattention mechanisms of transformers. However, existing hardware accelerators\nlack support for spiking attention, exhibit limited throughput in exploiting\nfine-grained sparsity, and struggle with scalable parallelism in sparse\ncomputation. To address these, we propose FireFly-T, a dual-engine overlay\narchitecture that integrates a sparse engine for activation sparsity and a\nbinary engine for spiking attention. In the sparse engine, we propose a\nhighthroughput sparse decoder that exploits fine-grained sparsity by\nconcurrently extracting multiple non-zero spikes. To complement this, we\nintroduce a scalable load balancing mechanism with weight dispatch and\nout-of-order execution, eliminating bank conflicts to support scalable\nmultidimensional parallelism. In the binary engine, we leverage the byte-level\nwrite capability of SRAMs to efficiently manipulate the 3D dataflows required\nfor spiking attention with minimal resource overhead. We also optimize the core\nAND-PopCount operation in spiking attention through a LUT6-based\nimplementation, improving timing closure and reducing LUT utilization on Xilinx\nFPGAs. As an overlay architecture, FireFly-T further incorporates an\norchestrator that dynamically manipulates input dataflows with flexible\nadaptation for diverse network topologies, while ensuring efficient resource\nutilization and maintaining high throughput. Experimental results demonstrate\nthat our accelerator achieves $1.39\\times$ and $2.40\\times$ higher energy\nefficiency, as well as $4.21\\times$ and $7.10\\times$ greater DSP efficiency,\ncompared to FireFly v2 and the transformer-enabled SpikeTA, respectively. These\nresults highlight its potential as an efficient hardware platform for spiking\ntransformer.",
    "published": "2025-05-19T07:00:18Z",
    "updated": "2025-05-19T07:00:18Z",
    "authors": [
      "Tenglong Li",
      "Jindong Li",
      "Guobin Shen",
      "Dongcheng Zhao",
      "Qian Zhang",
      "Yi Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13219v5",
    "title": "PiT: Progressive Diffusion Transformer",
    "summary": "Diffusion Transformers (DiTs) achieve remarkable performance within image\ngeneration via the transformer architecture. Conventionally, DiTs are\nconstructed by stacking serial isotropic global modeling transformers, which\nface significant quadratic computational cost. However, through empirical\nanalysis, we find that DiTs do not rely as heavily on global information as\npreviously believed. In fact, most layers exhibit significant redundancy in\nglobal computation. Additionally, conventional attention mechanisms suffer from\nlow-frequency inertia, limiting their efficiency. To address these issues, we\npropose Pseudo Shifted Window Attention (PSWA), which fundamentally mitigates\nglobal attention redundancy. PSWA achieves moderate global-local information\nthrough window attention. It further utilizes a high-frequency bridging branch\nto simulate shifted window operations, which both enrich the high-frequency\ninformation and strengthen inter-window connections. Furthermore, we propose\nthe Progressive Coverage Channel Allocation (PCCA) strategy that captures\nhigh-order attention without additional computational cost. Based on these\ninnovations, we propose a series of Pseudo Progressive Diffusion Transformer\n(PiT). Our extensive experiments show their superior performance; for example,\nour proposed PiT-L achieves 54% FID improvement over DiT-XL/2 while using less\ncomputation.",
    "published": "2025-05-19T15:02:33Z",
    "updated": "2025-08-14T03:15:18Z",
    "authors": [
      "Jiafu Wu",
      "Yabiao Wang",
      "Jian Li",
      "Jinlong Peng",
      "Yun Cao",
      "Chengjie Wang",
      "Jiangning Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22195v1",
    "title": "S2AFormer: Strip Self-Attention for Efficient Vision Transformer",
    "summary": "Vision Transformer (ViT) has made significant advancements in computer\nvision, thanks to its token mixer's sophisticated ability to capture global\ndependencies between all tokens. However, the quadratic growth in computational\ndemands as the number of tokens increases limits its practical efficiency.\nAlthough recent methods have combined the strengths of convolutions and\nself-attention to achieve better trade-offs, the expensive pairwise token\naffinity and complex matrix operations inherent in self-attention remain a\nbottleneck. To address this challenge, we propose S2AFormer, an efficient\nVision Transformer architecture featuring novel Strip Self-Attention (SSA). We\ndesign simple yet effective Hybrid Perception Blocks (HPBs) to effectively\nintegrate the local perception capabilities of CNNs with the global context\nmodeling of Transformer's attention mechanisms. A key innovation of SSA lies in\nits reducing the spatial dimensions of $K$ and $V$ while compressing the\nchannel dimensions of $Q$ and $K$. This design significantly reduces\ncomputational overhead while preserving accuracy, striking an optimal balance\nbetween efficiency and effectiveness. We evaluate the robustness and efficiency\nof S2AFormer through extensive experiments on multiple vision benchmarks,\nincluding ImageNet-1k for image classification, ADE20k for semantic\nsegmentation, and COCO for object detection and instance segmentation. Results\ndemonstrate that S2AFormer achieves significant accuracy gains with superior\nefficiency in both GPU and non-GPU environments, making it a strong candidate\nfor efficient vision Transformers.",
    "published": "2025-05-28T10:17:23Z",
    "updated": "2025-05-28T10:17:23Z",
    "authors": [
      "Guoan Xu",
      "Wenfeng Huang",
      "Wenjing Jia",
      "Jiamao Li",
      "Guangwei Gao",
      "Guo-Jun Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24333v2",
    "title": "Two failure modes of deep transformers and how to avoid them: a unified\n  theory of signal propagation at initialisation",
    "summary": "Finding the right initialisation for neural networks is crucial to ensure\nsmooth training and good performance. In transformers, the wrong initialisation\ncan lead to one of two failure modes of self-attention layers: rank collapse,\nwhere all tokens collapse into similar representations, and entropy collapse,\nwhere highly concentrated attention scores lead to training instability. While\nprevious work has studied different scaling regimes for transformers, an\nasymptotically exact, down-to-the constant prescription for how to initialise\ntransformers has so far been lacking. Here, we provide an analytical theory of\nsignal propagation through deep transformers with self-attention, layer\nnormalisation, skip connections and MLP. Our theory yields a simple algorithm\nto compute trainability diagrams that identify the correct choice of\ninitialisation hyper-parameters for a given architecture. We overcome the key\nchallenge, an exact treatment of the self-attention layer, by establishing a\nformal parallel with the Random Energy Model from statistical physics. We also\nanalyse gradients in the backward path and determine the regime where gradients\nvanish at initialisation. We demonstrate the versatility of our framework\nthrough three case studies. Our theoretical framework gives a unified\nperspective on the two failure modes of self-attention and gives quantitative\npredictions on the scale of both weights and residual connections that\nguarantee smooth training.",
    "published": "2025-05-30T08:18:23Z",
    "updated": "2025-09-26T16:22:08Z",
    "authors": [
      "Alessio Giorlandino",
      "Sebastian Goldt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10366v1",
    "title": "FSATFusion: Frequency-Spatial Attention Transformer for Infrared and\n  Visible Image Fusion",
    "summary": "The infrared and visible images fusion (IVIF) is receiving increasing\nattention from both the research community and industry due to its excellent\nresults in downstream applications. Existing deep learning approaches often\nutilize convolutional neural networks to extract image features. However, the\ninherently capacity of convolution operations to capture global context can\nlead to information loss, thereby restricting fusion performance. To address\nthis limitation, we propose an end-to-end fusion network named the\nFrequency-Spatial Attention Transformer Fusion Network (FSATFusion). The\nFSATFusion contains a frequency-spatial attention Transformer (FSAT) module\ndesigned to effectively capture discriminate features from source images. This\nFSAT module includes a frequency-spatial attention mechanism (FSAM) capable of\nextracting significant features from feature maps. Additionally, we propose an\nimproved Transformer module (ITM) to enhance the ability to extract global\ncontext information of vanilla Transformer. We conducted both qualitative and\nquantitative comparative experiments, demonstrating the superior fusion quality\nand efficiency of FSATFusion compared to other state-of-the-art methods.\nFurthermore, our network was tested on two additional tasks without any\nmodifications, to verify the excellent generalization capability of FSATFusion.\nFinally, the object detection experiment demonstrated the superiority of\nFSATFusion in downstream visual tasks. Our code is available at\nhttps://github.com/Lmmh058/FSATFusion.",
    "published": "2025-06-12T05:42:38Z",
    "updated": "2025-06-12T05:42:38Z",
    "authors": [
      "Tianpei Zhang",
      "Jufeng Zhao",
      "Yiming Zhu",
      "Guangmang Cui",
      "Yuhan Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15498v1",
    "title": "Mental Accounts for Actions: EWA-Inspired Attention in Decision\n  Transformers",
    "summary": "Transformers have emerged as a compelling architecture for sequential\ndecision-making by modeling trajectories via self-attention. In reinforcement\nlearning (RL), they enable return-conditioned control without relying on value\nfunction approximation. Decision Transformers (DTs) exploit this by casting RL\nas supervised sequence modeling, but they are restricted to offline data and\nlack exploration. Online Decision Transformers (ODTs) address this limitation\nthrough entropy-regularized training on on-policy rollouts, offering a stable\nalternative to traditional RL methods like Soft Actor-Critic, which depend on\nbootstrapped targets and reward shaping. Despite these advantages, ODTs use\nstandard attention, which lacks explicit memory of action-specific outcomes.\nThis leads to inefficiencies in learning long-term action effectiveness.\nInspired by cognitive models such as Experience-Weighted Attraction (EWA), we\npropose Experience-Weighted Attraction with Vector Quantization for Online\nDecision Transformers (EWA-VQ-ODT), a lightweight module that maintains\nper-action mental accounts summarizing recent successes and failures.\nContinuous actions are routed via direct grid lookup to a compact\nvector-quantized codebook, where each code stores a scalar attraction updated\nonline through decay and reward-based reinforcement. These attractions modulate\nattention by biasing the columns associated with action tokens, requiring no\nchange to the backbone or training objective. On standard continuous-control\nbenchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,\nparticularly in early training. The module is computationally efficient,\ninterpretable via per-code traces, and supported by theoretical guarantees that\nbound the attraction dynamics and its impact on attention drift.",
    "published": "2025-09-19T00:33:22Z",
    "updated": "2025-09-19T00:33:22Z",
    "authors": [
      "Zahra Aref",
      "Narayan B. Mandayam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.02610v2",
    "title": "On the Robustness of Vision Transformers to Adversarial Examples",
    "summary": "Recent advances in attention-based networks have shown that Vision\nTransformers can achieve state-of-the-art or near state-of-the-art results on\nmany image classification tasks. This puts transformers in the unique position\nof being a promising alternative to traditional convolutional neural networks\n(CNNs). While CNNs have been carefully studied with respect to adversarial\nattacks, the same cannot be said of Vision Transformers. In this paper, we\nstudy the robustness of Vision Transformers to adversarial examples. Our\nanalyses of transformer security is divided into three parts. First, we test\nthe transformer under standard white-box and black-box attacks. Second, we\nstudy the transferability of adversarial examples between CNNs and\ntransformers. We show that adversarial examples do not readily transfer between\nCNNs and transformers. Based on this finding, we analyze the security of a\nsimple ensemble defense of CNNs and transformers. By creating a new attack, the\nself-attention blended gradient attack, we show that such an ensemble is not\nsecure under a white-box adversary. However, under a black-box adversary, we\nshow that an ensemble can achieve unprecedented robustness without sacrificing\nclean accuracy. Our analysis for this work is done using six types of white-box\nattacks and two types of black-box attacks. Our study encompasses multiple\nVision Transformers, Big Transfer Models and CNN architectures trained on\nCIFAR-10, CIFAR-100 and ImageNet.",
    "published": "2021-03-31T00:29:12Z",
    "updated": "2021-06-05T00:31:29Z",
    "authors": [
      "Kaleel Mahmood",
      "Rigel Mahmood",
      "Marten van Dijk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.08167v2",
    "title": "Quantum Vision Transformers",
    "summary": "In this work, quantum transformers are designed and analysed in detail by\nextending the state-of-the-art classical transformer neural network\narchitectures known to be very performant in natural language processing and\nimage analysis. Building upon the previous work, which uses parametrised\nquantum circuits for data loading and orthogonal neural layers, we introduce\nthree types of quantum transformers for training and inference, including a\nquantum transformer based on compound matrices, which guarantees a theoretical\nadvantage of the quantum attention mechanism compared to their classical\ncounterpart both in terms of asymptotic run time and the number of model\nparameters. These quantum architectures can be built using shallow quantum\ncircuits and produce qualitatively different classification models. The three\nproposed quantum attention layers vary on the spectrum between closely\nfollowing the classical transformers and exhibiting more quantum\ncharacteristics. As building blocks of the quantum transformer, we propose a\nnovel method for loading a matrix as quantum states as well as two new\ntrainable quantum orthogonal layers adaptable to different levels of\nconnectivity and quality of quantum computers. We performed extensive\nsimulations of the quantum transformers on standard medical image datasets that\nshowed competitively, and at times better performance compared to the classical\nbenchmarks, including the best-in-class classical vision transformers. The\nquantum transformers we trained on these small-scale datasets require fewer\nparameters compared to standard classical benchmarks. Finally, we implemented\nour quantum transformers on superconducting quantum computers and obtained\nencouraging results for up to six qubit experiments.",
    "published": "2022-09-16T20:51:23Z",
    "updated": "2024-02-20T13:26:44Z",
    "authors": [
      "El Amine Cherrat",
      "Iordanis Kerenidis",
      "Natansh Mathur",
      "Jonas Landman",
      "Martin Strahm",
      "Yun Yvonna Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.16563v2",
    "title": "Higher-Order Transformer Derivative Estimates for Explicit Pathwise\n  Learning Guarantees",
    "summary": "An inherent challenge in computing fully-explicit generalization bounds for\ntransformers involves obtaining covering number estimates for the given\ntransformer class $T$. Crude estimates rely on a uniform upper bound on the\nlocal-Lipschitz constants of transformers in $T$, and finer estimates require\nan analysis of their higher-order partial derivatives. Unfortunately, these\nprecise higher-order derivative estimates for (realistic) transformer models\nare not currently available in the literature as they are combinatorially\ndelicate due to the intricate compositional structure of transformer blocks.\n  This paper fills this gap by precisely estimating all the higher-order\nderivatives of all orders for the transformer model. We consider realistic\ntransformers with multiple (non-linearized) attention heads per block and layer\nnormalization. We obtain fully-explicit estimates of all constants in terms of\nthe number of attention heads, the depth and width of each transformer block,\nand the number of normalization layers. Further, we explicitly analyze the\nimpact of various standard activation function choices (e.g. SWISH and GeLU).\nAs an application, we obtain explicit pathwise generalization bounds for\ntransformers on a single trajectory of an exponentially-ergodic Markov process\nvalid at a fixed future time horizon. We conclude that real-world transformers\ncan learn from $N$ (non-i.i.d.) samples of a single Markov process's trajectory\nat a rate of ${O}(\\operatorname{polylog}(N)/\\sqrt{N})$.",
    "published": "2024-05-26T13:19:32Z",
    "updated": "2025-02-06T12:02:36Z",
    "authors": [
      "Yannick Limmer",
      "Anastasis Kratsios",
      "Xuwei Yang",
      "Raeid Saqur",
      "Blanka Horvath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.04209v2",
    "title": "Equivariant Neural Functional Networks for Transformers",
    "summary": "This paper systematically explores neural functional networks (NFN) for\ntransformer architectures. NFN are specialized neural networks that treat the\nweights, gradients, or sparsity patterns of a deep neural network (DNN) as\ninput data and have proven valuable for tasks such as learnable optimizers,\nimplicit data representations, and weight editing. While NFN have been\nextensively developed for MLP and CNN, no prior work has addressed their design\nfor transformers, despite the importance of transformers in modern deep\nlearning. This paper aims to address this gap by providing a systematic study\nof NFN for transformers. We first determine the maximal symmetric group of the\nweights in a multi-head attention module as well as a necessary and sufficient\ncondition under which two sets of hyperparameters of the multi-head attention\nmodule define the same function. We then define the weight space of transformer\narchitectures and its associated group action, which leads to the design\nprinciples for NFN in transformers. Based on these, we introduce\nTransformer-NFN, an NFN that is equivariant under this group action.\nAdditionally, we release a dataset of more than 125,000 Transformers model\ncheckpoints trained on two datasets with two different tasks, providing a\nbenchmark for evaluating Transformer-NFN and encouraging further research on\ntransformer training and performance.",
    "published": "2024-10-05T15:56:57Z",
    "updated": "2025-03-07T14:32:12Z",
    "authors": [
      "Viet-Hoang Tran",
      "Thieu N. Vo",
      "An Nguyen The",
      "Tho Tran Huu",
      "Minh-Khoi Nguyen-Nhat",
      "Thanh Tran",
      "Duy-Tung Pham",
      "Tan Minh Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.11892v1",
    "title": "Attention-based Adversarial Robust Distillation in Radio Signal\n  Classifications for Low-Power IoT Devices",
    "summary": "Due to great success of transformers in many applications such as natural\nlanguage processing and computer vision, transformers have been successfully\napplied in automatic modulation classification. We have shown that\ntransformer-based radio signal classification is vulnerable to imperceptible\nand carefully crafted attacks called adversarial examples. Therefore, we\npropose a defense system against adversarial examples in transformer-based\nmodulation classifications. Considering the need for computationally efficient\narchitecture particularly for Internet of Things (IoT)-based applications or\noperation of devices in environment where power supply is limited, we propose a\ncompact transformer for modulation classification. The advantages of robust\ntraining such as adversarial training in transformers may not be attainable in\ncompact transformers. By demonstrating this, we propose a novel compact\ntransformer that can enhance robustness in the presence of adversarial attacks.\nThe new method is aimed at transferring the adversarial attention map from the\nrobustly trained large transformer to a compact transformer. The proposed\nmethod outperforms the state-of-the-art techniques for the considered white-box\nscenarios including fast gradient method and projected gradient descent\nattacks. We have provided reasoning of the underlying working mechanisms and\ninvestigated the transferability of the adversarial examples between different\narchitectures. The proposed method has the potential to protect the transformer\nfrom the transferability of adversarial examples.",
    "published": "2025-06-13T15:39:01Z",
    "updated": "2025-06-13T15:39:01Z",
    "authors": [
      "Lu Zhang",
      "Sangarapillai Lambotharan",
      "Gan Zheng",
      "Guisheng Liao",
      "Basil AsSadhan",
      "Fabio Roli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00711v1",
    "title": "Resting-state fMRI Analysis using Quantum Time-series Transformer",
    "summary": "Resting-state functional magnetic resonance imaging (fMRI) has emerged as a\npivotal tool for revealing intrinsic brain network connectivity and identifying\nneural biomarkers of neuropsychiatric conditions. However, classical\nself-attention transformer models--despite their formidable representational\npower--struggle with quadratic complexity, large parameter counts, and\nsubstantial data requirements. To address these barriers, we introduce a\nQuantum Time-series Transformer, a novel quantum-enhanced transformer\narchitecture leveraging Linear Combination of Unitaries and Quantum Singular\nValue Transformation. Unlike classical transformers, Quantum Time-series\nTransformer operates with polylogarithmic computational complexity, markedly\nreducing training overhead and enabling robust performance even with fewer\nparameters and limited sample sizes. Empirical evaluation on the largest-scale\nfMRI datasets from the Adolescent Brain Cognitive Development Study and the UK\nBiobank demonstrates that Quantum Time-series Transformer achieves comparable\nor superior predictive performance compared to state-of-the-art classical\ntransformer models, with especially pronounced gains in small-sample scenarios.\nInterpretability analyses using SHapley Additive exPlanations further reveal\nthat Quantum Time-series Transformer reliably identifies clinically meaningful\nneural biomarkers of attention-deficit/hyperactivity disorder (ADHD). These\nfindings underscore the promise of quantum-enhanced transformers in advancing\ncomputational neuroscience by more efficiently modeling complex spatio-temporal\ndynamics and improving clinical interpretability.",
    "published": "2025-08-31T06:08:57Z",
    "updated": "2025-08-31T06:08:57Z",
    "authors": [
      "Junghoon Justin Park",
      "Jungwoo Seo",
      "Sangyoon Bae",
      "Samuel Yen-Chi Chen",
      "Huan-Hsin Tseng",
      "Jiook Cha",
      "Shinjae Yoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.02613v1",
    "title": "Class Semantics-based Attention for Action Detection",
    "summary": "Action localization networks are often structured as a feature encoder\nsub-network and a localization sub-network, where the feature encoder learns to\ntransform an input video to features that are useful for the localization\nsub-network to generate reliable action proposals. While some of the encoded\nfeatures may be more useful for generating action proposals, prior action\nlocalization approaches do not include any attention mechanism that enables the\nlocalization sub-network to attend more to the more important features. In this\npaper, we propose a novel attention mechanism, the Class Semantics-based\nAttention (CSA), that learns from the temporal distribution of semantics of\naction classes present in an input video to find the importance scores of the\nencoded features, which are used to provide attention to the more useful\nencoded features. We demonstrate on two popular action detection datasets that\nincorporating our novel attention mechanism provides considerable performance\ngains on competitive action detection models (e.g., around 6.2% improvement\nover BMN action detection baseline to obtain 47.5% mAP on the THUMOS-14\ndataset), and a new state-of-the-art of 36.25% mAP on the ActivityNet v1.3\ndataset. Further, the CSA localization model family which includes BMN-CSA, was\npart of the second-placed submission at the 2021 ActivityNet action\nlocalization challenge. Our attention mechanism outperforms prior\nself-attention modules such as the squeeze-and-excitation in action detection\ntask. We also observe that our attention mechanism is complementary to such\nself-attention modules in that performance improvements are seen when both are\nused together.",
    "published": "2021-09-06T17:22:46Z",
    "updated": "2021-09-06T17:22:46Z",
    "authors": [
      "Deepak Sridhar",
      "Niamul Quader",
      "Srikanth Muralidharan",
      "Yaoxin Li",
      "Peng Dai",
      "Juwei Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.14080v1",
    "title": "X-Linear Attention Networks for Image Captioning",
    "summary": "Recent progress on fine-grained visual recognition and visual question\nanswering has featured Bilinear Pooling, which effectively models the 2$^{nd}$\norder interactions across multi-modal inputs. Nevertheless, there has not been\nevidence in support of building such interactions concurrently with attention\nmechanism for image captioning. In this paper, we introduce a unified attention\nblock -- X-Linear attention block, that fully employs bilinear pooling to\nselectively capitalize on visual information or perform multi-modal reasoning.\nTechnically, X-Linear attention block simultaneously exploits both the spatial\nand channel-wise bilinear attention distributions to capture the 2$^{nd}$ order\ninteractions between the input single-modal or multi-modal features. Higher and\neven infinity order feature interactions are readily modeled through stacking\nmultiple X-Linear attention blocks and equipping the block with Exponential\nLinear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we\npresent X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates\nX-Linear attention block(s) into image encoder and sentence decoder of image\ncaptioning model to leverage higher order intra- and inter-modal interactions.\nThe experiments on COCO benchmark demonstrate that our X-LAN obtains to-date\nthe best published CIDEr performance of 132.0% on COCO Karpathy test split.\nWhen further endowing Transformer with X-Linear attention blocks, CIDEr is\nboosted up to 132.8%. Source code is available at\n\\url{https://github.com/Panda-Peter/image-captioning}.",
    "published": "2020-03-31T10:35:33Z",
    "updated": "2020-03-31T10:35:33Z",
    "authors": [
      "Yingwei Pan",
      "Ting Yao",
      "Yehao Li",
      "Tao Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.13858v2",
    "title": "AReLU: Attention-based Rectified Linear Unit",
    "summary": "Element-wise activation functions play a critical role in deep neural\nnetworks via affecting the expressivity power and the learning dynamics.\nLearning-based activation functions have recently gained increasing attention\nand success. We propose a new perspective of learnable activation function\nthrough formulating them with element-wise attention mechanism. In each network\nlayer, we devise an attention module which learns an element-wise, sign-based\nattention map for the pre-activation feature map. The attention map scales an\nelement based on its sign. Adding the attention module with a rectified linear\nunit (ReLU) results in an amplification of positive elements and a suppression\nof negative ones, both with learned, data-adaptive parameters. We coin the\nresulting activation function Attention-based Rectified Linear Unit (AReLU).\nThe attention module essentially learns an element-wise residue of the\nactivated part of the input, as ReLU can be viewed as an identity\ntransformation. This makes the network training more resistant to gradient\nvanishing. The learned attentive activation leads to well-focused activation of\nrelevant regions of a feature map. Through extensive evaluations, we show that\nAReLU significantly boosts the performance of most mainstream network\narchitectures with only two extra learnable parameters per layer introduced.\nNotably, AReLU facilitates fast network training under small learning rates,\nwhich makes it especially suited in the case of transfer learning and meta\nlearning. Our source code has been released (see\nhttps://github.com/densechen/AReLU).",
    "published": "2020-06-24T16:39:16Z",
    "updated": "2020-10-02T09:16:51Z",
    "authors": [
      "Dengsheng Chen",
      "Jun Li",
      "Kai Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.00620v2",
    "title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection",
    "summary": "Object detection in three-dimensional (3D) space attracts much interest from\nacademia and industry since it is an essential task in AI-driven applications\nsuch as robotics, autonomous driving, and augmented reality. As the basic\nformat of 3D data, the point cloud can provide detailed geometric information\nabout the objects in the original 3D space. However, due to 3D data's sparsity\nand unorderedness, specially designed networks and modules are needed to\nprocess this type of data. Attention mechanism has achieved impressive\nperformance in diverse computer vision tasks; however, it is unclear how\nattention modules would affect the performance of 3D point cloud object\ndetection and what sort of attention modules could fit with the inherent\nproperties of 3D data. This work investigates the role of the attention\nmechanism in 3D point cloud object detection and provides insights into the\npotential of different attention modules. To achieve that, we comprehensively\ninvestigate classical 2D attentions, novel 3D attentions, including the latest\npoint cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the\ndetailed experiments and analysis, we conclude the effects of different\nattention modules. This paper is expected to serve as a reference source for\nbenefiting attention-embedded 3D point cloud object detection. The code and\ntrained models are available at:\nhttps://github.com/ShiQiu0419/attentions_in_3D_detection.",
    "published": "2021-08-02T03:54:39Z",
    "updated": "2021-10-14T07:08:59Z",
    "authors": [
      "Shi Qiu",
      "Yunfan Wu",
      "Saeed Anwar",
      "Chongyi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.01750v2",
    "title": "Towards Robust Semantic Segmentation against Patch-based Attack via\n  Attention Refinement",
    "summary": "The attention mechanism has been proven effective on various visual tasks in\nrecent years. In the semantic segmentation task, the attention mechanism is\napplied in various methods, including the case of both Convolution Neural\nNetworks (CNN) and Vision Transformer (ViT) as backbones. However, we observe\nthat the attention mechanism is vulnerable to patch-based adversarial attacks.\nThrough the analysis of the effective receptive field, we attribute it to the\nfact that the wide receptive field brought by global attention may lead to the\nspread of the adversarial patch. To address this issue, in this paper, we\npropose a Robust Attention Mechanism (RAM) to improve the robustness of the\nsemantic segmentation model, which can notably relieve the vulnerability\nagainst patch-based attacks. Compared to the vallina attention mechanism, RAM\nintroduces two novel modules called Max Attention Suppression and Random\nAttention Dropout, both of which aim to refine the attention matrix and limit\nthe influence of a single adversarial patch on the semantic segmentation\nresults of other positions. Extensive experiments demonstrate the effectiveness\nof our RAM to improve the robustness of semantic segmentation models against\nvarious patch-based attack methods under different attack settings.",
    "published": "2024-01-03T13:58:35Z",
    "updated": "2024-05-09T09:09:37Z",
    "authors": [
      "Zheng Yuan",
      "Jie Zhang",
      "Yude Wang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07563v1",
    "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid",
    "summary": "Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
    "published": "2025-02-11T14:01:39Z",
    "updated": "2025-02-11T14:01:39Z",
    "authors": [
      "Weigao Sun",
      "Disen Lan",
      "Yiran Zhong",
      "Xiaoye Qu",
      "Yu Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.07963v1",
    "title": "Customizing the Inductive Biases of Softmax Attention using Structured\n  Matrices",
    "summary": "The core component of attention is the scoring function, which transforms the\ninputs into low-dimensional queries and keys and takes the dot product of each\npair. While the low-dimensional projection improves efficiency, it causes\ninformation loss for certain tasks that have intrinsically high-dimensional\ninputs. Additionally, attention uses the same scoring function for all input\npairs, without imposing a distance-dependent compute bias for neighboring\ntokens in the sequence. In this work, we address these shortcomings by\nproposing new scoring functions based on computationally efficient structured\nmatrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level\nLow Rank (MLR) matrices. On in-context regression tasks with high-dimensional\ninputs, our proposed scoring functions outperform standard attention for any\nfixed compute budget. On language modeling, a task that exhibits locality\npatterns, our MLR-based attention method achieves improved scaling laws\ncompared to both standard attention and variants of sliding window attention.\nAdditionally, we show that both BTT and MLR fall under a broader family of\nefficient structured matrices capable of encoding either full-rank or\ndistance-dependent compute biases, thereby addressing significant shortcomings\nof standard attention. Finally, we show that MLR attention has promising\nresults for long-range time-series forecasting.",
    "published": "2025-09-09T17:50:58Z",
    "updated": "2025-09-09T17:50:58Z",
    "authors": [
      "Yilun Kuang",
      "Noah Amsel",
      "Sanae Lotfi",
      "Shikai Qiu",
      "Andres Potapczynski",
      "Andrew Gordon Wilson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.08483v5",
    "title": "ETC: Encoding Long and Structured Inputs in Transformers",
    "summary": "Transformer models have advanced the state of the art in many Natural\nLanguage Processing (NLP) tasks. In this paper, we present a new Transformer\narchitecture, Extended Transformer Construction (ETC), that addresses two key\nchallenges of standard Transformer architectures, namely scaling input length\nand encoding structured inputs. To scale attention to longer inputs, we\nintroduce a novel global-local attention mechanism between global tokens and\nregular input tokens. We also show that combining global-local attention with\nrelative position encodings and a Contrastive Predictive Coding (CPC)\npre-training objective allows ETC to encode structured inputs. We achieve\nstate-of-the-art results on four natural language datasets requiring long\nand/or structured inputs.",
    "published": "2020-04-17T23:10:18Z",
    "updated": "2020-10-27T16:54:17Z",
    "authors": [
      "Joshua Ainslie",
      "Santiago Ontanon",
      "Chris Alberti",
      "Vaclav Cvicek",
      "Zachary Fisher",
      "Philip Pham",
      "Anirudh Ravula",
      "Sumit Sanghai",
      "Qifan Wang",
      "Li Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.12877v2",
    "title": "Training data-efficient image transformers & distillation through\n  attention",
    "summary": "Recently, neural networks purely based on attention were shown to address\nimage understanding tasks such as image classification. However, these visual\ntransformers are pre-trained with hundreds of millions of images using an\nexpensive infrastructure, thereby limiting their adoption.\n  In this work, we produce a competitive convolution-free transformer by\ntraining on Imagenet only. We train them on a single computer in less than 3\ndays. Our reference vision transformer (86M parameters) achieves top-1 accuracy\nof 83.1% (single-crop evaluation) on ImageNet with no external data.\n  More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet (where\nwe obtain up to 85.2% accuracy) and when transferring to other tasks. We share\nour code and models.",
    "published": "2020-12-23T18:42:10Z",
    "updated": "2021-01-15T15:52:50Z",
    "authors": [
      "Hugo Touvron",
      "Matthieu Cord",
      "Matthijs Douze",
      "Francisco Massa",
      "Alexandre Sablayrolles",
      "HervÃ© JÃ©gou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.02377v2",
    "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
    "summary": "A recent variation of Transformer, Performer, scales Transformer to longer\nsequences with a linear attention mechanism. However, it is not compatible with\nrelative position encoding, which has advantages over absolute position\nencoding. In this paper, we discuss possible ways to add relative position\nencoding to Performer. Based on the analysis, we propose PermuteFormer, a\nPerformer-based model with relative position encoding that scales linearly on\nlong sequences. PermuteFormer applies position-dependent transformation on\nqueries and keys to encode positional information into the attention module.\nThis transformation is carefully crafted so that the final output of\nself-attention is not affected by absolute positions of tokens. PermuteFormer\nintroduces negligible computational overhead by design that it runs as fast as\nPerformer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long\nsequences, as well as WikiText-103, a language modeling dataset. The\nexperiments show that PermuteFormer uniformly improves the performance of\nPerformer with almost no computational overhead and outperforms vanilla\nTransformer on most of the tasks.",
    "published": "2021-09-06T11:49:22Z",
    "updated": "2021-09-08T13:17:49Z",
    "authors": [
      "Peng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.07098v1",
    "title": "Residual Swin Transformer Channel Attention Network for Image\n  Demosaicing",
    "summary": "Image demosaicing is problem of interpolating full- resolution color images\nfrom raw sensor (color filter array) data. During last decade, deep neural\nnetworks have been widely used in image restoration, and in particular, in\ndemosaicing, attaining significant performance improvement. In recent years,\nvision transformers have been designed and successfully used in various\ncomputer vision applications. One of the recent methods of image restoration\nbased on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art\nperformance with a smaller number of parameters than neural network-based\nmethods. Inspired by the success of SwinIR, we propose in this paper a novel\nSwin Transformer-based network for image demosaicing, called RSTCANet. To\nextract image features, RSTCANet stacks several residual Swin Transformer\nChannel Attention blocks (RSTCAB), introducing the channel attention for each\ntwo successive ST blocks. Extensive experiments demonstrate that RSTCANet out-\nperforms state-of-the-art image demosaicing methods, and has a smaller number\nof parameters.",
    "published": "2022-04-14T16:45:17Z",
    "updated": "2022-04-14T16:45:17Z",
    "authors": [
      "Wenzhu Xing",
      "Karen Egiazarian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.12091v2",
    "title": "Transformer-Based Attention Networks for Continuous Pixel-Wise\n  Prediction",
    "summary": "While convolutional neural networks have shown a tremendous impact on various\ncomputer vision tasks, they generally demonstrate limitations in explicitly\nmodeling long-range dependencies due to the intrinsic locality of the\nconvolution operation. Initially designed for natural language processing\ntasks, Transformers have emerged as alternative architectures with innate\nglobal self-attention mechanisms to capture long-range dependencies. In this\npaper, we propose TransDepth, an architecture that benefits from both\nconvolutional neural networks and transformers. To avoid the network losing its\nability to capture local-level details due to the adoption of transformers, we\npropose a novel decoder that employs attention mechanisms based on gates.\nNotably, this is the first paper that applies transformers to pixel-wise\nprediction problems involving continuous labels (i.e., monocular depth\nprediction and surface normal estimation). Extensive experiments demonstrate\nthat the proposed TransDepth achieves state-of-the-art performance on three\nchallenging datasets. Our code is available at:\nhttps://github.com/ygjwd12345/TransDepth.",
    "published": "2021-03-22T18:00:13Z",
    "updated": "2021-08-05T15:16:41Z",
    "authors": [
      "Guanglei Yang",
      "Hao Tang",
      "Mingli Ding",
      "Nicu Sebe",
      "Elisa Ricci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.05313v1",
    "title": "Memory transformers for full context and high-resolution 3D Medical\n  Segmentation",
    "summary": "Transformer models achieve state-of-the-art results for image segmentation.\nHowever, achieving long-range attention, necessary to capture global context,\nwith high-resolution 3D images is a fundamental challenge. This paper\nintroduces the Full resolutIoN mEmory (FINE) transformer to overcome this\nissue. The core idea behind FINE is to learn memory tokens to indirectly model\nfull range interactions while scaling well in both memory and computational\ncosts. FINE introduces memory tokens at two levels: the first one allows full\ninteraction between voxels within local image regions (patches), the second one\nallows full interactions between all regions of the 3D volume. Combined, they\nallow full attention over high resolution images, e.g. 512 x 512 x 256 voxels\nand above. Experiments on the BCV image segmentation dataset shows better\nperformances than state-of-the-art CNN and transformer baselines, highlighting\nthe superiority of our full attention mechanism compared to recent transformer\nbaselines, e.g. CoTr, and nnFormer.",
    "published": "2022-10-11T10:11:05Z",
    "updated": "2022-10-11T10:11:05Z",
    "authors": [
      "Loic Themyr",
      "ClÃ©ment Rambour",
      "Nicolas Thome",
      "Toby Collins",
      "Alexandre Hostettler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.05529v1",
    "title": "An Exploration of Hierarchical Attention Transformers for Efficient Long\n  Document Classification",
    "summary": "Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.",
    "published": "2022-10-11T15:17:56Z",
    "updated": "2022-10-11T15:17:56Z",
    "authors": [
      "Ilias Chalkidis",
      "Xiang Dai",
      "Manos Fergadiotis",
      "Prodromos Malakasiotis",
      "Desmond Elliott"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.12381v3",
    "title": "S2WAT: Image Style Transfer via Hierarchical Vision Transformer using\n  Strips Window Attention",
    "summary": "Transformer's recent integration into style transfer leverages its\nproficiency in establishing long-range dependencies, albeit at the expense of\nattenuated local modeling. This paper introduces Strips Window Attention\nTransformer (S2WAT), a novel hierarchical vision transformer designed for style\ntransfer. S2WAT employs attention computation in diverse window shapes to\ncapture both short- and long-range dependencies. The merged dependencies\nutilize the \"Attn Merge\" strategy, which adaptively determines spatial weights\nbased on their relevance to the target. Extensive experiments on representative\ndatasets show the proposed method's effectiveness compared to state-of-the-art\n(SOTA) transformer-based and other approaches. The code and pre-trained models\nare available at https://github.com/AlienZhang1996/S2WAT.",
    "published": "2022-10-22T07:56:13Z",
    "updated": "2023-12-15T07:21:56Z",
    "authors": [
      "Chiyu Zhang",
      "Xiaogang Xu",
      "Lei Wang",
      "Zaiyan Dai",
      "Jun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.11364v1",
    "title": "FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks",
    "summary": "Transformers achieve remarkable performance in various domains, including\nNLP, CV, audio processing, and graph analysis. However, they do not scale well\non long sequence tasks due to their quadratic complexity w.r.t. the inputs\nlength. Linear Transformers were proposed to address this limitation. However,\nthese models have shown weaker performance on the long sequence tasks comparing\nto the original one. In this paper, we explore Linear Transformer models,\nrethinking their two core components. Firstly, we improved Linear Transformer\nwith Shift-Invariant Kernel Function SIKF, which achieve higher accuracy\nwithout loss in speed. Secondly, we introduce FastRPB which stands for Fast\nRelative Positional Bias, which efficiently adds positional information to\nself-attention using Fast Fourier Transformation. FastRPB is independent of the\nself-attention mechanism and can be combined with an original self-attention\nand all its efficient variants. FastRPB has O(N log(N)) computational\ncomplexity, requiring O(N) memory w.r.t. input sequence length N.",
    "published": "2022-02-23T09:12:00Z",
    "updated": "2022-02-23T09:12:00Z",
    "authors": [
      "Maksim Zubkov",
      "Daniil Gavrilov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.00771v2",
    "title": "Dynamic Linear Transformer for 3D Biomedical Image Segmentation",
    "summary": "Transformer-based neural networks have surpassed promising performance on\nmany biomedical image segmentation tasks due to a better global information\nmodeling from the self-attention mechanism. However, most methods are still\ndesigned for 2D medical images while ignoring the essential 3D volume\ninformation. The main challenge for 3D transformer-based segmentation methods\nis the quadratic complexity introduced by the self-attention mechanism\n\\cite{vaswani2017attention}. In this paper, we propose a novel transformer\narchitecture for 3D medical image segmentation using an encoder-decoder style\narchitecture with linear complexity. Furthermore, we newly introduce a dynamic\ntoken concept to further reduce the token numbers for self-attention\ncalculation. Taking advantage of the global information modeling, we provide\nuncertainty maps from different hierarchy stages. We evaluate this method on\nmultiple challenging CT pancreas segmentation datasets. Our promising results\nshow that our novel 3D Transformer-based segmentor could provide promising\nhighly feasible segmentation performance and accurate uncertainty\nquantification using single annotation. Code is available\nhttps://github.com/freshman97/LinTransUNet.",
    "published": "2022-06-01T21:15:01Z",
    "updated": "2023-02-01T17:58:54Z",
    "authors": [
      "Zheyuan Zhang",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.06614v1",
    "title": "Transformers are Meta-Reinforcement Learners",
    "summary": "The transformer architecture and variants presented remarkable success across\nmany machine learning tasks in recent years. This success is intrinsically\nrelated to the capability of handling long sequences and the presence of\ncontext-dependent weights from the attention mechanism. We argue that these\ncapabilities suit the central role of a Meta-Reinforcement Learning algorithm.\nIndeed, a meta-RL agent needs to infer the task from a sequence of\ntrajectories. Furthermore, it requires a fast adaptation strategy to adapt its\npolicy for a new task -- which can be achieved using the self-attention\nmechanism. In this work, we present TrMRL (Transformers for Meta-Reinforcement\nLearning), a meta-RL agent that mimics the memory reinstatement mechanism using\nthe transformer architecture. It associates the recent past of working memories\nto build an episodic memory recursively through the transformer layers. We show\nthat the self-attention computes a consensus representation that minimizes the\nBayes Risk at each layer and provides meaningful features to compute the best\nactions. We conducted experiments in high-dimensional continuous control\nenvironments for locomotion and dexterous manipulation. Results show that TrMRL\npresents comparable or superior asymptotic performance, sample efficiency, and\nout-of-distribution generalization compared to the baselines in these\nenvironments.",
    "published": "2022-06-14T06:21:13Z",
    "updated": "2022-06-14T06:21:13Z",
    "authors": [
      "Luckeciano C. Melo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.09703v3",
    "title": "SnowFormer: Context Interaction Transformer with Scale-awareness for\n  Single Image Desnowing",
    "summary": "Due to various and complicated snow degradations, single image desnowing is a\nchallenging image restoration task. As prior arts can not handle it ideally, we\npropose a novel transformer, SnowFormer, which explores efficient\ncross-attentions to build local-global context interaction across patches and\nsurpasses existing works that employ local operators or vanilla transformers.\nCompared to prior desnowing methods and universal image restoration methods,\nSnowFormer has several benefits. Firstly, unlike the multi-head self-attention\nin recent image restoration Vision Transformers, SnowFormer incorporates the\nmulti-head cross-attention mechanism to perform local-global context\ninteraction between scale-aware snow queries and local-patch embeddings.\nSecond, the snow queries in SnowFormer are generated by the query generator\nfrom aggregated scale-aware features, which are rich in potential clean cues,\nleading to superior restoration results. Third, SnowFormer outshines advanced\nstate-of-the-art desnowing networks and the prevalent universal image\nrestoration transformers on six synthetic and real-world datasets. The code is\nreleased in \\url{https://github.com/Ephemeral182/SnowFormer}.",
    "published": "2022-08-20T15:01:09Z",
    "updated": "2022-11-13T07:47:45Z",
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Yun Liu",
      "Erkang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1909.02279v1",
    "title": "Accelerating Transformer Decoding via a Hybrid of Self-attention and\n  Recurrent Neural Network",
    "summary": "Due to the highly parallelizable architecture, Transformer is faster to train\nthan RNN-based models and popularly used in machine translation tasks. However,\nat inference time, each output word requires all the hidden states of the\npreviously generated words, which limits the parallelization capability, and\nmakes it much slower than RNN-based ones. In this paper, we systematically\nanalyze the time cost of different components of both the Transformer and\nRNN-based model. Based on it, we propose a hybrid network of self-attention and\nRNN structures, in which, the highly parallelizable self-attention is utilized\nas the encoder, and the simpler RNN structure is used as the decoder. Our\nhybrid network can decode 4-times faster than the Transformer. In addition,\nwith the help of knowledge distillation, our hybrid network achieves comparable\ntranslation quality to the original Transformer.",
    "published": "2019-09-05T09:22:25Z",
    "updated": "2019-09-05T09:22:25Z",
    "authors": [
      "Chengyi Wang",
      "Shuangzhi Wu",
      "Shujie Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1910.10352v1",
    "title": "A Transformer with Interleaved Self-attention and Convolution for Hybrid\n  Acoustic Models",
    "summary": "Transformer with self-attention has achieved great success in the area of\nnature language processing. Recently, there have been a few studies on\ntransformer for end-to-end speech recognition, while its application for hybrid\nacoustic model is still very limited. In this paper, we revisit the\ntransformer-based hybrid acoustic model, and propose a model structure with\ninterleaved self-attention and 1D convolution, which is proven to have faster\nconvergence and higher recognition accuracy. We also study several aspects of\nthe transformer model, including the impact of the positional encoding feature,\ndropout regularization, as well as training with and without time restriction.\nWe show competitive recognition results on the public Librispeech dataset when\ncompared to the Kaldi baseline at both cross entropy training and sequence\ntraining stages. For reproducible research, we release our source code and\nrecipe within the PyKaldi2 toolbox.",
    "published": "2019-10-23T04:57:51Z",
    "updated": "2019-10-23T04:57:51Z",
    "authors": [
      "Liang Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2008.12606v1",
    "title": "Deep Spatial Transformation for Pose-Guided Person Image Generation and\n  Animation",
    "summary": "Pose-guided person image generation and animation aim to transform a source\nperson image to target poses. These tasks require spatial manipulation of\nsource data. However, Convolutional Neural Networks are limited by the lack of\nability to spatially transform the inputs. In this paper, we propose a\ndifferentiable global-flow local-attention framework to reassemble the inputs\nat the feature level. This framework first estimates global flow fields between\nsources and targets. Then, corresponding local source feature patches are\nsampled with content-aware local attention coefficients. We show that our\nframework can spatially transform the inputs in an efficient manner. Meanwhile,\nwe further model the temporal consistency for the person image animation task\nto generate coherent videos. The experiment results of both image generation\nand animation tasks demonstrate the superiority of our model. Besides,\nadditional results of novel view synthesis and face image animation show that\nour model is applicable to other tasks requiring spatial transformation. The\nsource code of our project is available at\nhttps://github.com/RenYurui/Global-Flow-Local-Attention.",
    "published": "2020-08-27T08:59:44Z",
    "updated": "2020-08-27T08:59:44Z",
    "authors": [
      "Yurui Ren",
      "Ge Li",
      "Shan Liu",
      "Thomas H. Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2009.06097v2",
    "title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range\n  Dependency Encoding",
    "summary": "Transformer has become ubiquitous in the deep learning field. One of the key\ningredients that destined its success is the self-attention mechanism, which\nallows fully-connected contextual encoding over input tokens. However, despite\nits effectiveness in modeling short sequences, self-attention suffers when\nhandling inputs with extreme long-range dependencies, as its complexity grows\nquadratically with respect to the sequence length. Therefore, long sequences\nare often encoded by Transformer in chunks using a sliding window. In this\npaper, we propose Cluster-Former, a novel clustering-based sparse Transformer\nto perform attention across chunked sequences. The proposed framework is\npivoted on two unique types of Transformer layer: Sliding-Window Layer and\nCluster-Former Layer, which encode local sequence information and global\ncontext jointly and iteratively. This new design allows information integration\nbeyond local windows, which is especially beneficial for question answering\n(QA) tasks that rely on long-range dependencies. Experiments show that\nCluster-Former achieves state-of-the-art performance on several major QA\nbenchmarks.",
    "published": "2020-09-13T22:09:30Z",
    "updated": "2021-06-07T06:08:27Z",
    "authors": [
      "Shuohang Wang",
      "Luowei Zhou",
      "Zhe Gan",
      "Yen-Chun Chen",
      "Yuwei Fang",
      "Siqi Sun",
      "Yu Cheng",
      "Jingjing Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.01572v1",
    "title": "TransfoRNN: Capturing the Sequential Information in Self-Attention\n  Representations for Language Modeling",
    "summary": "In this paper, we describe the use of recurrent neural networks to capture\nsequential information from the self-attention representations to improve the\nTransformers. Although self-attention mechanism provides a means to exploit\nlong context, the sequential information, i.e. the arrangement of tokens, is\nnot explicitly captured. We propose to cascade the recurrent neural networks to\nthe Transformers, which referred to as the TransfoRNN model, to capture the\nsequential information. We found that the TransfoRNN models which consists of\nonly shallow Transformers stack is suffice to give comparable, if not better,\nperformance than a deeper Transformer model. Evaluated on the Penn Treebank and\nWikiText-2 corpora, the proposed TransfoRNN model has shown lower model\nperplexities with fewer number of model parameters. On the Penn Treebank\ncorpus, the model perplexities were reduced up to 5.5% with the model size\nreduced up to 10.5%. On the WikiText-2 corpus, the model perplexity was reduced\nup to 2.2% with a 27.7% smaller model. Also, the TransfoRNN model was applied\non the LibriSpeech speech recognition task and has shown comparable results\nwith the Transformer models.",
    "published": "2021-04-04T09:31:18Z",
    "updated": "2021-04-04T09:31:18Z",
    "authors": [
      "Tze Yuang Chong",
      "Xuyang Wang",
      "Lin Yang",
      "Junjie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.09084v6",
    "title": "Fastformer: Additive Attention Can Be All You Need",
    "summary": "Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.",
    "published": "2021-08-20T09:44:44Z",
    "updated": "2021-09-05T08:20:31Z",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang",
      "Xing Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.15473v2",
    "title": "Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with\n  Multi Modal Fusion",
    "summary": "We revisit the use of spectral techniques to replaces the attention mechanism\nin Transformers through Fourier Transform based token mixing, and present a\ncomprehensive and novel reformulation of this technique in next generation\ntransformer models. We provide expanded literature context, detailed\nmathematical formulations of Fourier mixing and causal masking, and introduce a\nnovel MultiDomain Fourier Wavelet Attention(MDFWA) that integrates frequency\nand time localized transforms to capture both global and local dependencies\nefficiently. We derive the complexity bounds, gradient formulas, and show that\nMDFWA achieves sub quadratic time and memory cost while improving expressive\npower. We validate our design on an abstractive summarization task using PubMed\ndataset, by enhancing the proposed approach with learned frequency bases,\nadaptive scale selection, and multi-modal extensions.",
    "published": "2021-11-25T18:03:41Z",
    "updated": "2025-04-22T18:24:20Z",
    "authors": [
      "Andrew Kiruluta",
      "Andreas Lemos",
      "Eric Lundy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.14087v1",
    "title": "APRIL: Finding the Achilles' Heel on Privacy for Vision Transformers",
    "summary": "Federated learning frameworks typically require collaborators to share their\nlocal gradient updates of a common model instead of sharing training data to\npreserve privacy. However, prior works on Gradient Leakage Attacks showed that\nprivate training data can be revealed from gradients. So far almost all\nrelevant works base their attacks on fully-connected or convolutional neural\nnetworks. Given the recent overwhelmingly rising trend of adapting Transformers\nto solve multifarious vision tasks, it is highly valuable to investigate the\nprivacy risk of vision transformers. In this paper, we analyse the gradient\nleakage risk of self-attention based mechanism in both theoretical and\npractical manners. Particularly, we propose APRIL - Attention PRIvacy Leakage,\nwhich poses a strong threat to self-attention inspired models such as ViT.\nShowing how vision Transformers are at the risk of privacy leakage via\ngradients, we urge the significance of designing privacy-safer Transformer\nmodels and defending schemes.",
    "published": "2021-12-28T10:51:26Z",
    "updated": "2021-12-28T10:51:26Z",
    "authors": [
      "Jiahao Lu",
      "Xi Sheryl Zhang",
      "Tianli Zhao",
      "Xiangyu He",
      "Jian Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.15087v1",
    "title": "ChunkFormer: Learning Long Time Series with Multi-stage Chunked\n  Transformer",
    "summary": "The analysis of long sequence data remains challenging in many real-world\napplications. We propose a novel architecture, ChunkFormer, that improves the\nexisting Transformer framework to handle the challenges while dealing with long\ntime series. Original Transformer-based models adopt an attention mechanism to\ndiscover global information along a sequence to leverage the contextual data.\nLong sequential data traps local information such as seasonality and\nfluctuations in short data sequences. In addition, the original Transformer\nconsumes more resources by carrying the entire attention matrix during the\ntraining course. To overcome these challenges, ChunkFormer splits the long\nsequences into smaller sequence chunks for the attention calculation,\nprogressively applying different chunk sizes in each stage. In this way, the\nproposed model gradually learns both local and global information without\nchanging the total length of the input sequences. We have extensively tested\nthe effectiveness of this new architecture on different business domains and\nhave proved the advantage of such a model over the existing Transformer-based\nmodels.",
    "published": "2021-12-30T15:06:32Z",
    "updated": "2021-12-30T15:06:32Z",
    "authors": [
      "Yue Ju",
      "Alka Isac",
      "Yimin Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.02358v1",
    "title": "ViT-P: Rethinking Data-efficient Vision Transformers from Locality",
    "summary": "Recent advances of Transformers have brought new trust to computer vision\ntasks. However, on small dataset, Transformers is hard to train and has lower\nperformance than convolutional neural networks. We make vision transformers as\ndata-efficient as convolutional neural networks by introducing multi-focal\nattention bias. Inspired by the attention distance in a well-trained ViT, we\nconstrain the self-attention of ViT to have multi-scale localized receptive\nfield. The size of receptive field is adaptable during training so that optimal\nconfiguration can be learned. We provide empirical evidence that proper\nconstrain of receptive field can reduce the amount of training data for vision\ntransformers. On Cifar100, our ViT-P Base model achieves the state-of-the-art\naccuracy (83.16%) trained from scratch. We also perform analysis on ImageNet to\nshow our method does not lose accuracy on large data sets.",
    "published": "2022-03-04T14:49:48Z",
    "updated": "2022-03-04T14:49:48Z",
    "authors": [
      "Bin Chen",
      "Ran Wang",
      "Di Ming",
      "Xin Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.00883v1",
    "title": "Improving Transformer-based Conversational ASR by Inter-Sentential\n  Attention Mechanism",
    "summary": "Transformer-based models have demonstrated their effectiveness in automatic\nspeech recognition (ASR) tasks and even shown superior performance over the\nconventional hybrid framework. The main idea of Transformers is to capture the\nlong-range global context within an utterance by self-attention layers.\nHowever, for scenarios like conversational speech, such utterance-level\nmodeling will neglect contextual dependencies that span across utterances. In\nthis paper, we propose to explicitly model the inter-sentential information in\na Transformer based end-to-end architecture for conversational speech\nrecognition. Specifically, for the encoder network, we capture the contexts of\nprevious speech and incorporate such historic information into current input by\na context-aware residual attention mechanism. For the decoder, the prediction\nof current utterance is also conditioned on the historic linguistic information\nthrough a conditional decoder framework. We show the effectiveness of our\nproposed method on several open-source dialogue corpora and the proposed method\nconsistently improved the performance from the utterance-level\nTransformer-based ASR models.",
    "published": "2022-07-02T17:17:47Z",
    "updated": "2022-07-02T17:17:47Z",
    "authors": [
      "Kun Wei",
      "Pengcheng Guo",
      "Ning Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.04535v2",
    "title": "Depthformer : Multiscale Vision Transformer For Monocular Depth\n  Estimation With Local Global Information Fusion",
    "summary": "Attention-based models such as transformers have shown outstanding\nperformance on dense prediction tasks, such as semantic segmentation, owing to\ntheir capability of capturing long-range dependency in an image. However, the\nbenefit of transformers for monocular depth prediction has seldom been explored\nso far. This paper benchmarks various transformer-based models for the depth\nestimation task on an indoor NYUV2 dataset and an outdoor KITTI dataset. We\npropose a novel attention-based architecture, Depthformer for monocular depth\nestimation that uses multi-head self-attention to produce the multiscale\nfeature maps, which are effectively combined by our proposed decoder network.\nWe also propose a Transbins module that divides the depth range into bins whose\ncenter value is estimated adaptively per image. The final depth estimated is a\nlinear combination of bin centers for each pixel. Transbins module takes\nadvantage of the global receptive field using the transformer module in the\nencoding stage. Experimental results on NYUV2 and KITTI depth estimation\nbenchmark demonstrate that our proposed method improves the state-of-the-art by\n3.3%, and 3.3% respectively in terms of Root Mean Squared Error (RMSE). Code is\navailable at https://github.com/ashutosh1807/Depthformer.git.",
    "published": "2022-07-10T20:49:11Z",
    "updated": "2022-07-12T07:39:10Z",
    "authors": [
      "Ashutosh Agarwal",
      "Chetan Arora"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.15107v2",
    "title": "A Light Touch Approach to Teaching Transformers Multi-view Geometry",
    "summary": "Transformers are powerful visual learners, in large part due to their\nconspicuous lack of manually-specified priors. This flexibility can be\nproblematic in tasks that involve multiple-view geometry, due to the\nnear-infinite possible variations in 3D shapes and viewpoints (requiring\nflexibility), and the precise nature of projective geometry (obeying rigid\nlaws). To resolve this conundrum, we propose a \"light touch\" approach, guiding\nvisual Transformers to learn multiple-view geometry but allowing them to break\nfree when needed. We achieve this by using epipolar lines to guide the\nTransformer's cross-attention maps, penalizing attention values outside the\nepipolar lines and encouraging higher attention along these lines since they\ncontain geometrically plausible matches. Unlike previous methods, our proposal\ndoes not require any camera pose information at test-time. We focus on\npose-invariant object instance retrieval, where standard Transformer networks\nstruggle, due to the large differences in viewpoint between query and retrieved\nimages. Experimentally, our method outperforms state-of-the-art approaches at\nobject retrieval, without needing pose information at test-time.",
    "published": "2022-11-28T07:54:06Z",
    "updated": "2023-04-02T12:15:52Z",
    "authors": [
      "Yash Bhalgat",
      "Joao F. Henriques",
      "Andrew Zisserman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.10554v1",
    "title": "A Length-Extrapolatable Transformer",
    "summary": "Position modeling plays a critical role in Transformers. In this paper, we\nfocus on length extrapolation, i.e., training on short texts while evaluating\nlonger sequences. We define attention resolution as an indicator of\nextrapolation. Then we propose two designs to improve the above metric of\nTransformers. Specifically, we introduce a relative position embedding to\nexplicitly maximize attention resolution. Moreover, we use blockwise causal\nattention during inference for better resolution. We evaluate different\nTransformer variants with language modeling. Experimental results show that our\nmodel achieves strong performance in both interpolation and extrapolation\nsettings. The code will be available at https://aka.ms/LeX-Transformer.",
    "published": "2022-12-20T18:56:20Z",
    "updated": "2022-12-20T18:56:20Z",
    "authors": [
      "Yutao Sun",
      "Li Dong",
      "Barun Patra",
      "Shuming Ma",
      "Shaohan Huang",
      "Alon Benhaim",
      "Vishrav Chaudhary",
      "Xia Song",
      "Furu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.12335v1",
    "title": "Temporal Fusion Transformers for Streamflow Prediction: Value of\n  Combining Attention with Recurrence",
    "summary": "Over the past few decades, the hydrology community has witnessed notable\nadvancements in streamflow prediction, particularly with the introduction of\ncutting-edge machine-learning algorithms. Recurrent neural networks, especially\nLong Short-Term Memory (LSTM) networks, have become popular due to their\ncapacity to create precise forecasts and realistically mimic the system\ndynamics. Attention-based models, such as Transformers, can learn from the\nentire data sequence concurrently, a feature that LSTM does not have. This work\ntests the hypothesis that combining recurrence with attention can improve\nstreamflow prediction. We set up the Temporal Fusion Transformer (TFT)\narchitecture, a model that combines both of these aspects and has never been\napplied in hydrology before. We compare the performance of LSTM, Transformers,\nand TFT over 2,610 globally distributed catchments from the recently available\nCaravan dataset. Our results demonstrate that TFT indeed exceeds the\nperformance benchmark set by the LSTM and Transformers for streamflow\nprediction. Additionally, being an explainable AI method, TFT helps in gaining\ninsights into the streamflow generation processes.",
    "published": "2023-05-21T03:58:16Z",
    "updated": "2023-05-21T03:58:16Z",
    "authors": [
      "Sinan Rasiya Koya",
      "Tirthankar Roy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.04842v1",
    "title": "InvPT++: Inverted Pyramid Multi-Task Transformer for Visual Scene\n  Understanding",
    "summary": "Multi-task scene understanding aims to design models that can simultaneously\npredict several scene understanding tasks with one versatile model. Previous\nstudies typically process multi-task features in a more local way, and thus\ncannot effectively learn spatially global and cross-task interactions, which\nhampers the models' ability to fully leverage the consistency of various tasks\nin multi-task learning. To tackle this problem, we propose an Inverted Pyramid\nmulti-task Transformer, capable of modeling cross-task interaction among\nspatial features of different tasks in a global context. Specifically, we first\nutilize a transformer encoder to capture task-generic features for all tasks.\nAnd then, we design a transformer decoder to establish spatial and cross-task\ninteraction globally, and a novel UP-Transformer block is devised to increase\nthe resolutions of multi-task features gradually and establish cross-task\ninteraction at different scales. Furthermore, two types of Cross-Scale\nSelf-Attention modules, i.e., Fusion Attention and Selective Attention, are\nproposed to efficiently facilitate cross-task interaction across different\nfeature scales. An Encoder Feature Aggregation strategy is further introduced\nto better model multi-scale information in the decoder. Comprehensive\nexperiments on several 2D/3D multi-task benchmarks clearly demonstrate our\nproposal's effectiveness, establishing significant state-of-the-art\nperformances.",
    "published": "2023-06-08T00:28:22Z",
    "updated": "2023-06-08T00:28:22Z",
    "authors": [
      "Hanrong Ye",
      "Dan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.14023v3",
    "title": "Are Transformers with One Layer Self-Attention Using Low-Rank Weight\n  Matrices Universal Approximators?",
    "summary": "Existing analyses of the expressive capacity of Transformer models have\nrequired excessively deep layers for data memorization, leading to a\ndiscrepancy with the Transformers actually used in practice. This is primarily\ndue to the interpretation of the softmax function as an approximation of the\nhardmax function. By clarifying the connection between the softmax function and\nthe Boltzmann operator, we prove that a single layer of self-attention with\nlow-rank weight matrices possesses the capability to perfectly capture the\ncontext of an entire input sequence. As a consequence, we show that one-layer\nand single-head Transformers have a memorization capacity for finite samples,\nand that Transformers consisting of one self-attention layer with two\nfeed-forward neural networks are universal approximators for continuous\npermutation equivariant functions on a compact domain.",
    "published": "2023-07-26T08:07:37Z",
    "updated": "2024-01-29T10:16:41Z",
    "authors": [
      "Tokio Kajitsuka",
      "Issei Sato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.03212v2",
    "title": "Average-Hard Attention Transformers are Constant-Depth Uniform Threshold\n  Circuits",
    "summary": "Transformers have emerged as a widely used neural network model for various\nnatural language processing tasks. Previous research explored their\nrelationship with constant-depth threshold circuits, making two assumptions:\naverage-hard attention and logarithmic precision for internal computations\nrelative to input length. Merrill et al. (2022) prove that average-hard\nattention transformers recognize languages that fall within the complexity\nclass TC0, denoting the set of languages that can be recognized by\nconstant-depth polynomial-size threshold circuits. Likewise, Merrill and\nSabharwal (2023) show that log-precision transformers recognize languages\nwithin the class of uniform TC0. This shows that both transformer models can be\nsimulated by constant-depth threshold circuits, with the latter being more\nrobust due to generating a uniform circuit family. Our paper shows that the\nfirst result can be extended to yield uniform circuits as well.",
    "published": "2023-08-06T21:23:22Z",
    "updated": "2023-08-21T18:54:56Z",
    "authors": [
      "Lena Strobl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.12462v1",
    "title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via\n  Attention Weights",
    "summary": "In the realm of deep learning, transformers have emerged as a dominant\narchitecture, particularly in natural language processing tasks. However, with\ntheir widespread adoption, concerns regarding the security and privacy of the\ndata processed by these models have arisen. In this paper, we address a pivotal\nquestion: Can the data fed into transformers be recovered using their attention\nweights and outputs? We introduce a theoretical framework to tackle this\nproblem. Specifically, we present an algorithm that aims to recover the input\ndata $X \\in \\mathbb{R}^{d \\times n}$ from given attention weights $W = QK^\\top\n\\in \\mathbb{R}^{d \\times d}$ and output $B \\in \\mathbb{R}^{n \\times n}$ by\nminimizing the loss function $L(X)$. This loss function captures the\ndiscrepancy between the expected output and the actual output of the\ntransformer. Our findings have significant implications for the Localized\nLayer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's\ndesign from a security and privacy perspective. This work underscores the\nimportance of understanding and safeguarding the internal workings of\ntransformers to ensure the confidentiality of processed data.",
    "published": "2023-10-19T04:41:01Z",
    "updated": "2023-10-19T04:41:01Z",
    "authors": [
      "Yichuan Deng",
      "Zhao Song",
      "Shenghao Xie",
      "Chiwun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.01990v1",
    "title": "SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust\n  Attention",
    "summary": "We present Self-Adaptive Robust Attention for Robotics Transformers\n(SARA-RT): a new paradigm for addressing the emerging challenge of scaling up\nRobotics Transformers (RT) for on-robot deployment. SARA-RT relies on the new\nmethod of fine-tuning proposed by us, called up-training. It converts\npre-trained or already fine-tuned Transformer-based robotic policies of\nquadratic time complexity (including massive billion-parameter\nvision-language-action models or VLAs), into their efficient linear-attention\ncounterparts maintaining high quality. We demonstrate the effectiveness of\nSARA-RT by speeding up: (a) the class of recently introduced RT-2 models, the\nfirst VLA robotic policies pre-trained on internet-scale data, as well as (b)\nPoint Cloud Transformer (PCT) robotic policies operating on large point clouds.\nWe complement our results with the rigorous mathematical analysis providing\ndeeper insight into the phenomenon of SARA.",
    "published": "2023-12-04T16:08:47Z",
    "updated": "2023-12-04T16:08:47Z",
    "authors": [
      "Isabel Leal",
      "Krzysztof Choromanski",
      "Deepali Jain",
      "Avinava Dubey",
      "Jake Varley",
      "Michael Ryoo",
      "Yao Lu",
      "Frederick Liu",
      "Vikas Sindhwani",
      "Quan Vuong",
      "Tamas Sarlos",
      "Ken Oslund",
      "Karol Hausman",
      "Kanishka Rao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.05712v1",
    "title": "DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion\n  Transformer",
    "summary": "Speech-driven 3D facial animation is important for many multimedia\napplications. Recent work has shown promise in using either Diffusion models or\nTransformer architectures for this task. However, their mere aggregation does\nnot lead to improved performance. We suspect this is due to a shortage of\npaired audio-4D data, which is crucial for the Transformer to effectively\nperform as a denoiser within the Diffusion framework. To tackle this issue, we\npresent DiffSpeaker, a Transformer-based network equipped with novel biased\nconditional attention modules. These modules serve as substitutes for the\ntraditional self/cross-attention in standard Transformers, incorporating\nthoughtfully designed biases that steer the attention mechanisms to concentrate\non both the relevant task-specific and diffusion-related conditions. We also\nexplore the trade-off between accurate lip synchronization and non-verbal\nfacial expressions within the Diffusion paradigm. Experiments show our model\nnot only achieves state-of-the-art performance on existing benchmarks, but also\nfast inference speed owing to its ability to generate facial motions in\nparallel.",
    "published": "2024-02-08T14:39:16Z",
    "updated": "2024-02-08T14:39:16Z",
    "authors": [
      "Zhiyuan Ma",
      "Xiangyu Zhu",
      "Guojun Qi",
      "Chen Qian",
      "Zhaoxiang Zhang",
      "Zhen Lei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.10288v1",
    "title": "Rough Transformers for Continuous and Efficient Time-Series Modelling",
    "summary": "Time-series data in real-world medical settings typically exhibit long-range\ndependencies and are observed at non-uniform intervals. In such contexts,\ntraditional sequence-based recurrent models struggle. To overcome this,\nresearchers replace recurrent architectures with Neural ODE-based models to\nmodel irregularly sampled data and use Transformer-based architectures to\naccount for long-range dependencies. Despite the success of these two\napproaches, both incur very high computational costs for input sequences of\nmoderate lengths and greater. To mitigate this, we introduce the Rough\nTransformer, a variation of the Transformer model which operates on\ncontinuous-time representations of input sequences and incurs significantly\nreduced computational costs, critical for addressing long-range dependencies\ncommon in medical contexts. In particular, we propose multi-view signature\nattention, which uses path signatures to augment vanilla attention and to\ncapture both local and global dependencies in input data, while remaining\nrobust to changes in the sequence length and sampling frequency. We find that\nRough Transformers consistently outperform their vanilla attention counterparts\nwhile obtaining the benefits of Neural ODE-based models using a fraction of the\ncomputational time and memory resources on synthetic and real-world time-series\ntasks.",
    "published": "2024-03-15T13:29:45Z",
    "updated": "2024-03-15T13:29:45Z",
    "authors": [
      "Fernando Moreno-Pino",
      "Ãlvaro Arroyo",
      "Harrison Waldon",
      "Xiaowen Dong",
      "Ãlvaro Cartea"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15310v5",
    "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
    "summary": "Linearization of attention using various kernel approximation and kernel\nlearning techniques has shown promise. Past methods used a subset of\ncombinations of component functions and weight matrices within the random\nfeature paradigm. We identify the need for a systematic comparison of different\ncombinations of weight matrices and component functions for attention learning\nin Transformer. Hence, we introduce Spectraformer, a unified framework for\napproximating and learning the kernel function in the attention mechanism of\nthe Transformer. Our empirical results demonstrate, for the first time, that a\nrandom feature-based approach can achieve performance comparable to\ntop-performing sparse and low-rank methods on the challenging Long Range Arena\nbenchmark. Thus, we establish a new state-of-the-art for random feature-based\nefficient Transformers. The framework also produces many variants that offer\ndifferent advantages in accuracy, training time, and memory consumption. Our\ncode is available at: https://github.com/cruiseresearchgroup/spectraformer .",
    "published": "2024-05-24T07:52:53Z",
    "updated": "2025-09-23T03:21:45Z",
    "authors": [
      "Duke Nguyen",
      "Du Yin",
      "Aditya Joshi",
      "Flora Salim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.18194v3",
    "title": "Delving into Differentially Private Transformer",
    "summary": "Deep learning with differential privacy (DP) has garnered significant\nattention over the past years, leading to the development of numerous methods\naimed at enhancing model accuracy and training efficiency. This paper delves\ninto the problem of training Transformer models with differential privacy. Our\ntreatment is modular: the logic is to `reduce' the problem of training DP\nTransformer to the more basic problem of training DP vanilla neural nets. The\nlatter is better understood and amenable to many model-agnostic methods. Such\n`reduction' is done by first identifying the hardness unique to DP Transformer\ntraining: the attention distraction phenomenon and a lack of compatibility with\nexisting techniques for efficient gradient clipping. To deal with these two\nissues, we propose the Re-Attention Mechanism and Phantom Clipping,\nrespectively. We believe that our work not only casts new light on training DP\nTransformers but also promotes a modular treatment to advance research in the\nfield of differentially private deep learning.",
    "published": "2024-05-28T14:04:09Z",
    "updated": "2024-08-26T09:35:54Z",
    "authors": [
      "Youlong Ding",
      "Xueyang Wu",
      "Yining Meng",
      "Yonggang Luo",
      "Hao Wang",
      "Weike Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.11250v1",
    "title": "Linear Recency Bias During Training Improves Transformers' Fit to\n  Reading Times",
    "summary": "Recent psycholinguistic research has compared human reading times to\nsurprisal estimates from language models to study the factors shaping human\nsentence processing difficulty. Previous studies have shown a strong fit\nbetween surprisal values from Transformers and reading times. However, standard\nTransformers work with a lossless representation of the entire previous\nlinguistic context, unlike models of human language processing that include\nmemory decay. To bridge this gap, this paper evaluates a modification of the\nTransformer model that uses ALiBi (Press et al., 2022), a recency bias added to\nattention scores. Surprisal estimates with ALiBi show an improved fit to human\nreading times compared to a standard Transformer baseline. A subsequent\nanalysis of attention heads suggests that ALiBi's mixture of slopes -- which\ndetermine the rate of memory decay in each attention head -- may play a role in\nthe improvement by helping models with ALiBi to track different kinds of\nlinguistic dependencies.",
    "published": "2024-09-17T14:57:51Z",
    "updated": "2024-09-17T14:57:51Z",
    "authors": [
      "Christian Clark",
      "Byung-Doh Oh",
      "William Schuler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.12118v4",
    "title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers",
    "summary": "In this paper, I introduce the retrieval problem, a simple yet common\nreasoning task that can be solved only by transformers with a minimum number of\nlayers, which grows logarithmically with the input size. I empirically show\nthat large language models can solve the task under different prompting\nformulations without any fine-tuning. To understand how transformers solve the\nretrieval problem, I train several transformers on a minimal formulation.\nSuccessful learning occurs only under the presence of an implicit curriculum. I\nuncover the learned mechanisms by studying the attention maps in the trained\ntransformers. I also study the training process, uncovering that attention\nheads always emerge in a specific sequence guided by the implicit curriculum.",
    "published": "2024-11-18T23:12:13Z",
    "updated": "2025-03-29T23:29:51Z",
    "authors": [
      "Tiberiu Musat"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.06538v1",
    "title": "Understanding Factual Recall in Transformers via Associative Memories",
    "summary": "Large language models have demonstrated an impressive ability to perform\nfactual recall. Prior work has found that transformers trained on factual\nrecall tasks can store information at a rate proportional to their parameter\ncount. In our work, we show that shallow transformers can use a combination of\nassociative memories to obtain such near optimal storage capacity. We begin by\nproving that the storage capacities of both linear and MLP associative memories\nscale linearly with parameter count. We next introduce a synthetic factual\nrecall task, and prove that a transformer with a single layer of self-attention\nfollowed by an MLP can obtain 100% accuracy on the task whenever either the\ntotal number of self-attention parameters or MLP parameters scales (up to log\nfactors) linearly with the number of facts. In particular, the transformer can\ntrade off between using the value matrices or the MLP as an associative memory\nto store the dataset of facts. We complement these expressivity results with an\nanalysis of the gradient flow trajectory of a simplified linear attention model\ntrained on our factual recall task, where we show that the model exhibits\nsequential learning behavior.",
    "published": "2024-12-09T14:48:14Z",
    "updated": "2024-12-09T14:48:14Z",
    "authors": [
      "Eshaan Nichani",
      "Jason D. Lee",
      "Alberto Bietti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.11834v3",
    "title": "Wonderful Matrices: Combining for a More Efficient and Effective\n  Foundation Model Architecture",
    "summary": "In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.",
    "published": "2024-12-16T14:56:28Z",
    "updated": "2024-12-20T11:43:13Z",
    "authors": [
      "Jingze Shi",
      "Bingheng Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.00823v2",
    "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular\n  Architecture with Generalized Cross-Attention",
    "summary": "Transformers have achieved remarkable success across diverse domains, but\ntheir monolithic architecture presents challenges in interpretability,\nadaptability, and scalability. This paper introduces a novel modular\nTransformer architecture that explicitly decouples knowledge and reasoning\nthrough a generalized cross-attention mechanism to a globally shared knowledge\nbase with layer-specific transformations, specifically designed for effective\nknowledge retrieval. Critically, we provide a rigorous mathematical derivation\ndemonstrating that the Feed-Forward Network (FFN) in a standard Transformer is\na specialized case (a closure) of this generalized cross-attention, revealing\nits role in implicit knowledge retrieval and validating our design. This\ntheoretical framework provides a new lens for understanding FFNs and lays the\nfoundation for future research exploring enhanced interpretability,\nadaptability, and scalability, enabling richer interplay with external\nknowledge bases and other systems.",
    "published": "2025-01-01T12:55:57Z",
    "updated": "2025-01-06T14:26:41Z",
    "authors": [
      "Zhenyu Guo",
      "Wenguang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16381v1",
    "title": "PaTH Attention: Position Encoding via Accumulating Householder\n  Transformations",
    "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines.",
    "published": "2025-05-22T08:36:09Z",
    "updated": "2025-05-22T08:36:09Z",
    "authors": [
      "Songlin Yang",
      "Yikang Shen",
      "Kaiyue Wen",
      "Shawn Tan",
      "Mayank Mishra",
      "Liliang Ren",
      "Rameswar Panda",
      "Yoon Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13043v1",
    "title": "The Power of Architecture: Deep Dive into Transformer Architectures for\n  Long-Term Time Series Forecasting",
    "summary": "Transformer-based models have recently become dominant in Long-term Time\nSeries Forecasting (LTSF), yet the variations in their architecture, such as\nencoder-only, encoder-decoder, and decoder-only designs, raise a crucial\nquestion: What Transformer architecture works best for LTSF tasks? However,\nexisting models are often tightly coupled with various time-series-specific\ndesigns, making it difficult to isolate the impact of the architecture itself.\nTo address this, we propose a novel taxonomy that disentangles these designs,\nenabling clearer and more unified comparisons of Transformer architectures. Our\ntaxonomy considers key aspects such as attention mechanisms, forecasting\naggregations, forecasting paradigms, and normalization layers. Through\nextensive experiments, we uncover several key insights: bi-directional\nattention with joint-attention is most effective; more complete forecasting\naggregation improves performance; and the direct-mapping paradigm outperforms\nautoregressive approaches. Furthermore, our combined model, utilizing optimal\narchitectural choices, consistently outperforms several existing models,\nreinforcing the validity of our conclusions. We hope these findings offer\nvaluable guidance for future research on Transformer architectural designs in\nLTSF. Our code is available at https://github.com/HALF111/TSF_architecture.",
    "published": "2025-07-17T12:16:04Z",
    "updated": "2025-07-17T12:16:04Z",
    "authors": [
      "Lefei Shen",
      "Mouxiang Chen",
      "Han Fu",
      "Xiaoxue Ren",
      "Xiaoyun Joy Wang",
      "Jianling Sun",
      "Zhuo Li",
      "Chenghao Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23641v1",
    "title": "Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging",
    "summary": "Transformers are very effective in capturing both global and local\ncorrelations within high-energy particle collisions, but they present\ndeployment challenges in high-data-throughput environments, such as the CERN\nLHC. The quadratic complexity of transformer models demands substantial\nresources and increases latency during inference. In order to address these\nissues, we introduce the Spatially Aware Linear Transformer (SAL-T), a\nphysics-inspired enhancement of the linformer architecture that maintains\nlinear attention. Our method incorporates spatially aware partitioning of\nparticles based on kinematic features, thereby computing attention between\nregions of physical significance. Additionally, we employ convolutional layers\nto capture local correlations, informed by insights from jet physics. In\naddition to outperforming the standard linformer in jet classification tasks,\nSAL-T also achieves classification results comparable to full-attention\ntransformers, while using considerably fewer resources with lower latency\nduring inference. Experiments on a generic point cloud classification dataset\n(ModelNet10) further confirm this trend. Our code is available at\nhttps://github.com/aaronw5/SAL-T4HEP.",
    "published": "2025-10-24T18:00:01Z",
    "updated": "2025-10-24T18:00:01Z",
    "authors": [
      "Aaron Wang",
      "Zihan Zhao",
      "Subash Katel",
      "Vivekanand Gyanchand Sahu",
      "Elham E Khoda",
      "Abhijith Gandrakota",
      "Jennifer Ngadiuba",
      "Richard Cavanaugh",
      "Javier Duarte"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.00576v1",
    "title": "FlashEVA: Accelerating LLM inference via Efficient Attention",
    "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference.",
    "published": "2025-11-01T14:38:57Z",
    "updated": "2025-11-01T14:38:57Z",
    "authors": [
      "Juan Gabriel Kostelec",
      "Qinghai Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.06193v2",
    "title": "Relation Transformer Network",
    "summary": "The extraction of a scene graph with objects as nodes and mutual\nrelationships as edges is the basis for a deep understanding of image content.\nDespite recent advances, such as message passing and joint classification, the\ndetection of visual relationships remains a challenging task due to sub-optimal\nexploration of the mutual interaction among the visual objects. In this work,\nwe propose a novel transformer formulation for scene graph generation and\nrelation prediction. We leverage the encoder-decoder architecture of the\ntransformer for rich feature embedding of nodes and edges. Specifically, we\nmodel the node-to-node interaction with the self-attention of the transformer\nencoder and the edge-to-node interaction with the cross-attention of the\ntransformer decoder. Further, we introduce a novel positional embedding\nsuitable to handle edges in the decoder. Finally, our relation prediction\nmodule classifies the directed relation from the learned node and edge\nembedding. We name this architecture as Relation Transformer Network (RTN). On\nthe Visual Genome and GQA dataset, we have achieved an overall mean of 4.85%\nand 3.1% point improvement in comparison with state-of-the-art methods. Our\nexperiments show that Relation Transformer can efficiently model context across\nvarious datasets with small, medium, and large-scale relation classification.",
    "published": "2020-04-13T20:47:01Z",
    "updated": "2021-07-20T21:10:56Z",
    "authors": [
      "Rajat Koner",
      "Suprosanna Shit",
      "Volker Tresp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.03930v1",
    "title": "Hierarchical Graph Transformer with Adaptive Node Sampling",
    "summary": "The Transformer architecture has achieved remarkable success in a number of\ndomains including natural language processing and computer vision. However,\nwhen it comes to graph-structured data, transformers have not achieved\ncompetitive performance, especially on large graphs. In this paper, we identify\nthe main deficiencies of current graph transformers:(1) Existing node sampling\nstrategies in Graph Transformers are agnostic to the graph characteristics and\nthe training process. (2) Most sampling strategies only focus on local\nneighbors and neglect the long-range dependencies in the graph. We conduct\nexperimental investigations on synthetic datasets to show that existing\nsampling strategies are sub-optimal. To tackle the aforementioned problems, we\nformulate the optimization strategies of node sampling in Graph Transformer as\nan adversary bandit problem, where the rewards are related to the attention\nweights and can vary in the training procedure. Meanwhile, we propose a\nhierarchical attention scheme with graph coarsening to capture the long-range\ninteractions while reducing computational complexity. Finally, we conduct\nextensive experiments on real-world datasets to demonstrate the superiority of\nour method over existing graph transformers and popular GNNs.",
    "published": "2022-10-08T05:53:25Z",
    "updated": "2022-10-08T05:53:25Z",
    "authors": [
      "Zaixi Zhang",
      "Qi Liu",
      "Qingyong Hu",
      "Chee-Kong Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.11251v3",
    "title": "Towards End-to-End Generative Modeling of Long Videos with\n  Memory-Efficient Bidirectional Transformers",
    "summary": "Autoregressive transformers have shown remarkable success in video\ngeneration. However, the transformers are prohibited from directly learning the\nlong-term dependency in videos due to the quadratic complexity of\nself-attention, and inherently suffering from slow inference time and error\npropagation due to the autoregressive process. In this paper, we propose\nMemory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of\nlong-term dependency in videos and fast inference. Based on recent advances in\nbidirectional transformers, our method learns to decode the entire\nspatio-temporal volume of a video in parallel from partially observed patches.\nThe proposed transformer achieves a linear time complexity in both encoding and\ndecoding, by projecting observable context tokens into a fixed number of latent\ntokens and conditioning them to decode the masked tokens through the\ncross-attention. Empowered by linear complexity and bidirectional modeling, our\nmethod demonstrates significant improvement over the autoregressive\nTransformers for generating moderately long videos in both quality and speed.\nVideos and code are available at https://sites.google.com/view/mebt-cvpr2023 .",
    "published": "2023-03-20T16:35:38Z",
    "updated": "2023-05-31T15:02:51Z",
    "authors": [
      "Jaehoon Yoo",
      "Semin Kim",
      "Doyup Lee",
      "Chiheon Kim",
      "Seunghoon Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.03824v4",
    "title": "FNet: Mixing Tokens with Fourier Transforms",
    "summary": "We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.",
    "published": "2021-05-09T03:32:48Z",
    "updated": "2022-05-26T18:50:20Z",
    "authors": [
      "James Lee-Thorp",
      "Joshua Ainslie",
      "Ilya Eckstein",
      "Santiago Ontanon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.09641v2",
    "title": "Evaluating Transformers for Lightweight Action Recognition",
    "summary": "In video action recognition, transformers consistently reach state-of-the-art\naccuracy. However, many models are too heavyweight for the average researcher\nwith limited hardware resources. In this work, we explore the limitations of\nvideo transformers for lightweight action recognition. We benchmark 13 video\ntransformers and baselines across 3 large-scale datasets and 10 hardware\ndevices. Our study is the first to evaluate the efficiency of action\nrecognition models in depth across multiple devices and train a wide range of\nvideo transformers under the same conditions. We categorize current methods\ninto three classes and show that composite transformers that augment\nconvolutional backbones are best at lightweight action recognition, despite\nlacking accuracy. Meanwhile, attention-only models need more motion modeling\ncapabilities and stand-alone attention block models currently incur too much\nlatency overhead. Our experiments conclude that current video transformers are\nnot yet capable of lightweight action recognition on par with traditional\nconvolutional baselines, and that the previously mentioned shortcomings need to\nbe addressed to bridge this gap. Code to reproduce our experiments will be made\npublicly available.",
    "published": "2021-11-18T11:45:42Z",
    "updated": "2021-12-07T21:12:24Z",
    "authors": [
      "Raivo Koot",
      "Markus Hennerbichler",
      "Haiping Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.13891v2",
    "title": "Transformers from an Optimization Perspective",
    "summary": "Deep learning models such as the Transformer are often constructed by\nheuristics and experience. To provide a complementary foundation, in this work\nwe study the following problem: Is it possible to find an energy function\nunderlying the Transformer model, such that descent steps along this energy\ncorrespond with the Transformer forward pass? By finding such a function, we\ncan view Transformers as the unfolding of an interpretable optimization process\nacross iterations. This unfolding perspective has been frequently adopted in\nthe past to elucidate more straightforward deep models such as MLPs and CNNs;\nhowever, it has thus far remained elusive obtaining a similar equivalence for\nmore complex models with self-attention mechanisms like the Transformer. To\nthis end, we first outline several major obstacles before providing companion\ntechniques to at least partially address them, demonstrating for the first time\na close association between energy function minimization and deep layers with\nself-attention. This interpretation contributes to our intuition and\nunderstanding of Transformers, while potentially laying the ground-work for new\nmodel designs.",
    "published": "2022-05-27T10:45:15Z",
    "updated": "2023-02-27T05:56:12Z",
    "authors": [
      "Yongyi Yang",
      "Zengfeng Huang",
      "David Wipf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.08639v2",
    "title": "Improving Transformer-based Networks With Locality For Automatic Speaker\n  Verification",
    "summary": "Recently, Transformer-based architectures have been explored for speaker\nembedding extraction. Although the Transformer employs the self-attention\nmechanism to efficiently model the global interaction between token embeddings,\nit is inadequate for capturing short-range local context, which is essential\nfor the accurate extraction of speaker information. In this study, we enhance\nthe Transformer with the enhanced locality modeling in two directions. First,\nwe propose the Locality-Enhanced Conformer (LE-Confomer) by introducing\ndepth-wise convolution and channel-wise attention into the Conformer blocks.\nSecond, we present the Speaker Swin Transformer (SST) by adapting the Swin\nTransformer, originally proposed for vision tasks, into speaker embedding\nnetwork. We evaluate the proposed approaches on the VoxCeleb datasets and a\nlarge-scale Microsoft internal multilingual (MS-internal) dataset. The proposed\nmodels achieve 0.75% EER on VoxCeleb 1 test set, outperforming the previously\nproposed Transformer-based models and CNN-based models, such as ResNet34 and\nECAPA-TDNN. When trained on the MS-internal dataset, the proposed models\nachieve promising results with 14.6% relative reduction in EER over the\nRes2Net50 model.",
    "published": "2023-02-17T01:04:51Z",
    "updated": "2023-02-28T23:32:08Z",
    "authors": [
      "Mufan Sang",
      "Yong Zhao",
      "Gang Liu",
      "John H. L. Hansen",
      "Jian Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.19129v1",
    "title": "Key-Value Transformer",
    "summary": "Transformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture.",
    "published": "2023-05-28T20:26:06Z",
    "updated": "2023-05-28T20:26:06Z",
    "authors": [
      "Ali Borji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.00297v2",
    "title": "Transformers learn to implement preconditioned gradient descent for\n  in-context learning",
    "summary": "Several recent works demonstrate that transformers can implement algorithms\nlike gradient descent. By a careful construction of weights, these works show\nthat multiple layers of transformers are expressive enough to simulate\niterations of gradient descent. Going beyond the question of expressivity, we\nask: Can transformers learn to implement such algorithms by training over\nrandom problem instances? To our knowledge, we make the first theoretical\nprogress on this question via an analysis of the loss landscape for linear\ntransformers trained over random instances of linear regression. For a single\nattention layer, we prove the global minimum of the training objective\nimplements a single iteration of preconditioned gradient descent. Notably, the\npreconditioning matrix not only adapts to the input distribution but also to\nthe variance induced by data inadequacy. For a transformer with $L$ attention\nlayers, we prove certain critical points of the training objective implement\n$L$ iterations of preconditioned gradient descent. Our results call for future\ntheoretical studies on learning algorithms by training transformers.",
    "published": "2023-06-01T02:35:57Z",
    "updated": "2023-11-09T21:46:18Z",
    "authors": [
      "Kwangjun Ahn",
      "Xiang Cheng",
      "Hadi Daneshmand",
      "Suvrit Sra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.05688v1",
    "title": "ModeT: Learning Deformable Image Registration via Motion Decomposition\n  Transformer",
    "summary": "The Transformer structures have been widely used in computer vision and have\nrecently made an impact in the area of medical image registration. However, the\nuse of Transformer in most registration networks is straightforward. These\nnetworks often merely use the attention mechanism to boost the feature learning\nas the segmentation networks do, but do not sufficiently design to be adapted\nfor the registration task. In this paper, we propose a novel motion\ndecomposition Transformer (ModeT) to explicitly model multiple motion\nmodalities by fully exploiting the intrinsic capability of the Transformer\nstructure for deformation estimation. The proposed ModeT naturally transforms\nthe multi-head neighborhood attention relationship into the multi-coordinate\nrelationship to model multiple motion modes. Then the competitive weighting\nmodule (CWM) fuses multiple deformation sub-fields to generate the resulting\ndeformation field. Extensive experiments on two public brain magnetic resonance\nimaging (MRI) datasets show that our method outperforms current\nstate-of-the-art registration networks and Transformers, demonstrating the\npotential of our ModeT for the challenging non-rigid deformation estimation\nproblem. The benchmarks and our code are publicly available at\nhttps://github.com/ZAX130/SmileCode.",
    "published": "2023-06-09T06:00:05Z",
    "updated": "2023-06-09T06:00:05Z",
    "authors": [
      "Haiqiao Wang",
      "Dong Ni",
      "Yi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.00751v1",
    "title": "Mitigating Over-smoothing in Transformers via Regularized Nonlocal\n  Functionals",
    "summary": "Transformers have achieved remarkable success in a wide range of natural\nlanguage processing and computer vision applications. However, the\nrepresentation capacity of a deep transformer model is degraded due to the\nover-smoothing issue in which the token representations become identical when\nthe model's depth grows. In this work, we show that self-attention layers in\ntransformers minimize a functional which promotes smoothness, thereby causing\ntoken uniformity. We then propose a novel regularizer that penalizes the norm\nof the difference between the smooth output tokens from self-attention and the\ninput tokens to preserve the fidelity of the tokens. Minimizing the resulting\nregularized energy functional, we derive the Neural Transformer with a\nRegularized Nonlocal Functional (NeuTRENO), a novel class of transformer models\nthat can mitigate the over-smoothing issue. We empirically demonstrate the\nadvantages of NeuTRENO over the baseline transformers and state-of-the-art\nmethods in reducing the over-smoothing of token representations on various\npractical tasks, including object classification, image segmentation, and\nlanguage modeling.",
    "published": "2023-12-01T17:52:47Z",
    "updated": "2023-12-01T17:52:47Z",
    "authors": [
      "Tam Nguyen",
      "Tan M. Nguyen",
      "Richard G. Baraniuk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.03183v1",
    "title": "How Well Can Transformers Emulate In-context Newton's Method?",
    "summary": "Transformer-based models have demonstrated remarkable in-context learning\ncapabilities, prompting extensive research into its underlying mechanisms.\nRecent studies have suggested that Transformers can implement first-order\noptimization algorithms for in-context learning and even second order ones for\nthe case of linear regression. In this work, we study whether Transformers can\nperform higher order optimization methods, beyond the case of linear\nregression. We establish that linear attention Transformers with ReLU layers\ncan approximate second order optimization algorithms for the task of logistic\nregression and achieve $\\epsilon$ error with only a logarithmic to the error\nmore layers. As a by-product we demonstrate the ability of even linear\nattention-only Transformers in implementing a single step of Newton's iteration\nfor matrix inversion with merely two layers. These results suggest the ability\nof the Transformer architecture to implement complex algorithms, beyond\ngradient descent.",
    "published": "2024-03-05T18:20:10Z",
    "updated": "2024-03-05T18:20:10Z",
    "authors": [
      "Angeliki Giannou",
      "Liu Yang",
      "Tianhao Wang",
      "Dimitris Papailiopoulos",
      "Jason D. Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.04543v1",
    "title": "Strengthening Structural Inductive Biases by Pre-training to Perform\n  Syntactic Transformations",
    "summary": "Models need appropriate inductive biases to effectively learn from small\namounts of data and generalize systematically outside of the training\ndistribution. While Transformers are highly versatile and powerful, they can\nstill benefit from enhanced structural inductive biases for seq2seq tasks,\nespecially those involving syntactic transformations, such as converting active\nto passive voice or semantic parsing. In this paper, we propose to strengthen\nthe structural inductive bias of a Transformer by intermediate pre-training to\nperform synthetically generated syntactic transformations of dependency trees\ngiven a description of the transformation. Our experiments confirm that this\nhelps with few-shot learning of syntactic tasks such as chunking, and also\nimproves structural generalization for semantic parsing. Our analysis shows\nthat the intermediate pre-training leads to attention heads that keep track of\nwhich syntactic transformation needs to be applied to which token, and that the\nmodel can leverage these attention heads on downstream tasks.",
    "published": "2024-07-05T14:29:44Z",
    "updated": "2024-07-05T14:29:44Z",
    "authors": [
      "Matthias Lindemann",
      "Alexander Koller",
      "Ivan Titov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.10483v1",
    "title": "PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series\n  Forecasting",
    "summary": "The self-attention mechanism in Transformer architecture, invariant to\nsequence order, necessitates positional embeddings to encode temporal order in\ntime series prediction. We argue that this reliance on positional embeddings\nrestricts the Transformer's ability to effectively represent temporal\nsequences, particularly when employing longer lookback windows. To address\nthis, we introduce an innovative approach that combines Pyramid RNN\nembeddings(PRE) for univariate time series with the Transformer's capability to\nmodel multivariate dependencies. PRE, utilizing pyramidal one-dimensional\nconvolutional layers, constructs multiscale convolutional features that\npreserve temporal order. Additionally, RNNs, layered atop these features, learn\nmultiscale time series representations sensitive to sequence order. This\nintegration into Transformer models with attention mechanisms results in\nsignificant performance enhancements. We present the PRformer, a model\nintegrating PRE with a standard Transformer encoder, demonstrating\nstate-of-the-art performance on various real-world datasets. This performance\nhighlights the effectiveness of our approach in leveraging longer lookback\nwindows and underscores the critical role of robust temporal representations in\nmaximizing Transformer's potential for prediction tasks. Code is available at\nthis repository: \\url{https://github.com/usualheart/PRformer}.",
    "published": "2024-08-20T01:56:07Z",
    "updated": "2024-08-20T01:56:07Z",
    "authors": [
      "Yongbo Yu",
      "Weizhong Yu",
      "Feiping Nie",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.08148v1",
    "title": "A Review of Intelligent Device Fault Diagnosis Technologies Based on\n  Machine Vision",
    "summary": "This paper provides a comprehensive review of mechanical equipment fault\ndiagnosis methods, focusing on the advancements brought by Transformer-based\nmodels. It details the structure, working principles, and benefits of\nTransformers, particularly their self-attention mechanism and parallel\ncomputation capabilities, which have propelled their widespread application in\nnatural language processing and computer vision. The discussion highlights key\nTransformer model variants, such as Vision Transformers (ViT) and their\nextensions, which leverage self-attention to improve accuracy and efficiency in\nvisual tasks. Furthermore, the paper examines the application of\nTransformer-based approaches in intelligent fault diagnosis for mechanical\nsystems, showcasing their superior ability to extract and recognize patterns\nfrom complex sensor data for precise fault identification. Despite these\nadvancements, challenges remain, including the reliance on extensive labeled\ndatasets, significant computational demands, and difficulties in deploying\nmodels on resource-limited devices. To address these limitations, the paper\nproposes future research directions, such as developing lightweight Transformer\narchitectures, integrating multimodal data sources, and enhancing adaptability\nto diverse operational conditions. These efforts aim to further expand the\napplication of Transformer-based methods in mechanical fault diagnosis, making\nthem more robust, efficient, and suitable for real-world industrial\nenvironments.",
    "published": "2024-12-11T07:06:53Z",
    "updated": "2024-12-11T07:06:53Z",
    "authors": [
      "Guiran Liu",
      "Binrong Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.21686v2",
    "title": "Molecular Quantum Transformer",
    "summary": "The Transformer model, renowned for its powerful attention mechanism, has\nachieved state-of-the-art performance in various artificial intelligence tasks\nbut faces challenges such as high computational cost and memory usage.\nResearchers are exploring quantum computing to enhance the Transformer's\ndesign, though it still shows limited success with classical data. With a\ngrowing focus on leveraging quantum machine learning for quantum data,\nparticularly in quantum chemistry, we propose the Molecular Quantum Transformer\n(MQT) for modeling interactions in molecular quantum systems. By utilizing\nquantum circuits to implement the attention mechanism on the molecular\nconfigurations, MQT can efficiently calculate ground-state energies for all\nconfigurations. Numerical demonstrations show that in calculating ground-state\nenergies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer,\nhighlighting the promise of quantum effects in Transformer structures.\nFurthermore, its pretraining capability on diverse molecular data facilitates\nthe efficient learning of new molecules, extending its applicability to complex\nmolecular systems with minimal additional effort. Our method offers an\nalternative to existing quantum algorithms for estimating ground-state\nenergies, opening new avenues in quantum chemistry and materials science.",
    "published": "2025-03-27T16:54:15Z",
    "updated": "2025-05-16T02:38:13Z",
    "authors": [
      "Yuichi Kamata",
      "Quoc Hoan Tran",
      "Yasuhiro Endo",
      "Hirotaka Oshima"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10425v1",
    "title": "Softmax $\\geq$ Linear: Transformers may learn to classify in-context by\n  kernel gradient descent",
    "summary": "The remarkable ability of transformers to learn new concepts solely by\nreading examples within the input prompt, termed in-context learning (ICL), is\na crucial aspect of intelligent behavior. Here, we focus on understanding the\nlearning algorithm transformers use to learn from context. Existing theoretical\nwork, often based on simplifying assumptions, has primarily focused on linear\nself-attention and continuous regression tasks, finding transformers can learn\nin-context by gradient descent. Given that transformers are typically trained\non discrete and complex tasks, we bridge the gap from this existing work to the\nsetting of classification, with non-linear (importantly, softmax) activation.\nWe find that transformers still learn to do gradient descent in-context, though\non functionals in the kernel feature space and with a context-adaptive learning\nrate in the case of softmax transformer. These theoretical findings suggest a\ngreater adaptability to context for softmax attention, which we empirically\nverify and study through ablations. Overall, we hope this enhances theoretical\nunderstanding of in-context learning algorithms in more realistic settings,\npushes forward our intuitions and enables further theory bridging to larger\nmodels.",
    "published": "2025-10-12T03:20:27Z",
    "updated": "2025-10-12T03:20:27Z",
    "authors": [
      "Sara DragutinoviÄ",
      "Andrew M. Saxe",
      "Aaditya K. Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.00182v2",
    "title": "Differentiable Soft-Masked Attention",
    "summary": "Transformers have become prevalent in computer vision due to their\nperformance and flexibility in modelling complex operations. Of particular\nsignificance is the 'cross-attention' operation, which allows a vector\nrepresentation (e.g. of an object in an image) to be learned by attending to an\narbitrarily sized set of input features. Recently, \"Masked Attention\" was\nproposed in which a given object representation only attends to those image\npixel features for which the segmentation mask of that object is active. This\nspecialization of attention proved beneficial for various image and video\nsegmentation tasks. In this paper, we propose another specialization of\nattention which enables attending over `soft-masks' (those with continuous mask\nprobabilities instead of binary values), and is also differentiable through\nthese mask probabilities, thus allowing the mask used for attention to be\nlearned within the network without requiring direct loss supervision. This can\nbe useful for several applications. Specifically, we employ our \"Differentiable\nSoft-Masked Attention\" for the task of Weakly-Supervised Video Object\nSegmentation (VOS), where we develop a transformer-based network for VOS which\nonly requires a single annotated image frame for training, but can also benefit\nfrom cycle consistency training on a video with just one annotated frame.\nAlthough there is no loss for masks in unlabeled frames, the network is still\nable to segment objects in those frames due to our novel attention formulation.\nCode:\nhttps://github.com/Ali2500/HODOR/blob/main/hodor/modelling/encoder/soft_masked_attention.py",
    "published": "2022-06-01T02:05:13Z",
    "updated": "2022-08-05T14:09:12Z",
    "authors": [
      "Ali Athar",
      "Jonathon Luiten",
      "Alexander Hermans",
      "Deva Ramanan",
      "Bastian Leibe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.06899v1",
    "title": "Memory-efficient Transformers via Top-$k$ Attention",
    "summary": "Following the success of dot-product attention in Transformers, numerous\napproximations have been recently proposed to address its quadratic complexity\nwith respect to the input length. While these variants are memory and compute\nefficient, it is not possible to directly use them with popular pre-trained\nlanguage models trained using vanilla attention, without an expensive\ncorrective pre-training stage. In this work, we propose a simple yet highly\naccurate approximation for vanilla attention. We process the queries in chunks,\nand for each query, compute the top-$k$ scores with respect to the keys. Our\napproach offers several advantages: (a) its memory usage is linear in the input\nsize, similar to linear attention variants, such as Performer and RFA (b) it is\na drop-in replacement for vanilla attention that does not require any\ncorrective pre-training, and (c) it can also lead to significant memory savings\nin the feed-forward layers after casting them into the familiar query-key-value\nframework. We evaluate the quality of top-$k$ approximation for multi-head\nattention layers on the Long Range Arena Benchmark, and for feed-forward layers\nof T5 and UnifiedQA on multiple QA datasets. We show our approach leads to\naccuracy that is nearly-identical to vanilla attention in multiple setups\nincluding training from scratch, fine-tuning, and zero-shot inference.",
    "published": "2021-06-13T02:30:23Z",
    "updated": "2021-06-13T02:30:23Z",
    "authors": [
      "Ankit Gupta",
      "Guy Dar",
      "Shaya Goodman",
      "David Ciprut",
      "Jonathan Berant"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.10637v3",
    "title": "More than Encoder: Introducing Transformer Decoder to Upsample",
    "summary": "Medical image segmentation methods downsample images for feature extraction\nand then upsample them to restore resolution for pixel-level predictions. In\nsuch a schema, upsample technique is vital in restoring information for better\nperformance. However, existing upsample techniques leverage little information\nfrom downsampling paths. The local and detailed feature from the shallower\nlayer such as boundary and tissue texture is particularly more important in\nmedical segmentation compared with natural image segmentation. To this end, we\npropose a novel upsample approach for medical image segmentation, Window\nAttention Upsample (WAU), which upsamples features conditioned on local and\ndetailed features from downsampling path in local windows by introducing\nattention decoders of Transformer. WAU could serve as a general upsample method\nand be incorporated into any segmentation model that possesses lateral\nconnections. We first propose the Attention Upsample which consists of\nAttention Decoder (AD) and bilinear upsample. AD leverages pixel-level\nattention to model long-range dependency and global information for a better\nupsample. Bilinear upsample is introduced as the residual connection to\ncomplement the upsampled features. Moreover, considering the extensive memory\nand computation cost of pixel-level attention, we further design a window\nattention scheme to restrict attention computation in local windows instead of\nthe global range. We evaluate our method (WAU) on classic U-Net structure with\nlateral connections and achieve state-of-the-art performance on Synapse\nmulti-organ segmentation, Medical Segmentation Decathlon (MSD) Brain, and\nAutomatic Cardiac Diagnosis Challenge (ACDC) datasets. We also validate the\neffectiveness of our method on multiple classic architectures and achieve\nconsistent improvement.",
    "published": "2021-06-20T06:58:58Z",
    "updated": "2022-11-24T08:05:25Z",
    "authors": [
      "Yijiang Li",
      "Wentian Cai",
      "Ying Gao",
      "Chengming Li",
      "Xiping Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.05980v1",
    "title": "Neural Attention Forests: Transformer-Based Forest Improvement",
    "summary": "A new approach called NAF (the Neural Attention Forest) for solving\nregression and classification tasks under tabular training data is proposed.\nThe main idea behind the proposed NAF model is to introduce the attention\nmechanism into the random forest by assigning attention weights calculated by\nneural networks of a specific form to data in leaves of decision trees and to\nthe random forest itself in the framework of the Nadaraya-Watson kernel\nregression. In contrast to the available models like the attention-based random\nforest, the attention weights and the Nadaraya-Watson regression are\nrepresented in the form of neural networks whose weights can be regarded as\ntrainable parameters. The first part of neural networks with shared weights is\ntrained for all trees and computes attention weights of data in leaves. The\nsecond part aggregates outputs of the tree networks and aims to minimize the\ndifference between the random forest prediction and the truth target value from\na training set. The neural network is trained in an end-to-end manner. The\ncombination of the random forest and neural networks implementing the attention\nmechanism forms a transformer for enhancing the forest predictions. Numerical\nexperiments with real datasets illustrate the proposed method. The code\nimplementing the approach is publicly available.",
    "published": "2023-04-12T17:01:38Z",
    "updated": "2023-04-12T17:01:38Z",
    "authors": [
      "Andrei V. Konstantinov",
      "Lev V. Utkin",
      "Alexey A. Lukashin",
      "Vladimir A. Muliukha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.10443v3",
    "title": "Integrating a Heterogeneous Graph with Entity-aware Self-attention using\n  Relative Position Labels for Reading Comprehension Model",
    "summary": "Despite the significant progress made by transformer models in machine\nreading comprehension tasks, they still fall short in handling complex\nreasoning tasks due to the absence of explicit knowledge in the input sequence.\nTo address this limitation, many recent works have proposed injecting external\nknowledge into the model. However, selecting relevant external knowledge,\nensuring its availability, and requiring additional processing steps remain\nchallenging. In this paper, we introduce a novel attention pattern that\nintegrates reasoning knowledge derived from a heterogeneous graph into the\ntransformer architecture without relying on external knowledge. The proposed\nattention pattern comprises three key elements: global-local attention for word\ntokens, graph attention for entity tokens that exhibit strong attention towards\ntokens connected in the graph as opposed to those unconnected, and the\nconsideration of the type of relationship between each entity token and word\ntoken. This results in optimized attention between the two if a relationship\nexists. The pattern is coupled with special relative position labels, allowing\nit to integrate with LUKE's entity-aware self-attention mechanism. The\nexperimental findings corroborate that our model outperforms both the\ncutting-edge LUKE-Graph and the baseline LUKE model across two distinct\ndatasets: ReCoRD, emphasizing commonsense reasoning, and WikiHop, focusing on\nmulti-hop reasoning challenges.",
    "published": "2023-07-19T20:17:37Z",
    "updated": "2024-01-13T15:24:59Z",
    "authors": [
      "Shima Foolad",
      "Kourosh Kiani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.18333v1",
    "title": "Continuous Sign Language Recognition Using Intra-inter Gloss Attention",
    "summary": "Many continuous sign language recognition (CSLR) studies adopt\ntransformer-based architectures for sequence modeling due to their powerful\ncapacity for capturing global contexts. Nevertheless, vanilla self-attention,\nwhich serves as the core module of the transformer, calculates a weighted\naverage over all time steps; therefore, the local temporal semantics of sign\nvideos may not be fully exploited. In this study, we introduce a novel module\nin sign language recognition studies, called intra-inter gloss attention\nmodule, to leverage the relationships among frames within glosses and the\nsemantic and grammatical dependencies between glosses in the video. In the\nintra-gloss attention module, the video is divided into equally sized chunks\nand a self-attention mechanism is applied within each chunk. This localized\nself-attention significantly reduces complexity and eliminates noise introduced\nby considering non-relative frames. In the inter-gloss attention module, we\nfirst aggregate the chunk-level features within each gloss chunk by average\npooling along the temporal dimension. Subsequently, multi-head self-attention\nis applied to all chunk-level features. Given the non-significance of the\nsigner-environment interaction, we utilize segmentation to remove the\nbackground of the videos. This enables the proposed model to direct its focus\ntoward the signer. Experimental results on the PHOENIX-2014 benchmark dataset\ndemonstrate that our method can effectively extract sign language features in\nan end-to-end manner without any prior knowledge, improve the accuracy of CSLR,\nand achieve the word error rate (WER) of 20.4 on the test set which is a\ncompetitive result compare to the state-of-the-art which uses additional\nsupervisions.",
    "published": "2024-06-26T13:21:08Z",
    "updated": "2024-06-26T13:21:08Z",
    "authors": [
      "Hossein Ranjbar",
      "Alireza Taheri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.01542v1",
    "title": "FactorizePhys: Matrix Factorization for Multidimensional Attention in\n  Remote Physiological Sensing",
    "summary": "Remote photoplethysmography (rPPG) enables non-invasive extraction of blood\nvolume pulse signals through imaging, transforming spatial-temporal data into\ntime series signals. Advances in end-to-end rPPG approaches have focused on\nthis transformation where attention mechanisms are crucial for feature\nextraction. However, existing methods compute attention disjointly across\nspatial, temporal, and channel dimensions. Here, we propose the Factorized\nSelf-Attention Module (FSAM), which jointly computes multidimensional attention\nfrom voxel embeddings using nonnegative matrix factorization. To demonstrate\nFSAM's effectiveness, we developed FactorizePhys, an end-to-end 3D-CNN\narchitecture for estimating blood volume pulse signals from raw video frames.\nOur approach adeptly factorizes voxel embeddings to achieve comprehensive\nspatial, temporal, and channel attention, enhancing performance of generic\nsignal extraction tasks. Furthermore, we deploy FSAM within an existing\n2D-CNN-based rPPG architecture to illustrate its versatility. FSAM and\nFactorizePhys are thoroughly evaluated against state-of-the-art rPPG methods,\neach representing different types of architecture and attention mechanism. We\nperform ablation studies to investigate the architectural decisions and\nhyperparameters of FSAM. Experiments on four publicly available datasets and\nintuitive visualization of learned spatial-temporal features substantiate the\neffectiveness of FSAM and enhanced cross-dataset generalization in estimating\nrPPG signals, suggesting its broader potential as a multidimensional attention\nmechanism. The code is accessible at\nhttps://github.com/PhysiologicAILab/FactorizePhys.",
    "published": "2024-11-03T12:22:58Z",
    "updated": "2024-11-03T12:22:58Z",
    "authors": [
      "Jitesh Joshi",
      "Sos S. Agaian",
      "Youngjun Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.16112v1",
    "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers\n  Up",
    "summary": "Diffusion Transformers (DiT) have become a leading architecture in image\ngeneration. However, the quadratic complexity of attention mechanisms, which\nare responsible for modeling token-wise relationships, results in significant\nlatency when generating high-resolution images. To address this issue, we aim\nat a linear attention mechanism in this paper that reduces the complexity of\npre-trained DiTs to linear. We begin our exploration with a comprehensive\nsummary of existing efficient attention mechanisms and identify four key\nfactors crucial for successful linearization of pre-trained DiTs: locality,\nformulation consistency, high-rank attention maps, and feature integrity. Based\non these insights, we introduce a convolution-like local attention strategy\ntermed CLEAR, which limits feature interactions to a local window around each\nquery token, and thus achieves linear complexity. Our experiments indicate\nthat, by fine-tuning the attention layer on merely 10K self-generated samples\nfor 10K iterations, we can effectively transfer knowledge from a pre-trained\nDiT to a student model with linear complexity, yielding results comparable to\nthe teacher model. Simultaneously, it reduces attention computations by 99.5%\nand accelerates generation by 6.3 times for generating 8K-resolution images.\nFurthermore, we investigate favorable properties in the distilled attention\nlayers, such as zero-shot generalization cross various models and plugins, and\nimproved support for multi-GPU parallel inference. Models and codes are\navailable here: https://github.com/Huage001/CLEAR.",
    "published": "2024-12-20T17:57:09Z",
    "updated": "2024-12-20T17:57:09Z",
    "authors": [
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.02931v2",
    "title": "Self-Attention as a Parametric Endofunctor: A Categorical Framework for\n  Transformer Architectures",
    "summary": "Self-attention mechanisms have revolutionised deep learning architectures,\nyet their core mathematical structures remain incompletely understood. In this\nwork, we develop a category-theoretic framework focusing on the linear\ncomponents of self-attention. Specifically, we show that the query, key, and\nvalue maps naturally define a parametric 1-morphism in the 2-category\n$\\mathbf{Para(Vect)}$. On the underlying 1-category $\\mathbf{Vect}$, these maps\ninduce an endofunctor whose iterated composition precisely models multi-layer\nattention. We further prove that stacking multiple self-attention layers\ncorresponds to constructing the free monad on this endofunctor. For positional\nencodings, we demonstrate that strictly additive embeddings correspond to\nmonoid actions in an affine sense, while standard sinusoidal encodings, though\nnot additive, retain a universal property among injective (faithful)\nposition-preserving maps. We also establish that the linear portions of\nself-attention exhibit natural equivariance to permutations of input tokens,\nand show how the \"circuits\" identified in mechanistic interpretability can be\ninterpreted as compositions of parametric 1-morphisms. This categorical\nperspective unifies geometric, algebraic, and interpretability-based approaches\nto transformer analysis, making explicit the underlying structures of\nattention. We restrict to linear maps throughout, deferring the treatment of\nnonlinearities such as softmax and layer normalisation, which require more\nadvanced categorical constructions. Our results build on and extend recent work\non category-theoretic foundations for deep learning, offering deeper insights\ninto the algebraic structure of attention mechanisms.",
    "published": "2025-01-06T11:14:18Z",
    "updated": "2025-01-14T10:01:41Z",
    "authors": [
      "Charles O'Neill"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.06098v4",
    "title": "ELFATT: Efficient Linear Fast Attention for Vision Transformers",
    "summary": "The attention mechanism is the key to the success of transformers in\ndifferent machine learning tasks. However, the quadratic complexity with\nrespect to the sequence length of the vanilla softmax-based attention mechanism\nbecomes the major bottleneck for the application of long sequence tasks, such\nas vision tasks. Although various efficient linear attention mechanisms have\nbeen proposed, they need to sacrifice performance to achieve high efficiency.\nWhat's more, memory-efficient methods, such as FlashAttention-1-3, still have\nquadratic computation complexity which can be further improved. In this paper,\nwe propose a novel efficient linear fast attention (ELFATT) mechanism to\nachieve low memory input/output operations, linear computational complexity,\nand high performance at the same time. ELFATT offers 4-7x speedups over the\nvanilla softmax-based attention mechanism in high-resolution vision tasks\nwithout losing performance. ELFATT is FlashAttention friendly. Using\nFlashAttention-2 acceleration, ELFATT still offers 2-3x speedups over the\nvanilla softmax-based attention mechanism on high-resolution vision tasks\nwithout losing performance. Even in some non-vision tasks of long-range arena,\nELFATT still achieves leading performance and offers 1.2-2.3x speedups over\nFlashAttention-2. Even on edge GPUs, ELFATT still offers 1.6x to 2.0x speedups\ncompared to state-of-the-art attention mechanisms in various power modes from\n5W to 60W. Furthermore, ELFATT can be used to enhance and accelerate diffusion\ntasks directly without training.",
    "published": "2025-01-10T16:51:19Z",
    "updated": "2025-08-02T04:35:35Z",
    "authors": [
      "Chong Wu",
      "Maolin Che",
      "Renjie Xu",
      "Zhuoheng Ran",
      "Hong Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.06425v5",
    "title": "Tensor Product Attention Is All You Need",
    "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
    "published": "2025-01-11T03:37:10Z",
    "updated": "2025-10-23T23:35:32Z",
    "authors": [
      "Yifan Zhang",
      "Yifeng Liu",
      "Huizhuo Yuan",
      "Zhen Qin",
      "Yang Yuan",
      "Quanquan Gu",
      "Andrew Chi-Chih Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.16428v1",
    "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
    "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
    "published": "2025-03-20T17:59:58Z",
    "updated": "2025-03-20T17:59:58Z",
    "authors": [
      "Ruyi Xu",
      "Guangxuan Xiao",
      "Haofeng Huang",
      "Junxian Guo",
      "Song Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15541v1",
    "title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and\n  Network Sparsity",
    "summary": "We examine the intrinsic (within the attention head) and extrinsic (amongst\nthe attention heads) structure of the self-attention mechanism in transformers.\nTheoretical evidence for invariance of the self-attention mechanism to softmax\nactivation is obtained by appealing to paradifferential calculus, (and is\nsupported by computational examples), which relies on the intrinsic\norganization of the attention heads. Furthermore, we use an existing\nmethodology for hierarchical organization of tensors to examine network\nstructure by constructing hierarchal partition trees with respect to the query,\nkey, and head axes of network 3-tensors. Such an organization is consequential\nsince it allows one to profitably execute common signal processing tasks on a\ngeometry where the organized network 3-tensors exhibit regularity. We exemplify\nthis qualitatively, by visualizing the hierarchical organization of the tree\ncomprised of attention heads and the diffusion map embeddings, and\nquantitatively by investigating network sparsity with the expansion\ncoefficients of individual attention heads and the entire network with respect\nto the bi and tri-haar bases (respectively) on the space of queries, keys, and\nheads of the network. To showcase the utility of our theoretical and\nmethodological findings, we provide computational examples using vision and\nlanguage transformers. The ramifications of these findings are two-fold: (1) a\nsubsequent step in interpretability analysis is theoretically admitted, and can\nbe exploited empirically for downstream interpretability tasks (2) one can use\nthe network 3-tensor organization for empirical network applications such as\nmodel pruning (by virtue of network sparsity) and network architecture\ncomparison.",
    "published": "2025-06-18T15:14:56Z",
    "updated": "2025-06-18T15:14:56Z",
    "authors": [
      "Oluwadamilola Fasina",
      "Ruben V. C. Pohle",
      "Pei-Chun Su",
      "Ronald R. Coifman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02957v1",
    "title": "CS-VLM: Compressed Sensing Attention for Efficient Vision-Language\n  Representation Learning",
    "summary": "Vision-Language Models (vLLMs) have emerged as powerful architectures for\njoint reasoning over visual and textual inputs, enabling breakthroughs in image\ncaptioning, cross modal retrieval, and multimodal dialogue. However, as these\nmodels scale to longer video sequences and richer language descriptions, the\nquadratic complexity of the standard attention mechanism presents a fundamental\ncomputational bottleneck. This challenge is exacerbated in vLLMs, where\nattention must be computed not only within modalities but also across them,\nleading to prohibitive memory and latency costs. In this work, we introduce the\nCompressed Sensing Attention Transformer (CSAT), a novel architecture that\nreimagines attention computation through the lens of compressed sensing. By\nprojecting high dimensional key and value representations into a\nlower-dimensional subspace via random measurement matrices and reconstructing\nthe attention outputs using sparse recovery algorithms, CSAT significantly\nreduces attention complexity while maintaining semantic fidelity. Applied to\nvLLMs, CSAT exploits the inherent compressibility of both visual and textual\nrepresentations especially evident in video, where temporal redundancy is high,\nand in language, where cross-modal grounding is often sparse. In contrast to\nLLMs, which must often model entangled symbolic dependencies, vLLMs benefit\nfrom structured sparsity in alignment and scene composition, making them\nparticularly well-suited to compressed attention. We provide a formal\nmathematical treatment of CSAT, demonstrate its integration into vision\nlanguage pipelines, and validate its performance on standard benchmarks,\nhighlighting its promise as a scalable, interpretable, and resource efficient\nsolution for next generation multimodal transformers.",
    "published": "2025-06-30T02:11:20Z",
    "updated": "2025-06-30T02:11:20Z",
    "authors": [
      "Andrew Kiruluta",
      "Preethi Raju",
      "Priscilla Burity"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.19807v1",
    "title": "DS-Det: Single-Query Paradigm and Attention Disentangled Learning for\n  Flexible Object Detection",
    "summary": "Popular transformer detectors have achieved promising performance through\nquery-based learning using attention mechanisms. However, the roles of existing\ndecoder query types (e.g., content query and positional query) are still\nunderexplored. These queries are generally predefined with a fixed number\n(fixed-query), which limits their flexibility. We find that the learning of\nthese fixed-query is impaired by Recurrent Opposing inTeractions (ROT) between\ntwo attention operations: Self-Attention (query-to-query) and Cross-Attention\n(query-to-encoder), thereby degrading decoder efficiency. Furthermore, \"query\nambiguity\" arises when shared-weight decoder layers are processed with both\none-to-one and one-to-many label assignments during training, violating DETR's\none-to-one matching principle. To address these challenges, we propose DS-Det,\na more efficient detector capable of detecting a flexible number of objects in\nimages. Specifically, we reformulate and introduce a new unified Single-Query\nparadigm for decoder modeling, transforming the fixed-query into flexible.\nFurthermore, we propose a simplified decoder framework through attention\ndisentangled learning: locating boxes with Cross-Attention (one-to-many\nprocess), deduplicating predictions with Self-Attention (one-to-one process),\naddressing \"query ambiguity\" and \"ROT\" issues directly, and enhancing decoder\nefficiency. We further introduce a unified PoCoo loss that leverages box size\npriors to prioritize query learning on hard samples such as small objects.\nExtensive experiments across five different backbone models on COCO2017 and\nWiderPerson datasets demonstrate the general effectiveness and superiority of\nDS-Det. The source codes are available at\nhttps://github.com/Med-Process/DS-Det/.",
    "published": "2025-07-26T05:40:04Z",
    "updated": "2025-07-26T05:40:04Z",
    "authors": [
      "Guiping Cao",
      "Xiangyuan Lan",
      "Wenjian Huang",
      "Jianguo Zhang",
      "Dongmei Jiang",
      "Yaowei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.17593v1",
    "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
    "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
    "published": "2025-08-25T01:33:18Z",
    "updated": "2025-08-25T01:33:18Z",
    "authors": [
      "Aadesh Deshmukh",
      "Venkata Yaswanth Raparti",
      "Samuel Hsu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1412.8391v1",
    "title": "On The Equivalence Problem for Geometric Structures, I",
    "summary": "We discuss the local and global problems for the equivalence of geometric\nstructures of an arbitrary order and, in later sections, attention is given to\nwhat really matters, namely the equivalence with respect to transformations\nbelonging to a given pseudo-group of transformations. We first give attention\nto general prolongation spaces and thereafter insert the structures in their\nmost appropriate ambient namely, as specific solutions of partial differential\nequations where the equivalence problem is then discussed. In the second part,\nwe discuss applications of all this abstract nonsense and take considerable\nadvantage in exploring \\'Elie Cartan's magical trump called transformations et\nprolongements m\\'eri\\'edriques that somehow seem absent in present day\ngeometry.",
    "published": "2014-12-29T16:57:17Z",
    "updated": "2014-12-29T16:57:17Z",
    "authors": [
      "Antonio Kumpera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.14788v1",
    "title": "Character-Level Translation with Self-attention",
    "summary": "We explore the suitability of self-attention models for character-level\nneural machine translation. We test the standard transformer model, as well as\na novel variant in which the encoder block combines information from nearby\ncharacters using convolutions. We perform extensive experiments on WMT and UN\ndatasets, testing both bilingual and multilingual translation to English using\nup to three input languages (French, Spanish, and Chinese). Our transformer\nvariant consistently outperforms the standard transformer at the\ncharacter-level and converges faster while learning more robust character-level\nalignments.",
    "published": "2020-04-30T14:05:26Z",
    "updated": "2020-04-30T14:05:26Z",
    "authors": [
      "Yingqiang Gao",
      "Nikola I. Nikolov",
      "Yuhuang Hu",
      "Richard H. R. Hahnloser"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.10764v1",
    "title": "Regularizing Transformers With Deep Probabilistic Layers",
    "summary": "Language models (LM) have grown with non-stop in the last decade, from\nsequence-to-sequence architectures to the state-of-the-art and utter\nattention-based Transformers. In this work, we demonstrate how the inclusion of\ndeep generative models within BERT can bring more versatile models, able to\nimpute missing/noisy words with richer text or even improve BLEU score. More\nprecisely, we use a Gaussian Mixture Variational Autoencoder (GMVAE) as a\nregularizer layer and prove its effectiveness not only in Transformers but also\nin the most relevant encoder-decoder based LM, seq2seq with and without\nattention.",
    "published": "2021-08-23T10:17:02Z",
    "updated": "2021-08-23T10:17:02Z",
    "authors": [
      "Aurora Cobo Aguilera",
      "Pablo MartÃ­nez Olmos",
      "Antonio ArtÃ©s-RodrÃ­guez",
      "Fernando PÃ©rez-Cruz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.03014v1",
    "title": "Transformer-based Korean Pretrained Language Models: A Survey on Three\n  Years of Progress",
    "summary": "With the advent of Transformer, which was used in translation models in 2017,\nattention-based architectures began to attract attention. Furthermore, after\nthe emergence of BERT, which strengthened the NLU-specific encoder part, which\nis a part of the Transformer, and the GPT architecture, which strengthened the\nNLG-specific decoder part, various methodologies, data, and models for learning\nthe Pretrained Language Model began to appear. Furthermore, in the past three\nyears, various Pretrained Language Models specialized for Korean have appeared.\nIn this paper, we intend to numerically and qualitatively compare and analyze\nvarious Korean PLMs released to the public.",
    "published": "2021-11-25T16:37:24Z",
    "updated": "2021-11-25T16:37:24Z",
    "authors": [
      "Kichang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.11239v2",
    "title": "Vision Transformer: Vit and its Derivatives",
    "summary": "Transformer, an attention-based encoder-decoder architecture, has not only\nrevolutionized the field of natural language processing (NLP), but has also\ndone some pioneering work in the field of computer vision (CV). Compared to\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\nexcellent modeling capabilities to achieve very good performance on several\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\nself-attention mechanism in natural language processing, where word embeddings\nare replaced with patch embeddings.\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\nwith other fields.",
    "published": "2022-05-12T14:02:39Z",
    "updated": "2022-05-24T14:08:01Z",
    "authors": [
      "Zujun Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.08158v2",
    "title": "Attention Mixtures for Time-Aware Sequential Recommendation",
    "summary": "Transformers emerged as powerful methods for sequential recommendation.\nHowever, existing architectures often overlook the complex dependencies between\nuser preferences and the temporal context. In this short paper, we introduce\nMOJITO, an improved Transformer sequential recommender system that addresses\nthis limitation. MOJITO leverages Gaussian mixtures of attention-based temporal\ncontext and item embedding representations for sequential modeling. Such an\napproach permits to accurately predict which items should be recommended next\nto users depending on past actions and the temporal context. We demonstrate the\nrelevance of our approach, by empirically outperforming existing Transformers\nfor sequential recommendation on several real-world datasets.",
    "published": "2023-04-17T11:11:19Z",
    "updated": "2023-07-03T08:52:37Z",
    "authors": [
      "Viet-Anh Tran",
      "Guillaume Salha-Galvan",
      "Bruno Sguerra",
      "Romain Hennequin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.03796v1",
    "title": "Transformer Working Memory Enables Regular Language Reasoning and\n  Natural Language Length Extrapolation",
    "summary": "Unlike recurrent models, conventional wisdom has it that Transformers cannot\nperfectly model regular languages. Inspired by the notion of working memory, we\npropose a new Transformer variant named RegularGPT. With its novel combination\nof Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT\nconstructs working memory along the depth dimension, thereby enabling efficient\nand successful modeling of regular languages such as PARITY. We further test\nRegularGPT on the task of natural language length extrapolation and\nsurprisingly find that it rediscovers the local windowed attention effect\ndeemed necessary in prior work for length extrapolation.",
    "published": "2023-05-05T18:54:40Z",
    "updated": "2023-05-05T18:54:40Z",
    "authors": [
      "Ta-Chung Chi",
      "Ting-Han Fan",
      "Alexander I. Rudnicky",
      "Peter J. Ramadge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.01610v1",
    "title": "Centered Self-Attention Layers",
    "summary": "The self-attention mechanism in transformers and the message-passing\nmechanism in graph neural networks are repeatedly applied within deep learning\narchitectures. We show that this application inevitably leads to oversmoothing,\ni.e., to similar representations at the deeper layers for different tokens in\ntransformers and different nodes in graph neural networks. Based on our\nanalysis, we present a correction term to the aggregating operator of these\nmechanisms. Empirically, this simple term eliminates much of the oversmoothing\nproblem in visual transformers, obtaining performance in weakly supervised\nsegmentation that surpasses elaborate baseline methods that introduce multiple\nauxiliary networks and training phrases. In graph neural networks, the\ncorrection term enables the training of very deep architectures more\neffectively than many recent solutions to the same problem.",
    "published": "2023-06-02T15:19:08Z",
    "updated": "2023-06-02T15:19:08Z",
    "authors": [
      "Ameen Ali",
      "Tomer Galanti",
      "Lior Wolf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.15415v1",
    "title": "The Impact of LoRA on the Emergence of Clusters in Transformers",
    "summary": "In this paper, we employ the mathematical framework on Transformers developed\nby\n\\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}\nto explore how variations in attention parameters and initial token values\nimpact the structural dynamics of token clusters. Our analysis demonstrates\nthat while the clusters within a modified attention matrix dynamics can exhibit\nsignificant divergence from the original over extended periods, they maintain\nclose similarities over shorter intervals, depending on the parameter\ndifferences. This work contributes to the fine-tuning field through practical\napplications to the LoRA algorithm \\cite{hu2021lora,peft}, enhancing our\nunderstanding of the behavior of LoRA-enhanced Transformer models.",
    "published": "2024-02-23T16:26:01Z",
    "updated": "2024-02-23T16:26:01Z",
    "authors": [
      "Hugo Koubbi",
      "Matthieu Boussard",
      "Louis Hernandez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.16108v3",
    "title": "A Transformer approach for Electricity Price Forecasting",
    "summary": "This paper presents a novel approach to electricity price forecasting (EPF)\nusing a pure Transformer model. As opposed to other alternatives, no other\nrecurrent network is used in combination to the attention mechanism. Hence,\nshowing that the attention layer is enough for capturing the temporal patterns.\nThe paper also provides fair comparison of the models using the open-source EPF\ntoolbox and provide the code to enhance reproducibility and transparency in EPF\nresearch. The results show that the Transformer model outperforms traditional\nmethods, offering a promising solution for reliable and sustainable power\nsystem operation.",
    "published": "2024-03-24T11:52:39Z",
    "updated": "2025-09-10T11:00:11Z",
    "authors": [
      "Oscar Llorente",
      "Jose Portela"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.13636v1",
    "title": "Audio Mamba: Pretrained Audio State Space Model For Audio Tagging",
    "summary": "Audio tagging is an important task of mapping audio samples to their\ncorresponding categories. Recently endeavours that exploit transformer models\nin this field have achieved great success. However, the quadratic\nself-attention cost limits the scaling of audio transformer models and further\nconstrains the development of more universal audio models. In this paper, we\nattempt to solve this problem by proposing Audio Mamba, a self-attention-free\napproach that captures long audio spectrogram dependency with state space\nmodels. Our experimental results on two audio-tagging datasets demonstrate the\nparameter efficiency of Audio Mamba, it achieves comparable results to SOTA\naudio spectrogram transformers with one third parameters.",
    "published": "2024-05-22T13:35:56Z",
    "updated": "2024-05-22T13:35:56Z",
    "authors": [
      "Jiaju Lin",
      "Haoxuan Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.15025v1",
    "title": "SiT: Symmetry-Invariant Transformers for Generalisation in Reinforcement\n  Learning",
    "summary": "An open challenge in reinforcement learning (RL) is the effective deployment\nof a trained policy to new or slightly different situations as well as\nsemantically-similar environments. We introduce Symmetry-Invariant Transformer\n(SiT), a scalable vision transformer (ViT) that leverages both local and global\ndata patterns in a self-supervised manner to improve generalisation. Central to\nour approach is Graph Symmetric Attention, which refines the traditional\nself-attention mechanism to preserve graph symmetries, resulting in invariant\nand equivariant latent representations. We showcase SiT's superior\ngeneralization over ViTs on MiniGrid and Procgen RL benchmarks, and its sample\nefficiency on Atari 100k and CIFAR10.",
    "published": "2024-06-21T10:03:14Z",
    "updated": "2024-06-21T10:03:14Z",
    "authors": [
      "Matthias Weissenbacher",
      "Rishabh Agarwal",
      "Yoshinobu Kawahara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07862v1",
    "title": "Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition",
    "summary": "Existing sequence to sequence models for structured language tasks rely\nheavily on the dot product self attention mechanism, which incurs quadratic\ncomplexity in both computation and memory for input length N. We introduce the\nGraph Wavelet Transformer (GWT), a novel architecture that replaces this\nbottleneck with a learnable, multi scale wavelet transform defined over an\nexplicit graph Laplacian derived from syntactic or semantic parses. Our\nanalysis shows that multi scale spectral decomposition offers an interpretable,\nefficient, and expressive alternative to quadratic self attention for graph\nstructured sequence modeling.",
    "published": "2025-05-09T00:13:23Z",
    "updated": "2025-05-09T00:13:23Z",
    "authors": [
      "Andrew Kiruluta",
      "Eric Lundy",
      "Priscilla Burity"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16550v3",
    "title": "A Free Probabilistic Framework for Analyzing the Transformer-based\n  Language Models",
    "summary": "We present a formal operator-theoretic framework for analyzing\nTransformer-based language models using free probability theory. By modeling\ntoken embeddings and attention mechanisms as self-adjoint operators in a\ntracial \\( W^* \\)-probability space, we reinterpret attention as\nnon-commutative convolution and describe representation propagation via free\nadditive convolution. This leads to a spectral dynamic system interpretation of\ndeep Transformers. We derive entropy-based generalization bounds under freeness\nassumptions and provide insight into positional encoding, spectral evolution,\nand representational complexity. This work offers a principled, though\ntheoretical, perspective on structural dynamics in large language models.",
    "published": "2025-06-19T19:13:02Z",
    "updated": "2025-08-15T18:39:22Z",
    "authors": [
      "Swagatam Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2001.02219v1",
    "title": "DAF-NET: a saliency based weakly supervised method of dual attention\n  fusion for fine-grained image classification",
    "summary": "Fine-grained image classification is a challenging problem, since the\ndifficulty of finding discriminative features. To handle this circumstance,\nbasically, there are two ways to go. One is use attention based method to focus\non informative areas, while the other one aims to find high order between\nfeatures. Further, for attention based method there are two directions,\nactivation based and detection based, which are proved effective by scholars.\nHowever ,rare work focus on fusing two types of attention with high order\nfeature. In this paper, we propose a novel DAF method which fuse two types of\nattention and use them to as PAF(part attention filter) in deep bilinear\ntransformation module to mine the relationship between separate parts of an\nobject. Briefly, our network constructed by a student net who attempt to output\ntwo attention maps and a teacher net uses these two maps as empirical\ninformation to refine the result. The experiment result shows that only student\nnet could get 87.6% accuracy in CUB dataset while cooperating with teacher net\ncould achieve 89.1% accuracy.",
    "published": "2020-01-04T12:59:48Z",
    "updated": "2020-01-04T12:59:48Z",
    "authors": [
      "ZiChao Dong",
      "JiLong Wu",
      "TingTing Ren",
      "Yue Wang",
      "MengYing Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.09028v1",
    "title": "On the Locality of Attention in Direct Speech Translation",
    "summary": "Transformers have achieved state-of-the-art results across multiple NLP\ntasks. However, the self-attention mechanism complexity scales quadratically\nwith the sequence length, creating an obstacle for tasks involving long\nsequences, like in the speech domain. In this paper, we discuss the usefulness\nof self-attention for Direct Speech Translation. First, we analyze the\nlayer-wise token contributions in the self-attention of the encoder, unveiling\nlocal diagonal patterns. To prove that some attention weights are avoidable, we\npropose to substitute the standard self-attention with a local efficient one,\nsetting the amount of context used based on the results of the analysis. With\nthis approach, our model matches the baseline performance, and improves the\nefficiency by skipping the computation of those weights that standard attention\ndiscards.",
    "published": "2022-04-19T17:43:37Z",
    "updated": "2022-04-19T17:43:37Z",
    "authors": [
      "Belen Alastruey",
      "Javier Ferrando",
      "Gerard I. GÃ¡llego",
      "Marta R. Costa-jussÃ "
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.04294v1",
    "title": "Orthogonal Attention: A Cloze-Style Approach to Negation Scope\n  Resolution",
    "summary": "Negation Scope Resolution is an extensively researched problem, which is used\nto locate the words affected by a negation cue in a sentence. Recent works have\nshown that simply finetuning transformer-based architectures yield\nstate-of-the-art results on this task. In this work, we look at Negation Scope\nResolution as a Cloze-Style task, with the sentence as the Context and the cue\nwords as the Query. We also introduce a novel Cloze-Style Attention mechanism\ncalled Orthogonal Attention, which is inspired by Self Attention. First, we\npropose a framework for developing Orthogonal Attention variants, and then\npropose 4 Orthogonal Attention variants: OA-C, OA-CA, OA-EM, and OA-EMB. Using\nthese Orthogonal Attention layers on top of an XLNet backbone, we outperform\nthe finetuned XLNet state-of-the-art for Negation Scope Resolution, achieving\nthe best results to date on all 4 datasets we experiment with: BioScope\nAbstracts, BioScope Full Papers, SFU Review Corpus and the *sem 2012 Dataset\n(Sherlock).",
    "published": "2021-03-07T08:10:33Z",
    "updated": "2021-03-07T08:10:33Z",
    "authors": [
      "Aditya Khandelwal",
      "Vahida Attar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.07523v2",
    "title": "Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis",
    "summary": "Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow\nfiner-grained inferences about sentiment to be drawn from the same text,\ndepending on context. For example, a given text can have different targets\n(e.g., neighborhoods) and different aspects (e.g., price or safety), with\ndifferent sentiment associated with each target-aspect pair. In this paper, we\ninvestigate whether adding context to self-attention models improves\nperformance on (T)ABSA. We propose two variants of Context-Guided BERT\n(CG-BERT) that learn to distribute attention under different contexts. We first\nadapt a context-aware Transformer to produce a CG-BERT that uses context-guided\nsoftmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model\nthat learns a compositional attention that supports subtractive attention. We\ntrain both models with pretrained BERT on two (T)ABSA datasets: SentiHood and\nSemEval-2014 (Task 4). Both models achieve new state-of-the-art results with\nour QACG-BERT model having the best performance. Furthermore, we provide\nanalyses of the impact of context in the our proposed models. Our work provides\nmore evidence for the utility of adding context-dependencies to pretrained\nself-attention-based language models for context-based natural language tasks.",
    "published": "2020-10-15T05:01:20Z",
    "updated": "2020-12-14T18:33:19Z",
    "authors": [
      "Zhengxuan Wu",
      "Desmond C. Ong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.09815v3",
    "title": "LWA-HAND: Lightweight Attention Hand for Interacting Hand Reconstruction",
    "summary": "Recent years have witnessed great success for hand reconstruction in\nreal-time applications such as visual reality and augmented reality while\ninteracting with two-hand reconstruction through efficient transformers is left\nunexplored. In this paper, we propose a method called lightweight attention\nhand (LWA-HAND) to reconstruct hands in low flops from a single RGB image. To\nsolve the occlusion and interaction problem in efficient attention\narchitectures, we propose three mobile attention modules in this paper. The\nfirst module is a lightweight feature attention module that extracts both local\nocclusion representation and global image patch representation in a\ncoarse-to-fine manner. The second module is a cross image and graph bridge\nmodule which fuses image context and hand vertex. The third module is a\nlightweight cross-attention mechanism that uses element-wise operation for the\ncross-attention of two hands in linear complexity. The resulting model achieves\ncomparable performance on the InterHand2.6M benchmark in comparison with the\nstate-of-the-art models. Simultaneously, it reduces the flops to $0.47GFlops$\nwhile the state-of-the-art models have heavy computations between $10GFlops$\nand $20GFlops$.",
    "published": "2022-08-21T06:25:56Z",
    "updated": "2022-08-27T13:06:34Z",
    "authors": [
      "Xinhan Di",
      "Pengqian Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.08114v1",
    "title": "Classifying Scientific Publications with BERT -- Is Self-Attention a\n  Feature Selection Method?",
    "summary": "We investigate the self-attention mechanism of BERT in a fine-tuning scenario\nfor the classification of scientific articles over a taxonomy of research\ndisciplines. We observe how self-attention focuses on words that are highly\nrelated to the domain of the article. Particularly, a small subset of\nvocabulary words tends to receive most of the attention. We compare and\nevaluate the subset of the most attended words with feature selection methods\nnormally used for text classification in order to characterize self-attention\nas a possible feature selection approach. Using ConceptNet as ground truth, we\nalso find that attended words are more related to the research fields of the\narticles. However, conventional feature selection methods are still a better\noption to learn classifiers from scratch. This result suggests that, while\nself-attention identifies domain-relevant terms, the discriminatory information\nin BERT is encoded in the contextualized outputs and the classification layer.\nIt also raises the question whether injecting feature selection methods in the\nself-attention mechanism could further optimize single sequence classification\nusing transformers.",
    "published": "2021-01-20T13:22:26Z",
    "updated": "2021-01-20T13:22:26Z",
    "authors": [
      "Andres Garcia-Silva",
      "Jose Manuel Gomez-Perez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.07012v2",
    "title": "Sparse Attention with Linear Units",
    "summary": "Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.",
    "published": "2021-04-14T17:52:38Z",
    "updated": "2021-10-06T14:04:59Z",
    "authors": [
      "Biao Zhang",
      "Ivan Titov",
      "Rico Sennrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.05799v2",
    "title": "Human Attention during Goal-directed Reading Comprehension Relies on\n  Task Optimization",
    "summary": "The computational principles underlying attention allocation in complex\ngoal-directed tasks remain elusive. Goal-directed reading, i.e., reading a\npassage to answer a question in mind, is a common real-world task that strongly\nengages attention. Here, we investigate what computational models can explain\nattention distribution in this complex task. We show that the reading time on\neach word is predicted by the attention weights in transformer-based deep\nneural networks (DNNs) optimized to perform the same reading task. Eye-tracking\nfurther reveals that readers separately attend to basic text features and\nquestion-relevant information during first-pass reading and rereading,\nrespectively. Similarly, text features and question relevance separately\nmodulate attention weights in shallow and deep DNN layers. Furthermore, when\nreaders scan a passage without a question in mind, their reading time is\npredicted by DNNs optimized for a word prediction task. Therefore, attention\nduring real-world reading can be interpreted as the consequence of task\noptimization.",
    "published": "2021-07-13T01:07:22Z",
    "updated": "2023-04-23T01:50:53Z",
    "authors": [
      "Jiajie Zou",
      "Yuran Zhang",
      "Jialu Li",
      "Xing Tian",
      "Nai Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.10252v3",
    "title": "Similarity and Content-based Phonetic Self Attention for Speech\n  Recognition",
    "summary": "Transformer-based speech recognition models have achieved great success due\nto the self-attention (SA) mechanism that utilizes every frame in the feature\nextraction process. Especially, SA heads in lower layers capture various\nphonetic characteristics by the query-key dot product, which is designed to\ncompute the pairwise relationship between frames. In this paper, we propose a\nvariant of SA to extract more representative phonetic features. The proposed\nphonetic self-attention (phSA) is composed of two different types of phonetic\nattention; one is similarity-based and the other is content-based. In short,\nsimilarity-based attention captures the correlation between frames while\ncontent-based attention only considers each frame without being affected by\nother frames. We identify which parts of the original dot product equation are\nrelated to two different attention patterns and improve each part with simple\nmodifications. Our experiments on phoneme classification and speech recognition\nshow that replacing SA with phSA for lower layers improves the recognition\nperformance without increasing the latency and the parameter size.",
    "published": "2022-03-19T05:35:26Z",
    "updated": "2022-07-12T03:12:55Z",
    "authors": [
      "Kyuhong Shim",
      "Wonyong Sung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.08681v3",
    "title": "U-Former: Improving Monaural Speech Enhancement with Multi-head Self and\n  Cross Attention",
    "summary": "For supervised speech enhancement, contextual information is important for\naccurate spectral mapping. However, commonly used deep neural networks (DNNs)\nare limited in capturing temporal contexts. To leverage long-term contexts for\ntracking a target speaker, this paper treats the speech enhancement as\nsequence-to-sequence mapping, and propose a novel monaural speech enhancement\nU-net structure based on Transformer, dubbed U-Former. The key idea is to model\nlong-term correlations and dependencies, which are crucial for accurate noisy\nspeech modeling, through the multi-head attention mechanisms. For this purpose,\nU-Former incorporates multi-head attention mechanisms at two levels: 1) a\nmulti-head self-attention module which calculate the attention map along both\ntime- and frequency-axis to generate time and frequency sub-attention maps for\nleveraging global interactions between encoder features, while 2) multi-head\ncross-attention module which are inserted in the skip connections allows a fine\nrecovery in the decoder by filtering out uncorrelated features. Experimental\nresults illustrate that the U-Former obtains consistently better performance\nthan recent models of PESQ, STOI, and SSNR scores.",
    "published": "2022-05-18T01:33:10Z",
    "updated": "2022-10-12T09:50:38Z",
    "authors": [
      "Xinmeng Xu",
      "Jianjun Hao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.00561v2",
    "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better\n  Audio Learners",
    "summary": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted\nwith a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates\nthe modelling of local-global interactions in every decoder transformer block\nthrough attention heads of several distinct local and global windows. Empirical\nresults on ten downstream audio tasks show that MW-MAEs consistently outperform\nstandard MAEs in overall performance and learn better general-purpose audio\nrepresentations, along with demonstrating considerably better scaling\ncharacteristics. Investigating attention distances and entropies reveals that\nMW-MAE encoders learn heads with broader local and global attention. Analyzing\nattention head feature representations through Projection Weighted Canonical\nCorrelation Analysis (PWCCA) shows that attention heads with the same window\nsizes across the decoder layers of the MW-MAE learn correlated feature\nrepresentations which enables each block to independently capture local and\nglobal information, leading to a decoupled decoder feature hierarchy. Code for\nfeature extraction and downstream experiments along with pre-trained models\nwill be released publically.",
    "published": "2023-06-01T11:20:59Z",
    "updated": "2023-10-01T21:53:36Z",
    "authors": [
      "Sarthak Yadav",
      "Sergios Theodoridis",
      "Lars Kai Hansen",
      "Zheng-Hua Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.14650v2",
    "title": "PhD Thesis: Exploring the role of (self-)attention in cognitive and\n  computer vision architecture",
    "summary": "We investigate the role of attention and memory in complex reasoning tasks.\nWe analyze Transformer-based self-attention as a model and extend it with\nmemory. By studying a synthetic visual reasoning test, we refine the taxonomy\nof reasoning tasks. Incorporating self-attention with ResNet50, we enhance\nfeature maps using feature-based and spatial attention, achieving efficient\nsolving of challenging visual reasoning tasks. Our findings contribute to\nunderstanding the attentional needs of SVRT tasks. Additionally, we propose\nGAMR, a cognitive architecture combining attention and memory, inspired by\nactive vision theory. GAMR outperforms other architectures in sample\nefficiency, robustness, and compositionality, and shows zero-shot\ngeneralization on new reasoning tasks.",
    "published": "2023-06-26T12:40:12Z",
    "updated": "2023-06-28T08:22:14Z",
    "authors": [
      "Mohit Vaishnav"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.14505v4",
    "title": "Sentiment analysis with adaptive multi-head attention in Transformer",
    "summary": "We propose a novel framework based on the attention mechanism to identify the\nsentiment of a movie review document. Previous efforts on deep neural networks\nwith attention mechanisms focus on encoder and decoder with fixed numbers of\nmulti-head attention. Therefore, we need a mechanism to stop the attention\nprocess automatically if no more useful information can be read from the\nmemory.In this paper, we propose an adaptive multi-head attention architecture\n(AdaptAttn) which varies the number of attention heads based on length of\nsentences. AdaptAttn has a data preprocessing step where each document is\nclassified into any one of the three bins small, medium or large based on\nlength of the sentence. The document classified as small goes through two heads\nin each layer, the medium group passes four heads and the large group is\nprocessed by eight heads. We examine the merit of our model on the Stanford\nlarge movie review dataset. The experimental results show that the F1 score\nfrom our model is on par with the baseline model.",
    "published": "2023-10-23T02:32:30Z",
    "updated": "2024-03-11T04:13:50Z",
    "authors": [
      "Fanfei Meng",
      "Chen-Ao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.15055v2",
    "title": "Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions",
    "summary": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.",
    "published": "2024-02-23T02:15:47Z",
    "updated": "2024-10-23T13:20:15Z",
    "authors": [
      "Clement Neo",
      "Shay B. Cohen",
      "Fazl Barez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.12235v2",
    "title": "Beyond Average: Individualized Visual Scanpath Prediction",
    "summary": "Understanding how attention varies across individuals has significant\nscientific and societal impacts. However, existing visual scanpath models treat\nattention uniformly, neglecting individual differences. To bridge this gap,\nthis paper focuses on individualized scanpath prediction (ISP), a new attention\nmodeling task that aims to accurately predict how different individuals shift\ntheir attention in diverse visual tasks. It proposes an ISP method featuring\nthree novel technical components: (1) an observer encoder to characterize and\nintegrate an observer's unique attention traits, (2) an observer-centric\nfeature integration approach that holistically combines visual features, task\nguidance, and observer-specific characteristics, and (3) an adaptive fixation\nprioritization mechanism that refines scanpath predictions by dynamically\nprioritizing semantic feature maps based on individual observers' attention\ntraits. These novel components allow scanpath models to effectively address the\nattention variations across different observers. Our method is generally\napplicable to different datasets, model architectures, and visual tasks,\noffering a comprehensive tool for transforming general scanpath models into\nindividualized ones. Comprehensive evaluations using value-based and\nranking-based metrics verify the method's effectiveness and generalizability.",
    "published": "2024-04-18T14:51:42Z",
    "updated": "2024-04-19T02:42:24Z",
    "authors": [
      "Xianyu Chen",
      "Ming Jiang",
      "Qi Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.20867v1",
    "title": "Automatic Channel Pruning for Multi-Head Attention",
    "summary": "Despite the strong performance of Transformers, their quadratic computation\ncomplexity presents challenges in applying them to vision tasks. Automatic\npruning is one of effective methods for reducing computation complexity without\nheuristic approaches. However, directly applying it to multi-head attention is\nnot straightforward due to channel misalignment. In this paper, we propose an\nautomatic channel pruning method to take into account the multi-head attention\nmechanism. First, we incorporate channel similarity-based weights into the\npruning indicator to preserve more informative channels in each head. Then, we\nadjust pruning indicator to enforce removal of channels in equal proportions\nacross all heads, preventing the channel misalignment. We also add a reweight\nmodule to compensate for information loss resulting from channel removal, and\nan effective initialization step for pruning indicator based on difference of\nattention between original structure and each channel. Our proposed method can\nbe used to not only original attention, but also linear attention, which is\nmore efficient as linear complexity with respect to the number of tokens. On\nImageNet-1K, applying our pruning method to the FLattenTransformer, which\nincludes both attention mechanisms, shows outperformed accuracy for several\nMACs compared with previous state-of-the-art efficient models and pruned\nmethods. Code will be available soon.",
    "published": "2024-05-31T14:47:20Z",
    "updated": "2024-05-31T14:47:20Z",
    "authors": [
      "Eunho Lee",
      "Youngbae Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.03159v3",
    "title": "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting",
    "summary": "We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism\nequipped with both Autoregressive (AR) and Moving-average (MA) components. It\ncan adapt to various attention mechanisms, enhancing and decoupling their\nability to capture long-range and local temporal patterns in time series data.\nIn this paper, we first demonstrate that, for the time series forecasting (TSF)\ntask, the previously overlooked decoder-only autoregressive Transformer model\ncan achieve results comparable to the best baselines when appropriate\ntokenization and training methods are applied. Moreover, inspired by the ARMA\nmodel from statistics and recent advances in linear attention, we introduce the\nfull ARMA structure into existing autoregressive attention mechanisms. By using\nan indirect MA weight generation method, we incorporate the MA term while\nmaintaining the time complexity and parameter size of the underlying efficient\nattention models. We further explore how indirect parameter generation can\nproduce implicit MA weights that align with the modeling requirements for local\ntemporal impacts. Experimental results show that WAVE attention that\nincorporates the ARMA structure consistently improves the performance of\nvarious AR attentions on TSF tasks, achieving state-of-the-art results.",
    "published": "2024-10-04T05:45:50Z",
    "updated": "2025-02-12T03:55:17Z",
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Yan Sun",
      "Shihao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.04801v1",
    "title": "Improving Image Clustering with Artifacts Attenuation via Inference-Time\n  Attention Engineering",
    "summary": "The goal of this paper is to improve the performance of pretrained Vision\nTransformer (ViT) models, particularly DINOv2, in image clustering task without\nrequiring re-training or fine-tuning. As model size increases, high-norm\nartifacts anomaly appears in the patches of multi-head attention. We observe\nthat this anomaly leads to reduced accuracy in zero-shot image clustering.\nThese artifacts are characterized by disproportionately large values in the\nattention map compared to other patch tokens. To address these artifacts, we\npropose an approach called Inference-Time Attention Engineering (ITAE), which\nmanipulates attention function during inference. Specifically, we identify the\nartifacts by investigating one of the Query-Key-Value (QKV) patches in the\nmulti-head attention and attenuate their corresponding attention values inside\nthe pretrained models. ITAE shows improved clustering accuracy on multiple\ndatasets by exhibiting more expressive features in latent space. Our findings\nhighlight the potential of ITAE as a practical solution for reducing artifacts\nin pretrained ViT models and improving model performance in clustering tasks\nwithout the need for re-training or fine-tuning.",
    "published": "2024-10-07T07:26:10Z",
    "updated": "2024-10-07T07:26:10Z",
    "authors": [
      "Kazumoto Nakamura",
      "Yuji Nozawa",
      "Yu-Chieh Lin",
      "Kengo Nakata",
      "Youyang Ng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.06950v1",
    "title": "Faithful Interpretation for Graph Neural Networks",
    "summary": "Currently, attention mechanisms have garnered increasing attention in Graph\nNeural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph\nTransformers (GTs). It is not only due to the commendable boost in performance\nthey offer but also its capacity to provide a more lucid rationale for model\nbehaviors, which are often viewed as inscrutable. However, Attention-based GNNs\nhave demonstrated instability in interpretability when subjected to various\nsources of perturbations during both training and testing phases, including\nfactors like additional edges or nodes. In this paper, we propose a solution to\nthis problem by introducing a novel notion called Faithful Graph\nAttention-based Interpretation (FGAI). In particular, FGAI has four crucial\nproperties regarding stability and sensitivity to interpretation and final\noutput distribution. Built upon this notion, we propose an efficient\nmethodology for obtaining FGAI, which can be viewed as an ad hoc modification\nto the canonical Attention-based GNNs. To validate our proposed solution, we\nintroduce two novel metrics tailored for graph interpretation assessment.\nExperimental results demonstrate that FGAI exhibits superior stability and\npreserves the interpretability of attention under various forms of\nperturbations and randomness, which makes FGAI a more faithful and reliable\nexplanation tool.",
    "published": "2024-10-09T14:47:12Z",
    "updated": "2024-10-09T14:47:12Z",
    "authors": [
      "Lijie Hu",
      "Tianhao Huang",
      "Lu Yu",
      "Wanyu Lin",
      "Tianhang Zheng",
      "Di Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.18936v2",
    "title": "Self-Cross Diffusion Guidance for Text-to-Image Synthesis of Similar\n  Subjects",
    "summary": "Diffusion models achieved unprecedented fidelity and diversity for\nsynthesizing image, video, 3D assets, etc. However, subject mixing is an\nunresolved issue for diffusion-based image synthesis, particularly for\nsynthesizing multiple similar-looking subjects. We propose Self-Cross Diffusion\nGuidance to penalize the overlap between cross-attention maps and the\naggregated self-attention map. Compared to previous methods based on\nself-attention or cross-attention alone, our guidance is more effective in\neliminating subject mixing. What's more, our guidance addresses subject mixing\nfor all relevant patches beyond the most discriminant one, e.g., the beak of a\nbird. For each subject, we aggregate self-attention maps of patches with higher\ncross-attention values. Thus, the aggregated self-attention map forms a region\nthat the whole subject attends to. Our training-free method boosts the\nperformance of both Unet-based and Transformer-based diffusion models such as\nthe Stable Diffusion series. We also release a similar subjects dataset (SSD),\na challenging benchmark, and utilize GPT-4o for automatic and reliable\nevaluation. Extensive qualitative and quantitative results demonstrate the\neffectiveness of our self-cross diffusion guidance.",
    "published": "2024-11-28T05:58:03Z",
    "updated": "2025-03-24T19:58:03Z",
    "authors": [
      "Weimin Qiu",
      "Jieke Wang",
      "Meng Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.16545v2",
    "title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context\n  Encoding with Full-attention-based Pre-trained Language Models",
    "summary": "Large language models have shown remarkable performance across a wide range\nof language tasks, owing to their exceptional capabilities in context modeling.\nThe most commonly used method of context modeling is full self-attention, as\nseen in standard decoder-only Transformers. Although powerful, this method can\nbe inefficient for long sequences and may overlook inherent input structures.\nTo address these problems, an alternative approach is parallel context\nencoding, which splits the context into sub-pieces and encodes them parallelly.\nBecause parallel patterns are not encountered during training, naively applying\nparallel encoding leads to performance degradation. However, the underlying\nreasons and potential mitigations are unclear. In this work, we provide a\ndetailed analysis of this issue and identify that unusually high attention\nentropy can be a key factor. Furthermore, we adopt two straightforward methods\nto reduce attention entropy by incorporating attention sinks and selective\nmechanisms. Experiments on various tasks reveal that these methods effectively\nlower irregular attention entropy and narrow performance gaps. We hope this\nstudy can illuminate ways to enhance context modeling mechanisms.",
    "published": "2024-12-21T09:04:51Z",
    "updated": "2025-06-25T02:28:36Z",
    "authors": [
      "Zhisong Zhang",
      "Yan Wang",
      "Xinting Huang",
      "Tianqing Fang",
      "Hongming Zhang",
      "Chenlong Deng",
      "Shuaiyi Li",
      "Dong Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.02013v1",
    "title": "Attention Mamba: Time Series Modeling with Adaptive Pooling Acceleration\n  and Receptive Field Enhancements",
    "summary": "\"This work has been submitted to the lEEE for possible publication. Copyright\nmay be transferred without noticeafter which this version may no longer be\naccessible.\" Time series modeling serves as the cornerstone of real-world\napplications, such as weather forecasting and transportation management.\nRecently, Mamba has become a promising model that combines near-linear\ncomputational complexity with high prediction accuracy in time series modeling,\nwhile facing challenges such as insufficient modeling of nonlinear dependencies\nin attention and restricted receptive fields caused by convolutions. To\novercome these limitations, this paper introduces an innovative framework,\nAttention Mamba, featuring a novel Adaptive Pooling block that accelerates\nattention computation and incorporates global information, effectively\novercoming the constraints of limited receptive fields. Furthermore, Attention\nMamba integrates a bidirectional Mamba block, efficiently capturing long-short\nfeatures and transforming inputs into the Value representations for attention\nmechanisms. Extensive experiments conducted on diverse datasets underscore the\neffectiveness of Attention Mamba in extracting nonlinear dependencies and\nenhancing receptive fields, establishing superior performance among leading\ncounterparts. Our codes will be available on GitHub.",
    "published": "2025-04-02T05:56:43Z",
    "updated": "2025-04-02T05:56:43Z",
    "authors": [
      "Sijie Xiong",
      "Shuqing Liu",
      "Cheng Tang",
      "Fumiya Okubo",
      "Haoling Xiong",
      "Atsushi Shimada"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21036v2",
    "title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional\n  Visual Redundancy",
    "summary": "Video generation using diffusion models is highly computationally intensive,\nwith 3D attention in Diffusion Transformer (DiT) models accounting for over\n80\\% of the total computational resources. In this work, we introduce {\\bf\nRainFusion}, a novel training-free sparse attention method that exploits\ninherent sparsity nature in visual data to accelerate attention computation\nwhile preserving video quality. Specifically, we identify three unique sparse\npatterns in video generation attention calculations--Spatial Pattern, Temporal\nPattern and Textural Pattern. The sparse pattern for each attention head is\ndetermined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our\nproposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed\n{\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated\ninto state-of-the-art 3D-attention video generation models without additional\ntraining or calibration. We evaluate our method on leading open-sourced models\nincluding HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its\nbroad applicability and effectiveness. Experimental results show that\nRainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation\nwhile maintaining video quality, with only a minimal impact on VBench scores\n(-0.2\\%).",
    "published": "2025-05-27T11:15:02Z",
    "updated": "2025-06-09T11:33:40Z",
    "authors": [
      "Aiyue Chen",
      "Bin Dong",
      "Jingru Li",
      "Jing Lin",
      "Kun Tian",
      "Yiwu Yao",
      "Gongyi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.18413v1",
    "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
    "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
    "published": "2025-10-21T08:44:47Z",
    "updated": "2025-10-21T08:44:47Z",
    "authors": [
      "Siyuan Yan",
      "Guo-Qing Jiang",
      "Yuchen Zhang",
      "Xiaoxing Ma",
      "Ran Zhu",
      "Chun Cao",
      "Jingwei Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27516v1",
    "title": "BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for\n  Scalable and Efficient Text Summarization",
    "summary": "Transformer-based architectures have advanced text summarization, yet their\nquadratic complexity limits scalability on long documents. This paper\nintroduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a\nnovel framework that combines sparse attention, adaptive spans, and bilinear\nattention to address these limitations. Sparse attention reduces computational\ncosts by focusing on the most relevant parts of the input, while adaptive spans\ndynamically adjust the attention ranges. Bilinear attention complements both by\nmodeling complex token interactions within this refined context. BiSparse-AAS\nconsistently outperforms state-of-the-art baselines in both extractive and\nabstractive summarization tasks, achieving average ROUGE improvements of about\n68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance\non OpenWebText and Gigaword datasets. By addressing efficiency, scalability,\nand long-sequence modeling, BiSparse-AAS provides a unified, practical solution\nfor real-world text summarization applications.",
    "published": "2025-10-31T14:42:19Z",
    "updated": "2025-10-31T14:42:19Z",
    "authors": [
      "Desta Haileselassie Hagos",
      "Legand L. Burge",
      "Anietie Andy",
      "Anis Yazidi",
      "Vladimir Vlassov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.10526v5",
    "title": "Castling-ViT: Compressing Self-Attention via Switching Towards\n  Linear-Angular Attention at Vision Transformer Inference",
    "summary": "Vision Transformers (ViTs) have shown impressive performance but still\nrequire a high computation cost as compared to convolutional neural networks\n(CNNs), one reason is that ViTs' attention measures global similarities and\nthus has a quadratic complexity with the number of input tokens. Existing\nefficient ViTs adopt local attention (e.g., Swin) or linear attention (e.g.,\nPerformer), which sacrifice ViTs' capabilities of capturing either global or\nlocal context. In this work, we ask an important research question: Can ViTs\nlearn both global and local context while being more efficient during\ninference? To this end, we propose a framework called Castling-ViT, which\ntrains ViTs using both linear-angular attention and masked softmax-based\nquadratic attention, but then switches to having only linear angular attention\nduring ViT inference. Our Castling-ViT leverages angular kernels to measure the\nsimilarities between queries and keys via spectral angles. And we further\nsimplify it with two techniques: (1) a novel linear-angular attention\nmechanism: we decompose the angular kernels into linear terms and high-order\nresiduals, and only keep the linear terms; and (2) we adopt two parameterized\nmodules to approximate high-order residuals: a depthwise convolution and an\nauxiliary masked softmax attention to help learn both global and local\ninformation, where the masks for softmax attention are regularized to gradually\nbecome zeros and thus incur no overhead during ViT inference. Extensive\nexperiments and ablation studies on three tasks consistently validate the\neffectiveness of the proposed Castling-ViT, e.g., achieving up to a 1.8% higher\naccuracy or 40% MACs reduction on ImageNet classification and 1.2 higher mAP on\nCOCO detection under comparable FLOPs, as compared to ViTs with vanilla\nsoftmax-based attentions.",
    "published": "2022-11-18T22:49:04Z",
    "updated": "2024-07-25T17:29:22Z",
    "authors": [
      "Haoran You",
      "Yunyang Xiong",
      "Xiaoliang Dai",
      "Bichen Wu",
      "Peizhao Zhang",
      "Haoqi Fan",
      "Peter Vajda",
      "Yingyan Celine Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.08330v2",
    "title": "Convolution-enhanced Evolving Attention Networks",
    "summary": "Attention-based neural networks, such as Transformers, have become ubiquitous\nin numerous applications, including computer vision, natural language\nprocessing, and time-series analysis. In all kinds of attention networks, the\nattention maps are crucial as they encode semantic dependencies between input\ntokens. However, most existing attention networks perform modeling or reasoning\nbased on representations , wherein the attention maps of different layers are\nlearned separately without explicit interactions. In this paper, we propose a\nnovel and generic evolving attention mechanism, which directly models the\nevolution of inter-token relationships through a chain of residual\nconvolutional modules. The major motivations are twofold. On the one hand, the\nattention maps in different layers share transferable knowledge, thus adding a\nresidual connection can facilitate the information flow of inter-token\nrelationships across layers. On the other hand, there is naturally an\nevolutionary trend among attention maps at different abstraction levels, so it\nis beneficial to exploit a dedicated convolution-based module to capture this\nprocess. Equipped with the proposed mechanism, the convolution-enhanced\nevolving attention networks achieve superior performance in various\napplications, including time-series representation, natural language\nunderstanding, machine translation, and image classification. Especially on\ntime-series representation tasks, Evolving Attention-enhanced Dilated\nConvolutional (EA-DC-) Transformer outperforms state-of-the-art models\nsignificantly, achieving an average of 17% improvement compared to the best\nSOTA. To the best of our knowledge, this is the first work that explicitly\nmodels the layer-wise evolution of attention maps. Our implementation is\navailable at https://github.com/pkuyym/EvolvingAttention.",
    "published": "2022-12-16T08:14:04Z",
    "updated": "2023-04-28T06:44:48Z",
    "authors": [
      "Yujing Wang",
      "Yaming Yang",
      "Zhuo Li",
      "Jiangang Bai",
      "Mingliang Zhang",
      "Xiangtai Li",
      "Jing Yu",
      "Ce Zhang",
      "Gao Huang",
      "Yunhai Tong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01668v1",
    "title": "Measuring and Predicting Where and When Pathologists Focus their Visual\n  Attention while Grading Whole Slide Images of Cancer",
    "summary": "The ability to predict the attention of expert pathologists could lead to\ndecision support systems for better pathology training. We developed methods to\npredict the spatio-temporal (where and when) movements of pathologists'\nattention as they grade whole slide images (WSIs) of prostate cancer. We\ncharacterize a pathologist's attention trajectory by their x, y, and m\n(magnification) movements of a viewport as they navigate WSIs using a digital\nmicroscope. This information was obtained from 43 pathologists across 123 WSIs,\nand we consider the task of predicting the pathologist attention scanpaths\nconstructed from the viewport centers. We introduce a fixation extraction\nalgorithm that simplifies an attention trajectory by extracting fixations in\nthe pathologist's viewing while preserving semantic information, and we use\nthese pre-processed data to train and test a two-stage model to predict the\ndynamic (scanpath) allocation of attention during WSI reading via intermediate\nattention heatmap prediction. In the first stage, a transformer-based\nsub-network predicts the attention heatmaps (static attention) across different\nmagnifications. In the second stage, we predict the attention scanpath by\nsequentially modeling the next fixation points in an autoregressive manner\nusing a transformer-based approach, starting at the WSI center and leveraging\nmulti-magnification feature representations from the first stage. Experimental\nresults show that our scanpath prediction model outperforms chance and baseline\nmodels. Tools developed from this model could assist pathology trainees in\nlearning to allocate their attention during WSI reading like an expert.",
    "published": "2025-08-03T08:53:45Z",
    "updated": "2025-08-03T08:53:45Z",
    "authors": [
      "Souradeep Chakraborty",
      "Ruoyu Xue",
      "Rajarsi Gupta",
      "Oksana Yaskiv",
      "Constantin Friedman",
      "Natallia Sheuka",
      "Dana Perez",
      "Paul Friedman",
      "Won-Tak Choi",
      "Waqas Mahmud",
      "Beatrice Knudsen",
      "Gregory Zelinsky",
      "Joel Saltz",
      "Dimitris Samaras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.11358v1",
    "title": "N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using\n  Neural Ordinary Differential Equations",
    "summary": "We use neural ordinary differential equations to formulate a variant of the\nTransformer that is depth-adaptive in the sense that an input-dependent number\nof time steps is taken by the ordinary differential equation solver. Our goal\nin proposing the N-ODE Transformer is to investigate whether its\ndepth-adaptivity may aid in overcoming some specific known theoretical\nlimitations of the Transformer in handling nonlocal effects. Specifically, we\nconsider the simple problem of determining the parity of a binary sequence, for\nwhich the standard Transformer has known limitations that can only be overcome\nby using a sufficiently large number of layers or attention heads. We find,\nhowever, that the depth-adaptivity of the N-ODE Transformer does not provide a\nremedy for the inherently nonlocal nature of the parity problem, and provide\nexplanations for why this is so. Next, we pursue regularization of the N-ODE\nTransformer by penalizing the arclength of the ODE trajectories, but find that\nthis fails to improve the accuracy or efficiency of the N-ODE Transformer on\nthe challenging parity problem. We suggest future avenues of research for\nmodifications and extensions of the N-ODE Transformer that may lead to improved\naccuracy and efficiency for sequence modelling tasks such as neural machine\ntranslation.",
    "published": "2020-10-22T00:48:24Z",
    "updated": "2020-10-22T00:48:24Z",
    "authors": [
      "Aaron Baier-Reinio",
      "Hans De Sterck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2007.06257v2",
    "title": "Rewiring the Transformer with Depth-Wise LSTMs",
    "summary": "Stacking non-linear layers allows deep neural networks to model complicated\nfunctions, and including residual connections in Transformer layers is\nbeneficial for convergence and performance. However, residual connections may\nmake the model \"forget\" distant layers and fail to fuse information from\nprevious layers effectively. Selectively managing the representation\naggregation of Transformer layers may lead to better performance. In this\npaper, we present a Transformer with depth-wise LSTMs connecting cascading\nTransformer layers and sub-layers. We show that layer normalization and\nfeed-forward computation within a Transformer layer can be absorbed into\ndepth-wise LSTMs connecting pure Transformer attention layers. Our experiments\nwith the 6-layer Transformer show significant BLEU improvements in both WMT 14\nEnglish-German / French tasks and the OPUS-100 many-to-many multilingual NMT\ntask, and our deep Transformer experiments demonstrate the effectiveness of\ndepth-wise LSTM on the convergence and performance of deep Transformers.",
    "published": "2020-07-13T09:19:34Z",
    "updated": "2024-04-04T07:17:11Z",
    "authors": [
      "Hongfei Xu",
      "Yang Song",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.03921v2",
    "title": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector",
    "summary": "Transformers are transforming the landscape of computer vision, especially\nfor recognition tasks. Detection transformers are the first fully end-to-end\nlearning systems for object detection, while vision transformers are the first\nfully transformer-based architecture for image classification. In this paper,\nwe integrate Vision and Detection Transformers (ViDT) to build an effective and\nefficient object detector. ViDT introduces a reconfigured attention module to\nextend the recent Swin Transformer to be a standalone object detector, followed\nby a computationally efficient transformer decoder that exploits multi-scale\nfeatures and auxiliary techniques essential to boost the detection performance\nwithout much increase in computational load. Extensive evaluation results on\nthe Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP\nand latency trade-off among existing fully transformer-based object detectors,\nand achieves 49.2AP owing to its high scalability for large models. We will\nrelease the code and trained models at https://github.com/naver-ai/vidt",
    "published": "2021-10-08T06:32:05Z",
    "updated": "2021-11-29T11:07:13Z",
    "authors": [
      "Hwanjun Song",
      "Deqing Sun",
      "Sanghyuk Chun",
      "Varun Jampani",
      "Dongyoon Han",
      "Byeongho Heo",
      "Wonjae Kim",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.00984v1",
    "title": "TGTOD: A Global Temporal Graph Transformer for Outlier Detection at\n  Scale",
    "summary": "While Transformers have revolutionized machine learning on various data,\nexisting Transformers for temporal graphs face limitations in (1) restricted\nreceptive fields, (2) overhead of subgraph extraction, and (3) suboptimal\ngeneralization capability beyond link prediction. In this paper, we rethink\ntemporal graph Transformers and propose TGTOD, a novel end-to-end Temporal\nGraph Transformer for Outlier Detection. TGTOD employs global attention to\nmodel both structural and temporal dependencies within temporal graphs. To\ntackle scalability, our approach divides large temporal graphs into\nspatiotemporal patches, which are then processed by a hierarchical Transformer\narchitecture comprising Patch Transformer, Cluster Transformer, and Temporal\nTransformer. We evaluate TGTOD on three public datasets under two settings,\ncomparing with a wide range of baselines. Our experimental results demonstrate\nthe effectiveness of TGTOD, achieving AP improvement of 61% on Elliptic.\nFurthermore, our efficiency evaluation shows that TGTOD reduces training time\nby 44x compared to existing Transformers for temporal graphs. To foster\nreproducibility, we make our implementation publicly available at\nhttps://github.com/kayzliu/tgtod.",
    "published": "2024-12-01T22:24:55Z",
    "updated": "2024-12-01T22:24:55Z",
    "authors": [
      "Kay Liu",
      "Jiahao Ding",
      "MohamadAli Torkamani",
      "Philip S. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.03269v2",
    "title": "Conformal Transformations for Symmetric Power Transformers",
    "summary": "Transformers with linear attention offer significant computational advantages\nover softmax-based transformers but often suffer from degraded performance. The\nsymmetric power (sympow) transformer, a particular type of linear transformer,\naddresses some of this performance gap by leveraging symmetric tensor\nembeddings, achieving comparable performance to softmax transformers. However,\nthe finite capacity of the recurrent state in sympow transformers limits their\nability to retain information, leading to performance degradation when scaling\nthe training or evaluation context length. To address this issue, we propose\nthe conformal-sympow transformer, which dynamically frees up capacity using\ndata-dependent multiplicative gating and adaptively stores information using\ndata-dependent rotary embeddings. Preliminary experiments on the LongCrawl64\ndataset demonstrate that conformal-sympow overcomes the limitations of sympow\ntransformers, achieving robust performance across scaled training and\nevaluation contexts.",
    "published": "2025-03-05T08:50:53Z",
    "updated": "2025-05-03T05:24:08Z",
    "authors": [
      "Saurabh Kumar",
      "Jacob Buckman",
      "Carles Gelada",
      "Sean Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.19125v1",
    "title": "Finding Clustering Algorithms in the Transformer Architecture",
    "summary": "The invention of the transformer architecture has revolutionized Artificial\nIntelligence (AI), yielding unprecedented success in areas such as natural\nlanguage processing, computer vision, and multimodal reasoning. Despite these\nadvances, it is unclear whether transformers are able to learn and implement\nprecise algorithms. Here, we demonstrate that transformers can exactly\nimplement a fundamental and widely used algorithm for $k$-means clustering:\nLloyd's algorithm. First, we theoretically prove the existence of such a\ntransformer architecture, which we term the $k$-means transformer, that exactly\nimplements Lloyd's algorithm for $k$-means clustering using the standard\ningredients of modern transformers: attention and residual connections. Next,\nwe numerically implement this transformer and demonstrate in experiments the\nexact correspondence between our architecture and Lloyd's algorithm, providing\na fully neural implementation of $k$-means clustering. Finally, we demonstrate\nthat interpretable alterations (e.g., incorporating layer normalizations or\nmultilayer perceptrons) to this architecture yields diverse and novel variants\nof clustering algorithms, such as soft $k$-means, spherical $k$-means, trimmed\n$k$-means, and more. Collectively, our findings demonstrate how transformer\nmechanisms can precisely map onto algorithmic procedures, offering a clear and\ninterpretable perspective on implementing precise algorithms in transformers.",
    "published": "2025-06-23T20:52:01Z",
    "updated": "2025-06-23T20:52:01Z",
    "authors": [
      "Kenneth L. Clarkson",
      "Lior Horesh",
      "Takuya Ito",
      "Charlotte Park",
      "Parikshit Ram"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.00112v3",
    "title": "Transformer in Transformer",
    "summary": "Transformer is a new kind of neural architecture which encodes the input data\nas powerful features via the attention mechanism. Basically, the visual\ntransformers first divide the input images into several local patches and then\ncalculate both representations and their relationship. Since natural images are\nof high complexity with abundant detail and color information, the granularity\nof the patch dividing is not fine enough for excavating features of objects in\ndifferent scales and locations. In this paper, we point out that the attention\ninside these local patches are also essential for building visual transformers\nwith high performance and we explore a new architecture, namely, Transformer iN\nTransformer (TNT). Specifically, we regard the local patches (e.g.,\n16$\\times$16) as \"visual sentences\" and present to further divide them into\nsmaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each\nword will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be\naggregated to enhance the representation ability. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed TNT architecture,\ne.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7%\nhigher than that of the state-of-the-art visual transformer with similar\ncomputational cost. The PyTorch code is available at\nhttps://github.com/huawei-noah/CV-Backbones, and the MindSpore code is\navailable at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.",
    "published": "2021-02-27T03:12:16Z",
    "updated": "2021-10-26T02:24:24Z",
    "authors": [
      "Kai Han",
      "An Xiao",
      "Enhua Wu",
      "Jianyuan Guo",
      "Chunjing Xu",
      "Yunhe Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.15668v1",
    "title": "AdaViT: Adaptive Vision Transformers for Efficient Image Recognition",
    "summary": "Built on top of self-attention mechanisms, vision transformers have\ndemonstrated remarkable performance on a variety of vision tasks recently.\nWhile achieving excellent performance, they still require relatively intensive\ncomputational cost that scales up drastically as the numbers of patches,\nself-attention heads and transformer blocks increase. In this paper, we argue\nthat due to the large variations among images, their need for modeling\nlong-range dependencies between patches differ. To this end, we introduce\nAdaViT, an adaptive computation framework that learns to derive usage policies\non which patches, self-attention heads and transformer blocks to use throughout\nthe backbone on a per-input basis, aiming to improve inference efficiency of\nvision transformers with a minimal drop of accuracy for image recognition.\nOptimized jointly with a transformer backbone in an end-to-end manner, a\nlight-weight decision network is attached to the backbone to produce decisions\non-the-fly. Extensive experiments on ImageNet demonstrate that our method\nobtains more than 2x improvement on efficiency compared to state-of-the-art\nvision transformers with only 0.8% drop of accuracy, achieving good\nefficiency/accuracy trade-offs conditioned on different computational budgets.\nWe further conduct quantitative and qualitative analysis on learned usage\npolices and provide more insights on the redundancy in vision transformers.",
    "published": "2021-11-30T18:57:02Z",
    "updated": "2021-11-30T18:57:02Z",
    "authors": [
      "Lingchen Meng",
      "Hengduo Li",
      "Bor-Chun Chen",
      "Shiyi Lan",
      "Zuxuan Wu",
      "Yu-Gang Jiang",
      "Ser-Nam Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.10762v2",
    "title": "StyleSwin: Transformer-based GAN for High-resolution Image Generation",
    "summary": "Despite the tantalizing success in a broad of vision tasks, transformers have\nnot yet demonstrated on-par ability as ConvNets in high-resolution image\ngenerative modeling. In this paper, we seek to explore using pure transformers\nto build a generative adversarial network for high-resolution image synthesis.\nTo this end, we believe that local attention is crucial to strike the balance\nbetween computational efficiency and modeling capacity. Hence, the proposed\ngenerator adopts Swin transformer in a style-based architecture. To achieve a\nlarger receptive field, we propose double attention which simultaneously\nleverages the context of the local and the shifted windows, leading to improved\ngeneration quality. Moreover, we show that offering the knowledge of the\nabsolute position that has been lost in window-based transformers greatly\nbenefits the generation quality. The proposed StyleSwin is scalable to high\nresolutions, with both the coarse geometry and fine structures benefit from the\nstrong expressivity of transformers. However, blocking artifacts occur during\nhigh-resolution synthesis because performing the local attention in a\nblock-wise manner may break the spatial coherency. To solve this, we\nempirically investigate various solutions, among which we find that employing a\nwavelet discriminator to examine the spectral discrepancy effectively\nsuppresses the artifacts. Extensive experiments show the superiority over prior\ntransformer-based GANs, especially on high resolutions, e.g., 1024x1024. The\nStyleSwin, without complex training strategies, excels over StyleGAN on\nCelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the\npromise of using transformers for high-resolution image generation. The code\nand models will be available at https://github.com/microsoft/StyleSwin.",
    "published": "2021-12-20T18:59:51Z",
    "updated": "2022-07-21T01:15:06Z",
    "authors": [
      "Bowen Zhang",
      "Shuyang Gu",
      "Bo Zhang",
      "Jianmin Bao",
      "Dong Chen",
      "Fang Wen",
      "Yong Wang",
      "Baining Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.17247v3",
    "title": "VL-InterpreT: An Interactive Visualization Tool for Interpreting\n  Vision-Language Transformers",
    "summary": "Breakthroughs in transformer-based models have revolutionized not only the\nNLP field, but also vision and multimodal systems. However, although\nvisualization and interpretability tools have become available for NLP models,\ninternal mechanisms of vision and multimodal transformers remain largely\nopaque. With the success of these transformers, it is increasingly critical to\nunderstand their inner workings, as unraveling these black-boxes will lead to\nmore capable and trustworthy models. To contribute to this quest, we propose\nVL-InterpreT, which provides novel interactive visualizations for interpreting\nthe attentions and hidden representations in multimodal transformers.\nVL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety\nof statistics in attention heads throughout all layers for both vision and\nlanguage components, (2) visualizes cross-modal and intra-modal attentions\nthrough easily readable heatmaps, and (3) plots the hidden representations of\nvision and language tokens as they pass through the transformer layers. In this\npaper, we demonstrate the functionalities of VL-InterpreT through the analysis\nof KD-VLP, an end-to-end pretraining vision-language multimodal\ntransformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and\nWebQA, two visual question answering benchmarks. Furthermore, we also present a\nfew interesting findings about multimodal transformer behaviors that were\nlearned through our tool.",
    "published": "2022-03-30T05:25:35Z",
    "updated": "2022-08-22T22:25:59Z",
    "authors": [
      "Estelle Aflalo",
      "Meng Du",
      "Shao-Yen Tseng",
      "Yongfei Liu",
      "Chenfei Wu",
      "Nan Duan",
      "Vasudev Lal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.13401v2",
    "title": "Your Transformer May Not be as Powerful as You Expect",
    "summary": "Relative Positional Encoding (RPE), which encodes the relative distance\nbetween any pair of tokens, is one of the most successful modifications to the\noriginal Transformer. As far as we know, theoretical understanding of the\nRPE-based Transformers is largely unexplored. In this work, we mathematically\nanalyze the power of RPE-based Transformers regarding whether the model is\ncapable of approximating any continuous sequence-to-sequence functions. One may\nnaturally assume the answer is in the affirmative -- RPE-based Transformers are\nuniversal function approximators. However, we present a negative result by\nshowing there exist continuous sequence-to-sequence functions that RPE-based\nTransformers cannot approximate no matter how deep and wide the neural network\nis. One key reason lies in that most RPEs are placed in the softmax attention\nthat always generates a right stochastic matrix. This restricts the network\nfrom capturing positional information in the RPEs and limits its capacity. To\novercome the problem and make the model more powerful, we first present\nsufficient conditions for RPE-based Transformers to achieve universal function\napproximation. With the theoretical guidance, we develop a novel attention\nmodule, called Universal RPE-based (URPE) Attention, which satisfies the\nconditions. Therefore, the corresponding URPE-based Transformers become\nuniversal function approximators. Extensive experiments covering typical\narchitectures and tasks demonstrate that our model is parameter-efficient and\ncan achieve superior performance to strong baselines in a wide range of\napplications. The code will be made publicly available at\nhttps://github.com/lsj2408/URPE.",
    "published": "2022-05-26T14:51:30Z",
    "updated": "2022-10-28T16:42:25Z",
    "authors": [
      "Shengjie Luo",
      "Shanda Li",
      "Shuxin Zheng",
      "Tie-Yan Liu",
      "Liwei Wang",
      "Di He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.03758v3",
    "title": "Towards a Deeper Understanding of Transformer for Residential\n  Non-intrusive Load Monitoring",
    "summary": "Transformer models have demonstrated impressive performance in Non-Intrusive\nLoad Monitoring (NILM) applications in recent years. Despite their success,\nexisting studies have not thoroughly examined the impact of various\nhyper-parameters on model performance, which is crucial for advancing\nhigh-performing transformer models. In this work, a comprehensive series of\nexperiments have been conducted to analyze the influence of these\nhyper-parameters in the context of residential NILM. This study delves into the\neffects of the number of hidden dimensions in the attention layer, the number\nof attention layers, the number of attention heads, and the dropout ratio on\ntransformer performance. Furthermore, the role of the masking ratio has\nexplored in BERT-style transformer training, providing a detailed investigation\ninto its impact on NILM tasks. Based on these experiments, the optimal\nhyper-parameters have been selected and used them to train a transformer model,\nwhich surpasses the performance of existing models. The experimental findings\noffer valuable insights and guidelines for optimizing transformer\narchitectures, aiming to enhance their effectiveness and efficiency in NILM\napplications. It is expected that this work will serve as a foundation for\nfuture research and development of more robust and capable transformer models\nfor NILM.",
    "published": "2024-10-02T09:14:50Z",
    "updated": "2024-10-13T12:40:37Z",
    "authors": [
      "Minhajur Rahman",
      "Yasir Arafat"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19488v1",
    "title": "Understanding Transformer from the Perspective of Associative Memory",
    "summary": "In this paper, we share our reflections and insights on understanding\nTransformer architectures through the lens of associative memory--a classic\npsychological concept inspired by human cognition. We start with the basics of\nassociative memory (think simple linear attention) and then dive into two\ndimensions:\n  Memory Capacity: How much can a Transformer really remember, and how well? We\nintroduce retrieval SNR to measure this and use a kernel perspective to\nmathematically reveal why Softmax Attention is so effective. We also show how\nFFNs can be seen as a type of associative memory, leading to insights on their\ndesign and potential improvements.\n  Memory Update: How do these memories learn and evolve? We present a unified\nframework for understanding how different Transformer variants (like DeltaNet\nand Softmax Attention) update their \"knowledge base\". This leads us to tackle\ntwo provocative questions: 1. Are Transformers fundamentally limited in what\nthey can express, and can we break these barriers? 2. If a Transformer had\ninfinite context, would it become infinitely intelligent?\n  We want to demystify Transformer architecture, offering a clearer\nunderstanding of existing designs. This exploration aims to provide fresh\ninsights and spark new avenues for Transformer innovation.",
    "published": "2025-05-26T04:15:38Z",
    "updated": "2025-05-26T04:15:38Z",
    "authors": [
      "Shu Zhong",
      "Mingyu Xu",
      "Tenglong Ao",
      "Guang Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.14899v2",
    "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image\n  Classification",
    "summary": "The recently developed vision transformer (ViT) has achieved promising\nresults on image classification compared to convolutional neural networks.\nInspired by this, in this paper, we study how to learn multi-scale feature\nrepresentations in transformer models for image classification. To this end, we\npropose a dual-branch transformer to combine image patches (i.e., tokens in a\ntransformer) of different sizes to produce stronger image features. Our\napproach processes small-patch and large-patch tokens with two separate\nbranches of different computational complexity and these tokens are then fused\npurely by attention multiple times to complement each other. Furthermore, to\nreduce computation, we develop a simple yet effective token fusion module based\non cross attention, which uses a single token for each branch as a query to\nexchange information with other branches. Our proposed cross-attention only\nrequires linear time for both computational and memory complexity instead of\nquadratic time otherwise. Extensive experiments demonstrate that our approach\nperforms better than or on par with several concurrent works on vision\ntransformer, in addition to efficient CNN models. For example, on the\nImageNet1K dataset, with some architectural changes, our approach outperforms\nthe recent DeiT by a large margin of 2\\% with a small to moderate increase in\nFLOPs and model parameters. Our source codes and models are available at\n\\url{https://github.com/IBM/CrossViT}.",
    "published": "2021-03-27T13:03:17Z",
    "updated": "2021-08-22T18:53:59Z",
    "authors": [
      "Chun-Fu Chen",
      "Quanfu Fan",
      "Rameswar Panda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.00222v1",
    "title": "Cross-domain Detection Transformer based on Spatial-aware and\n  Semantic-aware Token Alignment",
    "summary": "Detection transformers like DETR have recently shown promising performance on\nmany object detection tasks, but the generalization ability of those methods is\nstill quite challenging for cross-domain adaptation scenarios. To address the\ncross-domain issue, a straightforward way is to perform token alignment with\nadversarial training in transformers. However, its performance is often\nunsatisfactory as the tokens in detection transformers are quite diverse and\nrepresent different spatial and semantic information. In this paper, we propose\na new method called Spatial-aware and Semantic-aware Token Alignment (SSTA) for\ncross-domain detection transformers. In particular, we take advantage of the\ncharacteristics of cross-attention as used in detection transformer and propose\nthe spatial-aware token alignment (SpaTA) and the semantic-aware token\nalignment (SemTA) strategies to guide the token alignment across domains. For\nspatial-aware token alignment, we can extract the information from the\ncross-attention map (CAM) to align the distribution of tokens according to\ntheir attention to object queries. For semantic-aware token alignment, we\ninject the category information into the cross-attention map and construct\ndomain embedding to guide the learning of a multi-class discriminator so as to\nmodel the category relationship and achieve category-level token alignment\nduring the entire adaptation process. We conduct extensive experiments on\nseveral widely-used benchmarks, and the results clearly show the effectiveness\nof our proposed method over existing state-of-the-art baselines.",
    "published": "2022-06-01T04:13:22Z",
    "updated": "2022-06-01T04:13:22Z",
    "authors": [
      "Jinhong Deng",
      "Xiaoyue Zhang",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.14941v4",
    "title": "Streaming Transformer ASR with Blockwise Synchronous Beam Search",
    "summary": "The Transformer self-attention network has shown promising performance as an\nalternative to recurrent neural networks in end-to-end (E2E) automatic speech\nrecognition (ASR) systems. However, Transformer has a drawback in that the\nentire input sequence is required to compute both self-attention and\nsource--target attention. In this paper, we propose a novel blockwise\nsynchronous beam search algorithm based on blockwise processing of encoder to\nperform streaming E2E Transformer ASR. In the beam search, encoded feature\nblocks are synchronously aligned using a block boundary detection technique,\nwhere a reliability score of each predicted hypothesis is evaluated based on\nthe end-of-sequence and repeated tokens in the hypothesis. Evaluations of the\nHKUST and AISHELL-1 Mandarin, LibriSpeech English, and CSJ Japanese tasks show\nthat the proposed streaming Transformer algorithm outperforms conventional\nonline approaches, including monotonic chunkwise attention (MoChA), especially\nwhen using the knowledge distillation technique. An ablation study indicates\nthat our streaming approach contributes to reducing the response time, and the\nrepetition criterion contributes significantly in certain tasks. Our streaming\nASR models achieve comparable or superior performance to batch models and other\nstreaming-based Transformer methods in all tasks considered.",
    "published": "2020-06-25T06:49:00Z",
    "updated": "2020-11-18T01:03:24Z",
    "authors": [
      "Emiru Tsunoo",
      "Yosuke Kashiwagi",
      "Shinji Watanabe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.14702v3",
    "title": "Pyramid Medical Transformer for Medical Image Segmentation",
    "summary": "Deep neural networks have been a prevailing technique in the field of medical\nimage processing. However, the most popular convolutional neural networks\n(CNNs) based methods for medical image segmentation are imperfect because they\nmodel long-range dependencies by stacking layers or enlarging filters.\nTransformers and the self-attention mechanism are recently proposed to\neffectively learn long-range dependencies by modeling all pairs of word-to-word\nattention regardless of their positions. The idea has also been extended to the\ncomputer vision field by creating and treating image patches as embeddings.\nConsidering the computation complexity for whole image self-attention, current\ntransformer-based models settle for a rigid partitioning scheme that\npotentially loses informative relations. Besides, current medical transformers\nmodel global context on full resolution images, leading to unnecessary\ncomputation costs. To address these issues, we developed a novel method to\nintegrate multi-scale attention and CNN feature extraction using a pyramidal\nnetwork architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans\ncaptured multi-range relations by working on multi-resolution images. An\nadaptive partitioning scheme was implemented to retain informative relations\nand to access different receptive fields efficiently. Experimental results on\nthree medical image datasets (gland segmentation, MoNuSeg, and HECKTOR\ndatasets) showed that PMTrans outperformed the latest CNN-based and\ntransformer-based models for medical image segmentation.",
    "published": "2021-04-29T23:57:20Z",
    "updated": "2022-04-29T17:25:25Z",
    "authors": [
      "Zhuangzhuang Zhang",
      "Weixiong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.06847v3",
    "title": "Video Super-Resolution Transformer",
    "summary": "Video super-resolution (VSR), with the aim to restore a high-resolution video\nfrom its corresponding low-resolution version, is a spatial-temporal sequence\nprediction problem. Recently, Transformer has been gaining popularity due to\nits parallel computing ability for sequence-to-sequence modeling. Thus, it\nseems to be straightforward to apply the vision Transformer to solve VSR.\nHowever, the typical block design of Transformer with a fully connected\nself-attention layer and a token-wise feed-forward layer does not fit well for\nVSR due to the following two reasons. First, the fully connected self-attention\nlayer neglects to exploit the data locality because this layer relies on linear\nlayers to compute attention maps. Second, the token-wise feed-forward layer\nlacks the feature alignment which is important for VSR since this layer\nindependently processes each of the input token embeddings without any\ninteraction among them. In this paper, we make the first attempt to adapt\nTransformer for VSR. Specifically, to tackle the first issue, we present a\nspatial-temporal convolutional self-attention layer with a theoretical\nunderstanding to exploit the locality information. For the second issue, we\ndesign a bidirectional optical flow-based feed-forward layer to discover the\ncorrelations across different video frames and also align features. Extensive\nexperiments on several benchmark datasets demonstrate the effectiveness of our\nproposed method. The code will be available at\nhttps://github.com/caojiezhang/VSR-Transformer.",
    "published": "2021-06-12T20:00:32Z",
    "updated": "2023-07-04T15:30:58Z",
    "authors": [
      "Jiezhang Cao",
      "Yawei Li",
      "Kai Zhang",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.13776v1",
    "title": "Transformer Uncertainty Estimation with Hierarchical Stochastic\n  Attention",
    "summary": "Transformers are state-of-the-art in a wide range of NLP tasks and have also\nbeen applied to many real-world products. Understanding the reliability and\ncertainty of transformer model predictions is crucial for building trustable\nmachine learning applications, e.g., medical diagnosis. Although many recent\ntransformer extensions have been proposed, the study of the uncertainty\nestimation of transformer models is under-explored. In this work, we propose a\nnovel way to enable transformers to have the capability of uncertainty\nestimation and, meanwhile, retain the original predictive performance. This is\nachieved by learning a hierarchical stochastic self-attention that attends to\nvalues and a set of learnable centroids, respectively. Then new attention heads\nare formed with a mixture of sampled centroids using the Gumbel-Softmax trick.\nWe theoretically show that the self-attention approximation by sampling from a\nGumbel distribution is upper bounded. We empirically evaluate our model on two\ntext classification tasks with both in-domain (ID) and out-of-domain (OOD)\ndatasets. The experimental results demonstrate that our approach: (1) achieves\nthe best predictive performance and uncertainty trade-off among compared\nmethods; (2) exhibits very competitive (in most cases, improved) predictive\nperformance on ID datasets; (3) is on par with Monte Carlo dropout and ensemble\nmethods in uncertainty estimation on OOD datasets.",
    "published": "2021-12-27T16:43:31Z",
    "updated": "2021-12-27T16:43:31Z",
    "authors": [
      "Jiahuan Pei",
      "Cheng Wang",
      "GyÃ¶rgy Szarvas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.10435v1",
    "title": "Vision Transformer with Convolutions Architecture Search",
    "summary": "Transformers exhibit great advantages in handling computer vision tasks. They\nmodel image classification tasks by utilizing a multi-head attention mechanism\nto process a series of patches consisting of split images. However, for complex\ntasks, Transformer in computer vision not only requires inheriting a bit of\ndynamic attention and global context, but also needs to introduce features\nconcerning noise reduction, shifting, and scaling invariance of objects.\nTherefore, here we take a step forward to study the structural characteristics\nof Transformer and convolution and propose an architecture search method-Vision\nTransformer with Convolutions Architecture Search (VTCAS). The high-performance\nbackbone network searched by VTCAS introduces the desirable features of\nconvolutional neural networks into the Transformer architecture while\nmaintaining the benefits of the multi-head attention mechanism. The searched\nblock-based backbone network can extract feature maps at different scales.\nThese features are compatible with a wider range of visual tasks, such as image\nclassification (32 M parameters, 82.0% Top-1 accuracy on ImageNet-1K) and\nobject detection (50.4% mAP on COCO2017). The proposed topology based on the\nmulti-head attention mechanism and CNN adaptively associates relational\nfeatures of pixels with multi-scale features of objects. It enhances the\nrobustness of the neural network for object recognition, especially in the low\nillumination indoor scene.",
    "published": "2022-03-20T02:59:51Z",
    "updated": "2022-03-20T02:59:51Z",
    "authors": [
      "Haichao Zhang",
      "Kuangrong Hao",
      "Witold Pedrycz",
      "Lei Gao",
      "Xuesong Tang",
      "Bing Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.11441v1",
    "title": "Multi-Modal Learning for AU Detection Based on Multi-Head Fused\n  Transformers",
    "summary": "Multi-modal learning has been intensified in recent years, especially for\napplications in facial analysis and action unit detection whilst there still\nexist two main challenges in terms of 1) relevant feature learning for\nrepresentation and 2) efficient fusion for multi-modalities. Recently, there\nare a number of works have shown the effectiveness in utilizing the attention\nmechanism for AU detection, however, most of them are binding the region of\ninterest (ROI) with features but rarely apply attention between features of\neach AU. On the other hand, the transformer, which utilizes a more efficient\nself-attention mechanism, has been widely used in natural language processing\nand computer vision tasks but is not fully explored in AU detection tasks. In\nthis paper, we propose a novel end-to-end Multi-Head Fused Transformer (MFT)\nmethod for AU detection, which learns AU encoding features representation from\ndifferent modalities by transformer encoder and fuses modalities by another\nfusion transformer module. Multi-head fusion attention is designed in the\nfusion transformer module for the effective fusion of multiple modalities. Our\napproach is evaluated on two public multi-modal AU databases, BP4D, and BP4D+,\nand the results are superior to the state-of-the-art algorithms and baseline\nmodels. We further analyze the performance of AU detection from different\nmodalities.",
    "published": "2022-03-22T03:31:29Z",
    "updated": "2022-03-22T03:31:29Z",
    "authors": [
      "Xiang Zhang",
      "Lijun Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.04978v1",
    "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation\n  Learning",
    "summary": "Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for\ncomputer vision tasks, while the self-attention computation in Transformer\nscales quadratically w.r.t. the input patch number. Thus, existing solutions\ncommonly employ down-sampling operations (e.g., average pooling) over\nkeys/values to dramatically reduce the computational cost. In this work, we\nargue that such over-aggressive down-sampling design is not invertible and\ninevitably causes information dropping especially for high-frequency components\nin objects (e.g., texture details). Motivated by the wavelet theory, we\nconstruct a new Wavelet Vision Transformer (\\textbf{Wave-ViT}) that formulates\nthe invertible down-sampling with wavelet transforms and self-attention\nlearning in a unified way. This proposal enables self-attention learning with\nlossless down-sampling over keys/values, facilitating the pursuing of a better\nefficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are\nleveraged to strengthen self-attention outputs by aggregating local contexts\nwith enlarged receptive field. We validate the superiority of Wave-ViT through\nextensive experiments over multiple vision tasks (e.g., image recognition,\nobject detection and instance segmentation). Its performances surpass\nstate-of-the-art ViT backbones with comparable FLOPs. Source code is available\nat \\url{https://github.com/YehLi/ImageNetModel}.",
    "published": "2022-07-11T16:03:51Z",
    "updated": "2022-07-11T16:03:51Z",
    "authors": [
      "Ting Yao",
      "Yingwei Pan",
      "Yehao Li",
      "Chong-Wah Ngo",
      "Tao Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.15480v1",
    "title": "SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph\n  Attention",
    "summary": "Recently, Graph Transformers have emerged as a promising solution to\nalleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance\ngraph representation performance. Unfortunately, Graph Transformers are\ncomputationally expensive due to the quadratic complexity inherent in\nself-attention when applied over large-scale graphs, especially for node tasks.\nIn contrast, spiking neural networks (SNNs), with event-driven and binary\nspikes properties, can perform energy-efficient computation. In this work, we\npropose a novel insight into integrating SNNs with Graph Transformers and\ndesign a Spiking Graph Attention (SGA) module. The matrix multiplication is\nreplaced by sparse addition and mask operations. The linear complexity enables\nall-pair node interactions on large-scale graphs with limited GPU memory. To\nour knowledge, our work is the first attempt to introduce SNNs into Graph\nTransformers. Furthermore, we design SpikeGraphormer, a Dual-branch\narchitecture, combining a sparse GNN branch with our SGA-driven Graph\nTransformer branch, which can simultaneously perform all-pair node interactions\nand capture local neighborhoods. SpikeGraphormer consistently outperforms\nexisting state-of-the-art approaches across various datasets and makes\nsubstantial improvements in training time, inference time, and GPU memory cost\n(10 ~ 20x lower than vanilla self-attention). It also performs well in\ncross-domain applications (image and text classification). We release our code\nat https://github.com/PHD-lanyu/SpikeGraphormer.",
    "published": "2024-03-21T03:11:53Z",
    "updated": "2024-03-21T03:11:53Z",
    "authors": [
      "Yundong Sun",
      "Dongjie Zhu",
      "Yansong Wang",
      "Zhaoshuo Tian",
      "Ning Cao",
      "Gregory O'Hared"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.16552v2",
    "title": "QKFormer: Hierarchical Spiking Transformer using Q-K Attention",
    "summary": "Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with\nTransformer architectures, have attracted significant attention due to their\npotential for energy efficiency and high performance. However, existing models\nin this domain still suffer from suboptimal performance. We introduce several\ninnovations to improve the performance: i) We propose a novel spike-form Q-K\nattention mechanism, tailored for SNNs, which efficiently models the importance\nof token or channel dimensions through binary vectors with linear complexity.\nii) We incorporate the hierarchical structure, which significantly benefits the\nperformance of both the brain and artificial neural networks, into spiking\ntransformers to obtain multi-scale spiking representation. iii) We design a\nversatile and powerful patch embedding module with a deformed shortcut\nspecifically for spiking transformers. Together, we develop QKFormer, a\nhierarchical spiking transformer based on Q-K attention with direct training.\nQKFormer shows significantly superior performance over existing\nstate-of-the-art SNN models on various mainstream datasets. Notably, with\ncomparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a\ngroundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially\noutperforming Spikformer by 10.84%. To our best knowledge, this is the first\ntime that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The\ncode and models are publicly available at\nhttps://github.com/zhouchenlin2096/QKFormer",
    "published": "2024-03-25T08:57:27Z",
    "updated": "2024-10-08T09:29:01Z",
    "authors": [
      "Chenlin Zhou",
      "Han Zhang",
      "Zhaokun Zhou",
      "Liutao Yu",
      "Liwei Huang",
      "Xiaopeng Fan",
      "Li Yuan",
      "Zhengyu Ma",
      "Huihui Zhou",
      "Yonghong Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.01367v2",
    "title": "Transformers are Universal In-context Learners",
    "summary": "Transformers are deep architectures that define \"in-context mappings\" which\nenable predicting new tokens based on a given set of tokens (such as a prompt\nin NLP applications or a set of patches for a vision transformer). In this\nwork, we study in particular the ability of these architectures to handle an\narbitrarily large number of context tokens. To mathematically, uniformly\naddress their expressivity, we consider the case that the mappings are\nconditioned on a context represented by a probability distribution of tokens\nwhich becomes discrete for a finite number of these. The relevant notion of\nsmoothness then corresponds to continuity in terms of the Wasserstein distance\nbetween these contexts. We demonstrate that deep transformers are universal and\ncan approximate continuous in-context mappings to arbitrary precision,\nuniformly over compact token domains. A key aspect of our results, compared to\nexisting findings, is that for a fixed precision, a single transformer can\noperate on an arbitrary (even infinite) number of tokens. Additionally, it\noperates with a fixed embedding dimension of tokens (this dimension does not\nincrease with precision) and a fixed number of heads (proportional to the\ndimension). The use of MLPs between multi-head attention layers is also\nexplicitly controlled. We consider both unmasked attentions (as used for the\nvision transformer) and masked causal attentions (as used for NLP and time\nseries applications). We tackle the causal setting leveraging a space-time\nlifting to analyze causal attention as a mapping over probability distributions\nof tokens.",
    "published": "2024-08-02T16:21:48Z",
    "updated": "2024-10-03T02:43:59Z",
    "authors": [
      "Takashi Furuya",
      "Maarten V. de Hoop",
      "Gabriel PeyrÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.05477v3",
    "title": "Retrofitting Temporal Graph Neural Networks with Transformer",
    "summary": "Temporal graph neural networks (TGNNs) outperform regular GNNs by\nincorporating time information into graph-based operations. However, TGNNs\nadopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored\ntraining frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN,\nwhich uses Transformer decoder as the backbone model for TGNN to enjoy\nTransformer's codebase for efficient training. In particular, Transformer\nachieves tremendous success for language modeling, and thus the community\ndeveloped high-performance kernels (e.g., flash-attention and memory-efficient\nattention) and efficient distributed training schemes (e.g., PyTorch FSDP,\nDeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling,\ni.e., the message aggregation operation between chronologically occurring nodes\nand their temporal neighbors in TGNNs can be structured as sequence modeling.\nBeside this similarity, we also incorporate a series of algorithm designs\nincluding suffix infilling, temporal graph attention with self-loop, and causal\nmasking self-attention to make TF-TGN work. During training, existing systems\nare slow in transforming the graph topology and conducting graph sampling. As\nsuch, we propose methods to parallelize the CSR format conversion and graph\nsampling. We also adapt Transformer codebase to train TF-TGN efficiently with\nmultiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art\nTGNN training frameworks. The results show that TF-TGN can accelerate training\nby over 2.20 while providing comparable or even superior accuracy to existing\nSOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.",
    "published": "2024-09-09T10:11:25Z",
    "updated": "2024-09-18T09:15:10Z",
    "authors": [
      "Qiang Huang",
      "Xiao Yan",
      "Xin Wang",
      "Susie Xi Rao",
      "Zhichao Han",
      "Fangcheng Fu",
      "Wentao Zhang",
      "Jiawei Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.00630v1",
    "title": "STAA: Spatio-Temporal Attention Attribution for Real-Time Interpreting\n  Transformer-based Video Models",
    "summary": "Transformer-based models have achieved state-of-the-art performance in\nvarious computer vision tasks, including image and video analysis. However,\nTransformer's complex architecture and black-box nature pose challenges for\nexplainability, a crucial aspect for real-world applications and scientific\ninquiry. Current Explainable AI (XAI) methods can only provide one-dimensional\nfeature importance, either spatial or temporal explanation, with significant\ncomputational complexity. This paper introduces STAA (Spatio-Temporal Attention\nAttribution), an XAI method for interpreting video Transformer models. Differ\nfrom traditional methods that separately apply image XAI techniques for spatial\nfeatures or segment contribution analysis for temporal aspects, STAA offers\nboth spatial and temporal information simultaneously from attention values in\nTransformers. The study utilizes the Kinetics-400 dataset, a benchmark\ncollection of 400 human action classes used for action recognition research. We\nintroduce metrics to quantify explanations. We also apply optimization to\nenhance STAA's raw output. By implementing dynamic thresholding and attention\nfocusing mechanisms, we improve the signal-to-noise ratio in our explanations,\nresulting in more precise visualizations and better evaluation results. In\nterms of computational overhead, our method requires less than 3\\% of the\ncomputational resources of traditional XAI methods, making it suitable for\nreal-time video XAI analysis applications. STAA contributes to the growing\nfield of XAI by offering a method for researchers and practitioners to analyze\nTransformer models.",
    "published": "2024-11-01T14:40:07Z",
    "updated": "2024-11-01T14:40:07Z",
    "authors": [
      "Zerui Wang",
      "Yan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20443v2",
    "title": "Provable In-Context Learning of Nonlinear Regression with Transformers",
    "summary": "The transformer architecture, which processes sequences of input tokens to\nproduce outputs for query tokens, has revolutionized numerous areas of machine\nlearning. A defining feature of transformers is their ability to perform\npreviously unseen tasks using task specific prompts without updating\nparameters, a phenomenon known as in-context learning (ICL). Recent research\nhas actively explored the training dynamics behind ICL, with much of the focus\non relatively simple tasks such as linear regression and binary classification.\nTo advance the theoretical understanding of ICL, this paper investigates more\ncomplex nonlinear regression tasks, aiming to uncover how transformers acquire\nin-context learning capabilities in these settings. We analyze the stage-wise\ndynamics of attention during training: attention scores between a query token\nand its target features grow rapidly in the early phase, then gradually\nconverge to one, while attention to irrelevant features decays more slowly and\nexhibits oscillatory behavior. Our analysis introduces new proof techniques\nthat explicitly characterize how the nature of general non-degenerate\n$L$-Lipschitz task functions affects attention weights. Specifically, we\nidentify that the Lipschitz constant $L$ of nonlinear function classes as a key\nfactor governing the convergence dynamics of transformers in ICL. Leveraging\nthese insights, for two distinct regimes depending on whether $L$ is below or\nabove a threshold, we derive different time bounds to guarantee near-zero\nprediction error. Notably, despite the convergence time depending on the\nunderlying task functions, we prove that query tokens consistently attend to\nprompt tokens with highly relevant features at convergence, demonstrating the\nICL capability of transformers for unseen functions.",
    "published": "2025-07-28T00:09:28Z",
    "updated": "2025-10-01T00:43:34Z",
    "authors": [
      "Hongbo Li",
      "Lingjie Duan",
      "Yingbin Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.16575v1",
    "title": "ViT-Transformer: Self-attention mechanism based constitutive modeling\n  for nonlinear heterogeneous materials",
    "summary": "Multi-scale simulations of nonlinear heterogeneous materials and composites\nare challenging due to the prohibitive computational costs of high-fidelity\nsimulations. Recently, machine learning (ML) based approaches have emerged as\npromising alternatives to traditional multiscale methods. However, existing ML\nsurrogate constitutive models struggle in capturing long-range dependencies and\ngeneralization across microstructures. The recent advancements in\nattention-based Transformer architectures open the door to a more powerful\nclass of surrogate models. Attention mechanism has demonstrated remarkable\ncapabilities in natural language processing and computer vision. In this work,\nwe introduce a surrogate (meta) model, namely ViT-Transformer, using a Vision\nTransformer (ViT) encoder and a Transformer-based decoder which are both driven\nby the self-attention mechanism. The ViT encoder extracts microstructural\nfeatures from material images, while the decoder is a masked Transformer\nencoder that combines the latent geometrical features with the macroscopic\nstrain input sequence to predict the corresponding stress response. To enhance\ntraining, we propose a random extract training algorithm that improves\nrobustness to sequences of variable length. We design and construct a compact\nyet diverse dataset via data augmentation, and validate the surrogate model\nusing various composite material images and loading scenarios. Several\nnumerical examples are provided to show the effectiveness and accuracy of the\nViT-Transformer model and the training algorithm.",
    "published": "2025-10-18T16:52:57Z",
    "updated": "2025-10-18T16:52:57Z",
    "authors": [
      "Yijing Zhou",
      "Shabnam J. Semnani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.02651v1",
    "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
    "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
    "published": "2025-11-04T15:17:43Z",
    "updated": "2025-11-04T15:17:43Z",
    "authors": [
      "Oleksiy Ostapenko",
      "Luke Kumar",
      "Raymond Li",
      "Denis Kocetkov",
      "Joel Lamy-Poirier",
      "Shruthan Radhakrishna",
      "Soham Parikh",
      "Shambhavi Mishra",
      "Sebastien Paquet",
      "Srinivas Sunkara",
      "ValÃ©rie BÃ©caert",
      "Sathwik Tejaswi Madhusudhan",
      "Torsten Scholak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/0809.3230v2",
    "title": "SchrÃ¶dinger Operators Defined by Interval Exchange Transformations",
    "summary": "We discuss discrete one-dimensional Schr\\\"odinger operators whose potentials\nare generated by an invertible ergodic transformation of a compact metric space\nand a continuous real-valued sampling function. We pay particular attention to\nthe case where the transformation is a minimal interval exchange\ntransformation. Results about the spectrum and the spectral type of these\noperators are established. In particular, we provide the first examples of\ntransformations for which the associated Schr\\\"odinger operators have purely\nsingular spectrum for every non-constant continuous sampling function.",
    "published": "2008-09-18T19:22:44Z",
    "updated": "2009-05-15T19:48:23Z",
    "authors": [
      "Jon Chaika",
      "David Damanik",
      "Helge Krueger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1511.05014v1",
    "title": "Slice Fourier transform and convolutions",
    "summary": "Recently the construction of various integral transforms for slice monogenic\nfunctions has gained a lot of attention. In line with these developments, the\narticle at hand introduces the slice Fourier transform. In the first part, the\nkernel function of this integral transform is constructed using the Mehler\nformula. An explicit expression for the integral transform is obtained and\nallows for the study of its properties. In the second part, two kinds of\ncorresponding convolutions are examined: Mustard convolutions and convolutions\nbased on generalised translation operators. The paper finishes by demonstrating\nthe connection between both.",
    "published": "2015-11-16T16:11:06Z",
    "updated": "2015-11-16T16:11:06Z",
    "authors": [
      "Lander Cnudde",
      "Hendrik De Bie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.13295v1",
    "title": "Laplace Transform Effectiveness in the MGinf Queue Busy Period\n  Probabilistic Study",
    "summary": "The Laplace transform is a widely used tool in the study of probability\ndistributions, often allowing for a probability density functions and\ndistribution functions simpler determination and being a moments generating\nfunction. In this paper it is considered a situation not so simple, as it is\nthe case of the MGinf queue busy period length distribution. Attention will\nalso be given the respective tail Laplace transform. Then, in the context of an\nopen queues network, which nodes behave as MGinf queues, the Laplace transform\nwill be used to construct an algorithm to determine the Laplace transform of\nthe global service time length of a customer during their stay on the network\ndistribution.",
    "published": "2021-09-27T18:32:25Z",
    "updated": "2021-09-27T18:32:25Z",
    "authors": [
      "Manuel Alberto M. Ferreira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.04337v1",
    "title": "Adversarial Token Attacks on Vision Transformers",
    "summary": "Vision transformers rely on a patch token based self attention mechanism, in\ncontrast to convolutional networks. We investigate fundamental differences\nbetween these two families of models, by designing a block sparsity based\nadversarial token attack. We probe and analyze transformer as well as\nconvolutional models with token attacks of varying patch sizes. We infer that\ntransformer models are more sensitive to token attacks than convolutional\nmodels, with ResNets outperforming Transformer models by up to $\\sim30\\%$ in\nrobust accuracy for single token attacks.",
    "published": "2021-10-08T19:00:16Z",
    "updated": "2021-10-08T19:00:16Z",
    "authors": [
      "Ameya Joshi",
      "Gauri Jagatap",
      "Chinmay Hegde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.03041v1",
    "title": "Vision Transformers: State of the Art and Research Challenges",
    "summary": "Transformers have achieved great success in natural language processing. Due\nto the powerful capability of self-attention mechanism in transformers,\nresearchers develop the vision transformers for a variety of computer vision\ntasks, such as image recognition, object detection, image segmentation, pose\nestimation, and 3D reconstruction. This paper presents a comprehensive overview\nof the literature on different architecture designs and training tricks\n(including self-supervised learning) for vision transformers. Our goal is to\nprovide a systematic review with the open research opportunities.",
    "published": "2022-07-07T02:01:56Z",
    "updated": "2022-07-07T02:01:56Z",
    "authors": [
      "Bo-Kai Ruan",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.14678v1",
    "title": "Exploring Transformer Backbones for Image Diffusion Models",
    "summary": "We present an end-to-end Transformer based Latent Diffusion model for image\nsynthesis. On the ImageNet class conditioned generation task we show that a\nTransformer based Latent Diffusion model achieves a 14.1FID which is comparable\nto the 13.1FID score of a UNet based architecture. In addition to showing the\napplication of Transformer models for Diffusion based image synthesis this\nsimplification in architecture allows easy fusion and modeling of text and\nimage data. The multi-head attention mechanism of Transformers enables\nsimplified interaction between the image and text features which removes the\nrequirement for crossattention mechanism in UNet based Diffusion models.",
    "published": "2022-12-27T07:05:14Z",
    "updated": "2022-12-27T07:05:14Z",
    "authors": [
      "Princy Chahal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27118v1",
    "title": "Probability Distributions Computed by Hard-Attention Transformers",
    "summary": "Most expressivity results for transformers treat them as language recognizers\n(which accept or reject strings), and not as they are used in practice, as\nlanguage models (which generate strings autoregressively and\nprobabilistically). Here, we characterize the probability distributions that\ntransformer language models can express. We show that making transformer\nlanguage recognizers autoregressive can sometimes increase their expressivity,\nand that making them probabilistic can break equivalences that hold in the\nnon-probabilistic case. Our overall contribution is to tease apart what\nfunctions transformers are capable of expressing, in their most common use-case\nas language models.",
    "published": "2025-10-31T02:41:05Z",
    "updated": "2025-10-31T02:41:05Z",
    "authors": [
      "Andy Yang",
      "Anej Svete",
      "Jiaoda Li",
      "Anthony Widjaja Lin",
      "Jonathan Rawski",
      "Ryan Cotterell",
      "David Chiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2008.05828v1",
    "title": "On the Importance of Local Information in Transformer Based Models",
    "summary": "The self-attention module is a key component of Transformer-based models,\nwherein each token pays attention to every other token. Recent studies have\nshown that these heads exhibit syntactic, semantic, or local behaviour. Some\nstudies have also identified promise in restricting this attention to be local,\ni.e., a token attending to other tokens only in a small neighbourhood around\nit. However, no conclusive evidence exists that such local attention alone is\nsufficient to achieve high accuracy on multiple NLP tasks. In this work, we\nsystematically analyse the role of locality information in learnt models and\ncontrast it with the role of syntactic information. More specifically, we first\ndo a sensitivity analysis and show that, at every layer, the representation of\na token is much more sensitive to tokens in a small neighborhood around it than\nto tokens which are syntactically related to it. We then define an attention\nbias metric to determine whether a head pays more attention to local tokens or\nto syntactically related tokens. We show that a larger fraction of heads have a\nlocality bias as compared to a syntactic bias. Having established the\nimportance of local attention heads, we train and evaluate models where varying\nfractions of the attention heads are constrained to be local. Such models would\nbe more efficient as they would have fewer computations in the attention layer.\nWe evaluate these models on 4 GLUE datasets (QQP, SST-2, MRPC, QNLI) and 2 MT\ndatasets (En-De, En-Ru) and clearly demonstrate that such constrained models\nhave comparable performance to the unconstrained models. Through this\nsystematic evaluation we establish that attention in Transformer-based models\ncan be constrained to be local without affecting performance.",
    "published": "2020-08-13T11:32:47Z",
    "updated": "2020-08-13T11:32:47Z",
    "authors": [
      "Madhura Pande",
      "Aakriti Budhraja",
      "Preksha Nema",
      "Pratyush Kumar",
      "Mitesh M. Khapra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1809.04281v3",
    "title": "Music Transformer",
    "summary": "Music relies heavily on repetition to build structure and meaning.\nSelf-reference occurs on multiple timescales, from motifs to phrases to reusing\nof entire sections of music, such as in pieces with ABA structure. The\nTransformer (Vaswani et al., 2017), a sequence model based on self-attention,\nhas achieved compelling results in many generation tasks that require\nmaintaining long-range coherence. This suggests that self-attention might also\nbe well-suited to modeling music. In musical composition and performance,\nhowever, relative timing is critically important. Existing approaches for\nrepresenting relative positional information in the Transformer modulate\nattention based on pairwise distance (Shaw et al., 2018). This is impractical\nfor long sequences such as musical compositions since their memory complexity\nfor intermediate relative information is quadratic in the sequence length. We\npropose an algorithm that reduces their intermediate memory requirement to\nlinear in the sequence length. This enables us to demonstrate that a\nTransformer with our modified relative attention mechanism can generate\nminute-long compositions (thousands of steps, four times the length modeled in\nOore et al., 2018) with compelling structure, generate continuations that\ncoherently elaborate on a given motif, and in a seq2seq setup generate\naccompaniments conditioned on melodies. We evaluate the Transformer with our\nrelative attention mechanism on two datasets, JSB Chorales and\nPiano-e-Competition, and obtain state-of-the-art results on the latter.",
    "published": "2018-09-12T07:15:26Z",
    "updated": "2018-12-12T07:42:08Z",
    "authors": [
      "Cheng-Zhi Anna Huang",
      "Ashish Vaswani",
      "Jakob Uszkoreit",
      "Noam Shazeer",
      "Ian Simon",
      "Curtis Hawthorne",
      "Andrew M. Dai",
      "Matthew D. Hoffman",
      "Monica Dinculescu",
      "Douglas Eck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.12292v1",
    "title": "Contextual Transformer Networks for Visual Recognition",
    "summary": "Transformer with self-attention has led to the revolutionizing of natural\nlanguage processing field, and recently inspires the emergence of\nTransformer-style architecture design with competitive results in numerous\ncomputer vision tasks. Nevertheless, most of existing designs directly employ\nself-attention over a 2D feature map to obtain the attention matrix based on\npairs of isolated queries and keys at each spatial location, but leave the rich\ncontexts among neighbor keys under-exploited. In this work, we design a novel\nTransformer-style module, i.e., Contextual Transformer (CoT) block, for visual\nrecognition. Such design fully capitalizes on the contextual information among\ninput keys to guide the learning of dynamic attention matrix and thus\nstrengthens the capacity of visual representation. Technically, CoT block first\ncontextually encodes input keys via a $3\\times3$ convolution, leading to a\nstatic contextual representation of inputs. We further concatenate the encoded\nkeys with input queries to learn the dynamic multi-head attention matrix\nthrough two consecutive $1\\times1$ convolutions. The learnt attention matrix is\nmultiplied by input values to achieve the dynamic contextual representation of\ninputs. The fusion of the static and dynamic contextual representations are\nfinally taken as outputs. Our CoT block is appealing in the view that it can\nreadily replace each $3\\times3$ convolution in ResNet architectures, yielding a\nTransformer-style backbone named as Contextual Transformer Networks (CoTNet).\nThrough extensive experiments over a wide range of applications (e.g., image\nrecognition, object detection and instance segmentation), we validate the\nsuperiority of CoTNet as a stronger backbone. Source code is available at\n\\url{https://github.com/JDAI-CV/CoTNet}.",
    "published": "2021-07-26T16:00:21Z",
    "updated": "2021-07-26T16:00:21Z",
    "authors": [
      "Yehao Li",
      "Ting Yao",
      "Yingwei Pan",
      "Tao Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.07677v2",
    "title": "Transformers learn in-context by gradient descent",
    "summary": "At present, the mechanisms of in-context learning in Transformers are not\nwell understood and remain mostly an intuition. In this paper, we suggest that\ntraining Transformers on auto-regressive objectives is closely related to\ngradient-based meta-learning formulations. We start by providing a simple\nweight construction that shows the equivalence of data transformations induced\nby 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a\nregression loss. Motivated by that construction, we show empirically that when\ntraining self-attention-only Transformers on simple regression tasks either the\nmodels learned by GD and Transformers show great similarity or, remarkably, the\nweights found by optimization match the construction. Thus we show how trained\nTransformers become mesa-optimizers i.e. learn models by gradient descent in\ntheir forward pass. This allows us, at least in the domain of regression\nproblems, to mechanistically understand the inner workings of in-context\nlearning in optimized Transformers. Building on this insight, we furthermore\nidentify how Transformers surpass the performance of plain gradient descent by\nlearning an iterative curvature correction and learn linear models on deep data\nrepresentations to solve non-linear regression tasks. Finally, we discuss\nintriguing parallels to a mechanism identified to be crucial for in-context\nlearning termed induction-head (Olsson et al., 2022) and show how it could be\nunderstood as a specific case of in-context learning by gradient descent\nlearning within Transformers. Code to reproduce the experiments can be found at\nhttps://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "published": "2022-12-15T09:21:21Z",
    "updated": "2023-05-31T08:59:47Z",
    "authors": [
      "Johannes von Oswald",
      "Eyvind Niklasson",
      "Ettore Randazzo",
      "JoÃ£o Sacramento",
      "Alexander Mordvintsev",
      "Andrey Zhmoginov",
      "Max Vladymyrov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.16177v2",
    "title": "Transformer Meets Gated Residual Networks To Enhance Photoplethysmogram\n  Artifact Detection Informed by Mutual Information Neural Estimation",
    "summary": "This study delves into the effectiveness of various learning methods in\nimproving Transformer models, focusing particularly on the Gated Residual\nNetwork Transformer (GRN-Transformer) in the context of pediatric intensive\ncare units (PICU) with limited data availability. Our findings indicate that\nTransformers trained via supervised learning are less effective compared to\nMLP, CNN, and LSTM networks in such environments. Yet, leveraging unsupervised\nand self-supervised learning on unannotated data, with subsequent fine-tuning\non annotated data, notably enhances Transformer performance, although not to\nthe level of the GRN-Transformer. Central to our research is the analysis of\ndifferent activation functions for the Gated Linear Unit (GLU), a crucial\nelement of the GRN structure. We also employ Mutual Information Neural\nEstimation (MINE) to evaluate the GRN's contribution. Additionally, the study\nexamines the effects of integrating GRN within the Transformer's Attention\nmechanism versus using it as a separate intermediary layer. Our results\nhighlight that GLU with sigmoid activation stands out, achieving 0.98 accuracy,\n0.91 precision, 0.96 recall, and 0.94 F1 score. The MINE analysis supports the\nhypothesis that GRN enhances the mutual information between the hidden\nrepresentations and the output. Moreover, the use of GRN as an intermediate\nfilter layer proves more beneficial than incorporating it within the Attention\nmechanism. In summary, this research clarifies how GRN bolsters\nGRN-Transformer's performance, surpassing other learning techniques. These\nfindings offer a promising avenue for adopting sophisticated models like\nTransformers in data-constrained environments, such as PPG artifact detection\nin PICU settings.",
    "published": "2024-05-25T11:07:42Z",
    "updated": "2025-05-25T16:21:05Z",
    "authors": [
      "Thanh-Dung Le",
      "Clara Macabiau",
      "KÃ©vin Albert",
      "Symeon Chatzinotas",
      "Philippe Jouvet",
      "Rita Noumeir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16762v1",
    "title": "A Transformer-in-Transformer Network Utilizing Knowledge Distillation\n  for Image Recognition",
    "summary": "This paper presents a novel knowledge distillation neural architecture\nleveraging efficient transformer networks for effective image classification.\nNatural images display intricate arrangements encompassing numerous extraneous\nelements. Vision transformers utilize localized patches to compute attention.\nHowever, exclusive dependence on patch segmentation proves inadequate in\nsufficiently encompassing the comprehensive nature of the image. To address\nthis issue, we have proposed an inner-outer transformer-based architecture,\nwhich gives attention to the global and local aspects of the image. Moreover,\nThe training of transformer models poses significant challenges due to their\ndemanding resource, time, and data requirements. To tackle this, we integrate\nknowledge distillation into the architecture, enabling efficient learning.\nLeveraging insights from a larger teacher model, our approach enhances learning\nefficiency and effectiveness. Significantly, the transformer-in-transformer\nnetwork acquires lightweight characteristics by means of distillation conducted\nwithin the feature extraction layer. Our featured network's robustness is\nestablished through substantial experimentation on the MNIST, CIFAR10, and\nCIFAR100 datasets, demonstrating commendable top-1 and top-5 accuracy. The\nconducted ablative analysis comprehensively validates the effectiveness of the\nchosen parameters and settings, showcasing their superiority against\ncontemporary methodologies. Remarkably, the proposed Transformer-in-Transformer\nNetwork (TITN) model achieves impressive performance milestones across various\ndatasets: securing the highest top-1 accuracy of 74.71% and a top-5 accuracy of\n92.28% for the CIFAR100 dataset, attaining an unparalleled top-1 accuracy of\n92.03% and top-5 accuracy of 99.80% for the CIFAR-10 dataset, and registering\nan exceptional top-1 accuracy of 99.56% for the MNIST dataset.",
    "published": "2025-02-24T00:41:46Z",
    "updated": "2025-02-24T00:41:46Z",
    "authors": [
      "Dewan Tauhid Rahman",
      "Yeahia Sarker",
      "Antar Mazumder",
      "Md. Shamim Anower"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1907.01470v1",
    "title": "Augmenting Self-attention with Persistent Memory",
    "summary": "Transformer networks have lead to important progress in language modeling and\nmachine translation. These models include two consecutive modules, a\nfeed-forward layer and a self-attention layer. The latter allows the network to\ncapture long term dependencies and are often regarded as the key ingredient in\nthe success of Transformers. Building upon this intuition, we propose a new\nmodel that solely consists of attention layers. More precisely, we augment the\nself-attention layers with persistent memory vectors that play a similar role\nas the feed-forward layer. Thanks to these vectors, we can remove the\nfeed-forward layer without degrading the performance of a transformer. Our\nevaluation shows the benefits brought by our model on standard character and\nword level language modeling benchmarks.",
    "published": "2019-07-02T15:56:20Z",
    "updated": "2019-07-02T15:56:20Z",
    "authors": [
      "Sainbayar Sukhbaatar",
      "Edouard Grave",
      "Guillaume Lample",
      "Herve Jegou",
      "Armand Joulin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1908.11365v1",
    "title": "Improving Deep Transformer with Depth-Scaled Initialization and Merged\n  Attention",
    "summary": "The general trend in NLP is towards increasing model capacity and performance\nvia deeper neural networks. However, simply stacking more layers of the popular\nTransformer architecture for machine translation results in poor convergence\nand high computational overhead. Our empirical analysis suggests that\nconvergence is poor due to gradient vanishing caused by the interaction between\nresidual connections and layer normalization. We propose depth-scaled\ninitialization (DS-Init), which decreases parameter variance at the\ninitialization stage, and reduces output variance of residual connections so as\nto ease gradient back-propagation through normalization layers. To address\ncomputational cost, we propose a merged attention sublayer (MAtt) which\ncombines a simplified averagebased self-attention sublayer and the\nencoderdecoder attention sublayer on the decoder side. Results on WMT and IWSLT\ntranslation tasks with five translation directions show that deep Transformers\nwith DS-Init and MAtt can substantially outperform their base counterpart in\nterms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the\ndecoding speed of the baseline model thanks to the efficiency improvements of\nMAtt.",
    "published": "2019-08-29T17:50:55Z",
    "updated": "2019-08-29T17:50:55Z",
    "authors": [
      "Biao Zhang",
      "Ivan Titov",
      "Rico Sennrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.09164v2",
    "title": "Point Transformer",
    "summary": "Self-attention networks have revolutionized natural language processing and\nare making impressive strides in image analysis tasks such as image\nclassification and object detection. Inspired by this success, we investigate\nthe application of self-attention networks to 3D point cloud processing. We\ndesign self-attention layers for point clouds and use these to construct\nself-attention networks for tasks such as semantic scene segmentation, object\npart segmentation, and object classification. Our Point Transformer design\nimproves upon prior work across domains and tasks. For example, on the\nchallenging S3DIS dataset for large-scale semantic scene segmentation, the\nPoint Transformer attains an mIoU of 70.4% on Area 5, outperforming the\nstrongest prior model by 3.3 absolute percentage points and crossing the 70%\nmIoU threshold for the first time.",
    "published": "2020-12-16T18:58:56Z",
    "updated": "2021-09-26T15:33:47Z",
    "authors": [
      "Hengshuang Zhao",
      "Li Jiang",
      "Jiaya Jia",
      "Philip Torr",
      "Vladlen Koltun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.13673v1",
    "title": "Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS",
    "summary": "This paper presents Nana-HDR, a new non-attentive non-autoregressive model\nwith hybrid Transformer-based Dense-fuse encoder and RNN-based decoder for TTS.\nIt mainly consists of three parts: Firstly, a novel Dense-fuse encoder with\ndense connections between basic Transformer blocks for coarse feature fusion\nand a multi-head attention layer for fine feature fusion. Secondly, a\nsingle-layer non-autoregressive RNN-based decoder. Thirdly, a duration\npredictor instead of an attention model that connects the above hybrid encoder\nand decoder. Experiments indicate that Nana-HDR gives full play to the\nadvantages of each component, such as strong text encoding ability of\nTransformer-based encoder, stateful decoding without being bothered by exposure\nbias and local information preference, and stable alignment provided by\nduration predictor. Due to these advantages, Nana-HDR achieves competitive\nperformance in naturalness and robustness on two Mandarin corpora.",
    "published": "2021-09-28T12:45:14Z",
    "updated": "2021-09-28T12:45:14Z",
    "authors": [
      "Shilun Lin",
      "Wenchao Su",
      "Li Meng",
      "Fenglong Xie",
      "Xinhui Li",
      "Li Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.07731v3",
    "title": "Efficient Linear Attention for Fast and Accurate Keypoint Matching",
    "summary": "Recently Transformers have provided state-of-the-art performance in sparse\nmatching, crucial to realize high-performance 3D vision applications. Yet,\nthese Transformers lack efficiency due to the quadratic computational\ncomplexity of their attention mechanism. To solve this problem, we employ an\nefficient linear attention for the linear computational complexity. Then, we\npropose a new attentional aggregation that achieves high accuracy by\naggregating both the global and local information from sparse keypoints. To\nfurther improve the efficiency, we propose the joint learning of feature\nmatching and description. Our learning enables simpler and faster matching than\nSinkhorn, often used in matching the learned descriptors from Transformers. Our\nmethod achieves competitive performance with only 0.84M learnable parameters\nagainst the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M\nparameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.",
    "published": "2022-04-16T06:17:36Z",
    "updated": "2022-04-22T16:16:26Z",
    "authors": [
      "Suwichaya Suwanwimolkul",
      "Satoshi Komorita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1808.02128v2",
    "title": "Attentive Semantic Alignment with Offset-Aware Correlation Kernels",
    "summary": "Semantic correspondence is the problem of establishing correspondences across\nimages depicting different instances of the same object or scene class. One of\nrecent approaches to this problem is to estimate parameters of a global\ntransformation model that densely aligns one image to the other. Since an\nentire correlation map between all feature pairs across images is typically\nused to predict such a global transformation, noisy features from different\nbackgrounds, clutter, and occlusion distract the predictor from correct\nestimation of the alignment. This is a challenging issue, in particular, in the\nproblem of semantic correspondence where a large degree of image variations is\noften involved. In this paper, we introduce an attentive semantic alignment\nmethod that focuses on reliable correlations, filtering out distractors. For\neffective attention, we also propose an offset-aware correlation kernel that\nlearns to capture translation-invariant local transformations in computing\ncorrelation values over spatial locations. Experiments demonstrate the\neffectiveness of the attentive model and offset-aware kernel, and the proposed\nmodel combining both techniques achieves the state-of-the-art performance.",
    "published": "2018-08-06T21:42:57Z",
    "updated": "2018-10-26T07:55:18Z",
    "authors": [
      "Paul Hongsuck Seo",
      "Jongmin Lee",
      "Deunsol Jung",
      "Bohyung Han",
      "Minsu Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.05709v1",
    "title": "Shapley Head Pruning: Identifying and Removing Interference in\n  Multilingual Transformers",
    "summary": "Multilingual transformer-based models demonstrate remarkable zero and\nfew-shot transfer across languages by learning and reusing language-agnostic\nfeatures. However, as a fixed-size model acquires more languages, its\nperformance across all languages degrades, a phenomenon termed interference.\nOften attributed to limited model capacity, interference is commonly addressed\nby adding additional parameters despite evidence that transformer-based models\nare overparameterized. In this work, we show that it is possible to reduce\ninterference by instead identifying and pruning language-specific parameters.\nFirst, we use Shapley Values, a credit allocation metric from coalitional game\ntheory, to identify attention heads that introduce interference. Then, we show\nthat removing identified attention heads from a fixed model improves\nperformance for a target language on both sentence classification and\nstructural prediction, seeing gains as large as 24.7\\%. Finally, we provide\ninsights on language-agnostic and language-specific attention heads using\nattention visualization.",
    "published": "2022-10-11T18:11:37Z",
    "updated": "2022-10-11T18:11:37Z",
    "authors": [
      "William Held",
      "Diyi Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.14391v1",
    "title": "Can Transformer Attention Spread Give Insights Into Uncertainty of\n  Detected and Tracked Objects?",
    "summary": "Transformers have recently been utilized to perform object detection and\ntracking in the context of autonomous driving. One unique characteristic of\nthese models is that attention weights are computed in each forward pass,\ngiving insights into the model's interior, in particular, which part of the\ninput data it deemed interesting for the given task. Such an attention matrix\nwith the input grid is available for each detected (or tracked) object in every\ntransformer decoder layer. In this work, we investigate the distribution of\nthese attention weights: How do they change through the decoder layers and\nthrough the lifetime of a track? Can they be used to infer additional\ninformation about an object, such as a detection uncertainty? Especially in\nunstructured environments, or environments that were not common during\ntraining, a reliable measure of detection uncertainty is crucial to decide\nwhether the system can still be trusted or not.",
    "published": "2022-10-26T00:05:16Z",
    "updated": "2022-10-26T00:05:16Z",
    "authors": [
      "Felicia Ruppel",
      "Florian Faion",
      "Claudius GlÃ¤ser",
      "Klaus Dietmayer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1706.03581v1",
    "title": "Enriched Deep Recurrent Visual Attention Model for Multiple Object\n  Recognition",
    "summary": "We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an\nimproved attention-based architecture for multiple object recognition. The\nproposed model is a fully differentiable unit that can be optimized end-to-end\nby using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was\nemployed as visual attention mechanism which allows to learn the geometric\ntransformation of objects within images. With the combination of the Spatial\nTransformer and the powerful recurrent architecture, the proposed EDRAM can\nlocalize and recognize objects simultaneously. EDRAM has been evaluated on two\npublicly available datasets including MNIST Cluttered (with 70K cluttered\ndigits) and SVHN (with up to 250k real world images of house numbers).\nExperiments show that it obtains superior performance as compared with the\nstate-of-the-art models.",
    "published": "2017-06-12T11:55:35Z",
    "updated": "2017-06-12T11:55:35Z",
    "authors": [
      "Artsiom Ablavatski",
      "Shijian Lu",
      "Jianfei Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1904.10509v1",
    "title": "Generating Long Sequences with Sparse Transformers",
    "summary": "Transformers are powerful sequence models, but require time and memory that\ngrows quadratically with the sequence length. In this paper we introduce sparse\nfactorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We\nalso introduce a) a variation on architecture and initialization to train\ndeeper networks, b) the recomputation of attention matrices to save memory, and\nc) fast attention kernels for training. We call networks with these changes\nSparse Transformers, and show they can model sequences tens of thousands of\ntimesteps long using hundreds of layers. We use the same architecture to model\nimages, audio, and text from raw bytes, setting a new state of the art for\ndensity modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate\nunconditional samples that demonstrate global coherence and great diversity,\nand show it is possible in principle to use self-attention to model sequences\nof length one million or more.",
    "published": "2019-04-23T19:29:47Z",
    "updated": "2019-04-23T19:29:47Z",
    "authors": [
      "Rewon Child",
      "Scott Gray",
      "Alec Radford",
      "Ilya Sutskever"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.12023v1",
    "title": "Comprehensive Attention Self-Distillation for Weakly-Supervised Object\n  Detection",
    "summary": "Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to\ntrain object detectors using only the image-level category labels. However,\nwithout object-level labels, WSOD detectors are prone to detect bounding boxes\non salient objects, clustered objects and discriminative object parts.\nMoreover, the image-level category labels do not enforce consistent object\ndetection across different transformations of the same images. To address the\nabove issues, we propose a Comprehensive Attention Self-Distillation (CASD)\ntraining approach for WSOD. To balance feature learning among all object\ninstances, CASD computes the comprehensive attention aggregated from multiple\ntransformations and feature layers of the same images. To enforce consistent\nspatial supervision on objects, CASD conducts self-distillation on the WSOD\nnetworks, such that the comprehensive attention is approximated simultaneously\nby multiple transformations and feature layers of the same images. CASD\nproduces new state-of-the-art WSOD results on standard benchmarks such as\nPASCAL VOC 2007/2012 and MS-COCO.",
    "published": "2020-10-22T20:13:32Z",
    "updated": "2020-10-22T20:13:32Z",
    "authors": [
      "Zeyi Huang",
      "Yang Zou",
      "Vijayakumar Bhagavatula",
      "Dong Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.12423v3",
    "title": "GraphSpeech: Syntax-Aware Graph Attention Network For Neural Speech\n  Synthesis",
    "summary": "Attention-based end-to-end text-to-speech synthesis (TTS) is superior to\nconventional statistical methods in many ways. Transformer-based TTS is one of\nsuch successful implementations. While Transformer TTS models the speech frame\nsequence well with a self-attention mechanism, it does not associate input text\nwith output utterances from a syntactic point of view at sentence level. We\npropose a novel neural TTS model, denoted as GraphSpeech, that is formulated\nunder graph neural network framework. GraphSpeech encodes explicitly the\nsyntactic relation of input lexical tokens in a sentence, and incorporates such\ninformation to derive syntactically motivated character embeddings for TTS\nattention mechanism. Experiments show that GraphSpeech consistently outperforms\nthe Transformer TTS baseline in terms of spectrum and prosody rendering of\nutterances.",
    "published": "2020-10-23T14:14:06Z",
    "updated": "2021-03-26T13:21:02Z",
    "authors": [
      "Rui Liu",
      "Berrak Sisman",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.10271v1",
    "title": "Convolutional Xformers for Vision",
    "summary": "Vision transformers (ViTs) have found only limited practical use in\nprocessing images, in spite of their state-of-the-art accuracy on certain\nbenchmarks. The reason for their limited use include their need for larger\ntraining datasets and more computational resources compared to convolutional\nneural networks (CNNs), owing to the quadratic complexity of their\nself-attention mechanism. We propose a linear attention-convolution hybrid\narchitecture -- Convolutional X-formers for Vision (CXV) -- to overcome these\nlimitations. We replace the quadratic attention with linear attention\nmechanisms, such as Performer, Nystr\\\"omformer, and Linear Transformer, to\nreduce its GPU usage. Inductive prior for image data is provided by\nconvolutional sub-layers, thereby eliminating the need for class token and\npositional embeddings used by the ViTs. We also propose a new training method\nwhere we use two different optimizers during different phases of training and\nshow that it improves the top-1 image classification accuracy across different\narchitectures. CXV outperforms other architectures, token mixers (e.g.\nConvMixer, FNet and MLP Mixer), transformer models (e.g. ViT, CCT, CvT and\nhybrid Xformers), and ResNets for image classification in scenarios with\nlimited data and GPU resources (cores, RAM, power).",
    "published": "2022-01-25T12:32:09Z",
    "updated": "2022-01-25T12:32:09Z",
    "authors": [
      "Pranav Jeevan",
      "Amit sethi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.11781v2",
    "title": "RadioTransformer: A Cascaded Global-Focal Transformer for Visual\n  Attention-guided Disease Classification",
    "summary": "In this work, we present RadioTransformer, a novel visual attention-driven\ntransformer framework, that leverages radiologists' gaze patterns and models\ntheir visuo-cognitive behavior for disease diagnosis on chest radiographs.\nDomain experts, such as radiologists, rely on visual information for medical\nimage interpretation. On the other hand, deep neural networks have demonstrated\nsignificant promise in similar tasks even where visual interpretation is\nchallenging. Eye-gaze tracking has been used to capture the viewing behavior of\ndomain experts, lending insights into the complexity of visual search. However,\ndeep learning frameworks, even those that rely on attention mechanisms, do not\nleverage this rich domain information. RadioTransformer fills this critical gap\nby learning from radiologists' visual search patterns, encoded as 'human visual\nattention regions' in a cascaded global-focal transformer framework. The\noverall 'global' image characteristics and the more detailed 'local' features\nare captured by the proposed global and focal modules, respectively. We\nexperimentally validate the efficacy of our student-teacher approach for 8\ndatasets involving different disease classification tasks where eye-gaze data\nis not available during the inference phase. Code:\nhttps://github.com/bmi-imaginelab/radiotransformer.",
    "published": "2022-02-23T20:52:30Z",
    "updated": "2022-07-21T20:36:16Z",
    "authors": [
      "Moinak Bhattacharya",
      "Shubham Jain",
      "Prateek Prasanna"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.13138v1",
    "title": "ClusTR: Exploring Efficient Self-attention via Clustering for Vision\n  Transformers",
    "summary": "Although Transformers have successfully transitioned from their language\nmodelling origins to image-based applications, their quadratic computational\ncomplexity remains a challenge, particularly for dense prediction. In this\npaper we propose a content-based sparse attention method, as an alternative to\ndense self-attention, aiming to reduce the computation complexity while\nretaining the ability to model long-range dependencies. Specifically, we\ncluster and then aggregate key and value tokens, as a content-based method of\nreducing the total token count. The resulting clustered-token sequence retains\nthe semantic diversity of the original signal, but can be processed at a lower\ncomputational cost. Besides, we further extend the clustering-guided attention\nfrom single-scale to multi-scale, which is conducive to dense prediction tasks.\nWe label the proposed Transformer architecture ClusTR, and demonstrate that it\nachieves state-of-the-art performance on various vision tasks but at lower\ncomputational cost and with fewer parameters. For instance, our ClusTR small\nmodel with 22.7M parameters achieves 83.2\\% Top-1 accuracy on ImageNet. Source\ncode and ImageNet models will be made publicly available.",
    "published": "2022-08-28T04:18:27Z",
    "updated": "2022-08-28T04:18:27Z",
    "authors": [
      "Yutong Xie",
      "Jianpeng Zhang",
      "Yong Xia",
      "Anton van den Hengel",
      "Qi Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1906.02792v1",
    "title": "Attention is all you need for Videos: Self-attention based Video\n  Summarization using Universal Transformers",
    "summary": "Video Captioning and Summarization have become very popular in the recent\nyears due to advancements in Sequence Modelling, with the resurgence of\nLong-Short Term Memory networks (LSTMs) and introduction of Gated Recurrent\nUnits (GRUs). Existing architectures extract spatio-temporal features using\nCNNs and utilize either GRUs or LSTMs to model dependencies with soft attention\nlayers. These attention layers do help in attending to the most prominent\nfeatures and improve upon the recurrent units, however, these models suffer\nfrom the inherent drawbacks of the recurrent units themselves. The introduction\nof the Transformer model has driven the Sequence Modelling field into a new\ndirection. In this project, we implement a Transformer-based model for Video\ncaptioning, utilizing 3D CNN architectures like C3D and Two-stream I3D for\nvideo extraction. We also apply certain dimensionality reduction techniques so\nas to keep the overall size of the model within limits. We finally present our\nresults on the MSVD and ActivityNet datasets for Single and Dense video\ncaptioning tasks respectively.",
    "published": "2019-06-06T19:59:56Z",
    "updated": "2019-06-06T19:59:56Z",
    "authors": [
      "Manjot Bilkhu",
      "Siyang Wang",
      "Tushar Dobhal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1906.05708v1",
    "title": "2D Attentional Irregular Scene Text Recognizer",
    "summary": "Irregular scene text, which has complex layout in 2D space, is challenging to\nmost previous scene text recognizers. Recently, some irregular scene text\nrecognizers either rectify the irregular text to regular text image with\napproximate 1D layout or transform the 2D image feature map to 1D feature\nsequence. Though these methods have achieved good performance, the robustness\nand accuracy are still limited due to the loss of spatial information in the\nprocess of 2D to 1D transformation. Different from all of previous, we in this\npaper propose a framework which transforms the irregular text with 2D layout to\ncharacter sequence directly via 2D attentional scheme. We utilize a relation\nattention module to capture the dependencies of feature maps and a parallel\nattention module to decode all characters in parallel, which make our method\nmore effective and efficient. Extensive experiments on several public\nbenchmarks as well as our collected multi-line text dataset show that our\napproach is effective to recognize regular and irregular scene text and\noutperforms previous methods both in accuracy and speed.",
    "published": "2019-06-13T14:10:12Z",
    "updated": "2019-06-13T14:10:12Z",
    "authors": [
      "Pengyuan Lyu",
      "Zhicheng Yang",
      "Xinhang Leng",
      "Xiaojun Wu",
      "Ruiyu Li",
      "Xiaoyong Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.08046v1",
    "title": "Tree-structured Attention with Hierarchical Accumulation",
    "summary": "Incorporating hierarchical structures like constituency trees has been shown\nto be effective for various natural language processing (NLP) tasks. However,\nit is evident that state-of-the-art (SOTA) sequence-based models like the\nTransformer struggle to encode such structures inherently. On the other hand,\ndedicated models like the Tree-LSTM, while explicitly modeling hierarchical\nstructures, do not perform as efficiently as the Transformer. In this paper, we\nattempt to bridge this gap with \"Hierarchical Accumulation\" to encode parse\ntree structures into self-attention at constant time complexity. Our approach\noutperforms SOTA methods in four IWSLT translation tasks and the WMT'14\nEnglish-German translation task. It also yields improvements over Transformer\nand Tree-LSTM on three text classification tasks. We further demonstrate that\nusing hierarchical priors can compensate for data shortage, and that our model\nprefers phrase-level attentions over token-level attentions.",
    "published": "2020-02-19T08:17:00Z",
    "updated": "2020-02-19T08:17:00Z",
    "authors": [
      "Xuan-Phi Nguyen",
      "Shafiq Joty",
      "Steven C. H. Hoi",
      "Richard Socher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.08264v1",
    "title": "Molecule Attention Transformer",
    "summary": "Designing a single neural network architecture that performs competitively\nacross a range of molecule property prediction tasks remains largely an open\nchallenge, and its solution may unlock a widespread use of deep learning in the\ndrug discovery industry. To move towards this goal, we propose Molecule\nAttention Transformer (MAT). Our key innovation is to augment the attention\nmechanism in Transformer using inter-atomic distances and the molecular graph\nstructure. Experiments show that MAT performs competitively on a diverse set of\nmolecular prediction tasks. Most importantly, with a simple self-supervised\npretraining, MAT requires tuning of only a few hyperparameter values to achieve\nstate-of-the-art performance on downstream tasks. Finally, we show that\nattention weights learned by MAT are interpretable from the chemical point of\nview.",
    "published": "2020-02-19T16:14:48Z",
    "updated": "2020-02-19T16:14:48Z",
    "authors": [
      "Åukasz Maziarka",
      "Tomasz Danel",
      "SÅawomir Mucha",
      "Krzysztof Rataj",
      "Jacek Tabor",
      "StanisÅaw JastrzÄbski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.15222v3",
    "title": "BERTology Meets Biology: Interpreting Attention in Protein Language\n  Models",
    "summary": "Transformer architectures have proven to learn useful representations for\nprotein classification and generation tasks. However, these representations\npresent challenges in interpretability. In this work, we demonstrate a set of\nmethods for analyzing protein Transformer models through the lens of attention.\nWe show that attention: (1) captures the folding structure of proteins,\nconnecting amino acids that are far apart in the underlying sequence, but\nspatially close in the three-dimensional structure, (2) targets binding sites,\na key functional component of proteins, and (3) focuses on progressively more\ncomplex biophysical properties with increasing layer depth. We find this\nbehavior to be consistent across three Transformer architectures (BERT, ALBERT,\nXLNet) and two distinct protein datasets. We also present a three-dimensional\nvisualization of the interaction between attention and protein structure. Code\nfor visualization and analysis is available at\nhttps://github.com/salesforce/provis.",
    "published": "2020-06-26T21:50:17Z",
    "updated": "2021-03-28T21:56:26Z",
    "authors": [
      "Jesse Vig",
      "Ali Madani",
      "Lav R. Varshney",
      "Caiming Xiong",
      "Richard Socher",
      "Nazneen Fatema Rajani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.00529v1",
    "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal\n  Transformers",
    "summary": "Recently multimodal transformer models have gained popularity because their\nperformance on language and vision tasks suggest they learn rich\nvisual-linguistic representations. Focusing on zero-shot image retrieval tasks,\nwe study three important factors which can impact the quality of learned\nrepresentations: pretraining data, the attention mechanism, and loss functions.\nBy pretraining models on six datasets, we observe that dataset noise and\nlanguage similarity to our downstream task are important indicators of model\nperformance. Through architectural analysis, we learn that models with a\nmultimodal attention mechanism can outperform deeper models with modality\nspecific attention mechanisms. Finally, we show that successful contrastive\nlosses used in the self-supervised learning literature do not yield similar\nperformance gains when used in multimodal transformers",
    "published": "2021-01-31T20:36:41Z",
    "updated": "2021-01-31T20:36:41Z",
    "authors": [
      "Lisa Anne Hendricks",
      "John Mellor",
      "Rosalia Schneider",
      "Jean-Baptiste Alayrac",
      "Aida Nematzadeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.07680v1",
    "title": "Translational Equivariance in Kernelizable Attention",
    "summary": "While Transformer architectures have show remarkable success, they are bound\nto the computation of all pairwise interactions of input element and thus\nsuffer from limited scalability. Recent work has been successful by avoiding\nthe computation of the complete attention matrix, yet leads to problems down\nthe line. The absence of an explicit attention matrix makes the inclusion of\ninductive biases relying on relative interactions between elements more\nchallenging. An extremely powerful inductive bias is translational\nequivariance, which has been conjectured to be responsible for much of the\nsuccess of Convolutional Neural Networks on image recognition tasks. In this\nwork we show how translational equivariance can be implemented in efficient\nTransformers based on kernelizable attention - Performers. Our experiments\nhighlight that the devised approach significantly improves robustness of\nPerformers to shifts of input images compared to their naive application. This\nrepresents an important step on the path of replacing Convolutional Neural\nNetworks with more expressive Transformer architectures and will help to\nimprove sample efficiency and robustness in this realm.",
    "published": "2021-02-15T17:14:15Z",
    "updated": "2021-02-15T17:14:15Z",
    "authors": [
      "Max Horn",
      "Kumar Shridhar",
      "Elrich Groenewald",
      "Philipp F. M. Baumann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.12874v1",
    "title": "Accounting for Agreement Phenomena in Sentence Comprehension with\n  Transformer Language Models: Effects of Similarity-based Interference on\n  Surprisal and Attention",
    "summary": "We advance a novel explanation of similarity-based interference effects in\nsubject-verb and reflexive pronoun agreement processing, grounded in surprisal\nvalues computed from a pretrained large-scale Transformer model, GPT-2.\nSpecifically, we show that surprisal of the verb or reflexive pronoun predicts\nfacilitatory interference effects in ungrammatical sentences, where a\ndistractor noun that matches in number with the verb or pronoun leads to faster\nreading times, despite the distractor not participating in the agreement\nrelation. We review the human empirical evidence for such effects, including\nrecent meta-analyses and large-scale studies. We also show that attention\npatterns (indexed by entropy and other measures) in the Transformer show\npatterns of diffuse attention in the presence of similar distractors,\nconsistent with cue-based retrieval models of parsing. But in contrast to these\nmodels, the attentional cues and memory representations are learned entirely\nfrom the simple self-supervised task of predicting the next word.",
    "published": "2021-04-26T20:46:54Z",
    "updated": "2021-04-26T20:46:54Z",
    "authors": [
      "Soo Hyun Ryu",
      "Richard L. Lewis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.03817v1",
    "title": "TrTr: Visual Tracking with Transformer",
    "summary": "Template-based discriminative trackers are currently the dominant tracking\nmethods due to their robustness and accuracy, and the Siamese-network-based\nmethods that depend on cross-correlation operation between features extracted\nfrom template and search images show the state-of-the-art tracking performance.\nHowever, general cross-correlation operation can only obtain relationship\nbetween local patches in two feature maps. In this paper, we propose a novel\ntracker network based on a powerful attention mechanism called Transformer\nencoder-decoder architecture to gain global and rich contextual\ninterdependencies. In this new architecture, features of the template image is\nprocessed by a self-attention module in the encoder part to learn strong\ncontext information, which is then sent to the decoder part to compute\ncross-attention with the search image features processed by another\nself-attention module. In addition, we design the classification and regression\nheads using the output of Transformer to localize target based on\nshape-agnostic anchor. We extensively evaluate our tracker TrTr, on VOT2018,\nVOT2019, OTB-100, UAV, NfS, TrackingNet, and LaSOT benchmarks and our method\nperforms favorably against state-of-the-art algorithms. Training code and\npretrained models are available at https://github.com/tongtybj/TrTr.",
    "published": "2021-05-09T02:32:28Z",
    "updated": "2021-05-09T02:32:28Z",
    "authors": [
      "Moju Zhao",
      "Kei Okada",
      "Masayuki Inaba"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14110v2",
    "title": "MixerGAN: An MLP-Based Architecture for Unpaired Image-to-Image\n  Translation",
    "summary": "While attention-based transformer networks achieve unparalleled success in\nnearly all language tasks, the large number of tokens (pixels) found in images\ncoupled with the quadratic activation memory usage makes them prohibitive for\nproblems in computer vision. As such, while language-to-language translation\nhas been revolutionized by the transformer model, convolutional networks remain\nthe de facto solution for image-to-image translation. The recently proposed\nMLP-Mixer architecture alleviates some of the computational issues associated\nwith attention-based networks while still retaining the long-range connections\nthat make transformer models desirable. Leveraging this memory-efficient\nalternative to self-attention, we propose a new exploratory model in unpaired\nimage-to-image translation called MixerGAN: a simpler MLP-based architecture\nthat considers long-distance relationships between pixels without the need for\nexpensive attention mechanisms. Quantitative and qualitative analysis shows\nthat MixerGAN achieves competitive results when compared to prior\nconvolutional-based methods.",
    "published": "2021-05-28T21:12:52Z",
    "updated": "2021-08-19T07:38:02Z",
    "authors": [
      "George Cazenavette",
      "Manuel Ladron De Guevara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14850v1",
    "title": "Cascaded Head-colliding Attention",
    "summary": "Transformers have advanced the field of natural language processing (NLP) on\na variety of important tasks. At the cornerstone of the Transformer\narchitecture is the multi-head attention (MHA) mechanism which models pairwise\ninteractions between the elements of the sequence. Despite its massive success,\nthe current framework ignores interactions among different heads, leading to\nthe problem that many of the heads are redundant in practice, which greatly\nwastes the capacity of the model. To improve parameter efficiency, we\nre-formulate the MHA as a latent variable model from a probabilistic\nperspective. We present cascaded head-colliding attention (CODA) which\nexplicitly models the interactions between attention heads through a\nhierarchical variational distribution. We conduct extensive experiments and\ndemonstrate that CODA outperforms the transformer baseline, by $0.6$ perplexity\non \\texttt{Wikitext-103} in language modeling, and by $0.6$ BLEU on\n\\texttt{WMT14 EN-DE} in machine translation, due to its improvements on the\nparameter efficiency.\\footnote{Our implementation is publicly available at\n\\url{https://github.com/LZhengisme/CODA}.}",
    "published": "2021-05-31T10:06:42Z",
    "updated": "2021-05-31T10:06:42Z",
    "authors": [
      "Lin Zheng",
      "Zhiyong Wu",
      "Lingpeng Kong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.06455v3",
    "title": "PTT: Point-Track-Transformer Module for 3D Single Object Tracking in\n  Point Clouds",
    "summary": "3D single object tracking is a key issue for robotics. In this paper, we\npropose a transformer module called Point-Track-Transformer (PTT) for point\ncloud-based 3D single object tracking. PTT module contains three blocks for\nfeature embedding, position encoding, and self-attention feature computation.\nFeature embedding aims to place features closer in the embedding space if they\nhave similar semantic information. Position encoding is used to encode\ncoordinates of point clouds into high dimension distinguishable features.\nSelf-attention generates refined attention features by computing attention\nweights. Besides, we embed the PTT module into the open-source state-of-the-art\nmethod P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that\nour PTT-Net surpasses the state-of-the-art by a noticeable margin (~10%).\nAdditionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA\n1080Ti GPU. Our code is open-sourced for the robotics community at\nhttps://github.com/shanjiayao/PTT.",
    "published": "2021-08-14T03:24:10Z",
    "updated": "2021-10-07T07:07:56Z",
    "authors": [
      "Jiayao Shan",
      "Sifan Zhou",
      "Zheng Fang",
      "Yubo Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.13032v1",
    "title": "Shatter: An Efficient Transformer Encoder with Single-Headed\n  Self-Attention and Relative Sequence Partitioning",
    "summary": "The highly popular Transformer architecture, based on self-attention, is the\nfoundation of large pretrained models such as BERT, that have become an\nenduring paradigm in NLP. While powerful, the computational resources and time\nrequired to pretrain such models can be prohibitive. In this work, we present\nan alternative self-attention architecture, Shatter, that more efficiently\nencodes sequence information by softly partitioning the space of relative\npositions and applying different value matrices to different parts of the\nsequence. This mechanism further allows us to simplify the multi-headed\nattention in Transformer to single-headed. We conduct extensive experiments\nshowing that Shatter achieves better performance than BERT, with pretraining\nbeing faster per step (15% on TPU), converging in fewer steps, and offering\nconsiderable memory savings (>50%). Put together, Shatter can be pretrained on\n8 V100 GPUs in 7 days, and match the performance of BERT_Base -- making the\ncost of pretraining much more affordable.",
    "published": "2021-08-30T07:42:12Z",
    "updated": "2021-08-30T07:42:12Z",
    "authors": [
      "Ran Tian",
      "Joshua Maynez",
      "Ankur P. Parikh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.06920v2",
    "title": "Semantics-aware Attention Improves Neural Machine Translation",
    "summary": "The integration of syntactic structures into Transformer machine translation\nhas shown positive results, but to our knowledge, no work has attempted to do\nso with semantic structures. In this work we propose two novel parameter-free\nmethods for injecting semantic information into Transformers, both rely on\nsemantics-aware masking of (some of) the attention heads. One such method\noperates on the encoder, through a Scene-Aware Self-Attention (SASA) head.\nAnother on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We\nshow a consistent improvement over the vanilla Transformer and syntax-aware\nmodels for four language pairs. We further show an additional gain when using\nboth semantic and syntactic structures in some language pairs.",
    "published": "2021-10-13T17:58:22Z",
    "updated": "2022-05-24T11:02:23Z",
    "authors": [
      "Aviv Slobodkin",
      "Leshem Choshen",
      "Omri Abend"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.10778v1",
    "title": "Contrastive Document Representation Learning with Graph Attention\n  Networks",
    "summary": "Recent progress in pretrained Transformer-based language models has shown\ngreat success in learning contextual representation of text. However, due to\nthe quadratic self-attention complexity, most of the pretrained Transformers\nmodels can only handle relatively short text. It is still a challenge when it\ncomes to modeling very long documents. In this work, we propose to use a graph\nattention network on top of the available pretrained Transformers model to\nlearn document embeddings. This graph attention network allows us to leverage\nthe high-level semantic structure of the document. In addition, based on our\ngraph document model, we design a simple contrastive learning strategy to\npretrain our models on a large amount of unlabeled corpus. Empirically, we\ndemonstrate the effectiveness of our approaches in document classification and\ndocument retrieval tasks.",
    "published": "2021-10-20T21:05:02Z",
    "updated": "2021-10-20T21:05:02Z",
    "authors": [
      "Peng Xu",
      "Xinchi Chen",
      "Xiaofei Ma",
      "Zhiheng Huang",
      "Bing Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.01685v2",
    "title": "Interaction Transformer for Human Reaction Generation",
    "summary": "We address the challenging task of human reaction generation, which aims to\ngenerate a corresponding reaction based on an input action. Most of the\nexisting works do not focus on generating and predicting the reaction and\ncannot generate the motion when only the action is given as input. To address\nthis limitation, we propose a novel interaction Transformer (InterFormer)\nconsisting of a Transformer network with both temporal and spatial attention.\nSpecifically, temporal attention captures the temporal dependencies of the\nmotion of both characters and of their interaction, while spatial attention\nlearns the dependencies between the different body parts of each character and\nthose which are part of the interaction. Moreover, we propose using graphs to\nincrease the performance of spatial attention via an interaction distance\nmodule that helps focus on nearby joints from both characters. Extensive\nexperiments on the SBU interaction, K3HI, and DuetDance datasets demonstrate\nthe effectiveness of InterFormer. Our method is general and can be used to\ngenerate more complex and long-term interactions. We also provide videos of\ngenerated reactions and the code with pre-trained models at\nhttps://github.com/CRISTAL-3DSAM/InterFormer",
    "published": "2022-07-04T19:30:41Z",
    "updated": "2023-02-01T20:53:18Z",
    "authors": [
      "Baptiste Chopin",
      "Hao Tang",
      "Naima Otberdout",
      "Mohamed Daoudi",
      "Nicu Sebe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.10284v1",
    "title": "Multi Resolution Analysis (MRA) for Approximate Self-Attention",
    "summary": "Transformers have emerged as a preferred model for many tasks in natural\nlangugage processing and vision. Recent efforts on training and deploying\nTransformers more efficiently have identified many strategies to approximate\nthe self-attention matrix, a key module in a Transformer architecture.\nEffective ideas include various prespecified sparsity patterns, low-rank basis\nexpansions and combinations thereof. In this paper, we revisit classical\nMultiresolution Analysis (MRA) concepts such as Wavelets, whose potential value\nin this setting remains underexplored thus far. We show that simple\napproximations based on empirical feedback and design choices informed by\nmodern hardware and implementation challenges, eventually yield a MRA-based\napproach for self-attention with an excellent performance profile across most\ncriteria of interest. We undertake an extensive set of experiments and\ndemonstrate that this multi-resolution scheme outperforms most efficient\nself-attention proposals and is favorable for both short and long sequences.\nCode is available at \\url{https://github.com/mlpen/mra-attention}.",
    "published": "2022-07-21T03:36:30Z",
    "updated": "2022-07-21T03:36:30Z",
    "authors": [
      "Zhanpeng Zeng",
      "Sourav Pal",
      "Jeffery Kline",
      "Glenn M Fung",
      "Vikas Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.05405v4",
    "title": "VieCap4H-VLSP 2021: ObjectAoA-Enhancing performance of Object Relation\n  Transformer with Attention on Attention for Vietnamese image captioning",
    "summary": "Image captioning is currently a challenging task that requires the ability to\nboth understand visual information and use human language to describe this\nvisual information in the image. In this paper, we propose an efficient way to\nimprove the image understanding ability of transformer-based method by\nextending Object Relation Transformer architecture with Attention on Attention\nmechanism. Experiments on the VieCap4H dataset show that our proposed method\nsignificantly outperforms its original structure on both the public test and\nprivate test of the Image Captioning shared task held by VLSP.",
    "published": "2022-11-10T08:19:44Z",
    "updated": "2023-03-20T08:29:29Z",
    "authors": [
      "Nghia Hieu Nguyen",
      "Duong T. D. Vo",
      "Minh-Quan Ha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.09533v1",
    "title": "Parameter-Efficient Transformer with Hybrid Axial-Attention for Medical\n  Image Segmentation",
    "summary": "Transformers have achieved remarkable success in medical image analysis owing\nto their powerful capability to use flexible self-attention mechanism. However,\ndue to lacking intrinsic inductive bias in modeling visual structural\ninformation, they generally require a large-scale pre-training schedule,\nlimiting the clinical applications over expensive small-scale medical data. To\nthis end, we propose a parameter-efficient transformer to explore intrinsic\ninductive bias via position information for medical image segmentation.\nSpecifically, we empirically investigate how different position encoding\nstrategies affect the prediction quality of the region of interest (ROI), and\nobserve that ROIs are sensitive to the position encoding strategies. Motivated\nby this, we present a novel Hybrid Axial-Attention (HAA), a form of position\nself-attention that can be equipped with spatial pixel-wise information and\nrelative position information as inductive bias. Moreover, we introduce a\ngating mechanism to alleviate the burden of training schedule, resulting in\nefficient feature selection over small-scale datasets. Experiments on the BraTS\nand Covid19 datasets prove the superiority of our method over the baseline and\nprevious works. Internal workflow visualization with interpretability is\nconducted to better validate our success.",
    "published": "2022-11-17T13:54:55Z",
    "updated": "2022-11-17T13:54:55Z",
    "authors": [
      "Yiyue Hu",
      "Lei Zhang",
      "Nan Mu",
      "Lei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.12777v1",
    "title": "A Dual-scale Lead-seperated Transformer With Lead-orthogonal Attention\n  And Meta-information For Ecg Classification",
    "summary": "Auxiliary diagnosis of cardiac electrophysiological status can be obtained\nthrough the analysis of 12-lead electrocardiograms (ECGs). This work proposes a\ndual-scale lead-separated transformer with lead-orthogonal attention and\nmeta-information (DLTM-ECG) as a novel approach to address this challenge. ECG\nsegments of each lead are interpreted as independent patches, and together with\nthe reduced dimension signal, they form a dual-scale representation. As a\nmethod to reduce interference from segments with low correlation, two group\nattention mechanisms perform both lead-internal and cross-lead attention. Our\nmethod allows for the addition of previously discarded meta-information,\nfurther improving the utilization of clinical information. Experimental results\nshow that our DLTM-ECG yields significantly better classification scores than\nother transformer-based models,matching or performing better than\nstate-of-the-art (SOTA) deep learning methods on two benchmark datasets. Our\nwork has the potential for similar multichannel bioelectrical signal processing\nand physiological multimodal tasks.",
    "published": "2022-11-23T08:45:34Z",
    "updated": "2022-11-23T08:45:34Z",
    "authors": [
      "Yang Li",
      "Guijin Wang",
      "Zhourui Xia",
      "Wenming Yang",
      "Li Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.12879v2",
    "title": "Data Augmentation Vision Transformer for Fine-grained Image\n  Classification",
    "summary": "Recently, the vision transformer (ViT) has made breakthroughs in image\nrecognition. Its self-attention mechanism (MSA) can extract discriminative\nlabeling information of different pixel blocks to improve image classification\naccuracy. However, the classification marks in their deep layers tend to ignore\nlocal features between layers. In addition, the embedding layer will be\nfixed-size pixel blocks. Input network Inevitably introduces additional image\nnoise. To this end, we study a data augmentation vision transformer (DAVT)\nbased on data augmentation and proposes a data augmentation method for\nattention cropping, which uses attention weights as the guide to crop images\nand improve the ability of the network to learn critical features. Secondly, we\nalso propose a hierarchical attention selection (HAS) method, which improves\nthe ability of discriminative markers between levels of learning by filtering\nand fusing labels between levels. Experimental results show that the accuracy\nof this method on the two general datasets, CUB-200-2011, and Stanford Dogs, is\nbetter than the existing mainstream methods, and its accuracy is 1.4\\% and\n1.6\\% higher than the original ViT, respectively",
    "published": "2022-11-23T11:34:11Z",
    "updated": "2022-11-24T08:40:56Z",
    "authors": [
      "Chao Hu",
      "Liqiang Zhu",
      "Weibin Qiu",
      "Weijie Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.14090v1",
    "title": "Spatial-Spectral Transformer for Hyperspectral Image Denoising",
    "summary": "Hyperspectral image (HSI) denoising is a crucial preprocessing procedure for\nthe subsequent HSI applications. Unfortunately, though witnessing the\ndevelopment of deep learning in HSI denoising area, existing convolution-based\nmethods face the trade-off between computational efficiency and capability to\nmodel non-local characteristics of HSI. In this paper, we propose a\nSpatial-Spectral Transformer (SST) to alleviate this problem. To fully explore\nintrinsic similarity characteristics in both spatial dimension and spectral\ndimension, we conduct non-local spatial self-attention and global spectral\nself-attention with Transformer architecture. The window-based spatial\nself-attention focuses on the spatial similarity beyond the neighboring region.\nWhile, spectral self-attention exploits the long-range dependencies between\nhighly correlative bands. Experimental results show that our proposed method\noutperforms the state-of-the-art HSI denoising methods in quantitative quality\nand visual results.",
    "published": "2022-11-25T13:18:45Z",
    "updated": "2022-11-25T13:18:45Z",
    "authors": [
      "Miaoyu Li",
      "Ying Fu",
      "Yulun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.13729v1",
    "title": "DST: Deformable Speech Transformer for Emotion Recognition",
    "summary": "Enabled by multi-head self-attention, Transformer has exhibited remarkable\nresults in speech emotion recognition (SER). Compared to the original full\nattention mechanism, window-based attention is more effective in learning\nfine-grained features while greatly reducing model redundancy. However,\nemotional cues are present in a multi-granularity manner such that the\npre-defined fixed window can severely degrade the model flexibility. In\naddition, it is difficult to obtain the optimal window settings manually. In\nthis paper, we propose a Deformable Speech Transformer, named DST, for SER\ntask. DST determines the usage of window sizes conditioned on input speech via\na light-weight decision network. Meanwhile, data-dependent offsets derived from\nacoustic features are utilized to adjust the positions of the attention\nwindows, allowing DST to adaptively discover and attend to the valuable\ninformation embedded in the speech. Extensive experiments on IEMOCAP and MELD\ndemonstrate the superiority of DST.",
    "published": "2023-02-27T12:52:23Z",
    "updated": "2023-02-27T12:52:23Z",
    "authors": [
      "Weidong Chen",
      "Xiaofen Xing",
      "Xiangmin Xu",
      "Jianxin Pang",
      "Lan Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.02010v3",
    "title": "Memorization Capacity of Multi-Head Attention in Transformers",
    "summary": "Transformers have become the go-to architecture for language and vision\ntasks, yet their theoretical properties, especially memorization capacity,\nremain elusive. This paper investigates the memorization abilities of\nmulti-head attention mechanisms, examining how many example sequences they can\nmemorize, as a function of the number of heads and sequence length. Motivated\nby experimental findings on vision transformers, we introduce novel assumptions\nabout the linear independence of input data, distinct from the commonly used\ngeneral-position assumption. Under these assumptions, we demonstrate that an\nattention layer with $H$ heads, dimension $d$, and context size $n < d$,\nfeaturing $\\Theta(Hd^2)$ parameters, can memorize $\\Omega(Hn)$ examples. Our\nanalysis sheds light on how different attention heads handle various example\nsequences, aided by the softmax operator's saturation property. We validate our\nfindings through experiments on synthetic data.",
    "published": "2023-06-03T05:45:29Z",
    "updated": "2024-03-02T07:50:37Z",
    "authors": [
      "Sadegh Mahdavi",
      "Renjie Liao",
      "Christos Thrampoulidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.15989v3",
    "title": "Tensorformer: Normalized Matrix Attention Transformer for High-quality\n  Point Cloud Reconstruction",
    "summary": "Surface reconstruction from raw point clouds has been studied for decades in\nthe computer graphics community, which is highly demanded by modeling and\nrendering applications nowadays. Classic solutions, such as Poisson surface\nreconstruction, require point normals as extra input to perform reasonable\nresults. Modern transformer-based methods can work without normals, while the\nresults are less fine-grained due to limited encoding performance in local\nfusion from discrete points. We introduce a novel normalized matrix attention\ntransformer (Tensorformer) to perform high-quality reconstruction. The proposed\nmatrix attention allows for simultaneous point-wise and channel-wise message\npassing, while the previous vector attention loses neighbor point information\nacross different channels. It brings more degree of freedom in feature learning\nand thus facilitates better modeling of local geometries. Our method achieves\nstate-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and\nattains 4% improvements on IOU on ShapeNet. Code can be accessed\nhttps://github.com/THHHomas/Tensorformer6.",
    "published": "2023-06-28T07:59:34Z",
    "updated": "2023-10-10T08:45:35Z",
    "authors": [
      "Hui Tian",
      "Zheng Qin",
      "Renjiao Yi",
      "Chenyang Zhu",
      "Kai Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.02486v2",
    "title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens",
    "summary": "Scaling sequence length has become a critical demand in the era of large\nlanguage models. However, existing methods struggle with either computational\ncomplexity or model expressivity, rendering the maximum sequence length\nrestricted. To address this issue, we introduce LongNet, a Transformer variant\nthat can scale sequence length to more than 1 billion tokens, without\nsacrificing the performance on shorter sequences. Specifically, we propose\ndilated attention, which expands the attentive field exponentially as the\ndistance grows. LongNet has significant advantages: 1) it has a linear\ncomputation complexity and a logarithm dependency between any two tokens in a\nsequence; 2) it can be served as a distributed trainer for extremely long\nsequences; 3) its dilated attention is a drop-in replacement for standard\nattention, which can be seamlessly integrated with the existing\nTransformer-based optimization. Experiments results demonstrate that LongNet\nyields strong performance on both long-sequence modeling and general language\ntasks. Our work opens up new possibilities for modeling very long sequences,\ne.g., treating a whole corpus or even the entire Internet as a sequence.",
    "published": "2023-07-05T17:59:38Z",
    "updated": "2023-07-19T12:25:35Z",
    "authors": [
      "Jiayu Ding",
      "Shuming Ma",
      "Li Dong",
      "Xingxing Zhang",
      "Shaohan Huang",
      "Wenhui Wang",
      "Nanning Zheng",
      "Furu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.13807v2",
    "title": "Learning to (Learn at Test Time)",
    "summary": "We reformulate the problem of supervised learning as learning to learn with\ntwo nested loops (i.e. learning problems). The inner loop learns on each\nindividual instance with self-supervision before final prediction. The outer\nloop learns the self-supervised task used by the inner loop, such that its\nfinal prediction improves. Our inner loop turns out to be equivalent to linear\nattention when the inner-loop learner is only a linear model, and to\nself-attention when it is a kernel estimator. For practical comparison with\nlinear or self-attention layers, we replace each of them in a transformer with\nan inner loop, so our outer loop is equivalent to training the architecture.\nWhen each inner-loop learner is a neural network, our approach vastly\noutperforms transformers with linear attention on ImageNet from 224 x 224 raw\npixels in both accuracy and FLOPs, while (regular) transformers cannot run.",
    "published": "2023-10-20T20:42:00Z",
    "updated": "2024-01-07T22:32:39Z",
    "authors": [
      "Yu Sun",
      "Xinhao Li",
      "Karan Dalal",
      "Chloe Hsu",
      "Sanmi Koyejo",
      "Carlos Guestrin",
      "Xiaolong Wang",
      "Tatsunori Hashimoto",
      "Xinlei Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.05089v1",
    "title": "Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform",
    "summary": "Since its introduction, the transformers architecture has seen great adoption\nin NLP applications, but it also has limitations. Although the self-attention\nmechanism allows for generating very rich representations of the input text,\nits effectiveness may be limited in specialized domains such as legal, where,\nfor example, language models often have to process very long texts. In this\npaper, we explore alternatives to replace the attention-based layers with\nsimpler token-mixing mechanisms: Hartley and Fourier transforms. Using these\nnon-parametric techniques, we train models with long input documents from\nscratch in the legal domain setting. We also introduce a new hybrid Seq2Seq\narchitecture, a no-attention-based encoder connected with an attention-based\ndecoder, which performs quite well on existing summarization tasks with much\nless compute and memory requirements. We believe that similar, if not better\nperformance, as in the case of long correlations of abstractive text\nsummarization tasks, can be achieved by adopting these simpler infrastructures.\nThis not only makes training models from scratch accessible to more people, but\nalso contributes to the reduction of the carbon footprint during training.",
    "published": "2023-11-09T01:27:54Z",
    "updated": "2023-11-09T01:27:54Z",
    "authors": [
      "Daniele GiofrÃ©",
      "Sneha Ghantasala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.17340v1",
    "title": "Cross-Scope Spatial-Spectral Information Aggregation for Hyperspectral\n  Image Super-Resolution",
    "summary": "Hyperspectral image super-resolution has attained widespread prominence to\nenhance the spatial resolution of hyperspectral images. However,\nconvolution-based methods have encountered challenges in harnessing the global\nspatial-spectral information. The prevailing transformer-based methods have not\nadequately captured the long-range dependencies in both spectral and spatial\ndimensions. To alleviate this issue, we propose a novel cross-scope\nspatial-spectral Transformer (CST) to efficiently investigate long-range\nspatial and spectral similarities for single hyperspectral image\nsuper-resolution. Specifically, we devise cross-attention mechanisms in spatial\nand spectral dimensions to comprehensively model the long-range\nspatial-spectral characteristics. By integrating global information into the\nrectangle-window self-attention, we first design a cross-scope spatial\nself-attention to facilitate long-range spatial interactions. Then, by\nleveraging appropriately characteristic spatial-spectral features, we construct\na cross-scope spectral self-attention to effectively capture the intrinsic\ncorrelations among global spectral bands. Finally, we elaborate a concise\nfeed-forward neural network to enhance the feature representation capacity in\nthe Transformer structure. Extensive experiments over three hyperspectral\ndatasets demonstrate that the proposed CST is superior to other\nstate-of-the-art methods both quantitatively and visually. The code is\navailable at \\url{https://github.com/Tomchenshi/CST.git}.",
    "published": "2023-11-29T03:38:56Z",
    "updated": "2023-11-29T03:38:56Z",
    "authors": [
      "Shi Chen",
      "Lefei Zhang",
      "Liangpei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.09016v1",
    "title": "Pyramid Attention Network for Medical Image Registration",
    "summary": "The advent of deep-learning-based registration networks has addressed the\ntime-consuming challenge in traditional iterative methods.However, the\npotential of current registration networks for comprehensively capturing\nspatial relationships has not been fully explored, leading to inadequate\nperformance in large-deformation image registration.The pure convolutional\nneural networks (CNNs) neglect feature enhancement, while current\nTransformer-based networks are susceptible to information redundancy.To\nalleviate these issues, we propose a pyramid attention network (PAN) for\ndeformable medical image registration.Specifically, the proposed PAN\nincorporates a dual-stream pyramid encoder with channel-wise attention to boost\nthe feature representation.Moreover, a multi-head local attention Transformer\nis introduced as decoder to analyze motion patterns and generate deformation\nfields.Extensive experiments on two public brain magnetic resonance imaging\n(MRI) datasets and one abdominal MRI dataset demonstrate that our method\nachieves favorable registration performance, while outperforming several\nCNN-based and Transformer-based registration networks.Our code is publicly\navailable at https://github.com/JuliusWang-7/PAN.",
    "published": "2024-02-14T08:46:18Z",
    "updated": "2024-02-14T08:46:18Z",
    "authors": [
      "Zhuoyuan Wang",
      "Haiqiao Wang",
      "Yi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.16143v1",
    "title": "CFAT: Unleashing TriangularWindows for Image Super-resolution",
    "summary": "Transformer-based models have revolutionized the field of image\nsuper-resolution (SR) by harnessing their inherent ability to capture complex\ncontextual features. The overlapping rectangular shifted window technique used\nin transformer architecture nowadays is a common practice in super-resolution\nmodels to improve the quality and robustness of image upscaling. However, it\nsuffers from distortion at the boundaries and has limited unique shifting\nmodes. To overcome these weaknesses, we propose a non-overlapping triangular\nwindow technique that synchronously works with the rectangular one to mitigate\nboundary-level distortion and allows the model to access more unique sifting\nmodes. In this paper, we propose a Composite Fusion Attention Transformer\n(CFAT) that incorporates triangular-rectangular window-based local attention\nwith a channel-based global attention technique in image super-resolution. As a\nresult, CFAT enables attention mechanisms to be activated on more image pixels\nand captures long-range, multi-scale features to improve SR performance. The\nextensive experimental results and ablation study demonstrate the effectiveness\nof CFAT in the SR domain. Our proposed model shows a significant 0.7 dB\nperformance improvement over other state-of-the-art SR architectures.",
    "published": "2024-03-24T13:31:31Z",
    "updated": "2024-03-24T13:31:31Z",
    "authors": [
      "Abhisek Ray",
      "Gaurav Kumar",
      "Maheshkumar H. Kolekar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.17753v2",
    "title": "CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream\n  Enhanced Rectified Transformer Model",
    "summary": "Accurate, and effective traffic forecasting is vital for smart traffic\nsystems, crucial in urban traffic planning and management. Current\nSpatio-Temporal Transformer models, despite their prediction capabilities,\nstruggle with balancing computational efficiency and accuracy, favoring global\nover local information, and handling spatial and temporal data separately,\nlimiting insight into complex interactions. We introduce the Criss-Crossed\nDual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes\nthree innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),\nEnhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified\nTemporal Self-attention (ReTSA). These modules aim to lower computational needs\nvia sparse attention, focus on local information for better traffic dynamics\nunderstanding, and merge spatial and temporal insights through a unique\nlearning method. Extensive tests on six real-world datasets highlight\nCCDSReFormer's superior performance. An ablation study also confirms the\nsignificant impact of each component on the model's predictive accuracy,\nshowcasing our model's ability to forecast traffic flow effectively.",
    "published": "2024-03-26T14:43:57Z",
    "updated": "2024-03-29T06:48:37Z",
    "authors": [
      "Zhiqi Shao",
      "Michael G. H. Bell",
      "Ze Wang",
      "D. Glenn Geers",
      "Xusheng Yao",
      "Junbin Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.18152v1",
    "title": "Masked Attention as a Mechanism for Improving Interpretability of Vision\n  Transformers",
    "summary": "Vision Transformers are at the heart of the current surge of interest in\nfoundation models for histopathology. They process images by breaking them into\nsmaller patches following a regular grid, regardless of their content. Yet, not\nall parts of an image are equally relevant for its understanding. This is\nparticularly true in computational pathology where background is completely\nnon-informative and may introduce artefacts that could mislead predictions. To\naddress this issue, we propose a novel method that explicitly masks background\nin Vision Transformers' attention mechanism. This ensures tokens corresponding\nto background patches do not contribute to the final image representation,\nthereby improving model robustness and interpretability. We validate our\napproach using prostate cancer grading from whole-slide images as a case study.\nOur results demonstrate that it achieves comparable performance with plain\nself-attention while providing more accurate and clinically meaningful\nattention heatmaps.",
    "published": "2024-04-28T12:02:38Z",
    "updated": "2024-04-28T12:02:38Z",
    "authors": [
      "ClÃ©ment Grisi",
      "Geert Litjens",
      "Jeroen van der Laak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.05001v1",
    "title": "HMANet: Hybrid Multi-Axis Aggregation Network for Image Super-Resolution",
    "summary": "Transformer-based methods have demonstrated excellent performance on\nsuper-resolution visual tasks, surpassing conventional convolutional neural\nnetworks. However, existing work typically restricts self-attention computation\nto non-overlapping windows to save computational costs. This means that\nTransformer-based networks can only use input information from a limited\nspatial range. Therefore, a novel Hybrid Multi-Axis Aggregation network (HMA)\nis proposed in this paper to exploit feature potential information better. HMA\nis constructed by stacking Residual Hybrid Transformer Blocks(RHTB) and Grid\nAttention Blocks(GAB). On the one side, RHTB combines channel attention and\nself-attention to enhance non-local feature fusion and produce more attractive\nvisual results. Conversely, GAB is used in cross-domain information interaction\nto jointly model similar features and obtain a larger perceptual field. For the\nsuper-resolution task in the training phase, a novel pre-training method is\ndesigned to enhance the model representation capabilities further and validate\nthe proposed model's effectiveness through many experiments. The experimental\nresults show that HMA outperforms the state-of-the-art methods on the benchmark\ndataset. We provide code and models at https://github.com/korouuuuu/HMA.",
    "published": "2024-05-08T12:14:34Z",
    "updated": "2024-05-08T12:14:34Z",
    "authors": [
      "Shu-Chuan Chu",
      "Zhi-Chao Dou",
      "Jeng-Shyang Pan",
      "Shaowei Weng",
      "Junbao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.14880v4",
    "title": "Dissecting Query-Key Interaction in Vision Transformers",
    "summary": "Self-attention in vision transformers is often thought to perform perceptual\ngrouping where tokens attend to other tokens with similar embeddings, which\ncould correspond to semantically similar features of an object. However,\nattending to dissimilar tokens can be beneficial by providing contextual\ninformation. We propose to analyze the query-key interaction by the singular\nvalue decomposition of the interaction matrix (i.e.\n${\\textbf{W}_q}^\\top\\textbf{W}_k$). We find that in many ViTs, especially those\nwith classification training objectives, early layers attend more to similar\ntokens, while late layers show increased attention to dissimilar tokens,\nproviding evidence corresponding to perceptual grouping and contextualization,\nrespectively. Many of these interactions between features represented by\nsingular vectors are interpretable and semantic, such as attention between\nrelevant objects, between parts of an object, or between the foreground and\nbackground. This offers a novel perspective on interpreting the attention\nmechanism, which contributes to understanding how transformer models utilize\ncontext and salient features when processing images.",
    "published": "2024-04-04T20:06:07Z",
    "updated": "2025-01-14T01:57:44Z",
    "authors": [
      "Xu Pan",
      "Aaron Philip",
      "Ziqian Xie",
      "Odelia Schwartz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15903v1",
    "title": "UnitNorm: Rethinking Normalization for Transformers in Time Series",
    "summary": "Normalization techniques are crucial for enhancing Transformer models'\nperformance and stability in time series analysis tasks, yet traditional\nmethods like batch and layer normalization often lead to issues such as token\nshift, attention shift, and sparse attention. We propose UnitNorm, a novel\napproach that scales input vectors by their norms and modulates attention\npatterns, effectively circumventing these challenges. Grounded in existing\nnormalization frameworks, UnitNorm's effectiveness is demonstrated across\ndiverse time series analysis tasks, including forecasting, classification, and\nanomaly detection, via a rigorous evaluation on 6 state-of-the-art models and\n10 datasets. Notably, UnitNorm shows superior performance, especially in\nscenarios requiring robust attention mechanisms and contextual comprehension,\nevidenced by significant improvements by up to a 1.46 decrease in MSE for\nforecasting, and a 4.89% increase in accuracy for classification. This work not\nonly calls for a reevaluation of normalization strategies in time series\nTransformers but also sets a new direction for enhancing model performance and\nstability. The source code is available at\nhttps://anonymous.4open.science/r/UnitNorm-5B84.",
    "published": "2024-05-24T19:58:25Z",
    "updated": "2024-05-24T19:58:25Z",
    "authors": [
      "Nan Huang",
      "Christian KÃ¼mmerle",
      "Xiang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.19138v2",
    "title": "Multi-Channel Multi-Step Spectrum Prediction Using Transformer and\n  Stacked Bi-LSTM",
    "summary": "Spectrum prediction is considered as a key technology to assist spectrum\ndecision. Despite the great efforts that have been put on the construction of\nspectrum prediction, achieving accurate spectrum prediction emphasizes the need\nfor more advanced solutions. In this paper, we propose a new multichannel\nmulti-step spectrum prediction method using Transformer and stacked\nbidirectional LSTM (Bi- LSTM), named TSB. Specifically, we use multi-head\nattention and stacked Bi-LSTM to build a new Transformer based on\nencoder-decoder architecture. The self-attention mechanism composed of multiple\nlayers of multi-head attention can continuously attend to all positions of the\nmultichannel spectrum sequences. The stacked Bi-LSTM can learn these focused\ncoding features by multi-head attention layer by layer. The advantage of this\nfusion mode is that it can deeply capture the long-term dependence of\nmultichannel spectrum data. We have conducted extensive experiments on a\ndataset generated by a real simulation platform. The results show that the\nproposed algorithm performs better than the baselines.",
    "published": "2024-05-29T14:42:24Z",
    "updated": "2025-03-22T14:59:54Z",
    "authors": [
      "Guangliang Pan",
      "Jie Li",
      "Minglei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.03710v2",
    "title": "TwinS: Revisiting Non-Stationarity in Multivariate Time Series\n  Forecasting",
    "summary": "Recently, multivariate time series forecasting tasks have garnered increasing\nattention due to their significant practical applications, leading to the\nemergence of various deep forecasting models. However, real-world time series\nexhibit pronounced non-stationary distribution characteristics. These\ncharacteristics are not solely limited to time-varying statistical properties\nhighlighted by non-stationary Transformer but also encompass three key aspects:\nnested periodicity, absence of periodic distributions, and hysteresis among\ntime variables. In this paper, we begin by validating this theory through\nwavelet analysis and propose the Transformer-based TwinS model, which consists\nof three modules to address the non-stationary periodic distributions: Wavelet\nConvolution, Period-Aware Attention, and Channel-Temporal Mixed MLP.\nSpecifically, The Wavelet Convolution models nested periods by scaling the\nconvolution kernel size like wavelet transform. The Period-Aware Attention\nguides attention computation by generating period relevance scores through a\nconvolutional sub-network. The Channel-Temporal Mixed MLP captures the overall\nrelationships between time series through channel-time mixing learning. TwinS\nachieves SOTA performance compared to mainstream TS models, with a maximum\nimprovement in MSE of 25.8\\% over PatchTST.",
    "published": "2024-06-06T03:14:23Z",
    "updated": "2024-07-14T14:55:16Z",
    "authors": [
      "Jiaxi Hu",
      "Qingsong Wen",
      "Sijie Ruan",
      "Li Liu",
      "Yuxuan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.16513v1",
    "title": "Multi-Modal Vision Transformers for Crop Mapping from Satellite Image\n  Time Series",
    "summary": "Using images acquired by different satellite sensors has shown to improve\nclassification performance in the framework of crop mapping from satellite\nimage time series (SITS). Existing state-of-the-art architectures use\nself-attention mechanisms to process the temporal dimension and convolutions\nfor the spatial dimension of SITS. Motivated by the success of purely\nattention-based architectures in crop mapping from single-modal SITS, we\nintroduce several multi-modal multi-temporal transformer-based architectures.\nSpecifically, we investigate the effectiveness of Early Fusion, Cross Attention\nFusion and Synchronized Class Token Fusion within the Temporo-Spatial Vision\nTransformer (TSViT). Experimental results demonstrate significant improvements\nover state-of-the-art architectures with both convolutional and self-attention\ncomponents.",
    "published": "2024-06-24T10:40:46Z",
    "updated": "2024-06-24T10:40:46Z",
    "authors": [
      "Theresa Follath",
      "David Mickisch",
      "Jan Hemmerling",
      "Stefan Erasmi",
      "Marcel Schwieder",
      "BegÃ¼m Demir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.17837v1",
    "title": "Transformer Normalisation Layers and the Independence of Semantic\n  Subspaces",
    "summary": "Recent works have shown that transformers can solve contextual reasoning\ntasks by internally executing computational graphs called circuits. Circuits\noften use attention to logically match information from subspaces of the\nrepresentation, e.g. using position-in-sequence to identify the previous token.\nIn this work, we consider a semantic subspace to be any independent subspace of\nthe latent representation that can fully determine an attention distribution.\nWe show that Pre-Norm, the placement of normalisation layer used by\nstate-of-the-art transformers, violates this ability unless the model learns a\nstrict representation structure of orthogonal spheres. This is because it\ncauses linear subspaces to interfere through their common normalisation factor.\nTheoretically, we analyse circuit stability by modelling this interference as\nrandom noise on the $L_2$-norms of the query/key/value vectors, predicting a\nphenomenon of circuit collapse when sparse-attention shifts to a different\ntoken. Empirically, we investigate the sensitivity of real-world models trained\nfor mathematical addition, observing a 1% rate of circuit collapse when the\nnorms are artificially perturbed by $\\lesssim$10%. We contrast Pre-Norm with\nQKV-Norm, which places normalisation after the attention head's linear\noperators. Theoretically this relaxes the representational constraints.\nEmpirically we observe comparable in-distribution but worse out-of-distribution\nperformance.",
    "published": "2024-06-25T16:16:38Z",
    "updated": "2024-06-25T16:16:38Z",
    "authors": [
      "Stephen Menary",
      "Samuel Kaski",
      "Andre Freitas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.04218v1",
    "title": "Batch Transformer: Look for Attention in Batch",
    "summary": "Facial expression recognition (FER) has received considerable attention in\ncomputer vision, with \"in-the-wild\" environments such as human-computer\ninteraction. However, FER images contain uncertainties such as occlusion, low\nresolution, pose variation, illumination variation, and subjectivity, which\nincludes some expressions that do not match the target label. Consequently,\nlittle information is obtained from a noisy single image and it is not trusted.\nThis could significantly degrade the performance of the FER task. To address\nthis issue, we propose a batch transformer (BT), which consists of the proposed\nclass batch attention (CBA) module, to prevent overfitting in noisy data and\nextract trustworthy information by training on features reflected from several\nimages in a batch, rather than information from a single image. We also propose\nmulti-level attention (MLA) to prevent overfitting the specific features by\ncapturing correlations between each level. In this paper, we present a batch\ntransformer network (BTN) that combines the above proposals. Experimental\nresults on various FER benchmark datasets show that the proposed BTN\nconsistently outperforms the state-ofthe-art in FER datasets. Representative\nresults demonstrate the promise of the proposed BTN for FER.",
    "published": "2024-07-05T02:13:47Z",
    "updated": "2024-07-05T02:13:47Z",
    "authors": [
      "Myung Beom Her",
      "Jisu Jeong",
      "Hojoon Song",
      "Ji-Hyeong Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.03516v1",
    "title": "LMLT: Low-to-high Multi-Level Vision Transformer for Image\n  Super-Resolution",
    "summary": "Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have\ndemonstrated impressive performance. However, they suffer from significant\ncomplexity, resulting in high inference times and memory usage. Additionally,\nViT models using Window Self-Attention (WSA) face challenges in processing\nregions outside their windows. To address these issues, we propose the\nLow-to-high Multi-Level Transformer (LMLT), which employs attention with\nvarying feature sizes for each head. LMLT divides image features along the\nchannel dimension, gradually reduces spatial size for lower heads, and applies\nself-attention to each head. This approach effectively captures both local and\nglobal information. By integrating the results from lower heads into higher\nheads, LMLT overcomes the window boundary issues in self-attention. Extensive\nexperiments show that our model significantly reduces inference time and GPU\nmemory usage while maintaining or even surpassing the performance of\nstate-of-the-art ViT-based Image Super-Resolution methods. Our codes are\navailiable at https://github.com/jwgdmkj/LMLT.",
    "published": "2024-09-05T13:29:50Z",
    "updated": "2024-09-05T13:29:50Z",
    "authors": [
      "Jeongsoo Kim",
      "Jongho Nang",
      "Junsuk Choe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.04940v2",
    "title": "An Analog and Digital Hybrid Attention Accelerator for Transformers with\n  Charge-based In-memory Computing",
    "summary": "The attention mechanism is a key computing kernel of Transformers,\ncalculating pairwise correlations across the entire input sequence. The\ncomputing complexity and frequent memory access in computing self-attention put\na huge burden on the system especially when the sequence length increases. This\npaper presents an analog and digital hybrid processor to accelerate the\nattention mechanism for transformers in 65nm CMOS technology. We propose an\nanalog computing-in-memory (CIM) core, which prunes ~75% of low-score tokens on\naverage during runtime at ultra-low power and delay. Additionally, a digital\nprocessor performs precise computations only for ~25% unpruned tokens selected\nby the analog CIM core, preventing accuracy degradation. Measured results show\npeak energy efficiency of 14.8 and 1.65 TOPS/W, and peak area efficiency of\n976.6 and 79.4 GOPS/mm$^\\mathrm{2}$ in the analog core and the system-on-chip\n(SoC), respectively.",
    "published": "2024-09-08T01:27:56Z",
    "updated": "2024-09-20T21:02:21Z",
    "authors": [
      "Ashkan Moradifirouzabadi",
      "Divya Sri Dodla",
      "Mingu Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.08461v1",
    "title": "VistaFormer: Scalable Vision Transformers for Satellite Image Time\n  Series Segmentation",
    "summary": "We introduce VistaFormer, a lightweight Transformer-based model architecture\nfor the semantic segmentation of remote-sensing images. This model uses a\nmulti-scale Transformer-based encoder with a lightweight decoder that\naggregates global and local attention captured in the encoder blocks.\nVistaFormer uses position-free self-attention layers which simplifies the model\narchitecture and removes the need to interpolate temporal and spatial codes,\nwhich can reduce model performance when training and testing image resolutions\ndiffer. We investigate simple techniques for filtering noisy input signals like\nclouds and demonstrate that improved model scalability can be achieved by\nsubstituting Multi-Head Self-Attention (MHSA) with Neighbourhood Attention\n(NA). Experiments on the PASTIS and MTLCC crop-type segmentation benchmarks\nshow that VistaFormer achieves better performance than comparable models and\nrequires only 8% of the floating point operations using MHSA and 11% using NA\nwhile also using fewer trainable parameters. VistaFormer with MHSA improves on\nstate-of-the-art mIoU scores by 0.1% on the PASTIS benchmark and 3% on the\nMTLCC benchmark while VistaFormer with NA improves on the MTLCC benchmark by\n3.7%.",
    "published": "2024-09-13T01:19:53Z",
    "updated": "2024-09-13T01:19:53Z",
    "authors": [
      "Ezra MacDonald",
      "Derek Jacoby",
      "Yvonne Coady"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.12865v2",
    "title": "KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning",
    "summary": "Knowledge graph reasoning plays a vital role in various applications and has\ngarnered considerable attention. Recently, path-based methods have achieved\nimpressive performance. However, they may face limitations stemming from\nconstraints in message-passing neural networks, such as missing paths and\ninformation over-squashing. In this paper, we revisit the application of\ntransformers for knowledge graph reasoning to address the constraints faced by\npath-based methods and propose a novel method KnowFormer. KnowFormer utilizes a\ntransformer architecture to perform reasoning on knowledge graphs from the\nmessage-passing perspective, rather than reasoning by textual information like\nprevious pretrained language model based methods. Specifically, we define the\nattention computation based on the query prototype of knowledge graph\nreasoning, facilitating convenient construction and efficient optimization. To\nincorporate structural information into the self-attention mechanism, we\nintroduce structure-aware modules to calculate query, key, and value\nrespectively. Additionally, we present an efficient attention computation\nmethod for better scalability. Experimental results demonstrate the superior\nperformance of KnowFormer compared to prominent baseline methods on both\ntransductive and inductive benchmarks.",
    "published": "2024-09-19T16:08:10Z",
    "updated": "2024-12-17T18:27:01Z",
    "authors": [
      "Junnan Liu",
      "Qianren Mao",
      "Weifeng Jiang",
      "Jianxin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.17174v1",
    "title": "From Attention to Activation: Unravelling the Enigmas of Large Language\n  Models",
    "summary": "We study two strange phenomena in auto-regressive Transformers: (1) the\ndominance of the first token in attention heads; (2) the occurrence of large\noutlier activations in the hidden states. We find that popular large language\nmodels, such as Llama attend maximally to the first token in 98% of attention\nheads, a behaviour we attribute to the softmax function. To mitigate this\nissue, we propose a reformulation of softmax to softmax-1. Furthermore, we\nidentify adaptive optimisers, e.g. Adam, as the primary contributor to the\nlarge outlier activations and introduce OrthoAdam, a novel optimiser that\nutilises orthogonal matrices to transform gradients, to address this issue.\nFinally, not only do our methods prevent these phenomena from occurring, but\nadditionally, they enable Transformers to sustain their performance when\nquantised using basic algorithms, something that standard methods are unable to\ndo. In summary, our methods reduce the attention proportion on the first token\nfrom 65% to 3.3%, the activation kurtosis in the hidden states from 1657 to\n3.1, and perplexity penalty under 4-bit weight quantisation from 3565 to 0.3.",
    "published": "2024-10-22T16:51:27Z",
    "updated": "2024-10-22T16:51:27Z",
    "authors": [
      "Prannay Kaul",
      "Chengcheng Ma",
      "Ismail Elezi",
      "Jiankang Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.18613v2",
    "title": "Rethinking Attention: Polynomial Alternatives to Softmax in Transformers",
    "summary": "This paper questions whether the strong performance of softmax attention in\ntransformers stems from producing a probability distribution over inputs.\nInstead, we argue that softmax's effectiveness lies in its implicit\nregularization of the Frobenius norm of the attention matrix, which stabilizes\ntraining. Motivated by this, we explore alternative activations, specifically\npolynomials, that achieve a similar regularization effect. Our theoretical\nanalysis shows that certain polynomials can serve as effective substitutes for\nsoftmax, achieving strong performance across transformer applications despite\nviolating softmax's typical properties of positivity, normalization, and\nsparsity. Extensive experiments support these findings, offering a new\nperspective on attention mechanisms.",
    "published": "2024-10-24T10:08:25Z",
    "updated": "2025-05-19T08:16:17Z",
    "authors": [
      "Hemanth Saratchandran",
      "Jianqiao Zheng",
      "Yiping Ji",
      "Wenbo Zhang",
      "Simon Lucey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.10881v2",
    "title": "FIAS: Feature Imbalance-Aware Medical Image Segmentation with Dynamic\n  Fusion and Mixing Attention",
    "summary": "With the growing application of transformer in computer vision, hybrid\narchitecture that combine convolutional neural networks (CNNs) and transformers\ndemonstrates competitive ability in medical image segmentation. However, direct\nfusion of features from CNNs and transformers often leads to feature imbalance\nand redundant information. To address these issues, we propose a Feaure\nImbalance-Aware Segmentation (FIAS) network, which incorporates a dual-path\nencoder and a novel Mixing Attention (MixAtt) decoder. The dual-branches\nencoder integrates a DilateFormer for long-range global feature extraction and\na Depthwise Multi-Kernel (DMK) convolution for capturing fine-grained local\ndetails. A Context-Aware Fusion (CAF) block dynamically balances the\ncontribution of these global and local features, preventing feature imbalance.\nThe MixAtt decoder further enhances segmentation accuracy by combining\nself-attention and Monte Carlo attention, enabling the model to capture both\nsmall details and large-scale dependencies. Experimental results on the Synapse\nmulti-organ and ACDC datasets demonstrate the strong competitiveness of our\napproach in medical image segmentation tasks.",
    "published": "2024-11-16T20:30:44Z",
    "updated": "2024-11-27T19:33:44Z",
    "authors": [
      "Xiwei Liu",
      "Min Xu",
      "Qirong Ho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.01979v2",
    "title": "FGATT: A Robust Framework for Wireless Data Imputation Using Fuzzy Graph\n  Attention Networks and Transformer Encoders",
    "summary": "Missing data is a pervasive challenge in wireless networks and many other\ndomains, often compromising the performance of machine learning and deep\nlearning models. To address this, we propose a novel framework, FGATT, that\ncombines the Fuzzy Graph Attention Network (FGAT) with the Transformer encoder\nto perform robust and accurate data imputation. FGAT leverages fuzzy rough sets\nand graph attention mechanisms to capture spatial dependencies dynamically,\neven in scenarios where predefined spatial information is unavailable. The\nTransformer encoder is employed to model temporal dependencies, utilizing its\nself-attention mechanism to focus on significant time-series patterns. A\nself-adaptive graph construction method is introduced to enable dynamic\nconnectivity learning, ensuring the framework's applicability to a wide range\nof wireless datasets. Extensive experiments demonstrate that our approach\noutperforms state-of-the-art methods in imputation accuracy and robustness,\nparticularly in scenarios with substantial missing data. The proposed model is\nwell-suited for applications in wireless sensor networks and IoT environments,\nwhere data integrity is critical.",
    "published": "2024-12-02T21:16:47Z",
    "updated": "2025-02-01T22:44:21Z",
    "authors": [
      "Jinming Xing",
      "Chang Xue",
      "Dongwen Luo",
      "Ruilin Xing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.16515v1",
    "title": "VSFormer: Value and Shape-Aware Transformer with Prior-Enhanced\n  Self-Attention for Multivariate Time Series Classification",
    "summary": "Multivariate time series classification is a crucial task in data mining,\nattracting growing research interest due to its broad applications. While many\nexisting methods focus on discovering discriminative patterns in time series,\nreal-world data does not always present such patterns, and sometimes raw\nnumerical values can also serve as discriminative features. Additionally, the\nrecent success of Transformer models has inspired many studies. However, when\napplying to time series classification, the self-attention mechanisms in\nTransformer models could introduce classification-irrelevant features, thereby\ncompromising accuracy. To address these challenges, we propose a novel method,\nVSFormer, that incorporates both discriminative patterns (shape) and numerical\ninformation (value). In addition, we extract class-specific prior information\nderived from supervised information to enrich the positional encoding and\nprovide classification-oriented self-attention learning, thereby enhancing its\neffectiveness. Extensive experiments on all 30 UEA archived datasets\ndemonstrate the superior performance of our method compared to SOTA models.\nThrough ablation studies, we demonstrate the effectiveness of the improved\nencoding layer and the proposed self-attention mechanism. Finally, We provide a\ncase study on a real-world time series dataset without discriminative patterns\nto interpret our model.",
    "published": "2024-12-21T07:31:22Z",
    "updated": "2024-12-21T07:31:22Z",
    "authors": [
      "Wenjie Xi",
      "Rundong Zuo",
      "Alejandro Alvarez",
      "Jie Zhang",
      "Byron Choi",
      "Jessica Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07436v1",
    "title": "Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head\n  Attention without Alignment Barriers",
    "summary": "Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance.",
    "published": "2025-02-11T10:24:57Z",
    "updated": "2025-02-11T10:24:57Z",
    "authors": [
      "Zhaodong Bing",
      "Linze Li",
      "Jiajun Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.20337v1",
    "title": "Progressive Focused Transformer for Single Image Super-Resolution",
    "summary": "Transformer-based methods have achieved remarkable results in image\nsuper-resolution tasks because they can capture non-local dependencies in\nlow-quality input images. However, this feature-intensive modeling approach is\ncomputationally expensive because it calculates the similarities between\nnumerous features that are irrelevant to the query features when obtaining\nattention weights. These unnecessary similarity calculations not only degrade\nthe reconstruction performance but also introduce significant computational\noverhead. How to accurately identify the features that are important to the\ncurrent query features and avoid similarity calculations between irrelevant\nfeatures remains an urgent problem. To address this issue, we propose a novel\nand effective Progressive Focused Transformer (PFT) that links all isolated\nattention maps in the network through Progressive Focused Attention (PFA) to\nfocus attention on the most important tokens. PFA not only enables the network\nto capture more critical similar features, but also significantly reduces the\ncomputational cost of the overall network by filtering out irrelevant features\nbefore calculating similarities. Extensive experiments demonstrate the\neffectiveness of the proposed method, achieving state-of-the-art performance on\nvarious single image super-resolution benchmarks.",
    "published": "2025-03-26T09:02:37Z",
    "updated": "2025-03-26T09:02:37Z",
    "authors": [
      "Wei Long",
      "Xingyu Zhou",
      "Leheng Zhang",
      "Shuhang Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.06704v1",
    "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers",
    "summary": "Transformers have driven remarkable breakthroughs in natural language\nprocessing and computer vision, yet their standard attention mechanism still\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\nefficiently applies circular convolutions to reduce complexity without\nsacrificing representational power. CAT achieves O(NlogN) computations,\nrequires fewer learnable parameters by streamlining fully-connected layers, and\nintroduces no heavier operations, resulting in consistent accuracy improvements\nand about a 10% speedup in naive PyTorch implementations on large-scale\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\nengineering-isomorphism framework, CAT's design not only offers practical\nefficiency and ease of implementation but also provides insights to guide the\ndevelopment of next-generation, high-performance Transformer architectures.\nFinally, our ablation studies highlight the key conditions underlying CAT's\nsuccess, shedding light on broader principles for scalable attention\nmechanisms.",
    "published": "2025-04-09T09:08:26Z",
    "updated": "2025-04-09T09:08:26Z",
    "authors": [
      "Yoshihiro Yamada"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20481v1",
    "title": "CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG\n  Classification with Transformer Architecture",
    "summary": "Accurate ECG interpretation is vital, yet complex cardiac data and\n\"black-box\" AI models limit clinical utility. Inspired by Transformer\narchitectures' success in NLP for understanding sequential data, we frame ECG\nas the heart's unique \"language\" of temporal patterns. We present\nCardioPatternFormer, a novel Transformer-based model for interpretable ECG\nclassification. It employs a sophisticated attention mechanism to precisely\nidentify and classify diverse cardiac patterns, excelling at discerning subtle\nanomalies and distinguishing multiple co-occurring conditions. This\npattern-guided attention provides clear insights by highlighting influential\nsignal regions, effectively allowing the \"heart to talk\" through transparent\ninterpretations. CardioPatternFormer demonstrates robust performance on\nchallenging ECGs, including complex multi-pathology cases. Its interpretability\nvia attention maps enables clinicians to understand the model's rationale,\nfostering trust and aiding informed diagnostic decisions. This work offers a\npowerful, transparent solution for advanced ECG analysis, paving the way for\nmore reliable and clinically actionable AI in cardiology.",
    "published": "2025-05-26T19:36:58Z",
    "updated": "2025-05-26T19:36:58Z",
    "authors": [
      "Berat Kutay UÄraÅ",
      "Ãmer Nezih Gerek",
      "Ä°brahim Talha SaygÄ±"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08324v2",
    "title": "Hyperspectral Image Classification via Transformer-based\n  Spectral-Spatial Attention Decoupling and Adaptive Gating",
    "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more effectively extract\nand fuse spatial context with fine spectral information in hyperspectral image\n(HSI) classification, this paper proposes a novel network architecture called\nSTNet. The core advantage of STNet stems from the dual innovative design of its\nSpatial-Spectral Transformer module: first, the fundamental explicit decoupling\nof spatial and spectral attention ensures targeted capture of key information\nin HSI; second, two functionally distinct gating mechanisms perform intelligent\nregulation at both the fusion level of attention flows (adaptive attention\nfusion gating) and the internal level of feature transformation (GFFN). This\ncharacteristic demonstrates superior feature extraction and fusion capabilities\ncompared to traditional convolutional neural networks, while reducing\noverfitting risks in small-sample and high-noise scenarios. STNet enhances\nmodel representation capability without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.",
    "published": "2025-06-10T01:24:35Z",
    "updated": "2025-06-11T01:30:34Z",
    "authors": [
      "Guandong Li",
      "Mengxia Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10371v1",
    "title": "Revisiting Transformers with Insights from Image Filtering",
    "summary": "The self-attention mechanism, a cornerstone of Transformer-based\nstate-of-the-art deep learning architectures, is largely heuristic-driven and\nfundamentally challenging to interpret. Establishing a robust theoretical\nfoundation to explain its remarkable success and limitations has therefore\nbecome an increasingly prominent focus in recent research. Some notable\ndirections have explored understanding self-attention through the lens of image\ndenoising and nonparametric regression. While promising, existing frameworks\nstill lack a deeper mechanistic interpretation of various architectural\ncomponents that enhance self-attention, both in its original formulation and\nsubsequent variants. In this work, we aim to advance this understanding by\ndeveloping a unifying image processing framework, capable of explaining not\nonly the self-attention computation itself but also the role of components such\nas positional encoding and residual connections, including numerous later\nvariants. We also pinpoint potential distinctions between the two concepts\nbuilding upon our framework, and make effort to close this gap. We introduce\ntwo independent architectural modifications within transformers. While our\nprimary objective is interpretability, we empirically observe that image\nprocessing-inspired modifications can also lead to notably improved accuracy\nand robustness against data contamination and adversaries across language and\nvision tasks as well as better long sequence understanding.",
    "published": "2025-06-12T05:46:57Z",
    "updated": "2025-06-12T05:46:57Z",
    "authors": [
      "Laziz U. Abdullaev",
      "Maksim Tkachenko",
      "Tan M. Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18520v1",
    "title": "Enhancing Image Restoration Transformer via Adaptive Translation\n  Equivariance",
    "summary": "Translation equivariance is a fundamental inductive bias in image\nrestoration, ensuring that translated inputs produce translated outputs.\nAttention mechanisms in modern restoration transformers undermine this\nproperty, adversely impacting both training convergence and generalization. To\nalleviate this issue, we propose two key strategies for incorporating\ntranslation equivariance: slide indexing and component stacking. Slide indexing\nmaintains operator responses at fixed positions, with sliding window attention\nbeing a notable example, while component stacking enables the arrangement of\ntranslation-equivariant operators in parallel or sequentially, thereby building\ncomplex architectures while preserving translation equivariance. However, these\nstrategies still create a dilemma in model design between the high\ncomputational cost of self-attention and the fixed receptive field associated\nwith sliding window attention. To address this, we develop an adaptive sliding\nindexing mechanism to efficiently select key-value pairs for each query, which\nare then concatenated in parallel with globally aggregated key-value pairs. The\ndesigned network, called the Translation Equivariance Adaptive Transformer\n(TEAFormer), is assessed across a variety of image restoration tasks. The\nresults highlight its superiority in terms of effectiveness, training\nconvergence, and generalization.",
    "published": "2025-06-23T11:23:04Z",
    "updated": "2025-06-23T11:23:04Z",
    "authors": [
      "JiaKui Hu",
      "Zhengjian Yao",
      "Lujia Jin",
      "Hangzhou He",
      "Yanye Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02754v1",
    "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.",
    "published": "2025-07-03T16:16:34Z",
    "updated": "2025-07-03T16:16:34Z",
    "authors": [
      "Aurko Roy",
      "Timothy Chou",
      "Sai Surya Duvvuri",
      "Sijia Chen",
      "Jiecao Yu",
      "Xiaodong Wang",
      "Manzil Zaheer",
      "Rohan Anil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20259v1",
    "title": "L-MCAT: Unpaired Multimodal Transformer with Contrastive Attention for\n  Label-Efficient Satellite Image Classification",
    "summary": "We propose the Lightweight Multimodal Contrastive Attention Transformer\n(L-MCAT), a novel transformer-based framework for label-efficient remote\nsensing image classification using unpaired multimodal satellite data. L-MCAT\nintroduces two core innovations: (1) Modality-Spectral Adapters (MSA) that\ncompress high-dimensional sensor inputs into a unified embedding space, and (2)\nUnpaired Multimodal Attention Alignment (U-MAA), a contrastive self-supervised\nmechanism integrated into the attention layers to align heterogeneous\nmodalities without pixel-level correspondence or labels. L-MCAT achieves 95.4%\noverall accuracy on the SEN12MS dataset using only 20 labels per class,\noutperforming state-of-the-art baselines while using 47x fewer parameters and\n23x fewer FLOPs than MCTrans. It maintains over 92% accuracy even under 50%\nspatial misalignment, demonstrating robustness for real-world deployment. The\nmodel trains end-to-end in under 5 hours on a single consumer GPU.",
    "published": "2025-07-27T13:06:32Z",
    "updated": "2025-07-27T13:06:32Z",
    "authors": [
      "Mitul Goswami",
      "Mrinal Goswami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04782v1",
    "title": "VARMA-Enhanced Transformer for Time Series Forecasting",
    "summary": "Transformer-based models have significantly advanced time series forecasting.\nRecent work, like the Cross-Attention-only Time Series transformer (CATS),\nshows that removing self-attention can make the model more accurate and\nefficient. However, these streamlined architectures may overlook the\nfine-grained, local temporal dependencies effectively captured by classical\nstatistical models like Vector AutoRegressive Moving Average model (VARMA). To\naddress this gap, we propose VARMAformer, a novel architecture that synergizes\nthe efficiency of a cross-attention-only framework with the principles of\nclassical time series analysis. Our model introduces two key innovations: (1) a\ndedicated VARMA-inspired Feature Extractor (VFE) that explicitly models\nautoregressive (AR) and moving-average (MA) patterns at the patch level, and\n(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal\ngate to make queries more context-aware. By fusing these classical insights\ninto a modern backbone, VARMAformer captures both global, long-range\ndependencies and local, statistical structures. Through extensive experiments\non widely-used benchmark datasets, we demonstrate that our model consistently\noutperforms existing state-of-the-art methods. Our work validates the\nsignificant benefit of integrating classical statistical insights into modern\ndeep learning frameworks for time series forecasting.",
    "published": "2025-09-05T03:32:51Z",
    "updated": "2025-09-05T03:32:51Z",
    "authors": [
      "Jiajun Song",
      "Xiaoou Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06070v1",
    "title": "There is More to Attention: Statistical Filtering Enhances Explanations\n  in Vision Transformers",
    "summary": "Explainable AI (XAI) has become increasingly important with the rise of large\ntransformer models, yet many explanation methods designed for CNNs transfer\npoorly to Vision Transformers (ViTs). Existing ViT explanations often rely on\nattention weights, which tend to yield noisy maps as they capture\ntoken-to-token interactions within each layer.While attribution methods\nincorporating MLP blocks have been proposed, we argue that attention remains a\nvaluable and interpretable signal when properly filtered. We propose a method\nthat combines attention maps with a statistical filtering, initially proposed\nfor CNNs, to remove noisy or uninformative patterns and produce more faithful\nexplanations. We further extend our approach with a class-specific variant that\nyields discriminative explanations. Evaluation against popular state-of-the-art\nmethods demonstrates that our approach produces sharper and more interpretable\nmaps. In addition to perturbation-based faithfulness metrics, we incorporate\nhuman gaze data to assess alignment with human perception, arguing that human\ninterpretability remains essential for XAI. Across multiple datasets, our\napproach consistently outperforms or is comparable to the SOTA methods while\nremaining efficient and human plausible.",
    "published": "2025-10-07T15:59:04Z",
    "updated": "2025-10-07T15:59:04Z",
    "authors": [
      "Meghna P Ayyar",
      "Jenny Benois-Pineau",
      "Akka Zemmari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1811.00253v3",
    "title": "Hybrid Self-Attention Network for Machine Translation",
    "summary": "The encoder-decoder is the typical framework for Neural Machine Translation\n(NMT), and different structures have been developed for improving the\ntranslation performance. Transformer is one of the most promising structures,\nwhich can leverage the self-attention mechanism to capture the semantic\ndependency from global view. However, it cannot distinguish the relative\nposition of different tokens very well, such as the tokens located at the left\nor right of the current token, and cannot focus on the local information around\nthe current token either. To alleviate these problems, we propose a novel\nattention mechanism named Hybrid Self-Attention Network (HySAN) which\naccommodates some specific-designed masks for self-attention network to extract\nvarious semantic, such as the global/local information, the left/right part\ncontext. Finally, a squeeze gate is introduced to combine different kinds of\nSANs for fusion. Experimental results on three machine translation tasks show\nthat our proposed framework outperforms the Transformer baseline significantly\nand achieves superior results over state-of-the-art NMT systems.",
    "published": "2018-11-01T06:35:21Z",
    "updated": "2018-12-10T11:50:42Z",
    "authors": [
      "Kaitao Song",
      "Xu Tan",
      "Furong Peng",
      "Jianfeng Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1810.00825v3",
    "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant\n  Neural Networks",
    "summary": "Many machine learning tasks such as multiple instance learning, 3D shape\nrecognition, and few-shot image classification are defined on sets of\ninstances. Since solutions to such problems do not depend on the order of\nelements of the set, models used to address them should be permutation\ninvariant. We present an attention-based neural network module, the Set\nTransformer, specifically designed to model interactions among elements in the\ninput set. The model consists of an encoder and a decoder, both of which rely\non attention mechanisms. In an effort to reduce computational complexity, we\nintroduce an attention scheme inspired by inducing point methods from sparse\nGaussian process literature. It reduces the computation time of self-attention\nfrom quadratic to linear in the number of elements in the set. We show that our\nmodel is theoretically attractive and we evaluate it on a range of tasks,\ndemonstrating the state-of-the-art performance compared to recent methods for\nset-structured data.",
    "published": "2018-10-01T17:10:03Z",
    "updated": "2019-05-26T06:05:29Z",
    "authors": [
      "Juho Lee",
      "Yoonho Lee",
      "Jungtaek Kim",
      "Adam R. Kosiorek",
      "Seungjin Choi",
      "Yee Whye Teh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.04070v1",
    "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning",
    "summary": "The Transformer model is widely successful on many natural language\nprocessing tasks. However, the quadratic complexity of self-attention limit its\napplication on long text. In this paper, adopting a fine-to-coarse attention\nmechanism on multi-scale spans via binary partitioning (BP), we propose\nBP-Transformer (BPT for short). BPT yields $O(k\\cdot n\\log (n/k))$ connections\nwhere $k$ is a hyperparameter to control the density of attention. BPT has a\ngood balance between computation complexity and model capacity. A series of\nexperiments on text classification, machine translation and language modeling\nshows BPT has a superior performance for long text than previous self-attention\nmodels. Our code, hyperparameters and CUDA kernels for sparse attention are\navailable in PyTorch.",
    "published": "2019-11-11T04:31:23Z",
    "updated": "2019-11-11T04:31:23Z",
    "authors": [
      "Zihao Ye",
      "Qipeng Guo",
      "Quan Gan",
      "Xipeng Qiu",
      "Zheng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.00609v1",
    "title": "CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with\n  Multi-Head Self-Attention Weights based Counterfactual Detection",
    "summary": "In this paper, we describe an approach for modelling causal reasoning in\nnatural language by detecting counterfactuals in text using multi-head\nself-attention weights. We use pre-trained transformer models to extract\ncontextual embeddings and self-attention weights from the text. We show the use\nof convolutional layers to extract task-specific features from these\nself-attention weights. Further, we describe a fine-tuning approach with a\ncommon base model for knowledge sharing between the two closely related\nsub-tasks for counterfactual detection. We analyze and compare the performance\nof various transformer models in our experiments. Finally, we perform a\nqualitative analysis with the multi-head self-attention weights to interpret\nour models' dynamics.",
    "published": "2020-05-31T21:02:25Z",
    "updated": "2020-05-31T21:02:25Z",
    "authors": [
      "Rajaswa Patil",
      "Veeky Baths"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.03945v1",
    "title": "On Biasing Transformer Attention Towards Monotonicity",
    "summary": "Many sequence-to-sequence tasks in natural language processing are roughly\nmonotonic in the alignment between source and target sequence, and previous\nwork has facilitated or enforced learning of monotonic attention behavior via\nspecialized attention functions or pretraining. In this work, we introduce a\nmonotonicity loss function that is compatible with standard attention\nmechanisms and test it on several sequence-to-sequence tasks:\ngrapheme-to-phoneme conversion, morphological inflection, transliteration, and\ndialect normalization. Experiments show that we can achieve largely monotonic\nbehavior. Performance is mixed, with larger gains on top of RNN baselines.\nGeneral monotonicity does not benefit transformer multihead attention, however,\nwe see isolated improvements when only a subset of heads is biased towards\nmonotonic behavior.",
    "published": "2021-04-08T17:42:05Z",
    "updated": "2021-04-08T17:42:05Z",
    "authors": [
      "Annette Rios",
      "Chantal Amrhein",
      "NoÃ«mi Aepli",
      "Rico Sennrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.10090v2",
    "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms",
    "summary": "Self-attention, an architectural motif designed to model long-range\ninteractions in sequential data, has driven numerous recent breakthroughs in\nnatural language processing and beyond. This work provides a theoretical\nanalysis of the inductive biases of self-attention modules. Our focus is to\nrigorously establish which functions and long-range dependencies self-attention\nblocks prefer to represent. Our main result shows that bounded-norm Transformer\nnetworks \"create sparse variables\": a single self-attention head can represent\na sparse function of the input sequence, with sample complexity scaling only\nlogarithmically with the context length. To support our analysis, we present\nsynthetic experiments to probe the sample complexity of learning sparse Boolean\nfunctions with Transformers.",
    "published": "2021-10-19T16:36:19Z",
    "updated": "2022-06-24T02:32:42Z",
    "authors": [
      "Benjamin L. Edelman",
      "Surbhi Goel",
      "Sham Kakade",
      "Cyril Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.00138v1",
    "title": "Spatiotemporal Transformer Attention Network for 3D Voxel Level Joint\n  Segmentation and Motion Prediction in Point Cloud",
    "summary": "Environment perception including detection, classification, tracking, and\nmotion prediction are key enablers for automated driving systems and\nintelligent transportation applications. Fueled by the advances in sensing\ntechnologies and machine learning techniques, LiDAR-based sensing systems have\nbecome a promising solution. The current challenges of this solution are how to\neffectively combine different perception tasks into a single backbone and how\nto efficiently learn the spatiotemporal features directly from point cloud\nsequences. In this research, we propose a novel spatiotemporal attention\nnetwork based on a transformer self-attention mechanism for joint semantic\nsegmentation and motion prediction within a point cloud at the voxel level. The\nnetwork is trained to simultaneously outputs the voxel level class and\npredicted motion by learning directly from a sequence of point cloud datasets.\nThe proposed backbone includes both a temporal attention module (TAM) and a\nspatial attention module (SAM) to learn and extract the complex spatiotemporal\nfeatures. This approach has been evaluated with the nuScenes dataset, and\npromising performance has been achieved.",
    "published": "2022-02-28T23:18:27Z",
    "updated": "2022-02-28T23:18:27Z",
    "authors": [
      "Zhensong Wei",
      "Xuewei Qi",
      "Zhengwei Bai",
      "Guoyuan Wu",
      "Saswat Nayak",
      "Peng Hao",
      "Matthew Barth",
      "Yongkang Liu",
      "Kentaro Oguchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.12487v1",
    "title": "A Context-Aware Feature Fusion Framework for Punctuation Restoration",
    "summary": "To accomplish the punctuation restoration task, most existing approaches\nfocused on leveraging extra information (e.g., part-of-speech tags) or\naddressing the class imbalance problem. Recent works have widely applied the\ntransformer-based language models and significantly improved their\neffectiveness. To the best of our knowledge, an inherent issue has remained\nneglected: the attention of individual heads in the transformer will be diluted\nor powerless while feeding the long non-punctuation utterances. Since those\nprevious contexts, not the followings, are comparatively more valuable to the\ncurrent position, it's hard to achieve a good balance by independent attention.\nIn this paper, we propose a novel Feature Fusion framework based on two-type\nAttentions (FFA) to alleviate the shortage. It introduces a two-stream\narchitecture. One module involves interaction between attention heads to\nencourage the communication, and another masked attention module captures the\ndependent feature representation. Then, it aggregates two feature embeddings to\nfuse information and enhances context-awareness. The experiments on the popular\nbenchmark dataset IWSLT demonstrate that our approach is effective. Without\nadditional data, it obtains comparable performance to the current\nstate-of-the-art models.",
    "published": "2022-03-23T15:29:28Z",
    "updated": "2022-03-23T15:29:28Z",
    "authors": [
      "Yangjun Wu",
      "Kebin Fang",
      "Yao Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.15428v1",
    "title": "Explanation on Pretraining Bias of Finetuned Vision Transformer",
    "summary": "As the number of fine tuning of pretrained models increased, understanding\nthe bias of pretrained model is essential. However, there is little tool to\nanalyse transformer architecture and the interpretation of the attention maps\nis still challenging. To tackle the interpretability, we propose\nInput-Attribution and Attention Score Vector (IAV) which measures the\nsimilarity between attention map and input-attribution and shows the general\ntrend of interpretable attention patterns. We empirically explain the\npretraining bias of supervised and unsupervised pretrained ViT models, and show\nthat each head in ViT has a specific range of agreement on the decision of the\nclassification. We show that generalization, robustness and entropy of\nattention maps are not property of pretraining types. On the other hand, IAV\ntrend can separate the pretraining types.",
    "published": "2022-11-18T07:57:38Z",
    "updated": "2022-11-18T07:57:38Z",
    "authors": [
      "Bumjin Park",
      "Jaesik Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.13803v3",
    "title": "Fairness-aware Vision Transformer via Debiased Self-Attention",
    "summary": "Vision Transformer (ViT) has recently gained significant attention in solving\ncomputer vision (CV) problems due to its capability of extracting informative\nfeatures and modeling long-range dependencies through the attention mechanism.\nWhereas recent works have explored the trustworthiness of ViT, including its\nrobustness and explainability, the issue of fairness has not yet been\nadequately addressed. We establish that the existing fairness-aware algorithms\ndesigned for CNNs do not perform well on ViT, which highlights the need to\ndevelop our novel framework via Debiased Self-Attention (DSA). DSA is a\nfairness-through-blindness approach that enforces ViT to eliminate spurious\nfeatures correlated with the sensitive label for bias mitigation and\nsimultaneously retain real features for target prediction. Notably, DSA\nleverages adversarial examples to locate and mask the spurious features in the\ninput image patches with an additional attention weights alignment regularizer\nin the training objective to encourage learning real features for target\nprediction. Importantly, our DSA framework leads to improved fairness\nguarantees over prior works on multiple prediction tasks without compromising\ntarget prediction performance. Code is available at\n\\href{https://github.com/qiangyao1988/DSA}{https://github.com/qiangyao1988/DSA}.",
    "published": "2023-01-31T17:44:59Z",
    "updated": "2024-07-11T02:11:49Z",
    "authors": [
      "Yao Qiang",
      "Chengyin Li",
      "Prashant Khanduri",
      "Dongxiao Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.06971v1",
    "title": "Preserving Locality in Vision Transformers for Class Incremental\n  Learning",
    "summary": "Learning new classes without forgetting is crucial for real-world\napplications for a classification model. Vision Transformers (ViT) recently\nachieve remarkable performance in Class Incremental Learning (CIL). Previous\nworks mainly focus on block design and model expansion for ViTs. However, in\nthis paper, we find that when the ViT is incrementally trained, the attention\nlayers gradually lose concentration on local features. We call this interesting\nphenomenon as \\emph{Locality Degradation} in ViTs for CIL. Since the low-level\nlocal information is crucial to the transferability of the representation, it\nis beneficial to preserve the locality in attention layers. In this paper, we\nencourage the model to preserve more local information as the training\nprocedure goes on and devise a Locality-Preserved Attention (LPA) layer to\nemphasize the importance of local features. Specifically, we incorporate the\nlocal information directly into the vanilla attention and control the initial\ngradients of the vanilla attention by weighting it with a small initial value.\nExtensive experiments show that the representations facilitated by LPA capture\nmore low-level general information which is easier to transfer to follow-up\ntasks. The improved model gets consistently better performance on CIFAR100 and\nImageNet100.",
    "published": "2023-04-14T07:42:21Z",
    "updated": "2023-04-14T07:42:21Z",
    "authors": [
      "Bowen Zheng",
      "Da-Wei Zhou",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.07235v4",
    "title": "Mapping of attention mechanisms to a generalized Potts model",
    "summary": "Transformers are neural networks that revolutionized natural language\nprocessing and machine learning. They process sequences of inputs, like words,\nusing a mechanism called self-attention, which is trained via masked language\nmodeling (MLM). In MLM, a word is randomly masked in an input sequence, and the\nnetwork is trained to predict the missing word. Despite the practical success\nof transformers, it remains unclear what type of data distribution\nself-attention can learn efficiently. Here, we show analytically that if one\ndecouples the treatment of word positions and embeddings, a single layer of\nself-attention learns the conditionals of a generalized Potts model with\ninteractions between sites and Potts colors. Moreover, we show that training\nthis neural network is exactly equivalent to solving the inverse Potts problem\nby the so-called pseudo-likelihood method, well known in statistical physics.\nUsing this mapping, we compute the generalization error of self-attention in a\nmodel scenario analytically using the replica method.",
    "published": "2023-04-14T16:32:56Z",
    "updated": "2024-04-04T13:24:36Z",
    "authors": [
      "Riccardo Rende",
      "Federica Gerace",
      "Alessandro Laio",
      "Sebastian Goldt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.10875v1",
    "title": "Vision Transformer with Attention Map Hallucination and FFN Compaction",
    "summary": "Vision Transformer(ViT) is now dominating many vision tasks. The drawback of\nquadratic complexity of its token-wise multi-head self-attention (MHSA), is\nextensively addressed via either token sparsification or dimension reduction\n(in spatial or channel). However, the therein redundancy of MHSA is usually\noverlooked and so is the feed-forward network (FFN). To this end, we propose\nattention map hallucination and FFN compaction to fill in the blank.\nSpecifically, we observe similar attention maps exist in vanilla ViT and\npropose to hallucinate half of the attention maps from the rest with much\ncheaper operations, which is called hallucinated-MHSA (hMHSA). As for FFN, we\nfactorize its hidden-to-output projection matrix and leverage the\nre-parameterization technique to strengthen its capability, making it\ncompact-FFN (cFFN). With our proposed modules, a 10$\\%$-20$\\%$ reduction of\nfloating point operations (FLOPs) and parameters (Params) is achieved for\nvarious ViT-based backbones, including straight (DeiT), hybrid (NextViT) and\nhierarchical (PVT) structures, meanwhile, the performances are quite\ncompetitive.",
    "published": "2023-06-19T12:08:55Z",
    "updated": "2023-06-19T12:08:55Z",
    "authors": [
      "Haiyang Xu",
      "Zhichao Zhou",
      "Dongliang He",
      "Fu Li",
      "Jingdong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.12298v1",
    "title": "StarVQA+: Co-training Space-Time Attention for Video Quality Assessment",
    "summary": "Self-attention based Transformer has achieved great success in many computer\nvision tasks. However, its application to video quality assessment (VQA) has\nnot been satisfactory so far. Evaluating the quality of in-the-wild videos is\nchallenging due to the unknown of pristine reference and shooting distortion.\nThis paper presents a co-trained Space-Time Attention network for the VQA\nproblem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately\nconcatenating the divided space-time attention. Then, to facilitate the\ntraining of StarVQA+, we design a vectorized regression loss by encoding the\nmean opinion score (MOS) to the probability vector and embedding a special\ntoken as the learnable variable of MOS, leading to better fitting of human's\nrating process. Finally, to solve the data hungry problem with Transformer, we\npropose to co-train the spatial and temporal attention weights using both\nimages and videos. Various experiments are conducted on the de-facto\nin-the-wild video datasets, including LIVE-Qualcomm, LIVE-VQC, KoNViD-1k,\nYouTube-UGC, LSVQ, LSVQ-1080p, and DVL2021. Experimental results demonstrate\nthe superiority of the proposed StarVQA+ over the state-of-the-art.",
    "published": "2023-06-21T14:27:31Z",
    "updated": "2023-06-21T14:27:31Z",
    "authors": [
      "Fengchuang Xing",
      "Yuan-Gen Wang",
      "Weixuan Tang",
      "Guopu Zhu",
      "Sam Kwong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.14174v1",
    "title": "Only 5\\% Attention Is All You Need: Efficient Long-range Document-level\n  Neural Machine Translation",
    "summary": "Document-level Neural Machine Translation (DocNMT) has been proven crucial\nfor handling discourse phenomena by introducing document-level context\ninformation. One of the most important directions is to input the whole\ndocument directly to the standard Transformer model. In this case, efficiency\nbecomes a critical concern due to the quadratic complexity of the attention\nmodule. Existing studies either focus on the encoder part, which cannot be\ndeployed on sequence-to-sequence generation tasks, e.g., Machine Translation\n(MT), or suffer from a significant performance drop. In this work, we keep the\ntranslation performance while gaining 20\\% speed up by introducing extra\nselection layer based on lightweight attention that selects a small portion of\ntokens to be attended. It takes advantage of the original attention to ensure\nperformance and dimension reduction to accelerate inference. Experimental\nresults show that our method could achieve up to 95\\% sparsity (only 5\\% tokens\nattended) approximately, and save 93\\% computation cost on the attention module\ncompared with the original Transformer, while maintaining the performance.",
    "published": "2023-09-25T14:33:47Z",
    "updated": "2023-09-25T14:33:47Z",
    "authors": [
      "Zihan Liu",
      "Zewei Sun",
      "Shanbo Cheng",
      "Shujian Huang",
      "Mingxuan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.10318v1",
    "title": "Interpreting and Exploiting Functional Specialization in Multi-Head\n  Attention under Multi-task Learning",
    "summary": "Transformer-based models, even though achieving super-human performance on\nseveral downstream tasks, are often regarded as a black box and used as a\nwhole. It is still unclear what mechanisms they have learned, especially their\ncore module: multi-head attention. Inspired by functional specialization in the\nhuman brain, which helps to efficiently handle multiple tasks, this work\nattempts to figure out whether the multi-head attention module will evolve\nsimilar function separation under multi-tasking training. If it is, can this\nmechanism further improve the model performance? To investigate these\nquestions, we introduce an interpreting method to quantify the degree of\nfunctional specialization in multi-head attention. We further propose a simple\nmulti-task training method to increase functional specialization and mitigate\nnegative information transfer in multi-task learning. Experimental results on\nseven pre-trained transformer models have demonstrated that multi-head\nattention does evolve functional specialization phenomenon after multi-task\ntraining which is affected by the similarity of tasks. Moreover, the multi-task\ntraining strategy based on functional specialization boosts performance in both\nmulti-task learning and transfer learning without adding any parameters.",
    "published": "2023-10-16T11:55:53Z",
    "updated": "2023-10-16T11:55:53Z",
    "authors": [
      "Chong Li",
      "Shaonan Wang",
      "Yunhao Zhang",
      "Jiajun Zhang",
      "Chengqing Zong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.11265v1",
    "title": "Image Compression using only Attention based Neural Networks",
    "summary": "In recent research, Learned Image Compression has gained prominence for its\ncapacity to outperform traditional handcrafted pipelines, especially at low\nbit-rates. While existing methods incorporate convolutional priors with\noccasional attention blocks to address long-range dependencies, recent advances\nin computer vision advocate for a transformative shift towards fully\ntransformer-based architectures grounded in the attention mechanism. This paper\ninvestigates the feasibility of image compression exclusively using attention\nlayers within our novel model, QPressFormer. We introduce the concept of\nlearned image queries to aggregate patch information via cross-attention,\nfollowed by quantization and coding techniques. Through extensive evaluations,\nour work demonstrates competitive performance achieved by convolution-free\narchitectures across the popular Kodak, DIV2K, and CLIC datasets.",
    "published": "2023-10-17T13:38:38Z",
    "updated": "2023-10-17T13:38:38Z",
    "authors": [
      "Natacha Luka",
      "Romain Negrel",
      "David Picard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.09109v1",
    "title": "Stochastic Spiking Attention: Accelerating Attention with Stochastic\n  Computing in Spiking Networks",
    "summary": "Spiking Neural Networks (SNNs) have been recently integrated into Transformer\narchitectures due to their potential to reduce computational demands and to\nimprove power efficiency. Yet, the implementation of the attention mechanism\nusing spiking signals on general-purpose computing platforms remains\ninefficient. In this paper, we propose a novel framework leveraging stochastic\ncomputing (SC) to effectively execute the dot-product attention for SNN-based\nTransformers. We demonstrate that our approach can achieve high classification\naccuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to\nthe performance of a baseline artificial neural network implementation\n($83.66\\%$). We estimate that the proposed SC approach can lead to over\n$6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory\naccess costs for a digital CMOS-based ASIC design. We experimentally validate\nour stochastic attention block design through an FPGA implementation, which is\nshown to achieve $48\\times$ lower latency as compared to a GPU implementation,\nwhile consuming $15\\times$ less power.",
    "published": "2024-02-14T11:47:19Z",
    "updated": "2024-02-14T11:47:19Z",
    "authors": [
      "Zihang Song",
      "Prabodh Katti",
      "Osvaldo Simeone",
      "Bipin Rajendran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.00798v1",
    "title": "On Difficulties of Attention Factorization through Shared Memory",
    "summary": "Transformers have revolutionized deep learning in numerous fields, including\nnatural language processing, computer vision, and audio processing. Their\nstrength lies in their attention mechanism, which allows for the discovering of\ncomplex input relationships. However, this mechanism's quadratic time and\nmemory complexity pose challenges for larger inputs. Researchers are now\ninvestigating models like Linear Unified Nested Attention (Luna) or Memory\nAugmented Transformer, which leverage external learnable memory to either\nreduce the attention computation complexity down to linear, or to propagate\ninformation between chunks in chunk-wise processing. Our findings challenge the\nconventional thinking on these models, revealing that interfacing with the\nmemory directly through an attention operation is suboptimal, and that the\nperformance may be considerably improved by filtering the input signal before\ncommunicating with memory.",
    "published": "2024-03-31T21:02:50Z",
    "updated": "2024-03-31T21:02:50Z",
    "authors": [
      "Uladzislau Yorsh",
      "Martin HoleÅa",
      "OndÅej Bojar",
      "David Herel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.12031v2",
    "title": "Neighborhood Attention Transformer with Progressive Channel Fusion for\n  Speaker Verification",
    "summary": "Transformer-based architectures for speaker verification typically require\nmore training data than ECAPA-TDNN. Therefore, recent work has generally been\ntrained on VoxCeleb1&2. We propose a backbone network based on self-attention,\nwhich can achieve competitive results when trained on VoxCeleb2 alone. The\nnetwork alternates between neighborhood attention and global attention to\ncapture local and global features, then aggregates features of different\nhierarchical levels, and finally performs attentive statistics pooling.\nAdditionally, we employ a progressive channel fusion strategy to expand the\nreceptive field in the channel dimension as the network deepens. We trained the\nproposed PCF-NAT model on VoxCeleb2 and evaluated it on VoxCeleb1 and the\nvalidation sets of VoxSRC. The EER and minDCF of the shallow PCF-NAT are on\naverage more than 20% lower than those of similarly sized ECAPA-TDNN. Deep\nPCF-NAT achieves an EER lower than 0.5% on VoxCeleb1-O. The code and models are\npublicly available at https://github.com/ChenNan1996/PCF-NAT.",
    "published": "2024-05-20T13:55:19Z",
    "updated": "2024-05-30T02:37:51Z",
    "authors": [
      "Nian Li",
      "Jianguo Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.10845v2",
    "title": "LAIP: Learning Local Alignment from Image-Phrase Modeling for Text-based\n  Person Search",
    "summary": "Text-based person search aims at retrieving images of a particular person\nbased on a given textual description. A common solution for this task is to\ndirectly match the entire images and texts, i.e., global alignment, which fails\nto deal with discerning specific details that discriminate against\nappearance-similar people. As a result, some works shift their attention\ntowards local alignment. One group matches fine-grained parts using forward\nattention weights of the transformer yet underutilizes information. Another\nimplicitly conducts local alignment by reconstructing masked parts based on\nunmasked context yet with a biased masking strategy. All limit performance\nimprovement. This paper proposes the Local Alignment from Image-Phrase modeling\n(LAIP) framework, with Bidirectional Attention-weighted local alignment\n(BidirAtt) and Mask Phrase Modeling (MPM) module.BidirAtt goes beyond the\ntypical forward attention by considering the gradient of the transformer as\nbackward attention, utilizing two-sided information for local alignment. MPM\nfocuses on mask reconstruction within the noun phrase rather than the entire\ntext, ensuring an unbiased masking strategy. Extensive experiments conducted on\nthe CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets demonstrate the superiority\nof the LAIP framework over existing methods.",
    "published": "2024-06-16T08:37:24Z",
    "updated": "2024-06-23T05:31:57Z",
    "authors": [
      "Haiguang Wang",
      "Yu Wu",
      "Mengxia Wu",
      "Cao Min",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.10906v1",
    "title": "Breaking the Attention Bottleneck",
    "summary": "Attention-based transformers have become the standard architecture in many\ndeep learning fields, primarily due to their ability to model long-range\ndependencies and handle variable-length input sequences. However, the attention\nmechanism with its quadratic complexity is a significant bottleneck in the\ntransformer architecture. This algorithm is only uni-directional in the decoder\nand converges to a static pattern in over-parametrized decoder-only models. I\naddress this issue by developing a generative function as attention or\nactivation replacement. It still has the auto-regressive character by comparing\neach token with the previous one. In my test setting with nanoGPT this yields a\nsmaller loss while having a smaller model. The loss further drops by\nincorporating an average context vector. This concept of attention replacement\nis distributed under the GNU AGPL v3 license at\nhttps://gitlab.com/Bachstelze/causal_generation.",
    "published": "2024-06-16T12:06:58Z",
    "updated": "2024-06-16T12:06:58Z",
    "authors": [
      "Kalle Hilsenbek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.15327v2",
    "title": "Fine-grained Attention in Hierarchical Transformers for Tabular\n  Time-series",
    "summary": "Tabular data is ubiquitous in many real-life systems. In particular,\ntime-dependent tabular data, where rows are chronologically related, is\ntypically used for recording historical events, e.g., financial transactions,\nhealthcare records, or stock history. Recently, hierarchical variants of the\nattention mechanism of transformer architectures have been used to model\ntabular time-series data. At first, rows (or columns) are encoded separately by\ncomputing attention between their fields. Subsequently, encoded rows (or\ncolumns) are attended to one another to model the entire tabular time-series.\nWhile efficient, this approach constrains the attention granularity and limits\nits ability to learn patterns at the field-level across separate rows, or\ncolumns. We take a first step to address this gap by proposing Fieldy, a\nfine-grained hierarchical model that contextualizes fields at both the row and\ncolumn levels. We compare our proposal against state of the art models on\nregression and classification tasks using public tabular time-series datasets.\nOur results show that combining row-wise and column-wise attention improves\nperformance without increasing model size. Code and data are available at\nhttps://github.com/raphaaal/fieldy.",
    "published": "2024-06-21T17:40:46Z",
    "updated": "2024-08-02T13:25:16Z",
    "authors": [
      "Raphael Azorin",
      "Zied Ben Houidi",
      "Massimo Gallo",
      "Alessandro Finamore",
      "Pietro Michiardi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.03115v1",
    "title": "Probing self-attention in self-supervised speech models for\n  cross-linguistic differences",
    "summary": "Speech models have gained traction thanks to increase in accuracy from novel\ntransformer architectures. While this impressive increase in performance across\nautomatic speech recognition (ASR) benchmarks is noteworthy, there is still\nmuch that is unknown about the use of attention mechanisms for speech-related\ntasks. For example, while it is assumed that these models are learning\nlanguage-independent (i.e., universal) speech representations, there has not\nyet been an in-depth exploration of what it would mean for the models to be\nlanguage-independent. In the current paper, we explore this question within the\nrealm of self-attention mechanisms of one small self-supervised speech\ntransformer model (TERA). We find that even with a small model, the attention\nheads learned are diverse ranging from almost entirely diagonal to almost\nentirely global regardless of the training language. We highlight some notable\ndifferences in attention patterns between Turkish and English and demonstrate\nthat the models do learn important phonological information during pretraining.\nWe also present a head ablation study which shows that models across languages\nprimarily rely on diagonal heads to classify phonemes.",
    "published": "2024-09-04T22:47:33Z",
    "updated": "2024-09-04T22:47:33Z",
    "authors": [
      "Sai Gopinath",
      "Joselyn Rodriguez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.07914v3",
    "title": "InterACT: Inter-dependency Aware Action Chunking with Hierarchical\n  Attention Transformers for Bimanual Manipulation",
    "summary": "Bimanual manipulation presents unique challenges compared to unimanual tasks\ndue to the complexity of coordinating two robotic arms. In this paper, we\nintroduce InterACT: Inter-dependency aware Action Chunking with Hierarchical\nAttention Transformers, a novel imitation learning framework designed\nspecifically for bimanual manipulation. InterACT leverages hierarchical\nattention mechanisms to effectively capture inter-dependencies between dual-arm\njoint states and visual inputs. The framework comprises a Hierarchical\nAttention Encoder, which processes multi-modal inputs through segment-wise and\ncross-segment attention mechanisms, and a Multi-arm Decoder that generates each\narm's action predictions in parallel, while sharing information between the\narms through synchronization blocks by providing the other arm's intermediate\noutput as context. Our experiments, conducted on various simulated and\nreal-world bimanual manipulation tasks, demonstrate that InterACT outperforms\nexisting methods. Detailed ablation studies further validate the significance\nof key components, including the impact of CLS tokens, cross-segment encoders,\nand synchronization blocks on task performance. We provide supplementary\nmaterials and videos on our project page.",
    "published": "2024-09-12T10:30:44Z",
    "updated": "2024-10-16T08:52:42Z",
    "authors": [
      "Andrew Lee",
      "Ian Chuang",
      "Ling-Yuan Chen",
      "Iman Soltani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.13291v2",
    "title": "Localized Gaussians as Self-Attention Weights for Point Clouds\n  Correspondence",
    "summary": "Current data-driven methodologies for point cloud matching demand extensive\ntraining time and computational resources, presenting significant challenges\nfor model deployment and application. In the point cloud matching task, recent\nadvancements with an encoder-only Transformer architecture have revealed the\nemergence of semantically meaningful patterns in the attention heads,\nparticularly resembling Gaussian functions centered on each point of the input\nshape. In this work, we further investigate this phenomenon by integrating\nthese patterns as fixed attention weights within the attention heads of the\nTransformer architecture. We evaluate two variants: one utilizing predetermined\nvariance values for the Gaussians, and another where the variance values are\ntreated as learnable parameters. Additionally we analyze the performances on\nnoisy data and explore a possible way to improve robustness to noise. Our\nfindings demonstrate that fixing the attention weights not only accelerates the\ntraining process but also enhances the stability of the optimization.\nFurthermore, we conducted an ablation study to identify the specific layers\nwhere the infused information is most impactful and to understand the reliance\nof the network on this information.",
    "published": "2024-09-20T07:41:47Z",
    "updated": "2025-08-08T07:52:24Z",
    "authors": [
      "Alessandro Riva",
      "Alessandro Raganato",
      "Simone Melzi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.23608v2",
    "title": "Context-Aware Token Selection and Packing for Enhanced Vision\n  Transformer",
    "summary": "In recent years, the long-range attention mechanism of vision transformers\nhas driven significant performance breakthroughs across various computer vision\ntasks. However, the traditional self-attention mechanism, which processes both\ninformative and non-informative tokens, suffers from inefficiency and\ninaccuracies. While sparse attention mechanisms have been introduced to\nmitigate these issues by pruning tokens involved in attention, they often lack\ncontext-awareness and intelligence. These mechanisms frequently apply a uniform\ntoken selection strategy across different inputs for batch training or optimize\nefficiency only for the inference stage. To overcome these challenges, we\npropose a novel algorithm: Select and Pack Attention (SPA). SPA dynamically\nselects informative tokens using a low-cost gating layer supervised by\nselection labels and packs these tokens into new batches, enabling a variable\nnumber of tokens to be used in parallelized GPU batch training and inference.\nExtensive experiments across diverse datasets and computer vision tasks\ndemonstrate that SPA delivers superior performance and efficiency, including a\n0.6 mAP improvement in object detection and a 16.4% reduction in computational\ncosts.",
    "published": "2024-10-31T03:47:27Z",
    "updated": "2024-11-03T01:00:01Z",
    "authors": [
      "Tianyi Zhang",
      "Baoxin Li",
      "Jae-sun Seo",
      "Yu Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.02509v3",
    "title": "FCL-ViT: Task-Aware Attention Tuning for Continual Learning",
    "summary": "Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN)\nknowledge to new tasks, without forgetting the old ones. However, modern CL\ntechniques focus on provisioning memory capabilities to existing DNN models\nrather than designing new ones that are able to adapt according to the task at\nhand. This paper presents the novel Feedback Continual Learning Vision\nTransformer (FCL-ViT) that uses a feedback mechanism to generate real-time\ndynamic attention features tailored to the current task. The FCL-ViT operates\nin two Phases. In phase 1, the generic image features are produced and\ndetermine where the Transformer should attend on the current image. In phase 2,\ntask-specific image features are generated that leverage dynamic attention. To\nthis end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs)\nare introduced that operate in both phases and are responsible for tuning the\nTABs attention, respectively. The FCL-ViT surpasses state-of-the-art\nperformance on Continual Learning compared to benchmark methods, while\nretaining a small number of trainable DNN parameters.",
    "published": "2024-12-03T15:48:33Z",
    "updated": "2025-08-18T13:17:28Z",
    "authors": [
      "Anestis Kaimakamidis",
      "Ioannis Pitas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.03736v3",
    "title": "Decoding Human Attentive States from Spatial-temporal EEG Patches Using\n  Transformers",
    "summary": "Learning the spatial topology of electroencephalogram (EEG) channels and\ntheir temporal dynamics is crucial for decoding attention states. This paper\nintroduces EEG-PatchFormer, a transformer-based deep learning framework\ndesigned specifically for EEG attention classification in Brain-Computer\nInterface (BCI) applications. By integrating a Temporal CNN for frequency-based\nEEG feature extraction, a pointwise CNN for feature enhancement, and Spatial\nand Temporal Patching modules for organizing features into spatial-temporal\npatches, EEG-PatchFormer jointly learns spatial-temporal information from EEG\ndata. Leveraging the global learning capabilities of the self-attention\nmechanism, it captures essential features across brain regions over time,\nthereby enhancing EEG data decoding performance. Demonstrating superior\nperformance, EEG-PatchFormer surpasses existing benchmarks in accuracy, area\nunder the ROC curve (AUC), and macro-F1 score on a public cognitive attention\ndataset. The code can be found via:\nhttps://github.com/yi-ding-cs/EEG-PatchFormer .",
    "published": "2025-02-06T02:56:49Z",
    "updated": "2025-05-18T04:11:46Z",
    "authors": [
      "Yi Ding",
      "Joon Hei Lee",
      "Shuailei Zhang",
      "Tianze Luo",
      "Cuntai Guan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05076v1",
    "title": "Paying Attention to Facts: Quantifying the Knowledge Capacity of\n  Attention Layers",
    "summary": "In this paper, we investigate the ability of single-layer attention-only\ntransformers (i.e. attention layers) to memorize facts contained in databases\nfrom a linear-algebraic perspective. We associate with each database a\n3-tensor, propose the rank of this tensor as a measure of the size of the\ndatabase, and provide bounds on the rank in terms of properties of the\ndatabase. We also define a 3-tensor corresponding to an attention layer, and\nempirically demonstrate the relationship between its rank and database rank on\na dataset of toy models and random databases. By highlighting the roles played\nby the value-output and query-key weights, and the effects of argmax and\nsoftmax on rank, our results shed light on the `additive motif' of factual\nrecall in transformers, while also suggesting a way of increasing layer\ncapacity without increasing the number of parameters.",
    "published": "2025-02-07T16:50:27Z",
    "updated": "2025-02-07T16:50:27Z",
    "authors": [
      "Liang Ze Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.10835v1",
    "title": "Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large\n  Language Models",
    "summary": "We investigate how large language models perform latent multi-hop reasoning\nin prompts like \"Wolfgang Amadeus Mozart's mother's spouse is\". To analyze this\nprocess, we introduce logit flow, an interpretability method that traces how\nlogits propagate across layers and positions toward the final prediction. Using\nlogit flow, we identify four distinct stages in single-hop knowledge\nprediction: (A) entity subject enrichment, (B) entity attribute extraction, (C)\nrelation subject enrichment, and (D) relation attribute extraction. Extending\nthis analysis to multi-hop reasoning, we find that failures often stem from the\nrelation attribute extraction stage, where conflicting logits reduce prediction\naccuracy. To address this, we propose back attention, a novel mechanism that\nenables lower layers to leverage higher-layer hidden states from different\npositions during attention computation. With back attention, a 1-layer\ntransformer achieves the performance of a 2-layer transformer. Applied to four\nLLMs, back attention improves accuracy on five reasoning datasets,\ndemonstrating its effectiveness in enhancing latent multi-hop reasoning\nability.",
    "published": "2025-02-15T15:36:42Z",
    "updated": "2025-02-15T15:36:42Z",
    "authors": [
      "Zeping Yu",
      "Yonatan Belinkov",
      "Sophia Ananiadou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.15404v1",
    "title": "Improving Adversarial Transferability on Vision Transformers via Forward\n  Propagation Refinement",
    "summary": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR.",
    "published": "2025-03-19T16:44:23Z",
    "updated": "2025-03-19T16:44:23Z",
    "authors": [
      "Yuchen Ren",
      "Zhengyu Zhao",
      "Chenhao Lin",
      "Bo Yang",
      "Lu Zhou",
      "Zhe Liu",
      "Chao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.15758v2",
    "title": "ATTENTION2D: Communication Efficient Distributed Self-Attention\n  Mechanism",
    "summary": "Transformer-based models have emerged as a leading architecture for natural\nlanguage processing, natural language generation, and image generation tasks. A\nfundamental element of the transformer architecture is self-attention, which\nallows the model to capture intricate dependencies within the data. However,\nthe self-attention mechanism also incurs significant computational and memory\ncosts, particularly for long sequences.\n  In this paper, we introduce ATTENTION2D, a novel approach that exploits\nparallelism along two dimensions - query and key/value - of the self-attention\noperation. This method enables efficient distribution and parallelization of\ncomputations across multiple devices. Our approach facilitates asymptotically\nfaster training and inference phases compared to previous methods, without\nrelying on approximations or incurring additional computational or memory\noverheads. Furthermore, unlike existing techniques that struggle to scale with\nan increasing number of processing units, our approach effectively scales with\nadditional processing units.\n  Our experimental results confirm the effectiveness of our method in improving\ncommunication efficiency and scalability. Compared to Ring Attention, our\napproach demonstrated up to a 5x performance boost on a GPT-3-like model using\n64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64\nNVIDIA H100 GPUs across 64 nodes.",
    "published": "2025-03-20T00:25:44Z",
    "updated": "2025-06-28T22:35:32Z",
    "authors": [
      "Venmugil Elango"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18565v1",
    "title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures",
    "summary": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training.",
    "published": "2025-03-24T11:18:25Z",
    "updated": "2025-03-24T11:18:25Z",
    "authors": [
      "Abdoul Majid O. Thiombiano",
      "Brahim Hnich",
      "Ali Ben Mrad",
      "Mohamed Wiem Mkaouer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09663v1",
    "title": "Ordinary Least Squares as an Attention Mechanism",
    "summary": "I show that ordinary least squares (OLS) predictions can be rewritten as the\noutput of a restricted attention module, akin to those forming the backbone of\nlarge language models. This connection offers an alternative perspective on\nattention beyond the conventional information retrieval framework, making it\nmore accessible to researchers and analysts with a background in traditional\nstatistics. It falls into place when OLS is framed as a similarity-based method\nin a transformed regressor space, distinct from the standard view based on\npartial correlations. In fact, the OLS solution can be recast as the outcome of\nan alternative problem: minimizing squared prediction errors by optimizing the\nembedding space in which training and test vectors are compared via inner\nproducts. Rather than estimating coefficients directly, we equivalently learn\noptimal encoding and decoding operations for predictors. From this vantage\npoint, OLS maps naturally onto the query-key-value structure of attention\nmechanisms. Building on this foundation, I discuss key elements of\nTransformer-style attention and draw connections to classic ideas from time\nseries econometrics.",
    "published": "2025-04-13T17:26:44Z",
    "updated": "2025-04-13T17:26:44Z",
    "authors": [
      "Philippe Goulet Coulombe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.12088v2",
    "title": "AttentionDrop: A Novel Regularization Method for Transformer Models",
    "summary": "Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and speech\nprocessing. However, their immense capacity often leads to overfitting,\nespecially when training data is limited or noisy. In this research, a unified\nfamily of stochastic regularization techniques has been proposed, i.e.\nAttentionDrop with its three different variants, which operate directly on the\nself-attention distributions. Hard Attention Masking randomly zeroes out top-k\nattention logits per query to encourage diverse context utilization, Blurred\nAttention Smoothing applies a dynamic Gaussian convolution over attention\nlogits to diffuse overly peaked distributions, and Consistency-Regularized\nAttentionDrop enforces output stability under multiple independent\nAttentionDrop perturbations via a KL-based consistency loss. Results achieved\nin the study demonstrate that AttentionDrop consistently improves accuracy,\ncalibration, and adversarial robustness over standard Dropout, DropConnect, and\nR-Drop baselines",
    "published": "2025-04-16T13:51:16Z",
    "updated": "2025-09-19T11:47:37Z",
    "authors": [
      "Mirza Samad Ahmed Baig",
      "Syeda Anshrah Gillani",
      "Abdul Akbar Khan",
      "Shahid Munir Shah",
      "Muhammad Omer Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20966v2",
    "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
    "summary": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M and 1.8B parameter models\ndemonstrate that softpick achieves 0\\% sink rate consistently. The softpick\ntransformers produce hidden states with significantly lower kurtosis and\ncreates sparse attention maps. Quantized models using softpick outperform\nsoftmax on standard benchmarks, with a particularly pronounced advantage at\nlower bit precisions. Our analysis and discussion shows how softpick has the\npotential to open new possibilities for quantization, low-precision training,\nsparsity optimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention",
    "published": "2025-04-29T17:36:18Z",
    "updated": "2025-05-30T12:37:29Z",
    "authors": [
      "Zayd M. K. Zuhri",
      "Erland Hilman Fuadi",
      "Alham Fikri Aji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11157v1",
    "title": "Attention on the Sphere",
    "summary": "We introduce a generalized attention mechanism for spherical domains,\nenabling Transformer architectures to natively process data defined on the\ntwo-dimensional sphere - a critical need in fields such as atmospheric physics,\ncosmology, and robotics, where preserving spherical symmetries and topology is\nessential for physical accuracy. By integrating numerical quadrature weights\ninto the attention mechanism, we obtain a geometrically faithful spherical\nattention that is approximately rotationally equivariant, providing strong\ninductive biases and leading to better performance than Cartesian approaches.\nTo further enhance both scalability and model performance, we propose\nneighborhood attention on the sphere, which confines interactions to geodesic\nneighborhoods. This approach reduces computational complexity and introduces\nthe additional inductive bias for locality, while retaining the symmetry\nproperties of our method. We provide optimized CUDA kernels and\nmemory-efficient implementations to ensure practical applicability. The method\nis validated on three diverse tasks: simulating shallow water equations on the\nrotating sphere, spherical image segmentation, and spherical depth estimation.\nAcross all tasks, our spherical Transformers consistently outperform their\nplanar counterparts, highlighting the advantage of geometric priors for\nlearning on spherical domains.",
    "published": "2025-05-16T11:59:30Z",
    "updated": "2025-05-16T11:59:30Z",
    "authors": [
      "Boris Bonev",
      "Max Rietmann",
      "Andrea Paris",
      "Alberto Carpentieri",
      "Thorsten Kurth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23106v1",
    "title": "Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention\n  for Scalable and Interpretable Physics Discovery",
    "summary": "Attention mechanisms have emerged as transformative tools in core AI domains\nsuch as natural language processing and computer vision. Yet, their largely\nuntapped potential for modeling intricate physical systems presents a\ncompelling frontier. Learning such systems often entails discovering operators\nthat map between functional spaces using limited instances of function pairs --\na task commonly framed as a severely ill-posed inverse PDE problem. In this\nwork, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator\narchitecture that builds upon and enhances Nonlocal Attention Operators (NAO)\nin both predictive accuracy and computational efficiency. NIPS employs a linear\nattention mechanism to enable scalable learning and integrates a learnable\nkernel network that acts as a channel-independent convolution in Fourier space.\nAs a consequence, NIPS eliminates the need to explicitly compute and store\nlarge pairwise interactions, effectively amortizing the cost of handling\nspatial interactions into the Fourier transform. Empirical evaluations\ndemonstrate that NIPS consistently surpasses NAO and other baselines across\ndiverse benchmarks, heralding a substantial leap in scalable, interpretable,\nand efficient physics learning. Our code and data accompanying this paper are\navailable at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.",
    "published": "2025-05-29T05:18:30Z",
    "updated": "2025-05-29T05:18:30Z",
    "authors": [
      "Ning Liu",
      "Yue Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02944v1",
    "title": "Beyond Parallelism: Synergistic Computational Graph Effects in\n  Multi-Head Attention",
    "summary": "Multi-head attention powers Transformer networks, the primary deep learning\narchitecture behind the success of large language models (LLMs). Yet, the\ntheoretical advantages of multi-head versus single-head attention, beyond mere\nparallel processing, remain underexplored. In this paper, we reframe multi-head\nattention as a system of potentially synergistic computational graphs, where\neach head functions as a feedforward directed acyclic graph (DAG) with a common\nsink state. We provide intuition and preliminary theoretical analysis of mixing\ntime and minimax fidelity in this framework. Our results show that multi-head\nattention can synergistically enhance information propagation, yielding faster\nmixing times and minimax fidelity amplification under specific head-diversity\nconditions. Finally, we train single-head and multi-head Transformers, each\nwith the same total number of parameters, on sequence manipulation tasks and\nempirically verify the predicted effects.",
    "published": "2025-06-28T11:35:31Z",
    "updated": "2025-06-28T11:35:31Z",
    "authors": [
      "Haitz SÃ¡ez de OcÃ¡riz Borde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13546v1",
    "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention",
    "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA",
    "published": "2025-07-17T21:36:36Z",
    "updated": "2025-07-17T21:36:36Z",
    "authors": [
      "Dmitrii Mikhailov",
      "Aleksey Letunovskiy",
      "Maria Kovaleva",
      "Vladimir Arkhipkin",
      "Vladimir Korviakov",
      "Vladimir Polovnikov",
      "Viacheslav Vasilev",
      "Evelina Sidorova",
      "Denis Dimitrov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.14908v1",
    "title": "Partial Symmetry Enforced Attention Decomposition (PSEAD): A\n  Group-Theoretic Framework for Equivariant Transformers in Biological Systems",
    "summary": "This research introduces the Theory of Partial Symmetry Enforced Attention\nDecomposition (PSEAD), a new and rigorous group-theoretic framework designed to\nseamlessly integrate local symmetry awareness into the core architecture of\nself-attention mechanisms within Transformer models. We formalize the concept\nof local permutation subgroup actions on windows of biological data, proving\nthat under such actions, the attention mechanism naturally decomposes into a\ndirect sum of orthogonal irreducible components. Critically, these components\nare intrinsically aligned with the irreducible representations of the acting\npermutation subgroup, thereby providing a powerful mathematical basis for\ndisentangling symmetric and asymmetric features. We show that PSEAD offers\nsubstantial advantages. These include enhanced generalization capabilities to\nnovel biological motifs exhibiting similar partial symmetries, unprecedented\ninterpretability by allowing direct visualization and analysis of attention\ncontributions from different symmetry channels, and significant computational\nefficiency gains by focusing representational capacity on relevant symmetric\nsubspaces. Beyond static data analysis, we extend PSEAD's applicability to\ndynamic biological processes within reinforcement learning paradigms,\nshowcasing its potential to accelerate the discovery and optimization of\nbiologically meaningful policies in complex environments like protein folding\nand drug discovery. This work lays the groundwork for a new generation of\nbiologically informed, symmetry-aware artificial intelligence models.",
    "published": "2025-07-20T10:44:31Z",
    "updated": "2025-07-20T10:44:31Z",
    "authors": [
      "Daniel Ayomide Olanrewaju"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16018v1",
    "title": "Artifacts and Attention Sinks: Structured Approximations for Efficient\n  Vision Transformers",
    "summary": "Vision transformers have emerged as a powerful tool across a wide range of\napplications, yet their inner workings remain only partially understood. In\nthis work, we examine the phenomenon of massive tokens - tokens with\nexceptionally high activation norms that act as attention sinks - and artifact\ntokens that emerge as a byproduct during inference. Our analysis reveals that\nthese tokens mutually suppress one another through the attention mechanism,\nplaying a critical role in regulating information flow within the network.\nLeveraging these insights, we introduce Fast Nystr\\\"om Attention (FNA), a\ntraining-free method that approximates self-attention in linear time and space\nby exploiting the structured patterns formed by massive and artifact tokens.\nAdditionally, we propose a masking strategy to mitigate noise from these\ntokens, yielding modest performance gains at virtually no cost. We evaluate our\napproach on popular pretrained vision backbones and demonstrate competitive\nperformance on retrieval, classification, segmentation, and visual question\nanswering (VQA), all while reducing computational overhead.",
    "published": "2025-07-21T19:29:03Z",
    "updated": "2025-07-21T19:29:03Z",
    "authors": [
      "Andrew Lu",
      "Wentinn Liao",
      "Liuhui Wang",
      "Huzheng Yang",
      "Jianbo Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20096v2",
    "title": "EcoTransformer: Attention without Multiplication",
    "summary": "The Transformer, with its scaled dot-product attention mechanism, has become\na foundational architecture in modern AI. However, this mechanism is\ncomputationally intensive and incurs substantial energy costs. We propose a new\nTransformer architecture EcoTransformer, in which the output context vector is\nconstructed as the convolution of the values using a Laplacian kernel, where\nthe distances are measured by the L1 metric between the queries and keys.\nCompared to dot-product based attention, the new attention score calculation is\nfree of matrix multiplication. It performs on par with, or even surpasses,\nscaled dot-product attention in NLP, bioinformatics, and vision tasks, while\nconsuming significantly less energy.\n  (This version (v2) supersedes v1 and reflects the intended release and\nlicensing.)",
    "published": "2025-07-27T01:32:54Z",
    "updated": "2025-08-06T02:41:31Z",
    "authors": [
      "Xin Gao",
      "Xingming Xu",
      "Shirin Amiraslani",
      "Hong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23588v1",
    "title": "DiffLoRA: Differential Low-Rank Adapters for Large Language Models",
    "summary": "Differential Transformer has recently been proposed to improve performance in\nTransformer models by canceling out noise through a denoiser attention\nmechanism. In this work, we introduce DiffLoRA, a parameter-efficient\nadaptation of the differential attention mechanism, with low-rank adapters on\nboth positive and negative attention terms. This approach retains the\nefficiency of LoRA while aiming to benefit from the performance gains of\ndifferential attention. We evaluate DiffLoRA across a broad range of NLP tasks,\nincluding general benchmarks, many-shot in-context learning, RAG, and\nlong-context tests. We observe that, although DiffLoRA falls short of other\nparameter-efficient fine-tuning methods in most evaluation tasks, it shows\ninteresting results in certain domains (+11 pts on LoRA for HumanEval). We\nanalyze the attention patterns post-finetuning to identify the reasons for this\nbehavior.",
    "published": "2025-07-31T14:24:59Z",
    "updated": "2025-07-31T14:24:59Z",
    "authors": [
      "Alexandre Misrahi",
      "Nadezhda Chirkova",
      "Maxime Louis",
      "Vassilina Nikoulina"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.09709v1",
    "title": "MangaDiT: Reference-Guided Line Art Colorization with Hierarchical\n  Attention in Diffusion Transformers",
    "summary": "Recent advances in diffusion models have significantly improved the\nperformance of reference-guided line art colorization. However, existing\nmethods still struggle with region-level color consistency, especially when the\nreference and target images differ in character pose or motion. Instead of\nrelying on external matching annotations between the reference and target, we\npropose to discover semantic correspondences implicitly through internal\nattention mechanisms. In this paper, we present MangaDiT, a powerful model for\nreference-guided line art colorization based on Diffusion Transformers (DiT).\nOur model takes both line art and reference images as conditional inputs and\nintroduces a hierarchical attention mechanism with a dynamic attention\nweighting strategy. This mechanism augments the vanilla attention with an\nadditional context-aware path that leverages pooled spatial features,\neffectively expanding the model's receptive field and enhancing region-level\ncolor alignment. Experiments on two benchmark datasets demonstrate that our\nmethod significantly outperforms state-of-the-art approaches, achieving\nsuperior performance in both qualitative and quantitative evaluations.",
    "published": "2025-08-13T11:02:11Z",
    "updated": "2025-08-13T11:02:11Z",
    "authors": [
      "Qianru Qiu",
      "Jiafeng Mao",
      "Kento Masui",
      "Xueting Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25401v1",
    "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
    "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
    "published": "2025-09-29T18:57:14Z",
    "updated": "2025-09-29T18:57:14Z",
    "authors": [
      "Liang Qiao",
      "Yue Dai",
      "Yeqi Huang",
      "Hongyu Kan",
      "Jun Shi",
      "Hong An"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04944v1",
    "title": "On Structured State-Space Duality",
    "summary": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence\nbetween a simple Structured State-Space Model (SSM) and a masked attention\nmechanism. In particular, a state-space model with a scalar-times-identity\nstate matrix is equivalent to a masked self-attention with a $1$-semiseparable\ncausal mask. Consequently, the same sequence transformation (model) has two\nalgorithmic realizations: as a linear-time $O(T)$ recurrence or as a\nquadratic-time $O(T^2)$ attention. In this note, we formalize and generalize\nthis duality: (i) we extend SSD from the scalar-identity case to general\ndiagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs\nmatch the scalar case's training complexity lower bounds while supporting\nricher dynamics; (iii) we establish a necessary and sufficient condition under\nwhich an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we\nshow that such duality fails to extend to standard softmax attention due to\nrank explosion. Together, these results tighten bridge between recurrent SSMs\nand Transformers, and widen the design space for expressive yet efficient\nsequence models.",
    "published": "2025-10-06T15:46:50Z",
    "updated": "2025-10-06T15:46:50Z",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Xiwen Zhang",
      "Weimin Wu",
      "Han Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05901v2",
    "title": "Untangling Component Imbalance in Hybrid Linear Attention Conversion\n  Methods",
    "summary": "Transformers' quadratic computational complexity limits their scalability\ndespite remarkable performance. While linear attention reduces this to linear\ncomplexity, pre-training such models from scratch remains, in most cases,\nprohibitively expensive. Recent post-training linearisation methods convert\npre-trained Transformers to linear models efficiently, often using hybrid\napproaches that combine linear attention with sliding-window softmax. We\nidentify a critical flaw: existing hybrid methods inadvertently bypass the\nlinear component, relying almost entirely on SWA. Component-level diagnostics\nreveal this previously undetected behaviour stems from overlooked evaluation\npractices on common-sense benchmarks. We propose three solutions to ensure\nbalanced component usage: (i) inference-time hybridisation of linear-only\nconversions with sliding-window softmax; (ii) HedgeCATs, combining\nattention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled\nSliding-window Dropout (SSD), which stochastically suppresses the softmax\nbranch during training to prevent component collapse. Our methods maintain\ncomputational efficiency while recovering most base model performance and\nensuring genuine linear attention adoption, restoring the validity of\nperformance attributions in hybrid conversions.",
    "published": "2025-10-07T13:11:13Z",
    "updated": "2025-10-10T17:42:09Z",
    "authors": [
      "Martin Benfeghoul",
      "Teresa Delgado",
      "Adnan Oomerjee",
      "Haitham Bou Ammar",
      "Jun Wang",
      "Zafeirios Fountas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12443v2",
    "title": "Self-attention enabled quantum path analysis of high-harmonic generation\n  in solids",
    "summary": "High-harmonic generation (HHG) in solids provides a powerful platform to\nprobe ultrafast electron dynamics and interband--intraband coupling. However,\ndisentangling the complex many-body contributions in the HHG spectrum remains\nchallenging. Here we introduce a machine-learning approach based on a\nTransformer encoder to analyze and reconstruct HHG signals computed from a\none-dimensional Kronig--Penney model. The self-attention mechanism inherently\nhighlights correlations between temporal dipole dynamics and high-frequency\nspectral components, allowing us to identify signatures of nonadiabatic band\ncoupling that are otherwise obscured in standard Fourier analysis. By combining\nattention maps with Gabor time--frequency analysis, we extract and amplify weak\ncoupling channels that contribute to even-order harmonics and anomalous\nspectral features. Our results demonstrate that multi-head self-attention acts\nas a selective filter for strong-coupling events in the time domain, enabling a\nphysics-informed interpretation of high-dimensional quantum dynamics. This work\nestablishes Transformer-based attention as a versatile tool for solid-state\nstrong-field physics, opening new possibilities for interpretable machine\nlearning in attosecond spectroscopy and nonlinear photonics.",
    "published": "2025-10-14T12:25:00Z",
    "updated": "2025-10-15T01:46:18Z",
    "authors": [
      "Cong Zhao",
      "Xiaozhou Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.01237v1",
    "title": "Eyes on Target: Gaze-Aware Object Detection in Egocentric Video",
    "summary": "Human gaze offers rich supervisory signals for understanding visual attention\nin complex visual environments. In this paper, we propose Eyes on Target, a\nnovel depth-aware and gaze-guided object detection framework designed for\negocentric videos. Our approach injects gaze-derived features into the\nattention mechanism of a Vision Transformer (ViT), effectively biasing spatial\nfeature selection toward human-attended regions. Unlike traditional object\ndetectors that treat all regions equally, our method emphasises\nviewer-prioritised areas to enhance object detection. We validate our method on\nan egocentric simulator dataset where human visual attention is critical for\ntask assessment, illustrating its potential in evaluating human performance in\nsimulation scenarios. We evaluate the effectiveness of our gaze-integrated\nmodel through extensive experiments and ablation studies, demonstrating\nconsistent gains in detection accuracy over gaze-agnostic baselines on both the\ncustom simulator dataset and public benchmarks, including Ego4D Ego-Motion and\nEgo-CH-Gaze datasets. To interpret model behaviour, we also introduce a\ngaze-aware attention head importance metric, revealing how gaze cues modulate\ntransformer attention dynamics.",
    "published": "2025-11-03T05:21:58Z",
    "updated": "2025-11-03T05:21:58Z",
    "authors": [
      "Vishakha Lall",
      "Yisi Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.00452v2",
    "title": "Vision Transformer with Cross-attention by Temporal Shift for Efficient\n  Action Recognition",
    "summary": "Feature shifts have been shown to be useful for action recognition with\nCNN-based models since Temporal Shift Module (TSM) was proposed. It is based on\nframe-wise feature extraction with late fusion, and layer features are shifted\nalong the time direction for the temporal interaction. TokenShift, a recent\nmodel based on Vision Transformer (ViT), also uses the temporal feature shift\nmechanism, which, however, does not fully exploit the structure of Multi-head\nSelf-Attention (MSA) in ViT. In this paper, we propose Multi-head\nSelf/Cross-Attention (MSCA), which fully utilizes the attention structure.\nTokenShift is based on a frame-wise ViT with features temporally shifted with\nsuccessive frames (at time t+1 and t-1). In contrast, the proposed MSCA\nreplaces MSA in the frame-wise ViT, and some MSA heads attend to successive\nframes instead of the current frame. The computation cost is the same as the\nframe-wise ViT and TokenShift as it simply changes the target to which the\nattention is taken. There is a choice about which of key, query, and value are\ntaken from the successive frames, then we experimentally compared these\nvariants with Kinetics400. We also investigate other variants in which the\nproposed MSCA is used along the patch dimension of ViT, instead of the head\ndimension. Experimental results show that a variant, MSCA-KV, shows the best\nperformance and is better than TokenShift by 0.1% and then ViT by 1.2%.",
    "published": "2022-04-01T14:06:19Z",
    "updated": "2022-11-14T01:41:09Z",
    "authors": [
      "Ryota Hashiguchi",
      "Toru Tamaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.08446v2",
    "title": "VSA: Learning Varied-Size Window Attention in Vision Transformers",
    "summary": "Attention within windows has been widely explored in vision transformers to\nbalance the performance, computation complexity, and memory footprint. However,\ncurrent models adopt a hand-crafted fixed-size window design, which restricts\ntheir capacity of modeling long-term dependencies and adapting to objects of\ndifferent sizes. To address this drawback, we propose\n\\textbf{V}aried-\\textbf{S}ize Window \\textbf{A}ttention (VSA) to learn adaptive\nwindow configurations from data. Specifically, based on the tokens within each\ndefault window, VSA employs a window regression module to predict the size and\nlocation of the target window, i.e., the attention area where the key and value\ntokens are sampled. By adopting VSA independently for each attention head, it\ncan model long-term dependencies, capture rich context from diverse windows,\nand promote information exchange among overlapped windows. VSA is an\neasy-to-implement module that can replace the window attention in\nstate-of-the-art representative models with minor modifications and negligible\nextra computational cost while improving their performance by a large margin,\ne.g., 1.1\\% for Swin-T on ImageNet classification. In addition, the performance\ngain increases when using larger images for training and test. Experimental\nresults on more downstream tasks, including object detection, instance\nsegmentation, and semantic segmentation, further demonstrate the superiority of\nVSA over the vanilla window attention in dealing with objects of different\nsizes. The code will be released\nhttps://github.com/ViTAE-Transformer/ViTAE-VSA.",
    "published": "2022-04-18T17:56:07Z",
    "updated": "2023-07-03T07:49:59Z",
    "authors": [
      "Qiming Zhang",
      "Yufei Xu",
      "Jing Zhang",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.01075v1",
    "title": "OmniNet: Omnidirectional Representations from Transformers",
    "summary": "This paper proposes Omnidirectional Representations from Transformers\n(OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive\nfield, each token is allowed to attend to all tokens in the entire network.\nThis process can also be interpreted as a form of extreme or intensive\nattention mechanism that has the receptive field of the entire width and depth\nof the network. To this end, the omnidirectional attention is learned via a\nmeta-learner, which is essentially another self-attention based model. In order\nto mitigate the computationally expensive costs of full receptive field\nattention, we leverage efficient self-attention models such as kernel-based\n(Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer\net al.) as the meta-learner. Extensive experiments are conducted on\nautoregressive language modeling (LM1B, C4), Machine Translation, Long Range\nArena (LRA), and Image Recognition. The experiments show that OmniNet achieves\nconsiderable improvements across these tasks, including achieving\nstate-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena.\nMoreover, using omnidirectional representation in Vision Transformers leads to\nsignificant improvements on image recognition tasks on both few-shot learning\nand fine-tuning setups.",
    "published": "2021-03-01T15:31:54Z",
    "updated": "2021-03-01T15:31:54Z",
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Vamsi Aribandi",
      "Jai Gupta",
      "Philip Pham",
      "Zhen Qin",
      "Dara Bahri",
      "Da-Cheng Juan",
      "Donald Metzler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.03003v2",
    "title": "Transformer-based Personalized Attention Mechanism for Medical Images\n  with Clinical Records",
    "summary": "In medical image diagnosis, identifying the attention region, i.e., the\nregion of interest for which the diagnosis is made, is an important task.\nVarious methods have been developed to automatically identify target regions\nfrom given medical images. However, in actual medical practice, the diagnosis\nis made based not only on the images but also on a variety of clinical records.\nThis means that pathologists examine medical images with some prior knowledge\nof the patients and that the attention regions may change depending on the\nclinical records. In this study, we propose a method called the Personalized\nAttention Mechanism (PersAM), by which the attention regions in medical images\nare adaptively changed according to the clinical records. The primary idea of\nthe PersAM method is to encode the relationships between the medical images and\nclinical records using a variant of Transformer architecture. To demonstrate\nthe effectiveness of the PersAM method, we applied it to a large-scale digital\npathology problem of identifying the subtypes of 842 malignant lymphoma\npatients based on their gigapixel whole slide images and clinical records.",
    "published": "2022-06-07T04:35:22Z",
    "updated": "2023-01-27T09:54:07Z",
    "authors": [
      "Yusuke Takagi",
      "Noriaki Hashimoto",
      "Hiroki Masuda",
      "Hiroaki Miyoshi",
      "Koichi Ohshima",
      "Hidekata Hontani",
      "Ichiro Takeuchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.04301v3",
    "title": "Unveiling Transformers with LEGO: a synthetic reasoning task",
    "summary": "We propose a synthetic reasoning task, LEGO (Learning Equality and Group\nOperations), that encapsulates the problem of following a chain of reasoning,\nand we study how the Transformer architectures learn this task. We pay special\nattention to data effects such as pretraining (on seemingly unrelated NLP\ntasks) and dataset composition (e.g., differing chain length at training and\ntest time), as well as architectural variants such as weight-tied layers or\nadding convolutional components. We study how the trained models eventually\nsucceed at the task, and in particular, we manage to understand some of the\nattention heads as well as how the information flows in the network. In\nparticular, we have identified a novel \\emph{association} pattern that globally\nattends only to identical tokens. Based on these observations we propose a\nhypothesis that here pretraining helps for LEGO tasks due to certain structured\nattention patterns, and we experimentally verify this hypothesis. We also\nobserve that in some data regime the trained transformer finds ``shortcut\"\nsolutions to follow the chain of reasoning, which impedes the model's\nrobustness, and moreover we propose ways to prevent it. Motivated by our\nfindings on structured attention patterns, we propose the LEGO attention\nmodule, a drop-in replacement for vanilla attention heads. This architectural\nchange significantly reduces Flops and maintains or even \\emph{improves} the\nmodel's performance at large-scale pretraining.",
    "published": "2022-06-09T06:30:17Z",
    "updated": "2023-02-17T23:18:30Z",
    "authors": [
      "Yi Zhang",
      "Arturs Backurs",
      "SÃ©bastien Bubeck",
      "Ronen Eldan",
      "Suriya Gunasekar",
      "Tal Wagner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.08645v2",
    "title": "Local Slot Attention for Vision-and-Language Navigation",
    "summary": "Vision-and-language navigation (VLN), a frontier study aiming to pave the way\nfor general-purpose robots, has been a hot topic in the computer vision and\nnatural language processing community. The VLN task requires an agent to\nnavigate to a goal location following natural language instructions in\nunfamiliar environments.\n  Recently, transformer-based models have gained significant improvements on\nthe VLN task. Since the attention mechanism in the transformer architecture can\nbetter integrate inter- and intra-modal information of vision and language.\n  However, there exist two problems in current transformer-based models.\n  1) The models process each view independently without taking the integrity of\nthe objects into account.\n  2) During the self-attention operation in the visual modality, the views that\nare spatially distant can be inter-weaved with each other without explicit\nrestriction. This kind of mixing may introduce extra noise instead of useful\ninformation.\n  To address these issues, we propose 1) A slot-attention based module to\nincorporate information from segmentation of the same object. 2) A local\nattention mask mechanism to limit the visual attention span. The proposed\nmodules can be easily plugged into any VLN architecture and we use the\nRecurrent VLN-Bert as our base model. Experiments on the R2R dataset show that\nour model has achieved the state-of-the-art results.",
    "published": "2022-06-17T09:21:26Z",
    "updated": "2022-06-22T02:32:32Z",
    "authors": [
      "Yifeng Zhuang",
      "Qiang Sun",
      "Yanwei Fu",
      "Lifeng Chen",
      "Xiangyang Xue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.00524v1",
    "title": "CloudAttention: Efficient Multi-Scale Attention Scheme For 3D Point\n  Cloud Learning",
    "summary": "Processing 3D data efficiently has always been a challenge. Spatial\noperations on large-scale point clouds, stored as sparse data, require extra\ncost. Attracted by the success of transformers, researchers are using\nmulti-head attention for vision tasks. However, attention calculations in\ntransformers come with quadratic complexity in the number of inputs and miss\nspatial intuition on sets like point clouds. We redesign set transformers in\nthis work and incorporate them into a hierarchical framework for shape\nclassification and part and scene segmentation. We propose our local attention\nunit, which captures features in a spatial neighborhood. We also compute\nefficient and dynamic global cross attentions by leveraging sampling and\ngrouping at each iteration. Finally, to mitigate the non-heterogeneity of point\nclouds, we propose an efficient Multi-Scale Tokenization (MST), which extracts\nscale-invariant tokens for attention operations. The proposed hierarchical\nmodel achieves state-of-the-art shape classification in mean accuracy and\nyields results on par with the previous segmentation methods while requiring\nsignificantly fewer computations. Our proposed architecture predicts\nsegmentation labels with around half the latency and parameter count of the\nprevious most efficient method with comparable performance. The code is\navailable at https://github.com/YigeWang-WHU/CloudAttention.",
    "published": "2022-07-31T21:39:15Z",
    "updated": "2022-07-31T21:39:15Z",
    "authors": [
      "Mahdi Saleh",
      "Yige Wang",
      "Nassir Navab",
      "Benjamin Busam",
      "Federico Tombari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.03012v1",
    "title": "Learning Spatiotemporal Frequency-Transformer for Compressed Video\n  Super-Resolution",
    "summary": "Compressed video super-resolution (VSR) aims to restore high-resolution\nframes from compressed low-resolution counterparts. Most recent VSR approaches\noften enhance an input frame by borrowing relevant textures from neighboring\nvideo frames. Although some progress has been made, there are grand challenges\nto effectively extract and transfer high-quality textures from compressed\nvideos where most frames are usually highly degraded. In this paper, we propose\na novel Frequency-Transformer for compressed video super-resolution (FTVSR)\nthat conducts self-attention over a joint space-time-frequency domain. First,\nwe divide a video frame into patches, and transform each patch into DCT\nspectral maps in which each channel represents a frequency band. Such a design\nenables a fine-grained level self-attention on each frequency band, so that\nreal visual texture can be distinguished from artifacts, and further utilized\nfor video frame restoration. Second, we study different self-attention schemes,\nand discover that a divided attention which conducts a joint space-frequency\nattention before applying temporal attention on each frequency band, leads to\nthe best video enhancement quality. Experimental results on two widely-used\nvideo super-resolution benchmarks show that FTVSR outperforms state-of-the-art\napproaches on both uncompressed and compressed videos with clear visual\nmargins. Code is available at https://github.com/researchmm/FTVSR.",
    "published": "2022-08-05T07:02:30Z",
    "updated": "2022-08-05T07:02:30Z",
    "authors": [
      "Zhongwei Qiu",
      "Huan Yang",
      "Jianlong Fu",
      "Dongmei Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.09383v3",
    "title": "Unifying Top-down and Bottom-up Scanpath Prediction Using Transformers",
    "summary": "Most models of visual attention aim at predicting either top-down or\nbottom-up control, as studied using different visual search and free-viewing\ntasks. In this paper we propose the Human Attention Transformer (HAT), a single\nmodel that predicts both forms of attention control. HAT uses a novel\ntransformer-based architecture and a simplified foveated retina that\ncollectively create a spatio-temporal awareness akin to the dynamic visual\nworking memory of humans. HAT not only establishes a new state-of-the-art in\npredicting the scanpath of fixations made during target-present and\ntarget-absent visual search and ``taskless'' free viewing, but also makes human\ngaze behavior interpretable. Unlike previous methods that rely on a coarse grid\nof fixation cells and experience information loss due to fixation\ndiscretization, HAT features a sequential dense prediction architecture and\noutputs a dense heatmap for each fixation, thus avoiding discretizing\nfixations. HAT sets a new standard in computational attention, which emphasizes\neffectiveness, generality, and interpretability. HAT's demonstrated scope and\napplicability will likely inspire the development of new attention models that\ncan better predict human behavior in various attention-demanding scenarios.\nCode is available at https://github.com/cvlab-stonybrook/HAT.",
    "published": "2023-03-16T15:13:09Z",
    "updated": "2024-03-30T18:22:34Z",
    "authors": [
      "Zhibo Yang",
      "Sounak Mondal",
      "Seoyoung Ahn",
      "Ruoyu Xue",
      "Gregory Zelinsky",
      "Minh Hoai",
      "Dimitris Samaras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.04974v1",
    "title": "Transformer++",
    "summary": "Recent advancements in attention mechanisms have replaced recurrent neural\nnetworks and its variants for machine translation tasks. Transformer using\nattention mechanism solely achieved state-of-the-art results in sequence\nmodeling. Neural machine translation based on the attention mechanism is\nparallelizable and addresses the problem of handling long-range dependencies\namong words in sentences more effectively than recurrent neural networks. One\nof the key concepts in attention is to learn three matrices, query, key, and\nvalue, where global dependencies among words are learned through linearly\nprojecting word embeddings through these matrices. Multiple query, key, value\nmatrices can be learned simultaneously focusing on a different subspace of the\nembedded dimension, which is called multi-head in Transformer. We argue that\ncertain dependencies among words could be learned better through an\nintermediate context than directly modeling word-word dependencies. This could\nhappen due to the nature of certain dependencies or lack of patterns that lend\nthem difficult to be modeled globally using multi-head self-attention. In this\nwork, we propose a new way of learning dependencies through a context in\nmulti-head using convolution. This new form of multi-head attention along with\nthe traditional form achieves better results than Transformer on the WMT 2014\nEnglish-to-German and English-to-French translation tasks. We also introduce a\nframework to learn POS tagging and NER information during the training of\nencoder which further improves results achieving a new state-of-the-art of 32.1\nBLEU, better than existing best by 1.4 BLEU, on the WMT 2014 English-to-German\nand 44.6 BLEU, better than existing best by 1.1 BLEU, on the WMT 2014\nEnglish-to-French translation tasks. We call this Transformer++.",
    "published": "2020-03-02T13:00:16Z",
    "updated": "2020-03-02T13:00:16Z",
    "authors": [
      "Prakhar Thapak",
      "Prodip Hore"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.06697v1",
    "title": "Efficient Long-Range Attention Network for Image Super-resolution",
    "summary": "Recently, transformer-based methods have demonstrated impressive results in\nvarious vision tasks, including image super-resolution (SR), by exploiting the\nself-attention (SA) for feature extraction. However, the computation of SA in\nmost existing transformer based models is very expensive, while some employed\noperations may be redundant for the SR task. This limits the range of SA\ncomputation and consequently the SR performance. In this work, we propose an\nefficient long-range attention network (ELAN) for image SR. Specifically, we\nfirst employ shift convolution (shift-conv) to effectively extract the image\nlocal structural information while maintaining the same level of complexity as\n1x1 convolution, then propose a group-wise multi-scale self-attention (GMSA)\nmodule, which calculates SA on non-overlapped groups of features using\ndifferent window sizes to exploit the long-range image dependency. A highly\nefficient long-range attention block (ELAB) is then built by simply cascading\ntwo shift-conv with a GMSA module, which is further accelerated by using a\nshared attention mechanism. Without bells and whistles, our ELAN follows a\nfairly simple design by sequentially cascading the ELABs. Extensive experiments\ndemonstrate that ELAN obtains even better results against the transformer-based\nSR models but with significantly less complexity. The source code can be found\nat https://github.com/xindongzhang/ELAN.",
    "published": "2022-03-13T16:17:48Z",
    "updated": "2022-03-13T16:17:48Z",
    "authors": [
      "Xindong Zhang",
      "Hui Zeng",
      "Shi Guo",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.05463v1",
    "title": "Vision Transformer with Attentive Pooling for Robust Facial Expression\n  Recognition",
    "summary": "Facial Expression Recognition (FER) in the wild is an extremely challenging\ntask. Recently, some Vision Transformers (ViT) have been explored for FER, but\nmost of them perform inferiorly compared to Convolutional Neural Networks\n(CNN). This is mainly because the new proposed modules are difficult to\nconverge well from scratch due to lacking inductive bias and easy to focus on\nthe occlusion and noisy areas. TransFER, a representative transformer-based\nmethod for FER, alleviates this with multi-branch attention dropping but brings\nexcessive computations. On the contrary, we present two attentive pooling (AP)\nmodules to pool noisy features directly. The AP modules include Attentive Patch\nPooling (APP) and Attentive Token Pooling (ATP). They aim to guide the model to\nemphasize the most discriminative features while reducing the impacts of less\nrelevant features. The proposed APP is employed to select the most informative\npatches on CNN features, and ATP discards unimportant tokens in ViT. Being\nsimple to implement and without learnable parameters, the APP and ATP\nintuitively reduce the computational cost while boosting the performance by\nONLY pursuing the most discriminative features. Qualitative results demonstrate\nthe motivations and effectiveness of our attentive poolings. Besides,\nquantitative results on six in-the-wild datasets outperform other\nstate-of-the-art methods.",
    "published": "2022-12-11T10:33:19Z",
    "updated": "2022-12-11T10:33:19Z",
    "authors": [
      "Fanglei Xue",
      "Qiangchang Wang",
      "Zichang Tan",
      "Zhongsong Ma",
      "Guodong Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.09915v2",
    "title": "Embedded Heterogeneous Attention Transformer for Cross-lingual Image\n  Captioning",
    "summary": "Cross-lingual image captioning is a challenging task that requires addressing\nboth cross-lingual and cross-modal obstacles in multimedia analysis. The\ncrucial issue in this task is to model the global and the local matching\nbetween the image and different languages. Existing cross-modal embedding\nmethods based on the transformer architecture oversee the local matching\nbetween the image region and monolingual words, especially when dealing with\ndiverse languages. To overcome these limitations, we propose an Embedded\nHeterogeneous Attention Transformer (EHAT) to establish cross-domain\nrelationships and local correspondences between images and different languages\nby using a heterogeneous network. EHAT comprises Masked Heterogeneous\nCross-attention (MHCA), Heterogeneous Attention Reasoning Network (HARN), and\nHeterogeneous Co-attention (HCA). The HARN serves as the core network and it\ncaptures cross-domain relationships by leveraging visual bounding box\nrepresentation features to connect word features from two languages and to\nlearn heterogeneous maps. MHCA and HCA facilitate cross-domain integration in\nthe encoder through specialized heterogeneous attention mechanisms, enabling a\nsingle model to generate captions in two languages. We evaluate our approach on\nthe MSCOCO dataset to generate captions in English and Chinese, two languages\nthat exhibit significant differences in their language families. The\nexperimental results demonstrate the superior performance of our method\ncompared to existing advanced monolingual methods. Our proposed EHAT framework\neffectively addresses the challenges of cross-lingual image captioning, paving\nthe way for improved multilingual image analysis and understanding.",
    "published": "2023-07-19T11:35:21Z",
    "updated": "2024-04-05T15:45:48Z",
    "authors": [
      "Zijie Song",
      "Zhenzhen Hu",
      "Yuanen Zhou",
      "Ye Zhao",
      "Richang Hong",
      "Meng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.01232v2",
    "title": "Modality-aware Transformer for Financial Time series Forecasting",
    "summary": "Time series forecasting presents a significant challenge, particularly when\nits accuracy relies on external data sources rather than solely on historical\nvalues. This issue is prevalent in the financial sector, where the future\nbehavior of time series is often intricately linked to information derived from\nvarious textual reports and a multitude of economic indicators. In practice,\nthe key challenge lies in constructing a reliable time series forecasting model\ncapable of harnessing data from diverse sources and extracting valuable\ninsights to predict the target time series accurately. In this work, we tackle\nthis challenging problem and introduce a novel multimodal transformer-based\nmodel named the \\textit{Modality-aware Transformer}. Our model excels in\nexploring the power of both categorical text and numerical timeseries to\nforecast the target time series effectively while providing insights through\nits neural attention mechanism. To achieve this, we develop feature-level\nattention layers that encourage the model to focus on the most relevant\nfeatures within each data modality. By incorporating the proposed feature-level\nattention, we develop a novel Intra-modal multi-head attention (MHA),\nInter-modal MHA and Target-modal MHA in a way that both feature and temporal\nattentions are incorporated in MHAs. This enables the MHAs to generate temporal\nattentions with consideration of modality and feature importance which leads to\nmore informative embeddings. The proposed modality-aware structure enables the\nmodel to effectively exploit information within each modality as well as foster\ncross-modal understanding. Our extensive experiments on financial datasets\ndemonstrate that Modality-aware Transformer outperforms existing methods,\noffering a novel and practical solution to the complex challenges of\nmulti-modal financial time series forecasting.",
    "published": "2023-10-02T14:22:41Z",
    "updated": "2024-03-20T21:48:05Z",
    "authors": [
      "Hajar Emami",
      "Xuan-Hong Dang",
      "Yousaf Shah",
      "Petros Zerfos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.01233v2",
    "title": "GEN: A Practical Alternative to Graph Transformers for Long-Range Graph\n  Modeling",
    "summary": "Message Passing Neural Networks (MPNNs) model local relations effectively but\nstruggle to propagate information over long distances. Graph Transformers (GTs)\nmitigate this via global self-attention, yet their quadratic cost in the number\nof nodes limits scalability. We propose Graph Elimination Networks (GENs), an\nMPNN variant that approximates GT-like long-range modeling while maintaining\nhigh efficiency. GENs combine edge-wise and hop-wise self-attention in\nparallel; their multiplicative composition yields an attention kernel separable\nacross edge and hop factors within a bounded K-hop receptive field. To enable\nhop-wise attention, we introduce the Graph Elimination Algorithm (GEA), which\nprevents double counting across hops, ensuring that each round injects the\nk-hop incremental contribution exactly once. Taking differences between\nsuccessive rounds recovers the k-hop increment and yields disentangled\nmulti-hop features as inputs for hop-wise attention. This preserves clearer\nstructural distinctions across hop distances and enables more faithful modeling\nof pairwise dependencies between distant nodes within the K-hop neighborhood.\nOn the Long-Range Graph Benchmark (LRGB), GENs outperform strong MPNN baselines\nby 7.7 and 6.0 percentage points (pp) on PascalVOC-SP and COCO-SP, and achieve\nperformance on par with or better than state-of-the-art Graph Transformers. On\nOGBN-Products, GENs support full-batch training/inference, while\nsparse-attention baselines like Exphormer struggle with memory limits under\ncomparable budgets, highlighting GENs as a practical alternative for large,\nsparse graphs.",
    "published": "2024-01-02T14:58:59Z",
    "updated": "2025-09-01T10:02:53Z",
    "authors": [
      "Shuo Wang",
      "Ge Cheng",
      "Yun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.07846v4",
    "title": "Rethinking Transformer-Based Blind-Spot Network for Self-Supervised\n  Image Denoising",
    "summary": "Blind-spot networks (BSN) have been prevalent neural architectures in\nself-supervised image denoising (SSID). However, most existing BSNs are\nconducted with convolution layers. Although transformers have shown the\npotential to overcome the limitations of convolutions in many image restoration\ntasks, the attention mechanisms may violate the blind-spot requirement, thereby\nrestricting their applicability in BSN. To this end, we propose to analyze and\nredesign the channel and spatial attentions to meet the blind-spot requirement.\nSpecifically, channel self-attention may leak the blind-spot information in\nmulti-scale architectures, since the downsampling shuffles the spatial feature\ninto channel dimensions. To alleviate this problem, we divide the channel into\nseveral groups and perform channel attention separately. For spatial\nselfattention, we apply an elaborate mask to the attention matrix to restrict\nand mimic the receptive field of dilated convolution. Based on the redesigned\nchannel and window attentions, we build a Transformer-based Blind-Spot Network\n(TBSN), which shows strong local fitting and global perspective abilities.\nFurthermore, we introduce a knowledge distillation strategy that distills TBSN\ninto smaller denoisers to improve computational efficiency while maintaining\nperformance. Extensive experiments on real-world image denoising datasets show\nthat TBSN largely extends the receptive field and exhibits favorable\nperformance against state-of-theart SSID methods.",
    "published": "2024-04-11T15:39:10Z",
    "updated": "2025-08-19T07:51:16Z",
    "authors": [
      "Junyi Li",
      "Zhilu Zhang",
      "Wangmeng Zuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.12328v1",
    "title": "Multi-dimension Transformer with Attention-based Filtering for Medical\n  Image Segmentation",
    "summary": "The accurate segmentation of medical images is crucial for diagnosing and\ntreating diseases. Recent studies demonstrate that vision transformer-based\nmethods have significantly improved performance in medical image segmentation,\nprimarily due to their superior ability to establish global relationships among\nfeatures and adaptability to various inputs. However, these methods struggle\nwith the low signal-to-noise ratio inherent to medical images. Additionally,\nthe effective utilization of channel and spatial information, which are\nessential for medical image segmentation, is limited by the representation\ncapacity of self-attention. To address these challenges, we propose a\nmulti-dimension transformer with attention-based filtering (MDT-AF), which\nredesigns the patch embedding and self-attention mechanism for medical image\nsegmentation. MDT-AF incorporates an attention-based feature filtering\nmechanism into the patch embedding blocks and employs a coarse-to-fine process\nto mitigate the impact of low signal-to-noise ratio. To better capture complex\nstructures in medical images, MDT-AF extends the self-attention mechanism to\nincorporate spatial and channel dimensions, enriching feature representation.\nMoreover, we introduce an interaction mechanism to improve the feature\naggregation between spatial and channel dimensions. Experimental results on\nthree public medical image segmentation benchmarks show that MDT-AF achieves\nstate-of-the-art (SOTA) performance.",
    "published": "2024-05-20T18:52:41Z",
    "updated": "2024-05-20T18:52:41Z",
    "authors": [
      "Wentao Wang",
      "Xi Xiao",
      "Mingjie Liu",
      "Qing Tian",
      "Xuanyao Huang",
      "Qizhen Lan",
      "Swalpa Kumar Roy",
      "Tianyang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15926v2",
    "title": "Dissecting the Interplay of Attention Paths in a Statistical Mechanics\n  Theory of Transformers",
    "summary": "Despite the remarkable empirical performance of Transformers, their\ntheoretical understanding remains elusive. Here, we consider a deep multi-head\nself-attention network, that is closely related to Transformers yet\nanalytically tractable. We develop a statistical mechanics theory of Bayesian\nlearning in this model, deriving exact equations for the network's predictor\nstatistics under the finite-width thermodynamic limit, i.e.,\n$N,P\\rightarrow\\infty$, $P/N=\\mathcal{O}(1)$, where $N$ is the network width\nand $P$ is the number of training examples. Our theory shows that the predictor\nstatistics are expressed as a sum of independent kernels, each one pairing\ndifferent 'attention paths', defined as information pathways through different\nattention heads across layers. The kernels are weighted according to a\n'task-relevant kernel combination' mechanism that aligns the total kernel with\nthe task labels. As a consequence, this interplay between attention paths\nenhances generalization performance. Experiments confirm our findings on both\nsynthetic and real-world sequence classification tasks. Finally, our theory\nexplicitly relates the kernel combination mechanism to properties of the\nlearned weights, allowing for a qualitative transfer of its insights to models\ntrained via gradient descent. As an illustration, we demonstrate an efficient\nsize reduction of the network, by pruning those attention heads that are deemed\nless relevant by our theory.",
    "published": "2024-05-24T20:34:18Z",
    "updated": "2024-12-08T02:29:42Z",
    "authors": [
      "Lorenzo Tiberi",
      "Francesca Mignacco",
      "Kazuki Irie",
      "Haim Sompolinsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.06567v2",
    "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
    "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
    "published": "2024-06-03T13:28:43Z",
    "updated": "2024-12-07T13:23:39Z",
    "authors": [
      "Yilong Chen",
      "Linhao Zhang",
      "Junyuan Shang",
      "Zhenyu Zhang",
      "Tingwen Liu",
      "Shuohuan Wang",
      "Yu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.02367v9",
    "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration",
    "summary": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of $O(N^2)$,\ncompared to $O(N)$ for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention.",
    "published": "2024-10-03T10:25:23Z",
    "updated": "2025-10-01T16:09:05Z",
    "authors": [
      "Jintao Zhang",
      "Jia Wei",
      "Haofeng Huang",
      "Pengle Zhang",
      "Jun Zhu",
      "Jianfei Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.11836v1",
    "title": "UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption\n  Summarization Transformer",
    "summary": "Image captioning is the generation of natural language descriptions of images\nwhich have increased immense popularity in the recent past. With this different\ndeep-learning techniques are devised for the development of factual and\nstylized image captioning models. Previous models focused more on the\ngeneration of factual and stylized captions separately providing more than one\ncaption for a single image. The descriptions generated from these suffer from\nout-of-vocabulary and repetition issues. To the best of our knowledge, no such\nwork exists that provided a description that integrates different captioning\nmethods to describe the contents of an image with factual and stylized\n(romantic and humorous) elements. To overcome these limitations, this paper\npresents a novel Unified Attention and Multi-Head Attention-driven Caption\nSummarization Transformer (UnMA-CapSumT) based Captioning Framework. It\nutilizes both factual captions and stylized captions generated by the Modified\nAdaptive Attention-based factual image captioning model (MAA-FIC) and Style\nFactored Bi-LSTM with attention (SF-Bi-ALSTM) driven stylized image captioning\nmodel respectively. SF-Bi-ALSTM-based stylized IC model generates two prominent\nstyles of expression- {romance, and humor}. The proposed summarizer UnMHA-ST\ncombines both factual and stylized descriptions of an input image to generate\nstyled rich coherent summarized captions. The proposed UnMHA-ST transformer\nlearns and summarizes different linguistic styles efficiently by incorporating\nproposed word embedding fastText with Attention Word Embedding (fTA-WE) and\npointer-generator network with coverage mechanism concept to solve the\nout-of-vocabulary issues and repetition problem. Extensive experiments are\nconducted on Flickr8K and a subset of FlickrStyle10K with supporting ablation\nstudies to prove the efficiency and efficacy of the proposed framework.",
    "published": "2024-12-16T14:57:40Z",
    "updated": "2024-12-16T14:57:40Z",
    "authors": [
      "Dhruv Sharma",
      "Chhavi Dhiman",
      "Dinesh Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.20174v1",
    "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
    "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
    "published": "2025-03-26T02:58:41Z",
    "updated": "2025-03-26T02:58:41Z",
    "authors": [
      "Shihao Zhou",
      "Dayu Li",
      "Jinshan Pan",
      "Juncheng Zhou",
      "Jinglei Shi",
      "Jufeng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.11517v1",
    "title": "ConvShareViT: Enhancing Vision Transformers with Convolutional Attention\n  Mechanisms for Free-Space Optical Accelerators",
    "summary": "This paper introduces ConvShareViT, a novel deep learning architecture that\nadapts Vision Transformers (ViTs) to the 4f free-space optical system.\nConvShareViT replaces linear layers in multi-head self-attention (MHSA) and\nMultilayer Perceptrons (MLPs) with a depthwise convolutional layer with shared\nweights across input channels. Through the development of ConvShareViT, the\nbehaviour of convolutions within MHSA and their effectiveness in learning the\nattention mechanism were analysed systematically. Experimental results\ndemonstrate that certain configurations, particularly those using valid-padded\nshared convolutions, can successfully learn attention, achieving comparable\nattention scores to those obtained with standard ViTs. However, other\nconfigurations, such as those using same-padded convolutions, show limitations\nin attention learning and operate like regular CNNs rather than transformer\nmodels. ConvShareViT architectures are specifically optimised for the 4f\noptical system, which takes advantage of the parallelism and high-resolution\ncapabilities of optical systems. Results demonstrate that ConvShareViT can\ntheoretically achieve up to 3.04 times faster inference than GPU-based systems.\nThis potential acceleration makes ConvShareViT an attractive candidate for\nfuture optical deep learning applications and proves that our ViT\n(ConvShareViT) can be employed using only the convolution operation, via the\nnecessary optimisation of the ViT to balance performance and complexity.",
    "published": "2025-04-15T15:32:23Z",
    "updated": "2025-04-15T15:32:23Z",
    "authors": [
      "Riad Ibadulla",
      "Thomas M. Chen",
      "Constantino Carlos Reyes-Aldasoro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.17768v1",
    "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
    "published": "2025-04-24T17:39:25Z",
    "updated": "2025-04-24T17:39:25Z",
    "authors": [
      "Piotr Nawrot",
      "Robert Li",
      "Renjie Huang",
      "Sebastian Ruder",
      "Kelly Marchisio",
      "Edoardo M. Ponti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03065v1",
    "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
    "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
    "published": "2025-06-03T16:42:37Z",
    "updated": "2025-06-03T16:42:37Z",
    "authors": [
      "Pengtao Chen",
      "Xianfang Zeng",
      "Maosen Zhao",
      "Peng Ye",
      "Mingzhu Shen",
      "Wei Cheng",
      "Gang Yu",
      "Tao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02748v1",
    "title": "Linear Attention with Global Context: A Multipole Attention Mechanism\n  for Vision and Physics",
    "summary": "Transformers have become the de facto standard for a wide range of tasks,\nfrom image classification to physics simulations. Despite their impressive\nperformance, the quadratic complexity of standard Transformers in both memory\nand time with respect to the input length makes them impractical for processing\nhigh-resolution inputs. Therefore, several variants have been proposed, the\nmost successful relying on patchification, downsampling, or coarsening\ntechniques, often at the cost of losing the finest-scale details. In this work,\nwe take a different approach. Inspired by state-of-the-art techniques in\n$n$-body numerical simulations, we cast attention as an interaction problem\nbetween grid points. We introduce the Multipole Attention Neural Operator\n(MANO), which computes attention in a distance-based multiscale fashion. MANO\nmaintains, in each attention head, a global receptive field and achieves linear\ntime and memory complexity with respect to the number of grid points. Empirical\nresults on image classification and Darcy flows demonstrate that MANO rivals\nstate-of-the-art models such as ViT and Swin Transformer, while reducing\nruntime and peak memory usage by orders of magnitude. We open source our code\nfor reproducibility at https://github.com/AlexColagrande/MANO.",
    "published": "2025-07-03T16:05:26Z",
    "updated": "2025-07-03T16:05:26Z",
    "authors": [
      "Alex Colagrande",
      "Paul Caillon",
      "Eva Feillet",
      "Alexandre Allauzen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08289v1",
    "title": "Understanding Transformers through the Lens of Pavlovian Conditioning",
    "summary": "Transformer architectures have revolutionized artificial intelligence (AI)\nthrough their attention mechanisms, yet the computational principles underlying\ntheir success remain opaque. We present a novel theoretical framework that\nreinterprets the core computation of attention as Pavlovian conditioning. Our\nmodel finds a direct mathematical analogue in linear attention, which\nsimplifies the analysis of the underlying associative process. We demonstrate\nthat attention's queries, keys, and values can be mapped to the three elements\nof classical conditioning: test stimuli that probe associations, conditional\nstimuli (CS) that serve as retrieval cues, and unconditional stimuli (US) that\ncontain response information. Through this lens, we suggest that each attention\noperation constructs a transient associative memory via a Hebbian rule, where\nCS-US pairs form dynamic associations that test stimuli can later retrieve. Our\nframework yields several theoretical insights grounded in this linearized\nmodel: (1) a capacity theorem showing that attention heads can store\nO($\\sqrt{d_k}$) associations before interference degrades retrieval; (2) an\nerror propagation analysis revealing fundamental architectural trade-offs of\nbalancing model depth, width, and head redundancy to maintain reliability; and\n(3) an understanding of how biologically plausible learning rules could enhance\ntransformer architectures. By establishing this deep connection, we suggest\nthat the success of modern AI may stem not from architectural novelty alone,\nbut from implementing computational principles that biology optimized over\nmillions of years of evolution.",
    "published": "2025-08-05T05:00:00Z",
    "updated": "2025-08-05T05:00:00Z",
    "authors": [
      "Mu Qiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.00503v1",
    "title": "Signal-Aware Direction-of-Arrival Estimation Using Attention Mechanisms",
    "summary": "The direction-of-arrival (DOA) of sound sources is an essential acoustic\nparameter used, e.g., for multi-channel speech enhancement or source tracking.\nComplex acoustic scenarios consisting of sources-of-interest, interfering\nsources, reverberation, and noise make the estimation of the DOAs corresponding\nto the sources-of-interest a challenging task. Recently proposed attention\nmechanisms allow DOA estimators to focus on the sources-of-interest and\ndisregard interference and noise, i.e., they are signal-aware. The attention is\ntypically obtained by a deep neural network (DNN) from a short-time Fourier\ntransform (STFT) based representation of a single microphone signal.\nSubsequently, attention has been applied as binary or ratio weighting to\nSTFT-based microphone signal representations to reduce the impact of frequency\nbins dominated by noise, interference, or reverberation. The impact of\nattention on DOA estimators and different training strategies for attention and\nDOA DNNs are not yet studied in depth. In this paper, we evaluate systems\nconsisting of different DNNs and signal processing-based methods for DOA\nestimation when attention is applied. Additionally, we propose training\nstrategies for attention-based DOA estimation optimized via a DOA objective,\ni.e., end-to-end. The evaluation of the proposed and the baseline systems is\nperformed using data generated with simulated and measured room impulse\nresponses under various acoustic conditions, like reverberation times, noise,\nand source array distances. Overall, DOA estimation using attention in\ncombination with signal-processing methods exhibits a far lower computational\ncomplexity than a fully DNN-based system; however, it yields comparable\nresults.",
    "published": "2022-01-03T07:30:00Z",
    "updated": "2022-01-03T07:30:00Z",
    "authors": [
      "Wolfgang Mack",
      "Julian Wechsler",
      "EmanuÃ«l A. P. Habets"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.09741v5",
    "title": "Visual Attention Network",
    "summary": "While originally designed for natural language processing tasks, the\nself-attention mechanism has recently taken various computer vision areas by\nstorm. However, the 2D nature of images brings three challenges for applying\nself-attention in computer vision. (1) Treating images as 1D sequences neglects\ntheir 2D structures. (2) The quadratic complexity is too expensive for\nhigh-resolution images. (3) It only captures spatial adaptability but ignores\nchannel adaptability. In this paper, we propose a novel linear attention named\nlarge kernel attention (LKA) to enable self-adaptive and long-range\ncorrelations in self-attention while avoiding its shortcomings. Furthermore, we\npresent a neural network based on LKA, namely Visual Attention Network (VAN).\nWhile extremely simple, VAN surpasses similar size vision transformers(ViTs)\nand convolutional neural networks(CNNs) in various tasks, including image\nclassification, object detection, semantic segmentation, panoptic segmentation,\npose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet\nbenchmark and set new state-of-the-art performance (58.2 PQ) for panoptic\nsegmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for\nsemantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object\ndetection on COCO dataset. It provides a novel method and a simple yet strong\nbaseline for the community. Code is available at\nhttps://github.com/Visual-Attention-Network.",
    "published": "2022-02-20T06:35:18Z",
    "updated": "2022-07-11T04:21:37Z",
    "authors": [
      "Meng-Hao Guo",
      "Cheng-Ze Lu",
      "Zheng-Ning Liu",
      "Ming-Ming Cheng",
      "Shi-Min Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.10931v1",
    "title": "Attention-based Class Activation Diffusion for Weakly-Supervised\n  Semantic Segmentation",
    "summary": "Extracting class activation maps (CAM) is a key step for weakly-supervised\nsemantic segmentation (WSSS). The CAM of convolution neural networks fails to\ncapture long-range feature dependency on the image and result in the coverage\non only foreground object parts, i.e., a lot of false negatives. An intuitive\nsolution is ``coupling'' the CAM with the long-range attention matrix of visual\ntransformers (ViT) We find that the direct ``coupling'', e.g., pixel-wise\nmultiplication of attention and activation, achieves a more global coverage (on\nthe foreground), but unfortunately goes with a great increase of false\npositives, i.e., background pixels are mistakenly included. This paper aims to\ntackle this issue. It proposes a new method to couple CAM and Attention matrix\nin a probabilistic Diffusion way, and dub it AD-CAM. Intuitively, it integrates\nViT attention and CAM activation in a conservative and convincing way.\nConservative is achieved by refining the attention between a pair of pixels\nbased on their respective attentions to common neighbors, where the intuition\nis two pixels having very different neighborhoods are rarely dependent, i.e.,\ntheir attention should be reduced. Convincing is achieved by diffusing a\npixel's activation to its neighbors (on the CAM) in proportion to the\ncorresponding attentions (on the AM). In experiments, our results on two\nchallenging WSSS benchmarks PASCAL VOC and MS~COCO show that AD-CAM as pseudo\nlabels can yield stronger WSSS models than the state-of-the-art variants of\nCAM.",
    "published": "2022-11-20T10:06:32Z",
    "updated": "2022-11-20T10:06:32Z",
    "authors": [
      "Jianqiang Huang",
      "Jian Wang",
      "Qianru Sun",
      "Hanwang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.03431v1",
    "title": "Towards Understanding Cross and Self-Attention in Stable Diffusion for\n  Text-Guided Image Editing",
    "summary": "Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have\nrecently gained significant popularity for creative Text-to-image generation.\nYet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE)\nis of greater importance for application developers, which modify objects or\nobject properties in images by manipulating feature components in attention\nlayers during the generation process. However, little is known about what\nsemantic meanings these attention layers have learned and which parts of the\nattention maps contribute to the success of image editing. In this paper, we\nconduct an in-depth probing analysis and demonstrate that cross-attention maps\nin Stable Diffusion often contain object attribution information that can\nresult in editing failures. In contrast, self-attention maps play a crucial\nrole in preserving the geometric and shape details of the source image during\nthe transformation to the target image. Our analysis offers valuable insights\ninto understanding cross and self-attention maps in diffusion models. Moreover,\nbased on our findings, we simplify popular image editing methods and propose a\nmore straightforward yet more stable and efficient tuning-free procedure that\nonly modifies self-attention maps of the specified attention layers during the\ndenoising process. Experimental results show that our simplified method\nconsistently surpasses the performance of popular approaches on multiple\ndatasets.",
    "published": "2024-03-06T03:32:56Z",
    "updated": "2024-03-06T03:32:56Z",
    "authors": [
      "Bingyan Liu",
      "Chengyu Wang",
      "Tingfeng Cao",
      "Kui Jia",
      "Jun Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.17255v1",
    "title": "Decoding the visual attention of pathologists to reveal their level of\n  expertise",
    "summary": "We present a method for classifying the expertise of a pathologist based on\nhow they allocated their attention during a cancer reading. We engage this\ndecoding task by developing a novel method for predicting the attention of\npathologists as they read whole-slide Images (WSIs) of prostate and make cancer\ngrade classifications. Our ground truth measure of a pathologists' attention is\nthe x, y and z (magnification) movement of their viewport as they navigated\nthrough WSIs during readings, and to date we have the attention behavior of 43\npathologists reading 123 WSIs. These data revealed that specialists have higher\nagreement in both their attention and cancer grades compared to general\npathologists and residents, suggesting that sufficient information may exist in\ntheir attention behavior to classify their expertise level. To attempt this, we\ntrained a transformer-based model to predict the visual attention heatmaps of\nresident, general, and specialist (GU) pathologists during Gleason grading.\nBased solely on a pathologist's attention during a reading, our model was able\nto predict their level of expertise with 75.3%, 56.1%, and 77.2% accuracy,\nrespectively, better than chance and baseline models. Our model therefore\nenables a pathologist's expertise level to be easily and objectively evaluated,\nimportant for pathology training and competency assessment. Tools developed\nfrom our model could also be used to help pathology trainees learn how to read\nWSIs like an expert.",
    "published": "2024-03-25T23:03:51Z",
    "updated": "2024-03-25T23:03:51Z",
    "authors": [
      "Souradeep Chakraborty",
      "Dana Perez",
      "Paul Friedman",
      "Natallia Sheuka",
      "Constantin Friedman",
      "Oksana Yaskiv",
      "Rajarsi Gupta",
      "Gregory J. Zelinsky",
      "Joel H. Saltz",
      "Dimitris Samaras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.00281v2",
    "title": "Sigmoid Self-Attention has Lower Sample Complexity than Softmax\n  Self-Attention: A Mixture-of-Experts Perspective",
    "summary": "At the core of the popular Transformer architecture is the self-attention\nmechanism, which dynamically assigns softmax weights to each input token so\nthat the model can focus on the most salient information. However, the softmax\nstructure slows down the attention computation due to its row-wise nature, and\nit inherently introduces competition among tokens: as the weight assigned to\none token increases, the weights of others decrease. This competitive dynamic\nmay narrow the focus of self-attention to a limited set of features,\npotentially overlooking other informative characteristics. Recent experimental\nstudies have shown that using the element-wise sigmoid function helps eliminate\ntoken competition and reduce the computational overhead. Despite these\npromising empirical results, a rigorous comparison between sigmoid and softmax\nself-attention mechanisms remains absent in the literature. This paper closes\nthis gap by theoretically demonstrating that sigmoid self-attention is more\nsample-efficient than its softmax counterpart. Toward that goal, we represent\nthe self-attention matrix as a mixture of experts and show that ``experts'' in\nsigmoid self-attention require significantly less data to achieve the same\napproximation error as those in softmax self-attention.",
    "published": "2025-02-01T02:36:14Z",
    "updated": "2025-05-24T02:54:56Z",
    "authors": [
      "Fanqi Yan",
      "Huy Nguyen",
      "Pedram Akbarian",
      "Nhat Ho",
      "Alessandro Rinaldo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.04507v3",
    "title": "Fast Video Generation with Sliding Tile Attention",
    "summary": "Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art\nvideo generation, but suffer from prohibitive compute cost -- when generating\njust a 5-second 720P video, attention alone takes 800 out of 945 seconds of\ntotal inference time. This paper introduces sliding tile attention (STA) to\naddress this challenge. STA leverages the observation that attention scores in\npretrained video diffusion models predominantly concentrate within localized 3D\nwindows. By sliding and attending over the local spatial-temporal region, STA\neliminates redundancy from full attention. Unlike traditional token-wise\nsliding window attention (SWA), STA operates tile-by-tile with a novel\nhardware-aware sliding window design, preserving expressiveness while being\nhardware-efficient. With careful kernel-level optimizations, STA offers the\nfirst efficient 2D/3D sliding-window-like attention implementation, achieving\n58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over\nFlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading\nvideo DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s\nwithout quality degradation, requiring no training. Enabling finetuning further\nlowers latency to 268s with only a 0.09% drop on VBench. We make our codebase\npublic at https://github.com/hao-ai-lab/FastVideo.",
    "published": "2025-02-06T21:17:09Z",
    "updated": "2025-06-04T23:21:39Z",
    "authors": [
      "Peiyuan Zhang",
      "Yongqi Chen",
      "Runlong Su",
      "Hangliang Ding",
      "Ion Stoica",
      "Zhengzhong Liu",
      "Hao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13389v5",
    "title": "VSA: Faster Video Diffusion with Trainable Sparse Attention",
    "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models. Code will be available at\nhttps://github.com/hao-ai-lab/FastVideo.",
    "published": "2025-05-19T17:30:13Z",
    "updated": "2025-10-28T04:13:18Z",
    "authors": [
      "Peiyuan Zhang",
      "Yongqi Chen",
      "Haofeng Huang",
      "Will Lin",
      "Zhengzhong Liu",
      "Ion Stoica",
      "Eric Xing",
      "Hao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.23858v1",
    "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
    "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.",
    "published": "2025-06-30T13:52:31Z",
    "updated": "2025-06-30T13:52:31Z",
    "authors": [
      "Jianzong Wu",
      "Liang Hou",
      "Haotian Yang",
      "Xin Tao",
      "Ye Tian",
      "Pengfei Wan",
      "Di Zhang",
      "Yunhai Tong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24663v1",
    "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation",
    "summary": "Long-sequence processing is a critical capability for modern large language\nmodels. However, the self-attention mechanism in the standard Transformer\narchitecture faces severe computational and memory bottlenecks when processing\nlong sequences. While trainable sparse attention methods offer a promising\nsolution, existing approaches such as NSA introduce excessive extra parameters\nand disrupt the conventional \\textit{pretrain-on-short, finetune-on-long}\nworkflow, resulting in slow convergence and difficulty in acceleration. To\novercome these limitations, we introduce dense-sparse switchable attention\nframework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that\nseamlessly adapts models from short to long sequences. Specifically, InfLLM-V2\nreuses dense attention parameters through parameter-free architecture\nmodification, maintaining consistency between short and long sequence\nprocessing. Additionally, InfLLM-V2 ensures computational efficiency across all\nsequence lengths, by using dense attention for short inputs and smoothly\ntransitioning to sparse attention for long sequences. To achieve practical\nacceleration, we further introduce an efficient implementation of InfLLM-V2\nthat significantly reduces the computational overhead. Our experiments on\nlong-context understanding and chain-of-thought reasoning demonstrate that\nInfLLM-V2 is 4$\\times$ faster than dense attention while retaining 98.1% and\n99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we\nhave trained and open-sourced MiniCPM4.1\n(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,\nproviding a reproducible implementation for the research community.",
    "published": "2025-09-29T12:08:33Z",
    "updated": "2025-09-29T12:08:33Z",
    "authors": [
      "Weilin Zhao",
      "Zihan Zhou",
      "Zhou Su",
      "Chaojun Xiao",
      "Yuxuan Li",
      "Yanghao Li",
      "Yudi Zhang",
      "Weilun Zhao",
      "Zhen Li",
      "Yuxiang Huang",
      "Ao Sun",
      "Xu Han",
      "Zhiyuan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09883v1",
    "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
    "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
    "published": "2025-10-10T21:37:49Z",
    "updated": "2025-10-10T21:37:49Z",
    "authors": [
      "Hossein Entezari Zarch",
      "Lei Gao",
      "Chaoyi Jiang",
      "Murali Annavarm"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.11886v4",
    "title": "DeepViT: Towards Deeper Vision Transformer",
    "summary": "Vision transformers (ViTs) have been successfully applied in image\nclassification tasks recently. In this paper, we show that, unlike convolution\nneural networks (CNNs)that can be improved by stacking more convolutional\nlayers, the performance of ViTs saturate fast when scaled to be deeper. More\nspecifically, we empirically observe that such scaling difficulty is caused by\nthe attention collapse issue: as the transformer goes deeper, the attention\nmaps gradually become similar and even much the same after certain layers. In\nother words, the feature maps tend to be identical in the top layers of deep\nViT models. This fact demonstrates that in deeper layers of ViTs, the\nself-attention mechanism fails to learn effective concepts for representation\nlearning and hinders the model from getting expected performance gain. Based on\nabove observation, we propose a simple yet effective method, named\nRe-attention, to re-generate the attention maps to increase their diversity at\ndifferent layers with negligible computation and memory cost. The pro-posed\nmethod makes it feasible to train deeper ViT models with consistent performance\nimprovements via minor modification to existing ViT models. Notably, when\ntraining a deep ViT model with 32 transformer blocks, the Top-1 classification\naccuracy can be improved by 1.6% on ImageNet. Code is publicly available at\nhttps://github.com/zhoudaquan/dvit_repo.",
    "published": "2021-03-22T14:32:07Z",
    "updated": "2021-04-19T07:06:02Z",
    "authors": [
      "Daquan Zhou",
      "Bingyi Kang",
      "Xiaojie Jin",
      "Linjie Yang",
      "Xiaochen Lian",
      "Zihang Jiang",
      "Qibin Hou",
      "Jiashi Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.09048v4",
    "title": "Task Specific Attention is one more thing you need for object detection",
    "summary": "Various models have been proposed to perform object detection. However, most\nrequire many handdesigned components such as anchors and\nnon-maximum-suppression(NMS) to demonstrate good performance. To mitigate these\nissues, Transformer-based DETR and its variant, Deformable DETR, were\nsuggested. These have solved much of the complex issue in designing a head for\nobject detection models; however, doubts about performance still exist when\nconsidering Transformer-based models as state-of-the-art methods in object\ndetection for other models depending on anchors and NMS revealed better\nresults. Furthermore, it has been unclear whether it would be possible to build\nan end-to-end pipeline in combination only with attention modules, because the\nDETR-adapted Transformer method used a convolutional neural network (CNN) for\nthe backbone body. In this study, we propose that combining several attention\nmodules with our new Task Specific Split Transformer (TSST) is a powerful\nmethod to produce the state-of-the art performance on COCO results without\ntraditionally hand-designed components. By splitting the general-purpose\nattention module into two separated goal-specific attention modules, the\nproposed method allows for the design of simpler object detection models.\nExtensive experiments on the COCO benchmark demonstrate the effectiveness of\nour approach. Code is available at https://github.com/navervision/tsst",
    "published": "2022-02-18T07:09:33Z",
    "updated": "2022-06-15T04:02:27Z",
    "authors": [
      "Sang Yon Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.07349v1",
    "title": "XMorpher: Full Transformer for Deformable Medical Image Registration via\n  Cross Attention",
    "summary": "An effective backbone network is important to deep learning-based Deformable\nMedical Image Registration (DMIR), because it extracts and matches the features\nbetween two images to discover the mutual correspondence for fine registration.\nHowever, the existing deep networks focus on single image situation and are\nlimited in registration task which is performed on paired images. Therefore, we\nadvance a novel backbone network, XMorpher, for the effective corresponding\nfeature representation in DMIR. 1) It proposes a novel full transformer\narchitecture including dual parallel feature extraction networks which exchange\ninformation through cross attention, thus discovering multi-level semantic\ncorrespondence while extracting respective features gradually for final\neffective registration. 2) It advances the Cross Attention Transformer (CAT)\nblocks to establish the attention mechanism between images which is able to\nfind the correspondence automatically and prompts the features to fuse\nefficiently in the network. 3) It constrains the attention computation between\nbase windows and searching windows with different sizes, and thus focuses on\nthe local transformation of deformable registration and enhances the computing\nefficiency at the same time. Without any bells and whistles, our XMorpher gives\nVoxelmorph 2.8% improvement on DSC , demonstrating its effective representation\nof the features from the paired images in DMIR. We believe that our XMorpher\nhas great application potential in more paired medical images. Our XMorpher is\nopen on https://github.com/Solemoon/XMorpher",
    "published": "2022-06-15T08:00:12Z",
    "updated": "2022-06-15T08:00:12Z",
    "authors": [
      "Jiacheng Shi",
      "Yuting He",
      "Youyong Kong",
      "Jean-Louis Coatrieux",
      "Huazhong Shu",
      "Guanyu Yang",
      "Shuo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.06373v4",
    "title": "Recursive Generalization Transformer for Image Super-Resolution",
    "summary": "Transformer architectures have exhibited remarkable performance in image\nsuper-resolution (SR). Since the quadratic computational complexity of the\nself-attention (SA) in Transformer, existing methods tend to adopt SA in a\nlocal region to reduce overheads. However, the local design restricts the\nglobal context exploitation, which is crucial for accurate image\nreconstruction. In this work, we propose the Recursive Generalization\nTransformer (RGT) for image SR, which can capture global spatial information\nand is suitable for high-resolution images. Specifically, we propose the\nrecursive-generalization self-attention (RG-SA). It recursively aggregates\ninput features into representative feature maps, and then utilizes\ncross-attention to extract global information. Meanwhile, the channel\ndimensions of attention matrices (query, key, and value) are further scaled to\nmitigate the redundancy in the channel domain. Furthermore, we combine the\nRG-SA with local self-attention to enhance the exploitation of the global\ncontext, and propose the hybrid adaptive integration (HAI) for module\nintegration. The HAI allows the direct and effective fusion between features at\ndifferent levels (local or global). Extensive experiments demonstrate that our\nRGT outperforms recent state-of-the-art methods quantitatively and\nqualitatively. Code and pre-trained models are available at\nhttps://github.com/zhengchen1999/RGT.",
    "published": "2023-03-11T10:44:44Z",
    "updated": "2024-02-23T03:55:16Z",
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Linghe Kong",
      "Xiaokang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.03555v3",
    "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context\n  Transformers",
    "summary": "Transformer models have achieved state-of-the-art results across a diverse\nrange of domains. However, concern over the cost of training the attention\nmechanism to learn complex dependencies between distant inputs continues to\ngrow. In response, solutions that exploit the structure and sparsity of the\nlearned attention matrix have blossomed. However, real-world applications that\ninvolve long sequences, such as biological sequence analysis, may fall short of\nmeeting these assumptions, precluding exploration of these models. To address\nthis challenge, we present a new Transformer architecture, Performer, based on\nFast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales\nlinearly rather than quadratically in the number of tokens in the sequence, is\ncharacterized by sub-quadratic space complexity and does not incorporate any\nsparsity pattern priors. Furthermore, it provides strong theoretical\nguarantees: unbiased estimation of the attention matrix and uniform\nconvergence. It is also backwards-compatible with pre-trained regular\nTransformers. We demonstrate its effectiveness on the challenging task of\nprotein sequence modeling and provide detailed theoretical analysis.",
    "published": "2020-06-05T17:09:16Z",
    "updated": "2020-10-01T00:41:49Z",
    "authors": [
      "Krzysztof Choromanski",
      "Valerii Likhosherstov",
      "David Dohan",
      "Xingyou Song",
      "Andreea Gane",
      "Tamas Sarlos",
      "Peter Hawkins",
      "Jared Davis",
      "David Belanger",
      "Lucy Colwell",
      "Adrian Weller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.14385v1",
    "title": "Multi-Compound Transformer for Accurate Biomedical Image Segmentation",
    "summary": "The recent vision transformer(i.e.for image classification) learns non-local\nattentive interaction of different patch tokens. However, prior arts miss\nlearning the cross-scale dependencies of different pixels, the semantic\ncorrespondence of different labels, and the consistency of the feature\nrepresentations and semantic embeddings, which are critical for biomedical\nsegmentation. In this paper, we tackle the above issues by proposing a unified\ntransformer network, termed Multi-Compound Transformer (MCTrans), which\nincorporates rich feature learning and semantic structure mining into a unified\nframework. Specifically, MCTrans embeds the multi-scale convolutional features\nas a sequence of tokens and performs intra- and inter-scale self-attention,\nrather than single-scale attention in previous works. In addition, a learnable\nproxy embedding is also introduced to model semantic relationship and feature\nenhancement by using self-attention and cross-attention, respectively. MCTrans\ncan be easily plugged into a UNet-like network and attains a significant\nimprovement over the state-of-the-art methods in biomedical image segmentation\nin six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,\n3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,\nKavirs, ISIC2018 dataset, respectively. Code is available at\nhttps://github.com/JiYuanFeng/MCTrans.",
    "published": "2021-06-28T03:45:44Z",
    "updated": "2021-06-28T03:45:44Z",
    "authors": [
      "Yuanfeng Ji",
      "Ruimao Zhang",
      "Huijie Wang",
      "Zhen Li",
      "Lingyun Wu",
      "Shaoting Zhang",
      "Ping Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.10977v1",
    "title": "Tsformer: Time series Transformer for tourism demand forecasting",
    "summary": "AI-based methods have been widely applied to tourism demand forecasting.\nHowever, current AI-based methods are short of the ability to process long-term\ndependency, and most of them lack interpretability. The Transformer used\ninitially for machine translation shows an incredible ability to long-term\ndependency processing. Based on the Transformer, we proposed a time series\nTransformer (Tsformer) with Encoder-Decoder architecture for tourism demand\nforecasting. The proposed Tsformer encodes long-term dependency with encoder,\ncaptures short-term dependency with decoder, and simplifies the attention\ninteractions under the premise of highlighting dominant attention through a\nseries of attention masking mechanisms. These improvements make the multi-head\nattention mechanism process the input sequence according to the time\nrelationship, contributing to better interpretability. What's more, the context\nprocessing ability of the Encoder-Decoder architecture allows adopting the\ncalendar of days to be forecasted to enhance the forecasting performance.\nExperiments conducted on the Jiuzhaigou valley and Siguniang mountain tourism\ndemand datasets with other nine baseline methods indicate that the proposed\nTsformer outperformed all baseline models in the short-term and long-term\ntourism demand forecasting tasks. Moreover, ablation studies demonstrate that\nthe adoption of the calendar of days to be forecasted contributes to the\nforecasting performance of the proposed Tsformer. For better interpretability,\nthe attention weight matrix visualization is performed. It indicates that the\nTsformer concentrates on seasonal features and days close to days to be\nforecast in short-term forecasting.",
    "published": "2021-07-22T06:33:20Z",
    "updated": "2021-07-22T06:33:20Z",
    "authors": [
      "Siyuan Yi",
      "Xing Chen",
      "Chuanming Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.02664v2",
    "title": "Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic\n  Segmentation with Transformers",
    "summary": "Weakly-supervised semantic segmentation (WSSS) with image-level labels is an\nimportant and challenging task. Due to the high training efficiency, end-to-end\nsolutions for WSSS have received increasing attention from the community.\nHowever, current methods are mainly based on convolutional neural networks and\nfail to explore the global information properly, thus usually resulting in\nincomplete object regions. In this paper, to address the aforementioned\nproblem, we introduce Transformers, which naturally integrate global\ninformation, to generate more integral initial pseudo labels for end-to-end\nWSSS. Motivated by the inherent consistency between the self-attention in\nTransformers and the semantic affinity, we propose an Affinity from Attention\n(AFA) module to learn semantic affinity from the multi-head self-attention\n(MHSA) in Transformers. The learned affinity is then leveraged to refine the\ninitial pseudo labels for segmentation. In addition, to efficiently derive\nreliable affinity labels for supervising AFA and ensure the local consistency\nof pseudo labels, we devise a Pixel-Adaptive Refinement module that\nincorporates low-level image appearance information to refine the pseudo\nlabels. We perform extensive experiments and our method achieves 66.0% and\n38.9% mIoU on the PASCAL VOC 2012 and MS COCO 2014 datasets, respectively,\nsignificantly outperforming recent end-to-end methods and several multi-stage\ncompetitors. Code is available at https://github.com/rulixiang/afa.",
    "published": "2022-03-05T06:07:17Z",
    "updated": "2022-09-08T05:38:37Z",
    "authors": [
      "Lixiang Ru",
      "Yibing Zhan",
      "Baosheng Yu",
      "Bo Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.11255v2",
    "title": "3DGTN: 3D Dual-Attention GLocal Transformer Network for Point Cloud\n  Classification and Segmentation",
    "summary": "Although the application of Transformers in 3D point cloud processing has\nachieved significant progress and success, it is still challenging for existing\n3D Transformer methods to efficiently and accurately learn both valuable global\nfeatures and valuable local features for improved applications. This paper\npresents a novel point cloud representational learning network, called 3D Dual\nSelf-attention Global Local (GLocal) Transformer Network (3DGTN), for improved\nfeature learning in both classification and segmentation tasks, with the\nfollowing key contributions. First, a GLocal Feature Learning (GFL) block with\nthe dual self-attention mechanism (i.e., a novel Point-Patch Self-Attention,\ncalled PPSA, and a channel-wise self-attention) is designed to efficiently\nlearn the GLocal context information. Second, the GFL block is integrated with\na multi-scale Graph Convolution-based Local Feature Aggregation (LFA) block,\nleading to a Global-Local (GLocal) information extraction module that can\nefficiently capture critical information. Third, a series of GLocal modules are\nused to construct a new hierarchical encoder-decoder structure to enable the\nlearning of \"GLocal\" information in different scales in a hierarchical manner.\nThe proposed framework is evaluated on both classification and segmentation\ndatasets, demonstrating that the proposed method is capable of outperforming\nmany state-of-the-art methods on both classification and segmentation tasks.",
    "published": "2022-09-21T14:34:21Z",
    "updated": "2023-05-31T02:20:58Z",
    "authors": [
      "Dening Lu",
      "Kyle Gao",
      "Qian Xie",
      "Linlin Xu",
      "Jonathan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.12134v1",
    "title": "AMDET: Attention based Multiple Dimensions EEG Transformer for Emotion\n  Recognition",
    "summary": "Affective computing is an important branch of artificial intelligence, and\nwith the rapid development of brain computer interface technology, emotion\nrecognition based on EEG signals has received broad attention. It is still a\ngreat challenge to effectively explore the multi-dimensional information in the\nEEG data in spite of a large number of deep learning methods. In this paper, we\npropose a deep model called Attention-based Multiple Dimensions EEG Transformer\n(AMDET), which can exploit the complementarity among the\nspectral-spatial-temporal features of EEG data by employing the\nmulti-dimensional global attention mechanism. We transformed the original EEG\ndata into 3D temporal-spectral-spatial representations and then the AMDET would\nuse spectral-spatial transformer encoder layer to extract effective features in\nthe EEG signal and concentrate on the critical time frame with a temporal\nattention layer. We conduct extensive experiments on the DEAP, SEED, and\nSEED-IV datasets to evaluate the performance of AMDET and the results\noutperform the state-of-the-art baseline on three datasets. Accuracy rates of\n97.48%, 96.85%, 97.17%, 87.32% were achieved in the DEAP-Arousal, DEAP-Valence,\nSEED, and SEED-IV datasets, respectively. We also conduct extensive experiments\nto explore the possible brain regions that influence emotions and the coupling\nof EEG signals. AMDET can perform as well even with few channels which are\nidentified by visualizing what learned model focus on. The accuracy could\nachieve over 90% even with only eight channels and it is of great use and\nbenefit for practical applications.",
    "published": "2022-12-23T04:10:00Z",
    "updated": "2022-12-23T04:10:00Z",
    "authors": [
      "Yongling Xu",
      "Yang Du",
      "Jing Zou",
      "Tianying Zhou",
      "Lushan Xiao",
      "Li Liu",
      " Pengcheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.07803v2",
    "title": "EGformer: Equirectangular Geometry-biased Transformer for 360 Depth\n  Estimation",
    "summary": "Estimating the depths of equirectangular (i.e., 360) images (EIs) is\nchallenging given the distorted 180 x 360 field-of-view, which is hard to be\naddressed via convolutional neural network (CNN). Although a transformer with\nglobal attention achieves significant improvements over CNN for EI depth\nestimation task, it is computationally inefficient, which raises the need for\ntransformer with local attention. However, to apply local attention\nsuccessfully for EIs, a specific strategy, which addresses distorted\nequirectangular geometry and limited receptive field simultaneously, is\nrequired. Prior works have only cared either of them, resulting in\nunsatisfactory depths occasionally. In this paper, we propose an\nequirectangular geometry-biased transformer termed EGformer. While limiting the\ncomputational cost and the number of network parameters, EGformer enables the\nextraction of the equirectangular geometry-aware local attention with a large\nreceptive field. To achieve this, we actively utilize the equirectangular\ngeometry as the bias for the local attention instead of struggling to reduce\nthe distortion of EIs. As compared to the most recent EI depth estimation\nstudies, the proposed approach yields the best depth outcomes overall with the\nlowest computational cost and the fewest parameters, demonstrating the\neffectiveness of the proposed methods.",
    "published": "2023-04-16T15:14:17Z",
    "updated": "2023-09-07T05:51:15Z",
    "authors": [
      "Ilwi Yun",
      "Chanyong Shin",
      "Hyunku Lee",
      "Hyuk-Jae Lee",
      "Chae Eun Rhee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.03919v2",
    "title": "DBAT: Dynamic Backward Attention Transformer for Material Segmentation\n  with Cross-Resolution Patches",
    "summary": "The objective of dense material segmentation is to identify the material\ncategories for every image pixel. Recent studies adopt image patches to extract\nmaterial features. Although the trained networks can improve the segmentation\nperformance, their methods choose a fixed patch resolution which fails to take\ninto account the variation in pixel area covered by each material. In this\npaper, we propose the Dynamic Backward Attention Transformer (DBAT) to\naggregate cross-resolution features. The DBAT takes cropped image patches as\ninput and gradually increases the patch resolution by merging adjacent patches\nat each transformer stage, instead of fixing the patch resolution during\ntraining. We explicitly gather the intermediate features extracted from\ncross-resolution patches and merge them dynamically with predicted attention\nmasks. Experiments show that our DBAT achieves an accuracy of 86.85%, which is\nthe best performance among state-of-the-art real-time models. Like other\nsuccessful deep learning solutions with complex architectures, the DBAT also\nsuffers from lack of interpretability. To address this problem, this paper\nexamines the properties that the DBAT makes use of. By analysing the\ncross-resolution features and the attention weights, this paper interprets how\nthe DBAT learns from image patches. We further align features to semantic\nlabels, performing network dissection, to infer that the proposed model can\nextract material-related features better than other methods. We show that the\nDBAT model is more robust to network initialisation, and yields fewer variable\npredictions compared to other models. The project code is available at\nhttps://github.com/heng-yuwen/Dynamic-Backward-Attention-Transformer.",
    "published": "2023-05-06T03:47:20Z",
    "updated": "2024-02-28T10:22:03Z",
    "authors": [
      "Yuwen Heng",
      "Srinandan Dasmahapatra",
      "Hansung Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.07027v1",
    "title": "EfficientViT: Memory Efficient Vision Transformer with Cascaded Group\n  Attention",
    "summary": "Vision transformers have shown great success due to their high model\ncapabilities. However, their remarkable performance is accompanied by heavy\ncomputation costs, which makes them unsuitable for real-time applications. In\nthis paper, we propose a family of high-speed vision transformers named\nEfficientViT. We find that the speed of existing transformer models is commonly\nbounded by memory inefficient operations, especially the tensor reshaping and\nelement-wise functions in MHSA. Therefore, we design a new building block with\na sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN\nlayers, which improves memory efficiency while enhancing channel communication.\nMoreover, we discover that the attention maps share high similarities across\nheads, leading to computational redundancy. To address this, we present a\ncascaded group attention module feeding attention heads with different splits\nof the full feature, which not only saves computation cost but also improves\nattention diversity. Comprehensive experiments demonstrate EfficientViT\noutperforms existing efficient models, striking a good trade-off between speed\nand accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by\n1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia\nV100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficient\nmodel MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while\nrunning 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNX\nformat. Code and models are available at\nhttps://github.com/microsoft/Cream/tree/main/EfficientViT.",
    "published": "2023-05-11T17:59:41Z",
    "updated": "2023-05-11T17:59:41Z",
    "authors": [
      "Xinyu Liu",
      "Houwen Peng",
      "Ningxin Zheng",
      "Yuqing Yang",
      "Han Hu",
      "Yixuan Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.08724v1",
    "title": "Long-Range Grouping Transformer for Multi-View 3D Reconstruction",
    "summary": "Nowadays, transformer networks have demonstrated superior performance in many\ncomputer vision tasks. In a multi-view 3D reconstruction algorithm following\nthis paradigm, self-attention processing has to deal with intricate image\ntokens including massive information when facing heavy amounts of view input.\nThe curse of information content leads to the extreme difficulty of model\nlearning. To alleviate this problem, recent methods compress the token number\nrepresenting each view or discard the attention operations between the tokens\nfrom different views. Obviously, they give a negative impact on performance.\nTherefore, we propose long-range grouping attention (LGA) based on the\ndivide-and-conquer principle. Tokens from all views are grouped for separate\nattention operations. The tokens in each group are sampled from all views and\ncan provide macro representation for the resided view. The richness of feature\nlearning is guaranteed by the diversity among different groups. An effective\nand efficient encoder can be established which connects inter-view features\nusing LGA and extract intra-view features using the standard self-attention\nlayer. Moreover, a novel progressive upsampling decoder is also designed for\nvoxel generation with relatively high resolution. Hinging on the above, we\nconstruct a powerful transformer-based network, called LRGT. Experimental\nresults on ShapeNet verify our method achieves SOTA accuracy in multi-view\nreconstruction. Code will be available at\nhttps://github.com/LiyingCV/Long-Range-Grouping-Transformer.",
    "published": "2023-08-17T01:34:59Z",
    "updated": "2023-08-17T01:34:59Z",
    "authors": [
      "Liying Yang",
      "Zhenwei Zhu",
      "Xuxin Lin",
      "Jian Nong",
      "Yanyan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.12216v1",
    "title": "SG-Former: Self-guided Transformer with Evolving Token Reallocation",
    "summary": "Vision Transformer has demonstrated impressive success across various vision\ntasks. However, its heavy computation cost, which grows quadratically with\nrespect to the token sequence length, largely limits its power in handling\nlarge feature maps. To alleviate the computation cost, previous works rely on\neither fine-grained self-attentions restricted to local small regions, or\nglobal self-attentions but to shorten the sequence length resulting in coarse\ngranularity. In this paper, we propose a novel model, termed as Self-guided\nTransformer~(SG-Former), towards effective global self-attention with adaptive\nfine granularity. At the heart of our approach is to utilize a significance\nmap, which is estimated through hybrid-scale self-attention and evolves itself\nduring training, to reallocate tokens based on the significance of each region.\nIntuitively, we assign more tokens to the salient regions for achieving\nfine-grained attention, while allocating fewer tokens to the minor regions in\nexchange for efficiency and global receptive fields. The proposed SG-Former\nachieves performance superior to state of the art: our base size model achieves\n\\textbf{84.7\\%} Top-1 accuracy on ImageNet-1K, \\textbf{51.2mAP} bbAP on CoCo,\n\\textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \\textbf{+1.3\\% /\n+2.7 mAP/ +3 mIoU}, with lower computation costs and fewer parameters. The code\nis available at\n\\href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}",
    "published": "2023-08-23T15:52:45Z",
    "updated": "2023-08-23T15:52:45Z",
    "authors": [
      "Sucheng Ren",
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.05239v3",
    "title": "HAT: Hybrid Attention Transformer for Image Restoration",
    "summary": "Transformer-based methods have shown impressive performance in image\nrestoration tasks, such as image super-resolution and denoising. However, we\nfind that these networks can only utilize a limited spatial range of input\ninformation through attribution analysis. This implies that the potential of\nTransformer is still not fully exploited in existing networks. In order to\nactivate more input pixels for better restoration, we propose a new Hybrid\nAttention Transformer (HAT). It combines both channel attention and\nwindow-based self-attention schemes, thus making use of their complementary\nadvantages. Moreover, to better aggregate the cross-window information, we\nintroduce an overlapping cross-attention module to enhance the interaction\nbetween neighboring window features. In the training stage, we additionally\nadopt a same-task pre-training strategy to further exploit the potential of the\nmodel for further improvement. Extensive experiments have demonstrated the\neffectiveness of the proposed modules. We further scale up the model to show\nthat the performance of the SR task can be greatly improved. Besides, we extend\nHAT to more image restoration applications, including real-world image\nsuper-resolution, Gaussian image denoising and image compression artifacts\nreduction. Experiments on benchmark and real-world datasets demonstrate that\nour HAT achieves state-of-the-art performance both quantitatively and\nqualitatively. Codes and models are publicly available at\nhttps://github.com/XPixelGroup/HAT.",
    "published": "2023-09-11T05:17:55Z",
    "updated": "2025-10-31T02:07:56Z",
    "authors": [
      "Xiangyu Chen",
      "Xintao Wang",
      "Wenlong Zhang",
      "Xiangtao Kong",
      "Yu Qiao",
      "Jiantao Zhou",
      "Chao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.07102v2",
    "title": "Fovea Transformer: Efficient Long-Context Modeling with Structured\n  Fine-to-Coarse Attention",
    "summary": "The quadratic complexity of self-attention in Transformers has hindered the\nprocessing of long text. To alleviate this problem, previous works have\nproposed to sparsify the attention matrix, taking advantage of the observation\nthat crucial information about a token can be derived from its neighbors. These\nmethods typically combine one or another form of local attention and global\nattention. Such combinations introduce abrupt changes in contextual granularity\nwhen going from local to global, which may be undesirable. We believe that a\nsmoother transition could potentially enhance model's ability to capture\nlong-context dependencies. In this study, we introduce Fovea Transformer, a\nlong-context focused transformer that addresses the challenges of capturing\nglobal dependencies while maintaining computational efficiency. To achieve\nthis, we construct a multi-scale tree from the input sequence, and use\nrepresentations of context tokens with a progressively coarser granularity in\nthe tree, as their distance to the query token increases. We evaluate our model\non three long-context summarization tasks\\footnote{Our code is publicly\navailable at: \\textit{https://github.com/ZiweiHe/Fovea-Transformer}}. It\nachieves state-of-the-art performance on two of them, and competitive results\non the third with mixed improvement and setback of the evaluation metrics.",
    "published": "2023-11-13T06:24:27Z",
    "updated": "2024-01-11T14:24:54Z",
    "authors": [
      "Ziwei He",
      "Jian Yuan",
      "Le Zhou",
      "Jingwen Leng",
      "Bo Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.03288v1",
    "title": "STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention\n  Transformer for Skeleton-based Action Recognition",
    "summary": "Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. We think the key to\nskeleton-based action recognition is a skeleton hanging in frames, so we focus\non how the Graph Convolutional Convolution networks learn different topologies\nand effectively aggregate joint features in the global temporal and local\ntemporal. In this work, we propose three Channel-wise Tolopogy Graph\nConvolution based on Channel-wise Topology Refinement Graph Convolution\n(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture\nthe upper-lower body part and hand-foot relationship skeleton features. After\nthat, to capture features of human skeletons changing in frames we design the\nTemporal Attention Transformers to extract skeletons effectively. The Temporal\nAttention Transformers can learn the temporal features of human skeleton\nsequences. Finally, we fuse the temporal features output scale with MLP and\nclassification. We develop a powerful graph convolutional network named Spatial\nTemporal Effective Body-part Cross Attention Transformer which notably\nhigh-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models\nare available at https://github.com/maclong01/STEP-CATFormer",
    "published": "2023-12-06T04:36:58Z",
    "updated": "2023-12-06T04:36:58Z",
    "authors": [
      "Nguyen Huu Bao Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.08614v1",
    "title": "Factorization Vision Transformer: Modeling Long Range Dependency with\n  Local Window Cost",
    "summary": "Transformers have astounding representational power but typically consume\nconsiderable computation which is quadratic with image resolution. The\nprevailing Swin transformer reduces computational costs through a local window\nstrategy. However, this strategy inevitably causes two drawbacks: (1) the local\nwindow-based self-attention hinders global dependency modeling capability; (2)\nrecent studies point out that local windows impair robustness. To overcome\nthese challenges, we pursue a preferable trade-off between computational cost\nand performance. Accordingly, we propose a novel factorization self-attention\nmechanism (FaSA) that enjoys both the advantages of local window cost and\nlong-range dependency modeling capability. By factorizing the conventional\nattention matrix into sparse sub-attention matrices, FaSA captures long-range\ndependencies while aggregating mixed-grained information at a computational\ncost equivalent to the local window-based self-attention. Leveraging FaSA, we\npresent the factorization vision transformer (FaViT) with a hierarchical\nstructure. FaViT achieves high performance and robustness, with linear\ncomputational complexity concerning input image spatial resolution. Extensive\nexperiments have shown FaViT's advanced performance in classification and\ndownstream tasks. Furthermore, it also exhibits strong model robustness to\ncorrupted and biased data and hence demonstrates benefits in favor of practical\napplications. In comparison to the baseline model Swin-T, our FaViT-B2\nsignificantly improves classification accuracy by 1% and robustness by 7%,\nwhile reducing model parameters by 14%. Our code will soon be publicly\navailable at https://github.com/q2479036243/FaViT.",
    "published": "2023-12-14T02:38:12Z",
    "updated": "2023-12-14T02:38:12Z",
    "authors": [
      "Haolin Qin",
      "Daquan Zhou",
      "Tingfa Xu",
      "Ziyang Bian",
      "Jianan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.09596v5",
    "title": "Efficient generative adversarial networks using linear\n  additive-attention Transformers",
    "summary": "Although the capacity of deep generative models for image generation, such as\nDiffusion Models (DMs) and Generative Adversarial Networks (GANs), has\ndramatically improved in recent years, much of their success can be attributed\nto computationally expensive architectures. This has limited their adoption and\nuse to research laboratories and companies with large resources, while\nsignificantly raising the carbon footprint for training, fine-tuning, and\ninference. In this work, we present a novel GAN architecture which we call\nLadaGAN. This architecture is based on a linear attention Transformer block\nnamed Ladaformer. The main component of this block is a linear\nadditive-attention mechanism that computes a single attention vector per head\ninstead of the quadratic dot-product attention. We employ Ladaformer in both\nthe generator and discriminator, which reduces the computational complexity and\novercomes the training instabilities often associated with Transformer GANs.\nLadaGAN consistently outperforms existing convolutional and Transformer GANs on\nbenchmark datasets at different resolutions while being significantly more\nefficient. Moreover, LadaGAN shows competitive performance compared to\nstate-of-the-art multi-step generative models (e.g. DMs) using orders of\nmagnitude less computational resources.",
    "published": "2024-01-17T21:08:41Z",
    "updated": "2025-07-05T02:29:52Z",
    "authors": [
      "Emilio Morales-Juarez",
      "Gibran Fuentes-Pineda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.11675v2",
    "title": "ATFusion: An Alternate Cross-Attention Transformer Network for Infrared\n  and Visible Image Fusion",
    "summary": "The fusion of infrared and visible images is essential in remote sensing\napplications, as it combines the thermal information of infrared images with\nthe detailed texture of visible images for more accurate analysis in tasks like\nenvironmental monitoring, target detection, and disaster management. The\ncurrent fusion methods based on Transformer techniques for infrared and visible\n(IV) images have exhibited promising performance. However, the attention\nmechanism of the previous Transformer-based methods was prone to extract common\ninformation from source images without considering the discrepancy information,\nwhich limited fusion performance. In this paper, by reevaluating the\ncross-attention mechanism, we propose an alternate Transformer fusion network\n(ATFusion) to fuse IV images. Our ATFusion consists of one discrepancy\ninformation injection module (DIIM) and two alternate common information\ninjection modules (ACIIM). The DIIM is designed by modifying the vanilla\ncross-attention mechanism, which can promote the extraction of the discrepancy\ninformation of the source images. Meanwhile, the ACIIM is devised by\nalternately using the vanilla cross-attention mechanism, which can fully mine\ncommon information and integrate long dependencies. Moreover, the successful\ntraining of ATFusion is facilitated by a proposed segmented pixel loss\nfunction, which provides a good trade-off for texture detail and salient\nstructure preservation. The qualitative and quantitative results on public\ndatasets indicate our ATFusion is effective and superior compared to other\nstate-of-the-art methods.",
    "published": "2024-01-22T03:31:10Z",
    "updated": "2025-06-18T03:05:09Z",
    "authors": [
      "Han Yan",
      "Songlei Xiong",
      "Long Wang",
      "Lihua Jian",
      "Gemine Vivone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.04200v1",
    "title": "ACC-ViT : Atrous Convolution's Comeback in Vision Transformers",
    "summary": "Transformers have elevated to the state-of-the-art vision architectures\nthrough innovations in attention mechanism inspired from visual perception. At\npresent two classes of attentions prevail in vision transformers, regional and\nsparse attention. The former bounds the pixel interactions within a region; the\nlatter spreads them across sparse grids. The opposing natures of them have\nresulted in a dilemma between either preserving hierarchical relation or\nattaining a global context. In this work, taking inspiration from atrous\nconvolution, we introduce Atrous Attention, a fusion of regional and sparse\nattention, which can adaptively consolidate both local and global information,\nwhile maintaining hierarchical relations. As a further tribute to atrous\nconvolution, we redesign the ubiquitous inverted residual convolution blocks\nwith atrous convolution. Finally, we propose a generalized, hybrid vision\ntransformer backbone, named ACC-ViT, following conventional practices for\nstandard vision tasks. Our tiny version model achieves $\\sim 84 \\%$ accuracy on\nImageNet-1K, with less than $28.5$ million parameters, which is $0.42\\%$\nimprovement over state-of-the-art MaxViT while having $8.4\\%$ less parameters.\nIn addition, we have investigated the efficacy of ACC-ViT backbone under\ndifferent evaluation settings, such as finetuning, linear probing, and\nzero-shot learning on tasks involving medical image analysis, object detection,\nand language-image contrastive learning. ACC-ViT is therefore a strong vision\nbackbone, which is also competitive in mobile-scale versions, ideal for niche\napplications with small datasets.",
    "published": "2024-03-07T04:05:16Z",
    "updated": "2024-03-07T04:05:16Z",
    "authors": [
      "Nabil Ibtehaz",
      "Ning Yan",
      "Masood Mortazavi",
      "Daisuke Kihara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.00658v2",
    "title": "KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced\n  Transformer for 3D Human Pose Estimation",
    "summary": "This paper presents a novel Kinematics and Trajectory Prior\nKnowledge-Enhanced Transformer (KTPFormer), which overcomes the weakness in\nexisting transformer-based methods for 3D human pose estimation that the\nderivation of Q, K, V vectors in their self-attention mechanisms are all based\non simple linear mapping. We propose two prior attention modules, namely\nKinematics Prior Attention (KPA) and Trajectory Prior Attention (TPA) to take\nadvantage of the known anatomical structure of the human body and motion\ntrajectory information, to facilitate effective learning of global dependencies\nand features in the multi-head self-attention. KPA models kinematic\nrelationships in the human body by constructing a topology of kinematics, while\nTPA builds a trajectory topology to learn the information of joint motion\ntrajectory across frames. Yielding Q, K, V vectors with prior knowledge, the\ntwo modules enable KTPFormer to model both spatial and temporal correlations\nsimultaneously. Extensive experiments on three benchmarks (Human3.6M,\nMPI-INF-3DHP and HumanEva) show that KTPFormer achieves superior performance in\ncomparison to state-of-the-art methods. More importantly, our KPA and TPA\nmodules have lightweight plug-and-play designs and can be integrated into\nvarious transformer-based networks (i.e., diffusion-based) to improve the\nperformance with only a very small increase in the computational overhead. The\ncode is available at: https://github.com/JihuaPeng/KTPFormer.",
    "published": "2024-03-31T12:04:27Z",
    "updated": "2024-04-02T06:15:15Z",
    "authors": [
      "Jihua Peng",
      "Yanghong Zhou",
      "P. Y. Mok"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.01977v1",
    "title": "What Improves the Generalization of Graph Transformers? A Theoretical\n  Dive into the Self-attention and Positional Encoding",
    "summary": "Graph Transformers, which incorporate self-attention and positional encoding,\nhave recently emerged as a powerful architecture for various graph learning\ntasks. Despite their impressive performance, the complex non-convex\ninteractions across layers and the recursive graph structure have made it\nchallenging to establish a theoretical foundation for learning and\ngeneralization. This study introduces the first theoretical investigation of a\nshallow Graph Transformer for semi-supervised node classification, comprising a\nself-attention layer with relative positional encoding and a two-layer\nperceptron. Focusing on a graph data model with discriminative nodes that\ndetermine node labels and non-discriminative nodes that are class-irrelevant,\nwe characterize the sample complexity required to achieve a desirable\ngeneralization error by training with stochastic gradient descent (SGD). This\npaper provides the quantitative characterization of the sample complexity and\nnumber of iterations for convergence dependent on the fraction of\ndiscriminative nodes, the dominant patterns, and the initial model errors.\nFurthermore, we demonstrate that self-attention and positional encoding enhance\ngeneralization by making the attention map sparse and promoting the core\nneighborhood during training, which explains the superior feature\nrepresentation of Graph Transformers. Our theoretical results are supported by\nempirical experiments on synthetic and real-world benchmarks.",
    "published": "2024-06-04T05:30:16Z",
    "updated": "2024-06-04T05:30:16Z",
    "authors": [
      "Hongkang Li",
      "Meng Wang",
      "Tengfei Ma",
      "Sijia Liu",
      "Zaixi Zhang",
      "Pin-Yu Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.17530v1",
    "title": "Point Tree Transformer for Point Cloud Registration",
    "summary": "Point cloud registration is a fundamental task in the fields of computer\nvision and robotics. Recent developments in transformer-based methods have\ndemonstrated enhanced performance in this domain. However, the standard\nattention mechanism utilized in these methods often integrates many\nlow-relevance points, thereby struggling to prioritize its attention weights on\nsparse yet meaningful points. This inefficiency leads to limited local\nstructure modeling capabilities and quadratic computational complexity. To\novercome these limitations, we propose the Point Tree Transformer (PTT), a\nnovel transformer-based approach for point cloud registration that efficiently\nextracts comprehensive local and global features while maintaining linear\ncomputational complexity. The PTT constructs hierarchical feature trees from\npoint clouds in a coarse-to-dense manner, and introduces a novel Point Tree\nAttention (PTA) mechanism, which follows the tree structure to facilitate the\nprogressive convergence of attended regions towards salient points.\nSpecifically, each tree layer selectively identifies a subset of key points\nwith the highest attention scores. Subsequent layers focus attention on areas\nof significant relevance, derived from the child points of the selected point\nset. The feature extraction process additionally incorporates coarse point\nfeatures that capture high-level semantic information, thus facilitating local\nstructure modeling and the progressive integration of multiscale information.\nConsequently, PTA empowers the model to concentrate on crucial local structures\nand derive detailed local information while maintaining linear computational\ncomplexity. Extensive experiments conducted on the 3DMatch, ModelNet40, and\nKITTI datasets demonstrate that our method achieves superior performance over\nthe state-of-the-art methods.",
    "published": "2024-06-25T13:14:26Z",
    "updated": "2024-06-25T13:14:26Z",
    "authors": [
      "Meiling Wang",
      "Guangyan Chen",
      "Yi Yang",
      "Li Yuan",
      "Yufeng Yue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.01367v3",
    "title": "Domain Influence in MRI Medical Image Segmentation: spatial versus\n  k-space inputs",
    "summary": "Transformer-based networks applied to image patches have achieved\ncutting-edge performance in many vision tasks. However, lacking the built-in\nbias of convolutional neural networks (CNN) for local image statistics, they\nrequire large datasets and modifications to capture relationships between\npatches, especially in segmentation tasks. Images in the frequency domain might\nbe more suitable for the attention mechanism, as local features are represented\nglobally. By transforming images into the frequency domain, local features are\nrepresented globally. Due to MRI data acquisition properties, these images are\nparticularly suitable. This work investigates how the image domain (spatial or\nk-space) affects segmentation results of deep learning (DL) models, focusing on\nattention-based networks and other non-convolutional models based on MLPs. We\nalso examine the necessity of additional positional encoding for\nTransformer-based networks when input images are in the frequency domain. For\nevaluation, we pose a skull stripping task and a brain tissue segmentation\ntask. The attention-based models used are PerceiverIO and a vanilla Transformer\nencoder. To compare with non-attention-based models, an MLP and ResMLP are also\ntrained and tested. Results are compared with the Swin-Unet, the\nstate-of-the-art medical image segmentation model. Experimental results show\nthat using k-space for the input domain can significantly improve segmentation\nresults. Also, additional positional encoding does not seem beneficial for\nattention-based networks if the input is in the frequency domain. Although none\nof the models matched the Swin-Unet's performance, the less complex models\nshowed promising improvements with a different domain choice.",
    "published": "2024-07-01T15:21:41Z",
    "updated": "2024-08-05T06:58:01Z",
    "authors": [
      "Erik GÃ¶sche",
      "Reza Eghbali",
      "Florian Knoll",
      "Andreas M Rauschecker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.05391v2",
    "title": "SAMSA: Efficient Transformer for Many Data Modalities",
    "summary": "The versatility of self-attention mechanism earned transformers great success\nin almost all data modalities, with limitations on the quadratic complexity and\ndifficulty of training. Efficient transformers, on the other hand, often rely\non clever data-modality-dependent construction to get over the quadratic\ncomplexity of transformers. This greatly hinders their applications on\ndifferent data modalities, which is one of the pillars of contemporary\nfoundational modeling. In this paper, we lay the groundwork for efficient\nfoundational modeling by proposing SAMSA - SAMpling-Self-Attention, a\ncontext-aware linear complexity self-attention mechanism that works well on\nmultiple data modalities. Our mechanism is based on a differentiable sampling\nwithout replacement method we discovered. This enables the self-attention\nmodule to attend to the most important token set, where the importance is\ndefined by data. Moreover, as differentiability is not needed in inference, the\nsparse formulation of our method costs little time overhead, further lowering\ncomputational costs. In short, SAMSA achieved competitive or even SOTA results\non many benchmarks, while being faster in inference, compared to other very\nspecialized models. Against full self-attention, real inference time\nsignificantly decreases while performance ranges from negligible degradation to\noutperformance. We release our source code in the repository:\nhttps://github.com/HySonLab/SAMSA",
    "published": "2024-08-10T00:09:06Z",
    "updated": "2024-08-18T13:22:05Z",
    "authors": [
      "Minh Lenhat",
      "Viet Anh Nguyen",
      "Khoa Nguyen",
      "Duong Duc Hieu",
      "Dao Huu Hung",
      "Truong Son Hy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.05710v1",
    "title": "Efficient Diffusion Transformer with Step-wise Dynamic Attention\n  Mediators",
    "summary": "This paper identifies significant redundancy in the query-key interactions\nwithin self-attention mechanisms of diffusion transformer models, particularly\nduring the early stages of denoising diffusion steps. In response to this\nobservation, we present a novel diffusion transformer framework incorporating\nan additional set of mediator tokens to engage with queries and keys\nseparately. By modulating the number of mediator tokens during the denoising\ngeneration phases, our model initiates the denoising process with a precise,\nnon-ambiguous stage and gradually transitions to a phase enriched with detail.\nConcurrently, integrating mediator tokens simplifies the attention module's\ncomplexity to a linear scale, enhancing the efficiency of global attention\nprocesses. Additionally, we propose a time-step dynamic mediator token\nadjustment mechanism that further decreases the required computational FLOPs\nfor generation, simultaneously facilitating the generation of high-quality\nimages within the constraints of varied inference budgets. Extensive\nexperiments demonstrate that the proposed method can improve the generated\nimage quality while also reducing the inference cost of diffusion transformers.\nWhen integrated with the recent work SiT, our method achieves a\nstate-of-the-art FID score of 2.01. The source code is available at\nhttps://github.com/LeapLabTHU/Attention-Mediators.",
    "published": "2024-08-11T07:01:39Z",
    "updated": "2024-08-11T07:01:39Z",
    "authors": [
      "Yifan Pu",
      "Zhuofan Xia",
      "Jiayi Guo",
      "Dongchen Han",
      "Qixiu Li",
      "Duo Li",
      "Yuhui Yuan",
      "Ji Li",
      "Yizeng Han",
      "Shiji Song",
      "Gao Huang",
      "Xiu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.09605v1",
    "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via\n  Gradient Flow Analysis",
    "summary": "Understanding the training dynamics of transformers is important to explain\nthe impressive capabilities behind large language models. In this work, we\nstudy the dynamics of training a shallow transformer on a task of recognizing\nco-occurrence of two designated words. In the literature of studying training\ndynamics of transformers, several simplifications are commonly adopted such as\nweight reparameterization, attention linearization, special initialization, and\nlazy regime. In contrast, we analyze the gradient flow dynamics of\nsimultaneously training three attention matrices and a linear MLP layer from\nrandom initialization, and provide a framework of analyzing such dynamics via a\ncoupled dynamical system. We establish near minimum loss and characterize the\nattention model after training. We discover that gradient flow serves as an\ninherent mechanism that naturally divide the training process into two phases.\nIn Phase 1, the linear MLP quickly aligns with the two target signals for\ncorrect classification, whereas the softmax attention remains almost unchanged.\nIn Phase 2, the attention matrices and the MLP evolve jointly to enlarge the\nclassification margin and reduce the loss to a near minimum value. Technically,\nwe prove a novel property of the gradient flow, termed \\textit{automatic\nbalancing of gradients}, which enables the loss values of different samples to\ndecrease almost at the same rate and further facilitates the proof of near\nminimum training loss. We also conduct experiments to verify our theoretical\nresults.",
    "published": "2024-10-12T17:50:58Z",
    "updated": "2024-10-12T17:50:58Z",
    "authors": [
      "Hongru Yang",
      "Bhavya Kailkhura",
      "Zhangyang Wang",
      "Yingbin Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.03033v4",
    "title": "Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective",
    "summary": "State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.",
    "published": "2024-11-05T12:10:02Z",
    "updated": "2025-10-09T16:04:28Z",
    "authors": [
      "Qishuai Wen",
      "Chun-Guang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.11245v1",
    "title": "Transformer-Based Bearing Fault Detection using Temporal Decomposition\n  Attention Mechanism",
    "summary": "Bearing fault detection is a critical task in predictive maintenance, where\naccurate and timely fault identification can prevent costly downtime and\nequipment damage. Traditional attention mechanisms in Transformer neural\nnetworks often struggle to capture the complex temporal patterns in bearing\nvibration data, leading to suboptimal performance. To address this limitation,\nwe propose a novel attention mechanism, Temporal Decomposition Attention (TDA),\nwhich combines temporal bias encoding with seasonal-trend decomposition to\ncapture both long-term dependencies and periodic fluctuations in time series\ndata. Additionally, we incorporate the Hull Exponential Moving Average (HEMA)\nfor feature extraction, enabling the model to effectively capture meaningful\ncharacteristics from the data while reducing noise. Our approach integrates TDA\ninto the Transformer architecture, allowing the model to focus separately on\nthe trend and seasonal components of the data. Experimental results on the Case\nWestern Reserve University (CWRU) bearing fault detection dataset demonstrate\nthat our approach outperforms traditional attention mechanisms and achieves\nstate-of-the-art performance in terms of accuracy and interpretability. The\nHEMA-Transformer-TDA model achieves an accuracy of 98.1%, with exceptional\nprecision, recall, and F1-scores, demonstrating its effectiveness in bearing\nfault detection and its potential for application in other time series tasks\nwith seasonal patterns or trends.",
    "published": "2024-12-15T16:51:31Z",
    "updated": "2024-12-15T16:51:31Z",
    "authors": [
      "Marzieh Mirzaeibonehkhater",
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.06949v2",
    "title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. In particular, our method performs provably safe pruning via a\ndynamically set pruning threshold that guarantees the pruned attention weights\nare negligible. We apply ACP to language model pretraining with FoX and show it\nconsistently reduces the number of FLOPs and memory accesses in softmax\nattention by around 70% across different model sizes and context lengths,\nresulting in a roughly 50% to 70% reduction in attention runtime (or a\n2-3$\\times$ speedup) and a roughly 10% to 40% increase in end-to-end training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. Our code is available at\nhttps://github.com/zhixuan-lin/forgetting-transformer.",
    "published": "2025-04-09T14:57:55Z",
    "updated": "2025-08-11T19:43:22Z",
    "authors": [
      "Zhixuan Lin",
      "Johan Obando-Ceron",
      "Xu Owen He",
      "Aaron Courville"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10222v2",
    "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via\n  Head-Specific Complex Vector Attention",
    "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.",
    "published": "2025-05-15T12:30:33Z",
    "updated": "2025-05-27T08:30:45Z",
    "authors": [
      "Jintian Shao",
      "Hongyi Huang",
      "Jiayi Wu",
      "Beiwen Zhang",
      "ZhiYu Wu",
      "You Shan",
      "MingKai Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.14951v1",
    "title": "Latent-attention Based Transformer for Near ML Polar Decoding in\n  Short-code Regime",
    "summary": "Transformer architectures have emerged as promising deep learning (DL) tools\nfor modeling complex sequence-to-sequence interactions in channel decoding.\nHowever, current transformer-based decoders for error correction codes (ECCs)\ndemonstrate inferior performance and generalization capabilities compared to\nconventional algebraic decoders, especially in short-code regimes. In this\nwork, we propose a novel latent-attention based transformer (LAT) decoder for\npolar codes that addresses the limitations on performance and generalization\nthrough three pivotal innovations. First, we develop a latent-attention\nmechanism that supersedes the conventional self-attention mechanism. This\narchitectural modification enables independent learning of the Query and Key\nmatrices for code-aware attention computation, decoupling them from the Value\nmatrix to emphasize position-wise decoding interactions while reducing context\ncorrelation interference. Second, we devise an advanced training framework\nincorporating three synergistic components: entropy-aware importance sampling\nthat emphasizes low-probability regions in the signal constellation space,\nexperience reflow that introduces empirical labels to improve characterization\nof decoding boundaries, and dynamic label smoothing for likelihood-based\nregularization. Third, we propose a code-aware mask scheme which allows dynamic\nadaptation for varying code configurations. Numerical evaluations demonstrate\nthat the proposed LAT decoder achieves near maximum-likelihood (ML) performance\nin terms of both bit error rate (BER) and block error rate (BLER) for\nshort-length polar codes. Furthermore, the architecture exhibits robust\ngeneralization capabilities across diverse code rates and code lengths.",
    "published": "2025-07-20T13:19:43Z",
    "updated": "2025-07-20T13:19:43Z",
    "authors": [
      "Hongzhi Zhu",
      "Wei Xu",
      "Xiaohu You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14614v1",
    "title": "First Attentions Last: Better Exploiting First Attentions for Efficient\n  Transformer Training",
    "summary": "As training billion-scale transformers becomes increasingly common, employing\nmultiple distributed GPUs along with parallel training methods has become a\nstandard practice. However, existing transformer designs suffer from\nsignificant communication overhead, especially in Tensor Parallelism (TP),\nwhere each block's MHA-MLP connection requires an all-reduce communication.\nThrough our investigation, we show that the MHA-MLP connections can be bypassed\nfor efficiency, while the attention output of the first layer can serve as an\nalternative signal for the bypassed connection. Motivated by the observations,\nwe propose FAL (First Attentions Last), an efficient transformer architecture\nthat redirects the first MHA output to the MLP inputs of the following layers,\neliminating the per-block MHA-MLP connections. This removes the all-reduce\ncommunication and enables parallel execution of MHA and MLP on a single GPU. We\nalso introduce FAL+, which adds the normalized first attention output to the\nMHA outputs of the following layers to augment the MLP input for the model\nquality. Our evaluation shows that FAL reduces multi-GPU training time by up to\n44%, improves single-GPU throughput by up to 1.18x, and achieves better\nperplexity compared to the baseline GPT. FAL+ achieves even lower perplexity\nwithout increasing the training time than the baseline.",
    "published": "2025-10-16T12:21:46Z",
    "updated": "2025-10-16T12:21:46Z",
    "authors": [
      "Gyudong Kim",
      "Hyukju Na",
      "Jin Hyeon Kim",
      "Hyunsung Jang",
      "Jaemin Park",
      "Jaegi Hwang",
      "Namkoo Ha",
      "Seungryong Kim",
      "Young Geun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10615v1",
    "title": "FuXi-Î²: Towards a Lightweight and Fast Large-Scale Generative\n  Recommendation Model",
    "summary": "Scaling laws for autoregressive generative recommenders reveal potential for\nlarger, more versatile systems but mean greater latency and training costs. To\naccelerate training and inference, we investigated the recent generative\nrecommendation models HSTU and FuXi-$\\alpha$, identifying two efficiency\nbottlenecks: the indexing operations in relative temporal attention bias and\nthe computation of the query-key attention map. Additionally, we observed that\nrelative attention bias in self-attention mechanisms can also serve as\nattention maps. Previous works like Synthesizer have shown that alternative\nforms of attention maps can achieve similar performance, naturally raising the\nquestion of whether some attention maps are redundant. Through empirical\nexperiments, we discovered that using the query-key attention map might degrade\nthe model's performance in recommendation tasks. To address these bottlenecks,\nwe propose a new framework applicable to Transformer-like recommendation\nmodels. On one hand, we introduce Functional Relative Attention Bias, which\navoids the time-consuming operations of the original relative attention bias,\nthereby accelerating the process. On the other hand, we remove the query-key\nattention map from the original self-attention layer and design a new\nAttention-Free Token Mixer module. Furthermore, by applying this framework to\nFuXi-$\\alpha$, we introduce a new model, FuXi-$\\beta$. Experiments across\nmultiple datasets demonstrate that FuXi-$\\beta$ outperforms previous\nstate-of-the-art models and achieves significant acceleration compared to\nFuXi-$\\alpha$, while also adhering to the scaling law. Notably, FuXi-$\\beta$\nshows an improvement of 27% to 47% in the NDCG@10 metric on large-scale\nindustrial datasets compared to FuXi-$\\alpha$. Our code is available in a\npublic repository: https://github.com/USTC-StarTeam/FuXi-beta",
    "published": "2025-08-14T13:12:29Z",
    "updated": "2025-08-14T13:12:29Z",
    "authors": [
      "Yufei Ye",
      "Wei Guo",
      "Hao Wang",
      "Hong Zhu",
      "Yuyang Ye",
      "Yong Liu",
      "Huifeng Guo",
      "Ruiming Tang",
      "Defu Lian",
      "Enhong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/gr-qc/0107056v1",
    "title": "Gravitational self force and gauge transformations",
    "summary": "We explore how the gravitational self force (or ``radiation reaction''\nforce), acting on a pointlike test particle in curved spacetime, is modified in\na gauge transformation. We derive the general transformation law, describing\nthe change in the self force in terms of the infinitesimal displacement vector\nassociated with the gauge transformation. Based on this transformation law, we\nextend the regularization prescription by Mino et al. and Quinn and Wald\n(originally formulated within the harmonic gauge) to an arbitrary gauge. Then\nwe extend the method of mode-sum regularization (which provides a practical\nmeans for calculating the regularized self force and was recently applied to\nthe harmonic-gauge gravitational self force) to an arbitrary gauge. We find\nthat the regularization parameters involved in this method are\ngauge-independent. We also explore the gauge transformation of the self force\nfrom the harmonic gauge to the Regge-Wheeler gauge and to the radiation gauge,\nfocusing attention on the regularity of these gauge transformations. We\nconclude that the transformation of the self force to the Regge-Wheeler gauge\nin Schwarzschild spacetime is regular for radial orbits and irregular\notherwise, whereas the transformation to the radiation gauge is irregular for\nall orbits.",
    "published": "2001-07-16T13:29:44Z",
    "updated": "2001-07-16T13:29:44Z",
    "authors": [
      "Leor Barack",
      "Amos Ori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.02034v1",
    "title": "SSformer: A Lightweight Transformer for Semantic Segmentation",
    "summary": "It is well believed that Transformer performs better in semantic segmentation\ncompared to convolutional neural networks. Nevertheless, the original Vision\nTransformer may lack of inductive biases of local neighborhoods and possess a\nhigh time complexity. Recently, Swin Transformer sets a new record in various\nvision tasks by using hierarchical architecture and shifted windows while being\nmore efficient. However, as Swin Transformer is specifically designed for image\nclassification, it may achieve suboptimal performance on dense prediction-based\nsegmentation task. Further, simply combing Swin Transformer with existing\nmethods would lead to the boost of model size and parameters for the final\nsegmentation model. In this paper, we rethink the Swin Transformer for semantic\nsegmentation, and design a lightweight yet effective transformer model, called\nSSformer. In this model, considering the inherent hierarchical design of Swin\nTransformer, we propose a decoder to aggregate information from different\nlayers, thus obtaining both local and global attentions. Experimental results\nshow the proposed SSformer yields comparable mIoU performance with\nstate-of-the-art models, while maintaining a smaller model size and lower\ncompute.",
    "published": "2022-08-03T12:57:00Z",
    "updated": "2022-08-03T12:57:00Z",
    "authors": [
      "Wentao Shi",
      "Jing Xu",
      "Pan Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.11790v2",
    "title": "Addressing Token Uniformity in Transformers via Singular Value\n  Transformation",
    "summary": "Token uniformity is commonly observed in transformer-based models, in which\ndifferent tokens share a large proportion of similar information after going\nthrough stacked multiple self-attention layers in a transformer. In this paper,\nwe propose to use the distribution of singular values of outputs of each\ntransformer layer to characterise the phenomenon of token uniformity and\nempirically illustrate that a less skewed singular value distribution can\nalleviate the `token uniformity' problem. Base on our observations, we define\nseveral desirable properties of singular value distributions and propose a\nnovel transformation function for updating the singular values. We show that\napart from alleviating token uniformity, the transformation function should\npreserve the local neighbourhood structure in the original embedding space. Our\nproposed singular value transformation function is applied to a range of\ntransformer-based language models such as BERT, ALBERT, RoBERTa and DistilBERT,\nand improved performance is observed in semantic textual similarity evaluation\nand a range of GLUE tasks. Our source code is available at\nhttps://github.com/hanqi-qi/tokenUni.git.",
    "published": "2022-08-24T22:44:09Z",
    "updated": "2023-12-19T03:32:26Z",
    "authors": [
      "Hanqi Yan",
      "Lin Gui",
      "Wenjie Li",
      "Yulan He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.00957v1",
    "title": "Preference Transformer: Modeling Human Preferences using Transformers\n  for RL",
    "summary": "Preference-based reinforcement learning (RL) provides a framework to train\nagents using human preferences between two behaviors. However, preference-based\nRL has been challenging to scale since it requires a large amount of human\nfeedback to learn a reward function aligned with human intent. In this paper,\nwe present Preference Transformer, a neural architecture that models human\npreferences using transformers. Unlike prior approaches assuming human judgment\nis based on the Markovian rewards which contribute to the decision equally, we\nintroduce a new preference model based on the weighted sum of non-Markovian\nrewards. We then design the proposed preference model using a transformer\narchitecture that stacks causal and bidirectional self-attention layers. We\ndemonstrate that Preference Transformer can solve a variety of control tasks\nusing real human preferences, while prior approaches fail to work. We also show\nthat Preference Transformer can induce a well-specified reward and attend to\ncritical events in the trajectory by automatically capturing the temporal\ndependencies in human decision-making. Code is available on the project\nwebsite: https://sites.google.com/view/preference-transformer.",
    "published": "2023-03-02T04:24:29Z",
    "updated": "2023-03-02T04:24:29Z",
    "authors": [
      "Changyeon Kim",
      "Jongjin Park",
      "Jinwoo Shin",
      "Honglak Lee",
      "Pieter Abbeel",
      "Kimin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.00578v1",
    "title": "Systematic Generalization with Edge Transformers",
    "summary": "Recent research suggests that systematic generalization in natural language\nunderstanding remains a challenge for state-of-the-art neural models such as\nTransformers and Graph Neural Networks. To tackle this challenge, we propose\nEdge Transformer, a new model that combines inspiration from Transformers and\nrule-based symbolic AI. The first key idea in Edge Transformers is to associate\nvector states with every edge, that is, with every pair of input nodes -- as\nopposed to just every node, as it is done in the Transformer model. The second\nmajor innovation is a triangular attention mechanism that updates edge\nrepresentations in a way that is inspired by unification from logic\nprogramming. We evaluate Edge Transformer on compositional generalization\nbenchmarks in relational reasoning, semantic parsing, and dependency parsing.\nIn all three settings, the Edge Transformer outperforms Relation-aware,\nUniversal and classical Transformer baselines.",
    "published": "2021-12-01T15:50:45Z",
    "updated": "2021-12-01T15:50:45Z",
    "authors": [
      "Leon Bergen",
      "Timothy J. O'Donnell",
      "Dzmitry Bahdanau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.04225v1",
    "title": "Transformer Utilization in Medical Image Segmentation Networks",
    "summary": "Owing to success in the data-rich domain of natural images, Transformers have\nrecently become popular in medical image segmentation. However, the pairing of\nTransformers with convolutional blocks in varying architectural permutations\nleaves their relative effectiveness to open interpretation. We introduce\nTransformer Ablations that replace the Transformer blocks with plain linear\noperators to quantify this effectiveness. With experiments on 8 models on 2\nmedical image segmentation tasks, we explore -- 1) the replaceable nature of\nTransformer-learnt representations, 2) Transformer capacity alone cannot\nprevent representational replaceability and works in tandem with effective\ndesign, 3) The mere existence of explicit feature hierarchies in transformer\nblocks is more beneficial than accompanying self-attention modules, 4) Major\nspatial downsampling before Transformer modules should be used with caution.",
    "published": "2023-04-09T12:35:22Z",
    "updated": "2023-04-09T12:35:22Z",
    "authors": [
      "Saikat Roy",
      "Gregor Koehler",
      "Michael Baumgartner",
      "Constantin Ulrich",
      "Jens Petersen",
      "Fabian Isensee",
      "Klaus Maier-Hein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01533v1",
    "title": "Transformers trained on proteins can learn to attend to Euclidean\n  distance",
    "summary": "While conventional Transformers generally operate on sequence data, they can\nbe used in conjunction with structure models, typically SE(3)-invariant or\nequivariant graph neural networks (GNNs), for 3D applications such as protein\nstructure modelling. These hybrids typically involve either (1)\npreprocessing/tokenizing structural features as input for Transformers or (2)\ntaking Transformer embeddings and processing them within a structural\nrepresentation. However, there is evidence that Transformers can learn to\nprocess structural information on their own, such as the AlphaFold3 structural\ndiffusion model. In this work we show that Transformers can function\nindependently as structure models when passed linear embeddings of coordinates.\nWe first provide a theoretical explanation for how Transformers can learn to\nfilter attention as a 3D Gaussian with learned variance. We then validate this\ntheory using both simulated 3D points and in the context of masked token\nprediction for proteins. Finally, we show that pre-training protein Transformer\nencoders with structure improves performance on a downstream task, yielding\nbetter performance than custom structural models. Together, this work provides\na basis for using standard Transformers as hybrid structure-language models.",
    "published": "2025-02-03T17:12:44Z",
    "updated": "2025-02-03T17:12:44Z",
    "authors": [
      "Isaac Ellmen",
      "Constantin Schneider",
      "Matthew I. J. Raybould",
      "Charlotte M. Deane"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06167v1",
    "title": "Universal Approximation of Visual Autoregressive Transformers",
    "summary": "We investigate the fundamental limits of transformer-based foundation models,\nextending our analysis to include Visual Autoregressive (VAR) transformers. VAR\nrepresents a big step toward generating images using a novel, scalable,\ncoarse-to-fine ``next-scale prediction'' framework. These models set a new\nquality bar, outperforming all previous methods, including Diffusion\nTransformers, while having state-of-the-art performance for image synthesis\ntasks. Our primary contributions establish that, for single-head VAR\ntransformers with a single self-attention layer and single interpolation layer,\nthe VAR Transformer is universal. From the statistical perspective, we prove\nthat such simple VAR transformers are universal approximators for any\nimage-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based\nautoregressive transformers inherit similar approximation capabilities. Our\nresults provide important design principles for effective and computationally\nefficient VAR Transformer strategies that can be used to extend their utility\nto more sophisticated VAR models in image generation and other related areas.",
    "published": "2025-02-10T05:36:30Z",
    "updated": "2025-02-10T05:36:30Z",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11374v1",
    "title": "Transformer Enhanced Relation Classification: A Comparative Analysis of\n  Contextuality, Data Efficiency and Sequence Complexity",
    "summary": "In the era of large language model, relation extraction (RE) plays an\nimportant role in information extraction through the transformation of\nunstructured raw text into structured data (Wadhwa et al., 2023). In this\npaper, we systematically compare the performance of deep supervised learning\napproaches without transformers and those with transformers. We used a series\nof non-transformer architectures such as PA-LSTM(Zhang et al., 2017),\nC-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),\nand a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu\nand He, 2019). Our comparison included traditional metrics like micro F1, as\nwell as evaluations in different scenarios, varying sentence lengths, and\ndifferent percentages of the dataset for training. Our experiments were\nconducted on TACRED, TACREV, and RE-TACRED. The results show that\ntransformer-based models outperform non-transformer models, achieving micro F1\nscores of 80-90% compared to 64-67% for non-transformer models. Additionally,\nwe briefly review the research journey in supervised relation classification\nand discuss the role and current status of large language models (LLMs) in\nrelation extraction.",
    "published": "2025-09-14T18:11:31Z",
    "updated": "2025-09-14T18:11:31Z",
    "authors": [
      "Bowen Jing",
      "Yang Cui",
      "Tianpeng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1908.10408v1",
    "title": "Multiresolution Transformer Networks: Recurrence is Not Essential for\n  Modeling Hierarchical Structure",
    "summary": "The architecture of Transformer is based entirely on self-attention, and has\nbeen shown to outperform models that employ recurrence on sequence transduction\ntasks such as machine translation. The superior performance of Transformer has\nbeen attributed to propagating signals over shorter distances, between\npositions in the input and the output, compared to the recurrent architectures.\nWe establish connections between the dynamics in Transformer and recurrent\nnetworks to argue that several factors including gradient flow along an\nensemble of multiple weakly dependent paths play a paramount role in the\nsuccess of Transformer. We then leverage the dynamics to introduce {\\em\nMultiresolution Transformer Networks} as the first architecture that exploits\nhierarchical structure in data via self-attention. Our models significantly\noutperform state-of-the-art recurrent and hierarchical recurrent models on two\nreal-world datasets for query suggestion, namely, \\aol and \\amazon. In\nparticular, on AOL data, our model registers at least 20\\% improvement on each\nprecision score, and over 25\\% improvement on the BLEU score with respect to\nthe best performing recurrent model. We thus provide strong evidence that\nrecurrence is not essential for modeling hierarchical structure.",
    "published": "2019-08-27T18:51:50Z",
    "updated": "2019-08-27T18:51:50Z",
    "authors": [
      "Vikas K. Garg",
      "Inderjit S. Dhillon",
      "Hsiang-Fu Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.07154v1",
    "title": "MiniViT: Compressing Vision Transformers with Weight Multiplexing",
    "summary": "Vision Transformer (ViT) models have recently drawn much attention in\ncomputer vision due to their high model capability. However, ViT models suffer\nfrom huge number of parameters, restricting their applicability on devices with\nlimited memory. To alleviate this problem, we propose MiniViT, a new\ncompression framework, which achieves parameter reduction in vision\ntransformers while retaining the same performance. The central idea of MiniViT\nis to multiplex the weights of consecutive transformer blocks. More\nspecifically, we make the weights shared across layers, while imposing a\ntransformation on the weights to increase diversity. Weight distillation over\nself-attention is also applied to transfer knowledge from large-scale ViT\nmodels to weight-multiplexed compact models. Comprehensive experiments\ndemonstrate the efficacy of MiniViT, showing that it can reduce the size of the\npre-trained Swin-B transformer by 48\\%, while achieving an increase of 1.0\\% in\nTop-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,\nMiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,\nwithout seriously compromising the performance. Finally, we verify the\ntransferability of MiniViT by reporting its performance on downstream\nbenchmarks. Code and models are available at here.",
    "published": "2022-04-14T17:59:05Z",
    "updated": "2022-04-14T17:59:05Z",
    "authors": [
      "Jinnian Zhang",
      "Houwen Peng",
      "Kan Wu",
      "Mengchen Liu",
      "Bin Xiao",
      "Jianlong Fu",
      "Lu Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.08721v2",
    "title": "Multimodal Token Fusion for Vision Transformers",
    "summary": "Many adaptations of transformers have emerged to address the single-modal\nvision tasks, where self-attention modules are stacked to handle input sources\nlike images. Intuitively, feeding multiple modalities of data to vision\ntransformers could improve the performance, yet the inner-modal attentive\nweights may also be diluted, which could thus undermine the final performance.\nIn this paper, we propose a multimodal token fusion method (TokenFusion),\ntailored for transformer-based vision tasks. To effectively fuse multiple\nmodalities, TokenFusion dynamically detects uninformative tokens and\nsubstitutes these tokens with projected and aggregated inter-modal features.\nResidual positional alignment is also adopted to enable explicit utilization of\nthe inter-modal alignments after fusion. The design of TokenFusion allows the\ntransformer to learn correlations among multimodal features, while the\nsingle-modal transformer architecture remains largely intact. Extensive\nexperiments are conducted on a variety of homogeneous and heterogeneous\nmodalities and demonstrate that TokenFusion surpasses state-of-the-art methods\nin three typical vision tasks: multimodal image-to-image translation, RGB-depth\nsemantic segmentation, and 3D object detection with point cloud and images. Our\ncode is available at https://github.com/yikaiw/TokenFusion.",
    "published": "2022-04-19T07:47:50Z",
    "updated": "2022-07-15T11:00:23Z",
    "authors": [
      "Yikai Wang",
      "Xinghao Chen",
      "Lele Cao",
      "Wenbing Huang",
      "Fuchun Sun",
      "Yunhe Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.00400v2",
    "title": "Systematic Generalization and Emergent Structures in Transformers\n  Trained on Structured Tasks",
    "summary": "Transformer networks have seen great success in natural language processing\nand machine vision, where task objectives such as next word prediction and\nimage classification benefit from nuanced context sensitivity across\nhigh-dimensional inputs. However, there is an ongoing debate about how and when\ntransformers can acquire highly structured behavior and achieve systematic\ngeneralization. Here, we explore how well a causal transformer can perform a\nset of algorithmic tasks, including copying, sorting, and hierarchical\ncompositions of these operations. We demonstrate strong generalization to\nsequences longer than those used in training by replacing the standard\npositional encoding typically used in transformers with labels arbitrarily\npaired with items in the sequence. We search for the layer and head\nconfiguration sufficient to solve these tasks, then probe for signs of\nsystematic processing in latent representations and attention patterns. We show\nthat two-layer transformers learn reliable solutions to multi-level problems,\ndevelop signs of task decomposition, and encode input items in a way that\nencourages the exploitation of shared computation across related tasks. These\nresults provide key insights into how attention layers support structured\ncomputation both within a task and across multiple tasks.",
    "published": "2022-10-02T00:46:36Z",
    "updated": "2022-12-10T07:35:12Z",
    "authors": [
      "Yuxuan Li",
      "James L. McClelland"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.03546v1",
    "title": "Time-Space Transformers for Video Panoptic Segmentation",
    "summary": "We propose a novel solution for the task of video panoptic segmentation, that\nsimultaneously predicts pixel-level semantic and instance segmentation and\ngenerates clip-level instance tracks. Our network, named VPS-Transformer, with\na hybrid architecture based on the state-of-the-art panoptic segmentation\nnetwork Panoptic-DeepLab, combines a convolutional architecture for\nsingle-frame panoptic segmentation and a novel video module based on an\ninstantiation of the pure Transformer block. The Transformer, equipped with\nattention mechanisms, models spatio-temporal relations between backbone output\nfeatures of current and past frames for more accurate and consistent panoptic\nestimates. As the pure Transformer block introduces large computation overhead\nwhen processing high resolution images, we propose a few design changes for a\nmore efficient compute. We study how to aggregate information more effectively\nover the space-time volume and we compare several variants of the Transformer\nblock with different attention schemes. Extensive experiments on the\nCityscapes-VPS dataset demonstrate that our best model improves the temporal\nconsistency and video panoptic quality by a margin of 2.2%, with little extra\ncomputation.",
    "published": "2022-10-07T13:30:11Z",
    "updated": "2022-10-07T13:30:11Z",
    "authors": [
      "Andra Petrovai",
      "Sergiu Nedevschi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.03452v2",
    "title": "Can CNNs Be More Robust Than Transformers?",
    "summary": "The recent success of Vision Transformers is shaking the long dominance of\nConvolutional Neural Networks (CNNs) in image recognition for a decade.\nSpecifically, in terms of robustness on out-of-distribution samples, recent\nresearch finds that Transformers are inherently more robust than CNNs,\nregardless of different training setups. Moreover, it is believed that such\nsuperiority of Transformers should largely be credited to their\nself-attention-like architectures per se. In this paper, we question that\nbelief by closely examining the design of Transformers. Our findings lead to\nthree highly effective architecture designs for boosting robustness, yet simple\nenough to be implemented in several lines of code, namely a) patchifying input\nimages, b) enlarging kernel size, and c) reducing activation layers and\nnormalization layers. Bringing these components together, we are able to build\npure CNN architectures without any attention-like operations that are as robust\nas, or even more robust than, Transformers. We hope this work can help the\ncommunity better understand the design of robust neural architectures. The code\nis publicly available at https://github.com/UCSC-VLAA/RobustCNN.",
    "published": "2022-06-07T17:17:07Z",
    "updated": "2023-03-06T05:51:33Z",
    "authors": [
      "Zeyu Wang",
      "Yutong Bai",
      "Yuyin Zhou",
      "Cihang Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1906.05551v1",
    "title": "Lattice Transformer for Speech Translation",
    "summary": "Recent advances in sequence modeling have highlighted the strengths of the\ntransformer architecture, especially in achieving state-of-the-art machine\ntranslation results. However, depending on the up-stream systems, e.g., speech\nrecognition, or word segmentation, the input to translation system can vary\ngreatly. The goal of this work is to extend the attention mechanism of the\ntransformer to naturally consume the lattice in addition to the traditional\nsequential input. We first propose a general lattice transformer for speech\ntranslation where the input is the output of the automatic speech recognition\n(ASR) which contains multiple paths and posterior scores. To leverage the extra\ninformation from the lattice structure, we develop a novel controllable lattice\nattention mechanism to obtain latent representations. On the LDC\nSpanish-English speech translation corpus, our experiments show that lattice\ntransformer generalizes significantly better and outperforms both a transformer\nbaseline and a lattice LSTM. Additionally, we validate our approach on the WMT\n2017 Chinese-English translation task with lattice inputs from different BPE\nsegmentations. In this task, we also observe the improvements over strong\nbaselines.",
    "published": "2019-06-13T08:55:06Z",
    "updated": "2019-06-13T08:55:06Z",
    "authors": [
      "Pei Zhang",
      "Boxing Chen",
      "Niyu Ge",
      "Kai Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.07849v2",
    "title": "Co-Attentive Equivariant Neural Networks: Focusing Equivariance On\n  Transformations Co-Occurring In Data",
    "summary": "Equivariance is a nice property to have as it produces much more parameter\nefficient neural architectures and preserves the structure of the input through\nthe feature mapping. Even though some combinations of transformations might\nnever appear (e.g. an upright face with a horizontal nose), current equivariant\narchitectures consider the set of all possible transformations in a\ntransformation group when learning feature representations. Contrarily, the\nhuman visual system is able to attend to the set of relevant transformations\noccurring in the environment and utilizes this information to assist and\nimprove object recognition. Based on this observation, we modify conventional\nequivariant feature mappings such that they are able to attend to the set of\nco-occurring transformations in data and generalize this notion to act on\ngroups consisting of multiple symmetries. We show that our proposed\nco-attentive equivariant neural networks consistently outperform conventional\nrotation equivariant and rotation & reflection equivariant neural networks on\nrotated MNIST and CIFAR-10.",
    "published": "2019-11-18T12:41:12Z",
    "updated": "2020-02-10T13:56:10Z",
    "authors": [
      "David W. Romero",
      "Mark Hoogendoorn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.16236v3",
    "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear\n  Attention",
    "summary": "Transformers achieve remarkable performance in several tasks but due to their\nquadratic complexity, with respect to the input's length, they are\nprohibitively slow for very long sequences. To address this limitation, we\nexpress the self-attention as a linear dot-product of kernel feature maps and\nmake use of the associativity property of matrix products to reduce the\ncomplexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$,\nwhere $N$ is the sequence length. We show that this formulation permits an\niterative implementation that dramatically accelerates autoregressive\ntransformers and reveals their relationship to recurrent neural networks. Our\nlinear transformers achieve similar performance to vanilla transformers and\nthey are up to 4000x faster on autoregressive prediction of very long\nsequences.",
    "published": "2020-06-29T17:55:38Z",
    "updated": "2020-08-31T11:09:32Z",
    "authors": [
      "Angelos Katharopoulos",
      "Apoorv Vyas",
      "Nikolaos Pappas",
      "FranÃ§ois Fleuret"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.01909v2",
    "title": "Line Segment Detection Using Transformers without Edges",
    "summary": "In this paper, we present a joint end-to-end line segment detection algorithm\nusing Transformers that is post-processing and heuristics-guided intermediate\nprocessing (edge/junction/region detection) free. Our method, named LinE\nsegment TRansformers (LETR), takes advantages of having integrated tokenized\nqueries, a self-attention mechanism, and an encoding-decoding strategy within\nTransformers by skipping standard heuristic designs for the edge element\ndetection and perceptual grouping processes. We equip Transformers with a\nmulti-scale encoder/decoder strategy to perform fine-grained line segment\ndetection under a direct endpoint distance loss. This loss term is particularly\nsuitable for detecting geometric structures such as line segments that are not\nconveniently represented by the standard bounding box representations. The\nTransformers learn to gradually refine line segments through layers of\nself-attention. In our experiments, we show state-of-the-art results on\nWireframe and YorkUrban benchmarks.",
    "published": "2021-01-06T08:00:18Z",
    "updated": "2021-04-30T17:34:55Z",
    "authors": [
      "Yifan Xu",
      "Weijian Xu",
      "David Cheung",
      "Zhuowen Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.05624v2",
    "title": "NAST: Non-Autoregressive Spatial-Temporal Transformer for Time Series\n  Forecasting",
    "summary": "Although Transformer has made breakthrough success in widespread domains\nespecially in Natural Language Processing (NLP), applying it to time series\nforecasting is still a great challenge. In time series forecasting, the\nautoregressive decoding of canonical Transformer models could introduce huge\naccumulative errors inevitably. Besides, utilizing Transformer to deal with\nspatial-temporal dependencies in the problem still faces tough difficulties.~To\ntackle these limitations, this work is the first attempt to propose a\nNon-Autoregressive Transformer architecture for time series forecasting, aiming\nat overcoming the time delay and accumulative error issues in the canonical\nTransformer. Moreover, we present a novel spatial-temporal attention mechanism,\nbuilding a bridge by a learned temporal influence map to fill the gaps between\nthe spatial and temporal attention, so that spatial and temporal dependencies\ncan be processed integrally. Empirically, we evaluate our model on diversified\nego-centric future localization datasets and demonstrate state-of-the-art\nperformance on both real-time and accuracy.",
    "published": "2021-02-10T18:36:11Z",
    "updated": "2021-05-29T07:47:53Z",
    "authors": [
      "Kai Chen",
      "Guang Chen",
      "Dan Xu",
      "Lijun Zhang",
      "Yuyao Huang",
      "Alois Knoll"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.09079v3",
    "title": "A novel time-frequency Transformer based on self-attention mechanism and\n  its application in fault diagnosis of rolling bearings",
    "summary": "The scope of data-driven fault diagnosis models is greatly extended through\ndeep learning (DL). However, the classical convolution and recurrent structure\nhave their defects in computational efficiency and feature representation,\nwhile the latest Transformer architecture based on attention mechanism has not\nyet been applied in this field. To solve these problems, we propose a novel\ntime-frequency Transformer (TFT) model inspired by the massive success of\nvanilla Transformer in sequence processing. Specially, we design a fresh\ntokenizer and encoder module to extract effective abstractions from the\ntime-frequency representation (TFR) of vibration signals. On this basis, a new\nend-to-end fault diagnosis framework based on time-frequency Transformer is\npresented in this paper. Through the case studies on bearing experimental\ndatasets, we construct the optimal Transformer structure and verify its fault\ndiagnosis performance. The superiority of the proposed method is demonstrated\nin comparison with the benchmark models and other state-of-the-art methods.",
    "published": "2021-04-19T06:53:31Z",
    "updated": "2021-12-04T05:49:05Z",
    "authors": [
      "Yifei Ding",
      "Minping Jia",
      "Qiuhua Miao",
      "Yudong Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.08050v2",
    "title": "Pay Attention to MLPs",
    "summary": "Transformers have become one of the most important architectural innovations\nin deep learning and have enabled many breakthroughs over the past few years.\nHere we propose a simple network architecture, gMLP, based on MLPs with gating,\nand show that it can perform as well as Transformers in key language and vision\napplications. Our comparisons show that self-attention is not critical for\nVision Transformers, as gMLP can achieve the same accuracy. For BERT, our model\nachieves parity with Transformers on pretraining perplexity and is better on\nsome downstream NLP tasks. On finetuning tasks where gMLP performs worse,\nmaking the gMLP model substantially larger can close the gap with Transformers.\nIn general, our experiments show that gMLP can scale as well as Transformers\nover increased data and compute.",
    "published": "2021-05-17T17:55:04Z",
    "updated": "2021-06-01T20:24:06Z",
    "authors": [
      "Hanxiao Liu",
      "Zihang Dai",
      "David R. So",
      "Quoc V. Le"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.10189v3",
    "title": "Combining Transformer Generators with Convolutional Discriminators",
    "summary": "Transformer models have recently attracted much interest from computer vision\nresearchers and have since been successfully employed for several problems\ntraditionally addressed with convolutional neural networks. At the same time,\nimage synthesis using generative adversarial networks (GANs) has drastically\nimproved over the last few years. The recently proposed TransGAN is the first\nGAN using only transformer-based architectures and achieves competitive results\nwhen compared to convolutional GANs. However, since transformers are\ndata-hungry architectures, TransGAN requires data augmentation, an auxiliary\nsuper-resolution task during training, and a masking prior to guide the\nself-attention mechanism. In this paper, we study the combination of a\ntransformer-based generator and convolutional discriminator and successfully\nremove the need of the aforementioned required design choices. We evaluate our\napproach by conducting a benchmark of well-known CNN discriminators, ablate the\nsize of the transformer-based generator, and show that combining both\narchitectural elements into a hybrid model leads to better results.\nFurthermore, we investigate the frequency spectrum properties of generated\nimages and observe that our model retains the benefits of an attention based\ngenerator.",
    "published": "2021-05-21T07:56:59Z",
    "updated": "2021-07-10T10:16:47Z",
    "authors": [
      "Ricard Durall",
      "Stanislav Frolov",
      "JÃ¶rn Hees",
      "Federico Raue",
      "Franz-Josef Pfreundt",
      "Andreas Dengel",
      "Janis Keupe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.15156v1",
    "title": "Blending Anti-Aliasing into Vision Transformer",
    "summary": "The transformer architectures, based on self-attention mechanism and\nconvolution-free design, recently found superior performance and booming\napplications in computer vision. However, the discontinuous patch-wise\ntokenization process implicitly introduces jagged artifacts into attention\nmaps, arising the traditional problem of aliasing for vision transformers.\nAliasing effect occurs when discrete patterns are used to produce high\nfrequency or continuous information, resulting in the indistinguishable\ndistortions. Recent researches have found that modern convolution networks\nstill suffer from this phenomenon. In this work, we analyze the uncharted\nproblem of aliasing in vision transformer and explore to incorporate\nanti-aliasing properties. Specifically, we propose a plug-and-play\nAliasing-Reduction Module(ARM) to alleviate the aforementioned issue. We\ninvestigate the effectiveness and generalization of the proposed method across\nmultiple tasks and various vision transformer families. This lightweight design\nconsistently attains a clear boost over several famous structures. Furthermore,\nour module also improves data efficiency and robustness of vision transformers.",
    "published": "2021-10-28T14:30:02Z",
    "updated": "2021-10-28T14:30:02Z",
    "authors": [
      "Shengju Qian",
      "Hao Shao",
      "Yi Zhu",
      "Mu Li",
      "Jiaya Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.08314v1",
    "title": "TRIG: Transformer-Based Text Recognizer with Initial Embedding Guidance",
    "summary": "Scene text recognition (STR) is an important bridge between images and text,\nattracting abundant research attention. While convolutional neural networks\n(CNNS) have achieved remarkable progress in this task, most of the existing\nworks need an extra module (context modeling module) to help CNN to capture\nglobal dependencies to solve the inductive bias and strengthen the relationship\nbetween text features. Recently, the transformer has been proposed as a\npromising network for global context modeling by self-attention mechanism, but\none of the main shortcomings, when applied to recognition, is the efficiency.\nWe propose a 1-D split to address the challenges of complexity and replace the\nCNN with the transformer encoder to reduce the need for a context modeling\nmodule. Furthermore, recent methods use a frozen initial embedding to guide the\ndecoder to decode the features to text, leading to a loss of accuracy. We\npropose to use a learnable initial embedding learned from the transformer\nencoder to make it adaptive to different input images. Above all, we introduce\na novel architecture for text recognition, named TRansformer-based text\nrecognizer with Initial embedding Guidance (TRIG), composed of three stages\n(transformation, feature extraction, and prediction). Extensive experiments\nshow that our approach can achieve state-of-the-art on text recognition\nbenchmarks.",
    "published": "2021-11-16T09:10:39Z",
    "updated": "2021-11-16T09:10:39Z",
    "authors": [
      "Yue Tao",
      "Zhiwei Jia",
      "Runze Ma",
      "Shugong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.09686v4",
    "title": "Efficient Visual Tracking with Exemplar Transformers",
    "summary": "The design of more complex and powerful neural network models has\nsignificantly advanced the state-of-the-art in visual object tracking. These\nadvances can be attributed to deeper networks, or the introduction of new\nbuilding blocks, such as transformers. However, in the pursuit of increased\ntracking performance, runtime is often hindered. Furthermore, efficient\ntracking architectures have received surprisingly little attention. In this\npaper, we introduce the Exemplar Transformer, a transformer module utilizing a\nsingle instance level attention layer for realtime visual object tracking.\nE.T.Track, our visual tracker that incorporates Exemplar Transformer modules,\nruns at 47 FPS on a CPU. This is up to 8x faster than other transformer-based\nmodels. When compared to lightweight trackers that can operate in realtime on\nstandard CPUs, E.T.Track consistently outperforms all other methods on the\nLaSOT, OTB-100, NFS, TrackingNet, and VOT-ST2020 datasets. Code and models are\navailable at https://github.com/pblatter/ettrack.",
    "published": "2021-12-17T18:57:54Z",
    "updated": "2022-10-04T11:09:37Z",
    "authors": [
      "Philippe Blatter",
      "Menelaos Kanakis",
      "Martin Danelljan",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.02916v2",
    "title": "PanFormer: a Transformer Based Model for Pan-sharpening",
    "summary": "Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)\nimage from a low-resolution (LR) multi-spectral (MS) image and its\ncorresponding panchromatic (PAN) image acquired by a same satellite. Inspired\nby a new fashion in recent deep learning community, we propose a novel\nTransformer based model for pan-sharpening. We explore the potential of\nTransformer in image feature extraction and fusion. Following the successful\ndevelopment of vision transformers, we design a two-stream network with the\nself-attention to extract the modality-specific features from the PAN and MS\nmodalities and apply a cross-attention module to merge the spectral and spatial\nfeatures. The pan-sharpened image is produced from the enhanced fused features.\nExtensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our\nTransformer based model achieves impressive results and outperforms many\nexisting CNN based methods, which shows the great potential of introducing\nTransformer to the pan-sharpening task. Codes are available at\nhttps://github.com/zhysora/PanFormer.",
    "published": "2022-03-06T09:22:20Z",
    "updated": "2022-03-22T07:01:11Z",
    "authors": [
      "Huanyu Zhou",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.08421v1",
    "title": "WegFormer: Transformers for Weakly Supervised Semantic Segmentation",
    "summary": "Although convolutional neural networks (CNNs) have achieved remarkable\nprogress in weakly supervised semantic segmentation (WSSS), the effective\nreceptive field of CNN is insufficient to capture global context information,\nleading to sub-optimal results. Inspired by the great success of Transformers\nin fundamental vision areas, this work for the first time introduces\nTransformer to build a simple and effective WSSS framework, termed WegFormer.\nUnlike existing CNN-based methods, WegFormer uses Vision Transformer (ViT) as a\nclassifier to produce high-quality pseudo segmentation masks. To this end, we\nintroduce three tailored components in our Transformer-based framework, which\nare (1) a Deep Taylor Decomposition (DTD) to generate attention maps, (2) a\nsoft erasing module to smooth the attention maps, and (3) an efficient\npotential object mining (EPOM) to filter noisy activation in the background.\nWithout any bells and whistles, WegFormer achieves state-of-the-art 70.5% mIoU\non the PASCAL VOC dataset, significantly outperforming the previous best\nmethod. We hope WegFormer provides a new perspective to tap the potential of\nTransformer in weakly supervised semantic segmentation. Code will be released.",
    "published": "2022-03-16T06:50:31Z",
    "updated": "2022-03-16T06:50:31Z",
    "authors": [
      "Chunmeng Liu",
      "Enze Xie",
      "Wenjia Wang",
      "Wenhai Wang",
      "Guangyao Li",
      "Ping Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.05588v1",
    "title": "CenterFormer: Center-based Transformer for 3D Object Detection",
    "summary": "Query-based transformer has shown great potential in constructing long-range\nattention in many image-domain tasks, but has rarely been considered in\nLiDAR-based 3D object detection due to the overwhelming size of the point cloud\ndata. In this paper, we propose CenterFormer, a center-based transformer\nnetwork for 3D object detection. CenterFormer first uses a center heatmap to\nselect center candidates on top of a standard voxel-based point cloud encoder.\nIt then uses the feature of the center candidate as the query embedding in the\ntransformer. To further aggregate features from multiple frames, we design an\napproach to fuse features through cross-attention. Lastly, regression heads are\nadded to predict the bounding box on the output center feature representation.\nOur design reduces the convergence difficulty and computational complexity of\nthe transformer structure. The results show significant improvements over the\nstrong baseline of anchor-free object detection networks. CenterFormer achieves\nstate-of-the-art performance for a single model on the Waymo Open Dataset, with\n73.7% mAPH on the validation set and 75.6% mAPH on the test set, significantly\noutperforming all previously published CNN and transformer-based methods. Our\ncode is publicly available at https://github.com/TuSimple/centerformer",
    "published": "2022-09-12T20:15:11Z",
    "updated": "2022-09-12T20:15:11Z",
    "authors": [
      "Zixiang Zhou",
      "Xiangchen Zhao",
      "Yu Wang",
      "Panqu Wang",
      "Hassan Foroosh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.07221v1",
    "title": "Number of Attention Heads vs Number of Transformer-Encoders in Computer\n  Vision",
    "summary": "Determining an appropriate number of attention heads on one hand and the\nnumber of transformer-encoders, on the other hand, is an important choice for\nComputer Vision (CV) tasks using the Transformer architecture. Computing\nexperiments confirmed the expectation that the total number of parameters has\nto satisfy the condition of overdetermination (i.e., number of constraints\nsignificantly exceeding the number of parameters). Then, good generalization\nperformance can be expected. This sets the boundaries within which the number\nof heads and the number of transformers can be chosen. If the role of context\nin images to be classified can be assumed to be small, it is favorable to use\nmultiple transformers with a low number of heads (such as one or two). In\nclassifying objects whose class may heavily depend on the context within the\nimage (i.e., the meaning of a patch being dependent on other patches), the\nnumber of heads is equally important as that of transformers.",
    "published": "2022-09-15T11:26:44Z",
    "updated": "2022-09-15T11:26:44Z",
    "authors": [
      "Tomas Hrycej",
      "Bernhard Bermeitinger",
      "Siegfried Handschuh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.04323v2",
    "title": "Sequential Transformer for End-to-End Person Search",
    "summary": "Person Search aims to simultaneously localize and recognize a target person\nfrom realistic and uncropped gallery images. One major challenge of person\nsearch comes from the contradictory goals of the two sub-tasks, i.e., person\ndetection focuses on finding the commonness of all persons so as to distinguish\npersons from the background, while person re-identification (re-ID) focuses on\nthe differences among different persons. In this paper, we propose a novel\nSequential Transformer (SeqTR) for end-to-end person search to deal with this\nchallenge. Our SeqTR contains a detection transformer and a novel re-ID\ntransformer that sequentially addresses detection and re-ID tasks. The re-ID\ntransformer comprises the self-attention layer that utilizes contextual\ninformation and the cross-attention layer that learns local fine-grained\ndiscriminative features of the human body. Moreover, the re-ID transformer is\nshared and supervised by multi-scale features to improve the robustness of\nlearned person representations. Extensive experiments on two widely-used person\nsearch benchmarks, CUHK-SYSU and PRW, show that our proposed SeqTR not only\noutperforms all existing person search methods with a 59.3% mAP on PRW but also\nachieves comparable performance to the state-of-the-art results with an mAP of\n94.8% on CUHK-SYSU.",
    "published": "2022-11-06T09:32:30Z",
    "updated": "2022-11-13T05:11:49Z",
    "authors": [
      "Long Chen",
      "Jinhua Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.09262v2",
    "title": "AttMEMO : Accelerating Transformers with Memoization on Big Memory\n  Systems",
    "summary": "Transformer models gain popularity because of their superior inference\naccuracy and inference throughput. However, the transformer is\ncomputation-intensive, causing a long inference time. The existing works on\ntransformer inference acceleration have limitations caused by either the\nmodification of transformer architectures or the need of specialized hardware.\nIn this paper, we identify the opportunities of using memoization to accelerate\nthe self-attention mechanism in transformers without the above limitations.\nBuilt upon a unique observation that there is rich similarity in attention\ncomputation across inference sequences, we build a memoization database that\nleverages the emerging big memory system. We introduce a novel embedding\ntechnique to find semantically similar inputs to identify computation\nsimilarity. We also introduce a series of techniques such as memory mapping and\nselective memoization to avoid memory copy and unnecessary overhead. We enable\n22% inference-latency reduction on average (up to 68%) with negligible loss in\ninference accuracy.",
    "published": "2023-01-23T04:24:26Z",
    "updated": "2023-04-17T20:06:38Z",
    "authors": [
      "Yuan Feng",
      "Hyeran Jeon",
      "Filip Blagojevic",
      "Cyril Guyot",
      "Qing Li",
      "Dong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.01834v1",
    "title": "Coinductive guide to inductive transformer heads",
    "summary": "We argue that all building blocks of transformer models can be expressed with\na single concept: combinatorial Hopf algebra. Transformer learning emerges as a\nresult of the subtle interplay between the algebraic and coalgebraic operations\nof the combinatorial Hopf algebra. Viewed through this lens, the transformer\nmodel becomes a linear time-invariant system where the attention mechanism\ncomputes a generalized convolution transform and the residual stream serves as\na unit impulse. Attention-only transformers then learn by enforcing an\ninvariant between these two paths. We call this invariant Hopf coherence. Due\nto this, with a degree of poetic license, one could call combinatorial Hopf\nalgebras \"tensors with a built-in loss function gradient\". This loss function\ngradient occurs within the single layers and no backward pass is needed. This\nis in contrast to automatic differentiation which happens across the whole\ngraph and needs a explicit backward pass. This property is the result of the\nfact that combinatorial Hopf algebras have the surprising property of\ncalculating eigenvalues by repeated squaring.",
    "published": "2023-02-03T16:19:43Z",
    "updated": "2023-02-03T16:19:43Z",
    "authors": [
      "Adam Nemecek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.05548v1",
    "title": "Distilling Token-Pruned Pose Transformer for 2D Human Pose Estimation",
    "summary": "Human pose estimation has seen widespread use of transformer models in recent\nyears. Pose transformers benefit from the self-attention map, which captures\nthe correlation between human joint tokens and the image. However, training\nsuch models is computationally expensive. The recent token-Pruned Pose\nTransformer (PPT) solves this problem by pruning the background tokens of the\nimage, which are usually less informative. However, although it improves\nefficiency, PPT inevitably leads to worse performance than TokenPose due to the\npruning of tokens. To overcome this problem, we present a novel method called\nDistilling Pruned-Token Transformer for human pose estimation (DPPT). Our\nmethod leverages the output of a pre-trained TokenPose to supervise the\nlearning process of PPT. We also establish connections between the internal\nstructure of pose transformers and PPT, such as attention maps and joint\nfeatures. Our experimental results on the MPII datasets show that our DPPT can\nsignificantly improve PCK compared to previous PPT models while still reducing\ncomputational complexity.",
    "published": "2023-04-12T00:46:41Z",
    "updated": "2023-04-12T00:46:41Z",
    "authors": [
      "Feixiang Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.11812v1",
    "title": "NoiseTrans: Point Cloud Denoising with Transformers",
    "summary": "Point clouds obtained from capture devices or 3D reconstruction techniques\nare often noisy and interfere with downstream tasks. The paper aims to recover\nthe underlying surface of noisy point clouds. We design a novel model,\nNoiseTrans, which uses transformer encoder architecture for point cloud\ndenoising. Specifically, we obtain structural similarity of point-based point\nclouds with the assistance of the transformer's core self-attention mechanism.\nBy expressing the noisy point cloud as a set of unordered vectors, we convert\npoint clouds into point embeddings and employ Transformer to generate clean\npoint clouds. To make the Transformer preserve details when sensing the point\ncloud, we design the Local Point Attention to prevent the point cloud from\nbeing over-smooth. In addition, we also propose sparse encoding, which enables\nthe Transformer to better perceive the structural relationships of the point\ncloud and improve the denoising performance. Experiments show that our model\noutperforms state-of-the-art methods in various datasets and noise\nenvironments.",
    "published": "2023-04-24T04:01:23Z",
    "updated": "2023-04-24T04:01:23Z",
    "authors": [
      "Guangzhe Hou",
      "Guihe Qin",
      "Minghui Sun",
      "Yanhua Liang",
      "Jie Yan",
      "Zhonghan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.02866v2",
    "title": "Hierarchical Transformer for Scalable Graph Learning",
    "summary": "Graph Transformer is gaining increasing attention in the field of machine\nlearning and has demonstrated state-of-the-art performance on benchmarks for\ngraph representation learning. However, as current implementations of Graph\nTransformer primarily focus on learning representations of small-scale graphs,\nthe quadratic complexity of the global self-attention mechanism presents a\nchallenge for full-batch training when applied to larger graphs. Additionally,\nconventional sampling-based methods fail to capture necessary high-level\ncontextual information, resulting in a significant loss of performance. In this\npaper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a\nsolution to these challenges. HSGT successfully scales the Transformer\narchitecture to node representation learning tasks on large-scale graphs, while\nmaintaining high performance. By utilizing graph hierarchies constructed\nthrough coarsening techniques, HSGT efficiently updates and stores multi-scale\ninformation in node embeddings at different levels. Together with\nsampling-based training methods, HSGT effectively captures and aggregates\nmulti-level information on the hierarchical graph using only Transformer\nblocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-art\nperformance on large-scale benchmarks with graphs containing millions of nodes\nwith high efficiency.",
    "published": "2023-05-04T14:23:22Z",
    "updated": "2023-05-05T05:21:18Z",
    "authors": [
      "Wenhao Zhu",
      "Tianyu Wen",
      "Guojie Song",
      "Xiaojun Ma",
      "Liang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.10320v1",
    "title": "CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo",
    "summary": "The core of Multi-view Stereo(MVS) is the matching process among reference\nand source pixels. Cost aggregation plays a significant role in this process,\nwhile previous methods focus on handling it via CNNs. This may inherit the\nnatural limitation of CNNs that fail to discriminate repetitive or incorrect\nmatches due to limited local receptive fields. To handle the issue, we aim to\ninvolve Transformer into cost aggregation. However, another problem may occur\ndue to the quadratically growing computational complexity caused by\nTransformer, resulting in memory overflow and inference latency. In this paper,\nwe overcome these limits with an efficient Transformer-based cost aggregation\nnetwork, namely CostFormer. The Residual Depth-Aware Cost Transformer(RDACT) is\nproposed to aggregate long-range features on cost volume via self-attention\nmechanisms along the depth and spatial dimensions. Furthermore, Residual\nRegression Transformer(RRT) is proposed to enhance spatial attention. The\nproposed method is a universal plug-in to improve learning-based MVS methods.",
    "published": "2023-05-17T16:01:27Z",
    "updated": "2023-05-17T16:01:27Z",
    "authors": [
      "Weitao Chen",
      "Hongbin Xu",
      "Zhipeng Zhou",
      "Yang Liu",
      "Baigui Sun",
      "Wenxiong Kang",
      "Xuansong Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.11403v5",
    "title": "Efficient Mixed Transformer for Single Image Super-Resolution",
    "summary": "Recently, Transformer-based methods have achieved impressive results in\nsingle image super-resolution (SISR). However, the lack of locality mechanism\nand high complexity limit their application in the field of super-resolution\n(SR). To solve these problems, we propose a new method, Efficient Mixed\nTransformer (EMT) in this study. Specifically, we propose the Mixed Transformer\nBlock (MTB), consisting of multiple consecutive transformer layers, in some of\nwhich the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can\nenhance the local knowledge aggregation with pixel shifting operations. At the\nsame time, no additional complexity is introduced as PM has no parameters and\nfloating-point operations. Moreover, we employ striped window for SA (SWSA) to\ngain an efficient global dependency modelling by utilizing image anisotropy.\nExperimental results show that EMT outperforms the existing methods on\nbenchmark dataset and achieved state-of-the-art performance. The Code is\navailable at https://github.com/Fried-Rice-Lab/FriedRiceLab.",
    "published": "2023-05-19T03:19:38Z",
    "updated": "2023-06-19T06:56:23Z",
    "authors": [
      "Ling Zheng",
      "Jinchen Zhu",
      "Jinpeng Shi",
      "Shizhuang Weng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.18838v1",
    "title": "Client: Cross-variable Linear Integrated Enhanced Transformer for\n  Multivariate Long-Term Time Series Forecasting",
    "summary": "Long-term time series forecasting (LTSF) is a crucial aspect of modern\nsociety, playing a pivotal role in facilitating long-term planning and\ndeveloping early warning systems. While many Transformer-based models have\nrecently been introduced for LTSF, a doubt have been raised regarding the\neffectiveness of attention modules in capturing cross-time dependencies. In\nthis study, we design a mask-series experiment to validate this assumption and\nsubsequently propose the \"Cross-variable Linear Integrated ENhanced Transformer\nfor Multivariate Long-Term Time Series Forecasting\" (Client), an advanced model\nthat outperforms both traditional Transformer-based models and linear models.\nClient employs linear modules to learn trend information and attention modules\nto capture cross-variable dependencies. Meanwhile, it simplifies the embedding\nand position encoding layers and replaces the decoder module with a projection\nlayer. Essentially, Client incorporates non-linearity and cross-variable\ndependencies, which sets it apart from conventional linear models and\nTransformer-based models. Extensive experiments with nine real-world datasets\nhave confirmed the SOTA performance of Client with the least computation time\nand memory consumption compared with the previous Transformer-based models. Our\ncode is available at https://github.com/daxin007/Client.",
    "published": "2023-05-30T08:31:22Z",
    "updated": "2023-05-30T08:31:22Z",
    "authors": [
      "Jiaxin Gao",
      "Wenbo Hu",
      "Yuntian Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.13776v1",
    "title": "Swin-Free: Achieving Better Cross-Window Attention and Efficiency with\n  Size-varying Window",
    "summary": "Transformer models have shown great potential in computer vision, following\ntheir success in language tasks. Swin Transformer is one of them that\noutperforms convolution-based architectures in terms of accuracy, while\nimproving efficiency when compared to Vision Transformer (ViT) and its\nvariants, which have quadratic complexity with respect to the input size. Swin\nTransformer features shifting windows that allows cross-window connection while\nlimiting self-attention computation to non-overlapping local windows. However,\nshifting windows introduces memory copy operations, which account for a\nsignificant portion of its runtime. To mitigate this issue, we propose\nSwin-Free in which we apply size-varying windows across stages, instead of\nshifting windows, to achieve cross-connection among local windows. With this\nsimple design change, Swin-Free runs faster than the Swin Transformer at\ninference with better accuracy. Furthermore, we also propose a few of Swin-Free\nvariants that are faster than their Swin Transformer counterparts.",
    "published": "2023-06-23T20:19:58Z",
    "updated": "2023-06-23T20:19:58Z",
    "authors": [
      "Jinkyu Koo",
      "John Yang",
      "Le An",
      "Gwenaelle Cunha Sergio",
      "Su Inn Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.11129v4",
    "title": "Enhancing Graph Transformers with Hierarchical Distance Structural\n  Encoding",
    "summary": "Graph transformers need strong inductive biases to derive meaningful\nattention scores. Yet, current methods often fall short in capturing longer\nranges, hierarchical structures, or community structures, which are common in\nvarious graphs such as molecules, social networks, and citation networks. This\npaper presents a Hierarchical Distance Structural Encoding (HDSE) method to\nmodel node distances in a graph, focusing on its multi-level, hierarchical\nnature. We introduce a novel framework to seamlessly integrate HDSE into the\nattention mechanism of existing graph transformers, allowing for simultaneous\napplication with other positional encodings. To apply graph transformers with\nHDSE to large-scale graphs, we further propose a high-level HDSE that\neffectively biases the linear transformers towards graph hierarchies. We\ntheoretically prove the superiority of HDSE over shortest path distances in\nterms of expressivity and generalization. Empirically, we demonstrate that\ngraph transformers with HDSE excel in graph classification, regression on 7\ngraph-level datasets, and node classification on 11 large-scale graphs,\nincluding those with up to a billion nodes.",
    "published": "2023-08-22T02:22:34Z",
    "updated": "2024-05-27T11:04:29Z",
    "authors": [
      "Yuankai Luo",
      "Hongkang Li",
      "Lei Shi",
      "Xiao-Ming Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.12069v2",
    "title": "Transformers for scientific data: a pedagogical review for astronomers",
    "summary": "The deep learning architecture associated with ChatGPT and related generative\nAI products is known as transformers. Initially applied to Natural Language\nProcessing, transformers and the self-attention mechanism they exploit have\ngained widespread interest across the natural sciences. The goal of this\npedagogical and informal review is to introduce transformers to scientists. The\nreview includes the mathematics underlying the attention mechanism, a\ndescription of the original transformer architecture, and a section on\napplications to time series and imaging data in astronomy. We include a\nFrequently Asked Questions section for readers who are curious about generative\nAI or interested in getting started with transformers for their research\nproblem.",
    "published": "2023-10-18T16:02:32Z",
    "updated": "2023-10-19T02:24:59Z",
    "authors": [
      "Dimitrios Tanoglidis",
      "Bhuvnesh Jain",
      "Helen Qu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.08141v2",
    "title": "GMTR: Graph Matching Transformers",
    "summary": "Vision transformers (ViTs) have recently been used for visual matching beyond\nobject detection and segmentation. However, the original grid dividing strategy\nof ViTs neglects the spatial information of the keypoints, limiting the\nsensitivity to local information. Therefore, we propose QueryTrans (Query\nTransformer), which adopts a cross-attention module and keypoints-based center\ncrop strategy for better spatial information extraction. We further integrate\nthe graph attention module and devise a transformer-based graph matching\napproach GMTR (Graph Matching TRansformers) whereby the combinatorial nature of\nGM is addressed by a graph transformer neural GM solver. On standard GM\nbenchmarks, GMTR shows competitive performance against the SOTA frameworks.\nSpecifically, on Pascal VOC, GMTR achieves $\\mathbf{83.6\\%}$ accuracy,\n$\\mathbf{0.9\\%}$ higher than the SOTA framework. On Spair-71k, GMTR shows great\npotential and outperforms most of the previous works. Meanwhile, on Pascal VOC,\nQueryTrans improves the accuracy of NGMv2 from $80.1\\%$ to $\\mathbf{83.3\\%}$,\nand BBGM from $79.0\\%$ to $\\mathbf{84.5\\%}$. On Spair-71k, QueryTrans improves\nNGMv2 from $80.6\\%$ to $\\mathbf{82.5\\%}$, and BBGM from $82.1\\%$ to\n$\\mathbf{83.9\\%}$. Source code will be made publicly available.",
    "published": "2023-11-14T13:12:47Z",
    "updated": "2023-12-14T10:44:37Z",
    "authors": [
      "Jinpei Guo",
      "Shaofeng Zhang",
      "Runzhong Wang",
      "Chang Liu",
      "Junchi Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.06182v2",
    "title": "Why \"classic\" Transformers are shallow and how to make them go deep",
    "summary": "Since its introduction in 2017, Transformer has emerged as the leading neural\nnetwork architecture, catalyzing revolutionary advancements in many AI\ndisciplines. The key innovation in Transformer is a Self-Attention (SA)\nmechanism designed to capture contextual information. However, extending the\noriginal Transformer design to models of greater depth has proven exceedingly\nchallenging, if not impossible. Even though various modifications have been\nproposed in order to stack more layers of SA mechanism into deeper models, a\nfull understanding of this depth problem remains lacking. In this paper, we\nconduct a comprehensive investigation, both theoretically and empirically, to\nsubstantiate the claim that the depth problem is caused by \\emph{token\nsimilarity escalation}; that is, tokens grow increasingly alike after repeated\napplications of the SA mechanism. Our analysis reveals that, driven by the\ninvariant leading eigenspace and large spectral gaps of attention matrices,\ntoken similarity provably escalates at a linear rate. Based on the gained\ninsight, we propose a new strategy of surgically removing excessive similarity\nin contrast to the existing approach of diminishing the SA mechanism explicitly\nor implicitly (such as in pre-norm transformers). Preliminary experimental\nresults confirm the effectiveness of the proposed strategy in small-scale\npost-norm Transformer models.",
    "published": "2023-12-11T07:49:16Z",
    "updated": "2024-02-02T02:53:22Z",
    "authors": [
      "Yueyao Yu",
      "Yin Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.14267v3",
    "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time",
    "summary": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers.",
    "published": "2024-01-25T16:01:49Z",
    "updated": "2024-08-16T14:56:36Z",
    "authors": [
      "Lyle Muller",
      "Patricia S. Churchland",
      "Terrence J. Sejnowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.03148v1",
    "title": "Aligning Transformers with Weisfeiler-Leman",
    "summary": "Graph neural network architectures aligned with the $k$-dimensional\nWeisfeiler--Leman ($k$-WL) hierarchy offer theoretically well-understood\nexpressive power. However, these architectures often fail to deliver\nstate-of-the-art predictive performance on real-world graphs, limiting their\npractical utility. While recent works aligning graph transformer architectures\nwith the $k$-WL hierarchy have shown promising empirical results, employing\ntransformers for higher orders of $k$ remains challenging due to a prohibitive\nruntime and memory complexity of self-attention as well as impractical\narchitectural assumptions, such as an infeasible number of attention heads.\nHere, we advance the alignment of transformers with the $k$-WL hierarchy,\nshowing stronger expressivity results for each $k$, making them more feasible\nin practice. In addition, we develop a theoretical framework that allows the\nstudy of established positional encodings such as Laplacian PEs and SPE. We\nevaluate our transformers on the large-scale PCQM4Mv2 dataset, showing\ncompetitive predictive performance with the state-of-the-art and demonstrating\nstrong downstream performance when fine-tuning them on small-scale molecular\ndatasets. Our code is available at\nhttps://github.com/luis-mueller/wl-transformers.",
    "published": "2024-06-05T11:06:33Z",
    "updated": "2024-06-05T11:06:33Z",
    "authors": [
      "Luis MÃ¼ller",
      "Christopher Morris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.04532v1",
    "title": "How Transformers Utilize Multi-Head Attention in In-Context Learning? A\n  Case Study on Sparse Linear Regression",
    "summary": "Despite the remarkable success of transformer-based models in various\nreal-world tasks, their underlying mechanisms remain poorly understood. Recent\nstudies have suggested that transformers can implement gradient descent as an\nin-context learner for linear regression problems and have developed various\ntheoretical analyses accordingly. However, these works mostly focus on the\nexpressive power of transformers by designing specific parameter constructions,\nlacking a comprehensive understanding of their inherent working mechanisms\npost-training. In this study, we consider a sparse linear regression problem\nand investigate how a trained multi-head transformer performs in-context\nlearning. We experimentally discover that the utilization of multi-heads\nexhibits different patterns across layers: multiple heads are utilized and\nessential in the first layer, while usually only a single head is sufficient\nfor subsequent layers. We provide a theoretical explanation for this\nobservation: the first layer preprocesses the context data, and the following\nlayers execute simple optimization steps based on the preprocessed context.\nMoreover, we demonstrate that such a preprocess-then-optimize algorithm can\nsignificantly outperform naive gradient descent and ridge regression\nalgorithms. Further experimental results support our explanations. Our findings\noffer insights into the benefits of multi-head attention and contribute to\nunderstanding the more intricate mechanisms hidden within trained transformers.",
    "published": "2024-08-08T15:33:02Z",
    "updated": "2024-08-08T15:33:02Z",
    "authors": [
      "Xingwu Chen",
      "Lei Zhao",
      "Difan Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.12175v1",
    "title": "Expanding Expressivity in Transformer Models with MÃ¶biusAttention",
    "summary": "Attention mechanisms and Transformer architectures have revolutionized\nNatural Language Processing (NLP) by enabling exceptional modeling of\nlong-range dependencies and capturing intricate linguistic patterns. However,\ntheir inherent reliance on linear operations in the form of matrix\nmultiplications limits their ability to fully capture inter-token relationships\non their own. We propose M\\\"obiusAttention, a novel approach that integrates\nM\\\"obius transformations within the attention mechanism of Transformer-based\nmodels. M\\\"obius transformations are non-linear operations in spaces over\ncomplex numbers with the ability to map between various geometries. By\nincorporating these properties, M\\\"obiusAttention empowers models to learn more\nintricate geometric relationships between tokens and capture a wider range of\ninformation through complex-valued weight vectors. We build and pre-train a\nBERT and a RoFormer version enhanced with M\\\"obiusAttention, which we then\nfinetune on the GLUE benchmark. We evaluate empirically our approach against\nthe baseline BERT and RoFormer models on a range of downstream tasks. Our\napproach compares favorably against the baseline models, even with smaller\nnumber of parameters suggesting the enhanced expressivity of M\\\"obiusAttention.\nThis research paves the way for exploring the potential of M\\\"obius\ntransformations in the complex projective space to enhance the expressivity and\nperformance of foundation models.",
    "published": "2024-09-08T16:56:33Z",
    "updated": "2024-09-08T16:56:33Z",
    "authors": [
      "Anna-Maria Halacheva",
      "Mojtaba Nayyeri",
      "Steffen Staab"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.10830v1",
    "title": "One-Layer Transformer Provably Learns One-Nearest Neighbor In Context",
    "summary": "Transformers have achieved great success in recent years. Interestingly,\ntransformers have shown particularly strong in-context learning capability --\neven without fine-tuning, they are still able to solve unseen tasks well purely\nbased on task-specific prompts. In this paper, we study the capability of\none-layer transformers in learning one of the most classical nonparametric\nestimators, the one-nearest neighbor prediction rule. Under a theoretical\nframework where the prompt contains a sequence of labeled training data and\nunlabeled test data, we show that, although the loss function is nonconvex when\ntrained with gradient descent, a single softmax attention layer can\nsuccessfully learn to behave like a one-nearest neighbor classifier. Our result\ngives a concrete example of how transformers can be trained to implement\nnonparametric machine learning algorithms, and sheds light on the role of\nsoftmax attention in transformer models.",
    "published": "2024-11-16T16:12:42Z",
    "updated": "2024-11-16T16:12:42Z",
    "authors": [
      "Zihao Li",
      "Yuan Cao",
      "Cheng Gao",
      "Yihan He",
      "Han Liu",
      "Jason M. Klusowski",
      "Jianqing Fan",
      "Mengdi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.05904v1",
    "title": "Binary Event-Driven Spiking Transformer",
    "summary": "Transformer-based Spiking Neural Networks (SNNs) introduce a novel\nevent-driven self-attention paradigm that combines the high performance of\nTransformers with the energy efficiency of SNNs. However, the larger model size\nand increased computational demands of the Transformer structure limit their\npracticality in resource-constrained scenarios. In this paper, we integrate\nbinarization techniques into Transformer-based SNNs and propose the Binary\nEvent-Driven Spiking Transformer, i.e. BESTformer. The proposed BESTformer can\nsignificantly reduce storage and computational demands by representing weights\nand attention maps with a mere 1-bit. However, BESTformer suffers from a severe\nperformance drop from its full-precision counterpart due to the limited\nrepresentation capability of binarization. To address this issue, we propose a\nCoupled Information Enhancement (CIE) method, which consists of a reversible\nframework and information enhancement distillation. By maximizing the mutual\ninformation between the binary model and its full-precision counterpart, the\nCIE method effectively mitigates the performance degradation of the BESTformer.\nExtensive experiments on static and neuromorphic datasets demonstrate that our\nmethod achieves superior performance to other binary SNNs, showcasing its\npotential as a compact yet high-performance model for resource-limited edge\ndevices.",
    "published": "2025-01-10T12:00:11Z",
    "updated": "2025-01-10T12:00:11Z",
    "authors": [
      "Honglin Cao",
      "Zijian Zhou",
      "Wenjie Wei",
      "Ammar Belatreche",
      "Yu Liang",
      "Dehao Zhang",
      "Malu Zhang",
      "Yang Yang",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.00585v3",
    "title": "Converting Transformers into DGNNs Form",
    "summary": "Recent advances in deep learning have established Transformer architectures\nas the predominant modeling paradigm. Central to the success of Transformers is\nthe self-attention mechanism, which scores the similarity between query and key\nmatrices to modulate a value matrix. This operation bears striking similarities\nto digraph convolution, prompting an investigation into whether digraph\nconvolution could serve as an alternative to self-attention. In this study, we\nformalize this concept by introducing a synthetic unitary digraph convolution\nbased on the digraph Fourier transform. The resulting model, which we term\nConverter, effectively converts a Transformer into a Directed Graph Neural\nNetwork (DGNN) form. We have tested Converter on Long-Range Arena benchmark,\nlong document classification, and DNA sequence-based taxonomy classification.\nOur experimental results demonstrate that Converter achieves superior\nperformance while maintaining computational efficiency and architectural\nsimplicity, which establishes it as a lightweight yet powerful Transformer\nvariant.",
    "published": "2025-02-01T22:44:46Z",
    "updated": "2025-03-03T23:19:39Z",
    "authors": [
      "Jie Zhang",
      "Mao-Hsuan Mao",
      "Bo-Wei Chiu",
      "Min-Te Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.24067v1",
    "title": "TransMamba: Flexibly Switching between Transformer and Mamba",
    "summary": "Transformers are the cornerstone of modern large language models, but their\nquadratic computational complexity limits efficiency in long-sequence\nprocessing. Recent advancements in Mamba, a state space model (SSM) with linear\ncomplexity, offer promising efficiency gains but suffer from unstable\ncontextual learning and multitask generalization. This paper proposes\nTransMamba, a novel framework that unifies Transformer and Mamba through shared\nparameter matrices (e.g., QKV and CBx), and thus could dynamically switch\nbetween attention and SSM mechanisms at different token lengths and layers. We\ndesign the Memory converter to bridge Transformer and Mamba by converting\nattention outputs into SSM-compatible states, ensuring seamless information\nflow at TransPoints where the transformation happens. The TransPoint scheduling\nis also thoroughly explored for further improvements. We conducted extensive\nexperiments demonstrating that TransMamba achieves superior training efficiency\nand performance compared to baselines, and validated the deeper consistency\nbetween Transformer and Mamba paradigms, offering a scalable solution for\nnext-generation sequence modeling.",
    "published": "2025-03-31T13:26:24Z",
    "updated": "2025-03-31T13:26:24Z",
    "authors": [
      "Yixing Li",
      "Ruobing Xie",
      "Zhen Yang",
      "Xingwu Sun",
      "Shuaipeng Li",
      "Weidong Han",
      "Zhanhui Kang",
      "Yu Cheng",
      "Chengzhong Xu",
      "Di Wang",
      "Jie Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.10845v1",
    "title": "Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive\n  Language Generators",
    "summary": "Large Language Models (LLMs), powered by Transformers, have demonstrated\nhuman-like intelligence capabilities, yet their underlying mechanisms remain\npoorly understood. This paper presents a novel framework for interpreting LLMs\nas probabilistic left context-sensitive languages (CSLs) generators. We\nhypothesize that Transformers can be effectively decomposed into three\nfundamental components: context windows, attention mechanisms, and\nautoregressive generation frameworks. This decomposition allows for the\ndevelopment of more flexible and interpretable computational models, moving\nbeyond the traditional view of attention and autoregression as inseparable\nprocesses. We argue that next-token predictions can be understood as\nprobabilistic, dynamic approximations of left CSL production rules, providing\nan intuitive explanation for how simple token predictions can yield human-like\nintelligence outputs. Given that all CSLs are left context-sensitive\n(Penttonen, 1974), we conclude that Transformers stochastically approximate\nCSLs, which are widely recognized as models of human-like intelligence. This\ninterpretation bridges the gap between Formal Language Theory and the observed\ngenerative power of Transformers, laying a foundation for future advancements\nin generative AI theory and applications. Our novel perspective on Transformer\narchitectures will foster a deeper understanding of LLMs and their future\npotentials.",
    "published": "2025-04-15T04:06:27Z",
    "updated": "2025-04-15T04:06:27Z",
    "authors": [
      "Phill Kyu Rhee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23623v1",
    "title": "Characterizing the Expressivity of Transformer Language Models",
    "summary": "Transformer-based language models (LMs) have achieved widespread empirical\nsuccess, but their theoretical expressive power remains only partially\nunderstood. Prior work often relies on idealized models with assumptions --\nsuch as arbitrary numerical precision and hard attention -- that diverge from\nreal-world transformers. In this work, we provide an exact characterization of\nfixed-precision transformers with strict future masking and soft attention, an\nidealization that more closely mirrors practical implementations. We show that\nthese models are precisely as expressive as a specific fragment of linear\ntemporal logic that includes only a single temporal operator: the past\noperator. We further relate this logic to established classes in formal\nlanguage theory, automata theory, and algebra, yielding a rich and unified\ntheoretical framework for understanding transformer expressivity. Finally, we\npresent empirical results that align closely with our theory: transformers\ntrained on languages within their theoretical capacity generalize perfectly\nover lengths, while they consistently fail to generalize on languages beyond\nit.",
    "published": "2025-05-29T16:30:30Z",
    "updated": "2025-05-29T16:30:30Z",
    "authors": [
      "Jiaoda Li",
      "Ryan Cotterell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.06165v4",
    "title": "CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation",
    "summary": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to a different unlabeled target domain. Most existing\nUDA methods focus on learning domain-invariant feature representation, either\nfrom the domain level or category level, using convolution neural networks\n(CNNs)-based frameworks. One fundamental problem for the category level based\nUDA is the production of pseudo labels for samples in target domain, which are\nusually too noisy for accurate domain alignment, inevitably compromising the\nUDA performance. With the success of Transformer in various tasks, we find that\nthe cross-attention in Transformer is robust to the noisy input pairs for\nbetter feature alignment, thus in this paper Transformer is adopted for the\nchallenging UDA task. Specifically, to generate accurate input pairs, we design\na two-way center-aware labeling algorithm to produce pseudo labels for target\nsamples. Along with the pseudo labels, a weight-sharing triple-branch\ntransformer framework is proposed to apply self-attention and cross-attention\nfor source/target feature learning and source-target domain alignment,\nrespectively. Such design explicitly enforces the framework to learn\ndiscriminative domain-specific and domain-invariant representations\nsimultaneously. The proposed method is dubbed CDTrans (cross-domain\ntransformer), and it provides one of the first attempts to solve UDA tasks with\na pure transformer solution. Experiments show that our proposed method achieves\nthe best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet.\nCode and models are available at https://github.com/CDTrans/CDTrans.",
    "published": "2021-09-13T17:59:07Z",
    "updated": "2022-03-19T11:02:21Z",
    "authors": [
      "Tongkun Xu",
      "Weihua Chen",
      "Pichao Wang",
      "Fan Wang",
      "Hao Li",
      "Rong Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.15142v3",
    "title": "Redesigning the Transformer Architecture with Insights from\n  Multi-particle Dynamical Systems",
    "summary": "The Transformer and its variants have been proven to be efficient sequence\nlearners in many different domains. Despite their staggering success, a\ncritical issue has been the enormous number of parameters that must be trained\n(ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of\ndot-product attention. In this work, we investigate the problem of\napproximating the two central components of the Transformer -- multi-head\nself-attention and point-wise feed-forward transformation, with reduced\nparameter space and computational complexity. We build upon recent developments\nin analyzing deep neural networks as numerical solvers of ordinary differential\nequations. Taking advantage of an analogy between Transformer stages and the\nevolution of a dynamical system of multiple interacting particles, we formulate\na temporal evolution scheme, TransEvolve, to bypass costly dot-product\nattention over multiple stacked layers. We perform exhaustive experiments with\nTransEvolve on well-known encoder-decoder as well as encoder-only tasks. We\nobserve that the degree of approximation (or inversely, the degree of parameter\nreduction) has different effects on the performance, depending on the task.\nWhile in the encoder-decoder regime, TransEvolve delivers performances\ncomparable to the original Transformer, in encoder-only tasks it consistently\noutperforms Transformer along with several subsequent variants.",
    "published": "2021-09-30T14:01:06Z",
    "updated": "2021-10-27T07:33:47Z",
    "authors": [
      "Subhabrata Dutta",
      "Tanya Gautam",
      "Soumen Chakrabarti",
      "Tanmoy Chakraborty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.13076v2",
    "title": "Finetuning Pretrained Transformers into RNNs",
    "summary": "Transformers have outperformed recurrent neural networks (RNNs) in natural\nlanguage generation. But this comes with a significant computational cost, as\nthe attention mechanism's complexity scales quadratically with sequence length.\nEfficient transformer variants have received increasing interest in recent\nworks. Among them, a linear-complexity recurrent variant has proven well suited\nfor autoregressive generation. It approximates the softmax attention with\nrandomized or heuristic feature maps, but can be difficult to train and may\nyield suboptimal accuracy. This work aims to convert a pretrained transformer\ninto its efficient recurrent counterpart, improving efficiency while\nmaintaining accuracy. Specifically, we propose a swap-then-finetune procedure:\nin an off-the-shelf pretrained transformer, we replace the softmax attention\nwith its linear-complexity recurrent alternative and then finetune. With a\nlearned feature map, our approach provides an improved tradeoff between\nefficiency and accuracy over the standard transformer and other recurrent\nvariants. We also show that the finetuning process has lower training cost\nrelative to training these recurrent variants from scratch. As many models for\nnatural language tasks are increasingly dependent on large-scale pretrained\ntransformers, this work presents a viable approach to improving inference\nefficiency without repeating the expensive pretraining process.",
    "published": "2021-03-24T10:50:43Z",
    "updated": "2021-09-20T06:45:09Z",
    "authors": [
      "Jungo Kasai",
      "Hao Peng",
      "Yizhe Zhang",
      "Dani Yogatama",
      "Gabriel Ilharco",
      "Nikolaos Pappas",
      "Yi Mao",
      "Weizhu Chen",
      "Noah A. Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.02693v1",
    "title": "Focal and Global Spatial-Temporal Transformer for Skeleton-based Action\n  Recognition",
    "summary": "Despite great progress achieved by transformer in various vision tasks, it is\nstill underexplored for skeleton-based action recognition with only a few\nattempts. Besides, these methods directly calculate the pair-wise global\nself-attention equally for all the joints in both the spatial and temporal\ndimensions, undervaluing the effect of discriminative local joints and the\nshort-range temporal dynamics. In this work, we propose a novel Focal and\nGlobal Spatial-Temporal Transformer network (FG-STFormer), that is equipped\nwith two key components: (1) FG-SFormer: focal joints and global parts coupling\nspatial transformer. It forces the network to focus on modelling correlations\nfor both the learned discriminative spatial joints and human body parts\nrespectively. The selective focal joints eliminate the negative effect of\nnon-informative ones during accumulating the correlations. Meanwhile, the\ninteractions between the focal joints and body parts are incorporated to\nenhance the spatial dependencies via mutual cross-attention. (2) FG-TFormer:\nfocal and global temporal transformer. Dilated temporal convolution is\nintegrated into the global self-attention mechanism to explicitly capture the\nlocal temporal motion patterns of joints or body parts, which is found to be\nvital important to make temporal transformer work. Extensive experimental\nresults on three benchmarks, namely NTU-60, NTU-120 and NW-UCLA, show our\nFG-STFormer surpasses all existing transformer-based methods, and compares\nfavourably with state-of-the art GCN-based methods.",
    "published": "2022-10-06T05:57:15Z",
    "updated": "2022-10-06T05:57:15Z",
    "authors": [
      "Zhimin Gao",
      "Peitao Wang",
      "Pei Lv",
      "Xiaoheng Jiang",
      "Qidong Liu",
      "Pichao Wang",
      "Mingliang Xu",
      "Wanqing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.09139v1",
    "title": "Dual-Flattening Transformers through Decomposed Row and Column Queries\n  for Semantic Segmentation",
    "summary": "It is critical to obtain high resolution features with long range dependency\nfor dense prediction tasks such as semantic segmentation. To generate\nhigh-resolution output of size $H\\times W$ from a low-resolution feature map of\nsize $h\\times w$ ($hw\\ll HW$), a naive dense transformer incurs an intractable\ncomplexity of $\\mathcal{O}(hwHW)$, limiting its application on high-resolution\ndense prediction. We propose a Dual-Flattening Transformer (DFlatFormer) to\nenable high-resolution output by reducing complexity to $\\mathcal{O}(hw(H+W))$\nthat is multiple orders of magnitude smaller than the naive dense transformer.\nDecomposed queries are presented to retrieve row and column attentions\ntractably through separate transformers, and their outputs are combined to form\na dense feature map at high resolution. To this end, the input sequence fed\nfrom an encoder is row-wise and column-wise flattened to align with decomposed\nqueries by preserving their row and column structures, respectively. Row and\ncolumn transformers also interact with each other to capture their mutual\nattentions with the spatial crossings between rows and columns. We also propose\nto perform attentions through efficient grouping and pooling to further reduce\nthe model complexity. Extensive experiments on ADE20K and Cityscapes datasets\ndemonstrate the superiority of the proposed dual-flattening transformer\narchitecture with higher mIoUs.",
    "published": "2022-01-22T22:38:15Z",
    "updated": "2022-01-22T22:38:15Z",
    "authors": [
      "Ying Wang",
      "Chiuman Ho",
      "Wenju Xu",
      "Ziwei Xuan",
      "Xudong Liu",
      "Guo-Jun Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.09332v4",
    "title": "How Expressive are Transformers in Spectral Domain for Graphs?",
    "summary": "The recent works proposing transformer-based models for graphs have proven\nthe inadequacy of Vanilla Transformer for graph representation learning. To\nunderstand this inadequacy, there is a need to investigate if spectral analysis\nof the transformer will reveal insights into its expressive power. Similar\nstudies already established that spectral analysis of Graph neural networks\n(GNNs) provides extra perspectives on their expressiveness. In this work, we\nsystematically study and establish the link between the spatial and spectral\ndomain in the realm of the transformer. We further provide a theoretical\nanalysis and prove that the spatial attention mechanism in the transformer\ncannot effectively capture the desired frequency response, thus, inherently\nlimiting its expressiveness in spectral space. Therefore, we propose FeTA, a\nframework that aims to perform attention over the entire graph spectrum (i.e.,\nactual frequency components of the graphs) analogous to the attention in\nspatial space. Empirical results suggest that FeTA provides homogeneous\nperformance gain against vanilla transformer across all tasks on standard\nbenchmarks and can easily be extended to GNN-based models with low-pass\ncharacteristics (e.g., GAT).",
    "published": "2022-01-23T18:03:22Z",
    "updated": "2022-07-15T18:08:57Z",
    "authors": [
      "Anson Bastos",
      "Abhishek Nadgeri",
      "Kuldeep Singh",
      "Hiroki Kanezashi",
      "Toyotaro Suzumura",
      "Isaiah Onando Mulang'"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.14607v1",
    "title": "SIM-Trans: Structure Information Modeling Transformer for Fine-grained\n  Visual Categorization",
    "summary": "Fine-grained visual categorization (FGVC) aims at recognizing objects from\nsimilar subordinate categories, which is challenging and practical for human's\naccurate automatic recognition needs. Most FGVC approaches focus on the\nattention mechanism research for discriminative regions mining while neglecting\ntheir interdependencies and composed holistic object structure, which are\nessential for model's discriminative information localization and understanding\nability. To address the above limitations, we propose the Structure Information\nModeling Transformer (SIM-Trans) to incorporate object structure information\ninto transformer for enhancing discriminative representation learning to\ncontain both the appearance information and structure information.\nSpecifically, we encode the image into a sequence of patch tokens and build a\nstrong vision transformer framework with two well-designed modules: (i) the\nstructure information learning (SIL) module is proposed to mine the spatial\ncontext relation of significant patches within the object extent with the help\nof the transformer's self-attention weights, which is further injected into the\nmodel for importing structure information; (ii) the multi-level feature\nboosting (MFB) module is introduced to exploit the complementary of multi-level\nfeatures and contrastive learning among classes to enhance feature robustness\nfor accurate recognition. The proposed two modules are light-weighted and can\nbe plugged into any transformer network and trained end-to-end easily, which\nonly depends on the attention weights that come with the vision transformer\nitself. Extensive experiments and analyses demonstrate that the proposed\nSIM-Trans achieves state-of-the-art performance on fine-grained visual\ncategorization benchmarks. The code is available at\nhttps://github.com/PKU-ICST-MIPL/SIM-Trans_ACMMM2022.",
    "published": "2022-08-31T03:00:07Z",
    "updated": "2022-08-31T03:00:07Z",
    "authors": [
      "Hongbo Sun",
      "Xiangteng He",
      "Yuxin Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.10774v4",
    "title": "Focused Decoding Enables 3D Anatomical Detection by Transformers",
    "summary": "Detection Transformers represent end-to-end object detection approaches based\non a Transformer encoder-decoder architecture, exploiting the attention\nmechanism for global relation modeling. Although Detection Transformers deliver\nresults on par with or even superior to their highly optimized CNN-based\ncounterparts operating on 2D natural images, their success is closely coupled\nto access to a vast amount of training data. This, however, restricts the\nfeasibility of employing Detection Transformers in the medical domain, as\naccess to annotated data is typically limited. To tackle this issue and\nfacilitate the advent of medical Detection Transformers, we propose a novel\nDetection Transformer for 3D anatomical structure detection, dubbed Focused\nDecoder. Focused Decoder leverages information from an anatomical region atlas\nto simultaneously deploy query anchors and restrict the cross-attention's field\nof view to regions of interest, which allows for a precise focus on relevant\nanatomical structures. We evaluate our proposed approach on two publicly\navailable CT datasets and demonstrate that Focused Decoder not only provides\nstrong detection results and thus alleviates the need for a vast amount of\nannotated data but also exhibits exceptional and highly intuitive\nexplainability of results via attention weights. Our code is available at\nhttps://github.com/bwittmann/transoar.",
    "published": "2022-07-21T22:17:21Z",
    "updated": "2023-02-26T19:43:42Z",
    "authors": [
      "Bastian Wittmann",
      "Fernando Navarro",
      "Suprosanna Shit",
      "Bjoern Menze"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.04991v1",
    "title": "Sim-T: Simplify the Transformer Network by Multiplexing Technique for\n  Speech Recognition",
    "summary": "In recent years, a great deal of attention has been paid to the Transformer\nnetwork for speech recognition tasks due to its excellent model performance.\nHowever, the Transformer network always involves heavy computation and large\nnumber of parameters, causing serious deployment problems in devices with\nlimited computation sources or storage memory. In this paper, a new lightweight\nmodel called Sim-T has been proposed to expand the generality of the\nTransformer model. Under the help of the newly developed multiplexing\ntechnique, the Sim-T can efficiently compress the model with negligible\nsacrifice on its performance. To be more precise, the proposed technique\nincludes two parts, that are, module weight multiplexing and attention score\nmultiplexing. Moreover, a novel decoder structure has been proposed to\nfacilitate the attention score multiplexing. Extensive experiments have been\nconducted to validate the effectiveness of Sim-T. In Aishell-1 dataset, when\nthe proposed Sim-T is 48% parameter less than the baseline Transformer, 0.4%\nCER improvement can be obtained. Alternatively, 69% parameter reduction can be\nachieved if the Sim-T gives the same performance as the baseline Transformer.\nWith regard to the HKUST and WSJ eval92 datasets, CER and WER will be improved\nby 0.3% and 0.2%, respectively, when parameters in Sim-T are 40% less than the\nbaseline Transformer.",
    "published": "2023-04-11T05:25:00Z",
    "updated": "2023-04-11T05:25:00Z",
    "authors": [
      "Guangyong Wei",
      "Zhikui Duan",
      "Shiren Li",
      "Guangguang Yang",
      "Xinmei Yu",
      "Junhua Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.09353v1",
    "title": "Blind Image Quality Assessment via Transformer Predicted Error Map and\n  Perceptual Quality Token",
    "summary": "Image quality assessment is a fundamental problem in the field of image\nprocessing, and due to the lack of reference images in most practical\nscenarios, no-reference image quality assessment (NR-IQA), has gained\nincreasing attention recently. With the development of deep learning\ntechnology, many deep neural network-based NR-IQA methods have been developed,\nwhich try to learn the image quality based on the understanding of database\ninformation. Currently, Transformer has achieved remarkable progress in various\nvision tasks. Since the characteristics of the attention mechanism in\nTransformer fit the global perceptual impact of artifacts perceived by a human,\nTransformer is thus well suited for image quality assessment tasks. In this\npaper, we propose a Transformer based NR-IQA model using a predicted objective\nerror map and perceptual quality token. Specifically, we firstly generate the\npredicted error map by pre-training one model consisting of a Transformer\nencoder and decoder, in which the objective difference between the distorted\nand the reference images is used as supervision. Then, we freeze the parameters\nof the pre-trained model and design another branch using the vision Transformer\nto extract the perceptual quality token for feature fusion with the predicted\nerror map. Finally, the fused features are regressed to the final image quality\nscore. Extensive experiments have shown that our proposed method outperforms\nthe current state-of-the-art in both authentic and synthetic image databases.\nMoreover, the attentional map extracted by the perceptual quality token also\ndoes conform to the characteristics of the human visual system.",
    "published": "2023-05-16T11:17:54Z",
    "updated": "2023-05-16T11:17:54Z",
    "authors": [
      "Jinsong Shi",
      "Pan Gao",
      "Aljosa Smolic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.06625v4",
    "title": "iTransformer: Inverted Transformers Are Effective for Time Series\n  Forecasting",
    "summary": "The recent boom of linear forecasting models questions the ongoing passion\nfor architectural modifications of Transformer-based forecasters. These\nforecasters leverage Transformers to model the global dependencies over\ntemporal tokens of time series, with each token formed by multiple variates of\nthe same timestamp. However, Transformers are challenged in forecasting series\nwith larger lookback windows due to performance degradation and computation\nexplosion. Besides, the embedding for each temporal token fuses multiple\nvariates that represent potential delayed events and distinct physical\nmeasurements, which may fail in learning variate-centric representations and\nresult in meaningless attention maps. In this work, we reflect on the competent\nduties of Transformer components and repurpose the Transformer architecture\nwithout any modification to the basic components. We propose iTransformer that\nsimply applies the attention and feed-forward network on the inverted\ndimensions. Specifically, the time points of individual series are embedded\ninto variate tokens which are utilized by the attention mechanism to capture\nmultivariate correlations; meanwhile, the feed-forward network is applied for\neach variate token to learn nonlinear representations. The iTransformer model\nachieves state-of-the-art on challenging real-world datasets, which further\nempowers the Transformer family with promoted performance, generalization\nability across different variates, and better utilization of arbitrary lookback\nwindows, making it a nice alternative as the fundamental backbone of time\nseries forecasting. Code is available at this repository:\nhttps://github.com/thuml/iTransformer.",
    "published": "2023-10-10T13:44:09Z",
    "updated": "2024-03-14T11:45:57Z",
    "authors": [
      "Yong Liu",
      "Tengge Hu",
      "Haoran Zhang",
      "Haixu Wu",
      "Shiyu Wang",
      "Lintao Ma",
      "Mingsheng Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.14753v2",
    "title": "Learning with SASQuaTCh: a Novel Variational Quantum Transformer\n  Architecture with Kernel-Based Self-Attention",
    "summary": "The recent exploding growth in size of state-of-the-art machine learning\nmodels highlights a well-known issue where exponential parameter growth, which\nhas grown to trillions as in the case of the Generative Pre-trained Transformer\n(GPT), leads to training time and memory requirements which limit their\nadvancement in the near term. The predominant models use the so-called\ntransformer network and have a large field of applicability, including\npredicting text and images, classification, and even predicting solutions to\nthe dynamics of physical systems. Here we present a variational quantum circuit\narchitecture named Self-Attention Sequential Quantum Transformer Channel\n(SASQuaTCh), which builds networks of qubits that perform analogous operations\nof the transformer network, namely the keystone self-attention operation, and\nleads to an exponential improvement in parameter complexity and run-time\ncomplexity over its classical counterpart. Our approach leverages recent\ninsights from kernel-based operator learning in the context of predicting\nspatiotemporal systems to represent deep layers of a vision transformer network\nusing simple gate operations and a set of multi-dimensional quantum Fourier\ntransforms. To validate our approach, we consider image classification tasks in\nsimulation and with hardware, where with only 9 qubits and a handful of\nparameters we are able to simultaneously embed and classify a grayscale image\nof handwritten digits with high accuracy.",
    "published": "2024-03-21T18:00:04Z",
    "updated": "2025-02-05T16:56:04Z",
    "authors": [
      "Ethan N. Evans",
      "Matthew Cook",
      "Zachary P. Bradshaw",
      "Margarite L. LaBorde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.03380v1",
    "title": "On the Theoretical Expressive Power and the Design Space of Higher-Order\n  Graph Transformers",
    "summary": "Graph transformers have recently received significant attention in graph\nlearning, partly due to their ability to capture more global interaction via\nself-attention. Nevertheless, while higher-order graph neural networks have\nbeen reasonably well studied, the exploration of extending graph transformers\nto higher-order variants is just starting. Both theoretical understanding and\nempirical results are limited. In this paper, we provide a systematic study of\nthe theoretical expressive power of order-$k$ graph transformers and sparse\nvariants. We first show that, an order-$k$ graph transformer without additional\nstructural information is less expressive than the $k$-Weisfeiler Lehman\n($k$-WL) test despite its high computational cost. We then explore strategies\nto both sparsify and enhance the higher-order graph transformers, aiming to\nimprove both their efficiency and expressiveness. Indeed, sparsification based\non neighborhood information can enhance the expressive power, as it provides\nadditional information about input graph structures. In particular, we show\nthat a natural neighborhood-based sparse order-$k$ transformer model is not\nonly computationally efficient, but also expressive -- as expressive as $k$-WL\ntest. We further study several other sparse graph attention models that are\ncomputationally efficient and provide their expressiveness analysis. Finally,\nwe provide experimental results to show the effectiveness of the different\nsparsification strategies.",
    "published": "2024-04-04T11:26:51Z",
    "updated": "2024-04-04T11:26:51Z",
    "authors": [
      "Cai Zhou",
      "Rose Yu",
      "Yusu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.16166v2",
    "title": "The Power of Hard Attention Transformers on Data Sequences: A Formal\n  Language Theoretic Perspective",
    "summary": "Formal language theory has recently been successfully employed to unravel the\npower of transformer encoders. This setting is primarily applicable in Natural\nLanguage Processing (NLP), as a token embedding function (where a bounded\nnumber of tokens is admitted) is first applied before feeding the input to the\ntransformer. On certain kinds of data (e.g. time series), we want our\ntransformers to be able to handle arbitrary input sequences of numbers (or\ntuples thereof) without a priori limiting the values of these numbers. In this\npaper, we initiate the study of the expressive power of transformer encoders on\nsequences of data (i.e. tuples of numbers). Our results indicate an increase in\nexpressive power of hard attention transformers over data sequences, in stark\ncontrast to the case of strings. In particular, we prove that Unique Hard\nAttention Transformers (UHAT) over inputs as data sequences no longer lie\nwithin the circuit complexity class $AC^0$ (even without positional encodings),\nunlike the case of string inputs, but are still within the complexity class\n$TC^0$ (even with positional encodings). Over strings, UHAT without positional\nencodings capture only regular languages. In contrast, we show that over data\nsequences UHAT can capture non-regular properties. Finally, we show that UHAT\ncapture languages definable in an extension of linear temporal logic with unary\nnumeric predicates and arithmetics.",
    "published": "2024-05-25T10:18:48Z",
    "updated": "2024-11-12T16:38:44Z",
    "authors": [
      "Pascal BergstrÃ¤Ãer",
      "Chris KÃ¶cher",
      "Anthony Widjaja Lin",
      "Georg Zetzsche"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.10986v2",
    "title": "What Does It Mean to Be a Transformer? Insights from a Theoretical\n  Hessian Analysis",
    "summary": "The Transformer architecture has inarguably revolutionized deep learning,\novertaking classical architectures like multi-layer perceptrons (MLPs) and\nconvolutional neural networks (CNNs). At its core, the attention block differs\nin form and functionality from most other architectural components in deep\nlearning--to the extent that, in comparison to MLPs/CNNs, Transformers are more\noften accompanied by adaptive optimizers, layer normalization, learning rate\nwarmup, etc. The root causes behind these outward manifestations and the\nprecise mechanisms that govern them remain poorly understood. In this work, we\nbridge this gap by providing a fundamental understanding of what distinguishes\nthe Transformer from the other architectures--grounded in a theoretical\ncomparison of the (loss) Hessian. Concretely, for a single self-attention\nlayer, (a) we first entirely derive the Transformer's Hessian and express it in\nmatrix derivatives; (b) we then characterize it in terms of data, weight, and\nattention moment dependencies; and (c) while doing so further highlight the\nimportant structural differences to the Hessian of classical networks. Our\nresults suggest that various common architectural and optimization choices in\nTransformers can be traced back to their highly non-linear dependencies on the\ndata and weight matrices, which vary heterogeneously across parameters.\nUltimately, our findings provide a deeper understanding of the Transformer's\nunique optimization landscape and the challenges it poses.",
    "published": "2024-10-14T18:15:02Z",
    "updated": "2025-03-17T17:32:06Z",
    "authors": [
      "Weronika Ormaniec",
      "Felix Dangel",
      "Sidak Pal Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.20156v1",
    "title": "Distilled Transformers with Locally Enhanced Global Representations for\n  Face Forgery Detection",
    "summary": "Face forgery detection (FFD) is devoted to detecting the authenticity of face\nimages. Although current CNN-based works achieve outstanding performance in\nFFD, they are susceptible to capturing local forgery patterns generated by\nvarious manipulation methods. Though transformer-based detectors exhibit\nimprovements in modeling global dependencies, they are not good at exploring\nlocal forgery artifacts. Hybrid transformer-based networks are designed to\ncapture local and global manipulated traces, but they tend to suffer from the\nattention collapse issue as the transformer block goes deeper. Besides, soft\nlabels are rarely available. In this paper, we propose a distilled transformer\nnetwork (DTN) to capture both rich local and global forgery traces and learn\ngeneral and common representations for different forgery faces. Specifically,\nwe design a mixture of expert (MoE) module to mine various robust forgery\nembeddings. Moreover, a locally-enhanced vision transformer (LEVT) module is\nproposed to learn locally-enhanced global representations. We design a\nlightweight multi-attention scaling (MAS) module to avoid attention collapse,\nwhich can be plugged and played in any transformer-based models with only a\nslight increase in computational costs. In addition, we propose a deepfake\nself-distillation (DSD) scheme to provide the model with abundant soft label\ninformation. Extensive experiments show that the proposed method surpasses the\nstate of the arts on five deepfake datasets.",
    "published": "2024-12-28T14:00:27Z",
    "updated": "2024-12-28T14:00:27Z",
    "authors": [
      "Yaning Zhang",
      "Qiufu Li",
      "Zitong Yu",
      "Linlin Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.08803v1",
    "title": "A temporal scale transformer framework for precise remaining useful life\n  prediction in fuel cells",
    "summary": "In exploring Predictive Health Management (PHM) strategies for Proton\nExchange Membrane Fuel Cells (PEMFC), the Transformer model, widely used in\ndata-driven approaches, excels in many fields but struggles with time series\nanalysis due to its self-attention mechanism, which yields a complexity of the\ninput sequence squared and low computational efficiency. It also faces\nchallenges in capturing both global long-term dependencies and local details\neffectively. To tackle this, we propose the Temporal Scale Transformer\n(TSTransformer), an enhanced version of the inverted Transformer\n(iTransformer). Unlike traditional Transformers that treat each timestep as an\ninput token, TSTransformer maps sequences of varying lengths into tokens at\ndifferent stages for inter-sequence modeling, using attention to capture\nmultivariate correlations and feed-forward networks (FFN) to encode sequence\nrepresentations. By integrating a one-dimensional convolutional layer into the\nmultivariate attention for multi-level scaling of K and V matrices, it improves\nlocal feature extraction, captures temporal scale characteristics, and reduces\ntoken count and computational costs. Experiments comparing TSTransformer with\nmodels like Long Short-Term Memory, iTransformer, and Transformer demonstrate\nits potential as a powerful tool for advancing PHM in renewable energy,\neffectively addressing the limitations of pure Transformer models in\ndata-driven time series tasks.",
    "published": "2025-04-08T23:42:54Z",
    "updated": "2025-04-08T23:42:54Z",
    "authors": [
      "Zezhi Tang",
      "Xiaoyu Chen",
      "Xin Jin",
      "Benyuan Zhang",
      "Wenyu Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16275v1",
    "title": "Quantum Doubly Stochastic Transformers",
    "summary": "At the core of the Transformer, the Softmax normalizes the attention matrix\nto be right stochastic. Previous research has shown that this often\ndestabilizes training and that enforcing the attention matrix to be doubly\nstochastic (through Sinkhorn's algorithm) consistently improves performance\nacross different tasks, domains and Transformer flavors. However, Sinkhorn's\nalgorithm is iterative, approximative, non-parametric and thus inflexible\nw.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been\nproven that DSMs can be obtained with a parametric quantum circuit, yielding a\nnovel quantum inductive bias for DSMs with no known classical analogue.\nMotivated by this, we demonstrate the feasibility of a hybrid classical-quantum\ndoubly stochastic Transformer (QDSFormer) that replaces the Softmax in the\nself-attention layer with a variational quantum circuit. We study the\nexpressive power of the circuit and find that it yields more diverse DSMs that\nbetter preserve information than classical operators. Across multiple\nsmall-scale object recognition tasks, we find that our QDSFormer consistently\nsurpasses both a standard Vision Transformer and other doubly stochastic\nTransformers. Beyond the established Sinkformer, this comparison includes a\nnovel quantum-inspired doubly stochastic Transformer (based on QR\ndecomposition) that can be of independent interest. The QDSFormer also shows\nimproved training stability and lower performance variation suggesting that it\nmay mitigate the notoriously unstable training of ViTs on small-scale data.",
    "published": "2025-04-22T21:15:45Z",
    "updated": "2025-04-22T21:15:45Z",
    "authors": [
      "Jannis Born",
      "Filip Skogh",
      "Kahn Rhrissorrakrai",
      "Filippo Utro",
      "Nico Wagner",
      "Aleksandros Sobczyk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.23832v1",
    "title": "Low-latency vision transformers via large-scale multi-head attention",
    "summary": "The emergence of spontaneous symmetry breaking among a few heads of\nmulti-head attention (MHA) across transformer blocks in classification tasks\nwas recently demonstrated through the quantification of single-nodal\nperformance (SNP). This finding indicates that each head focuses its attention\non a subset of labels through cooperation among its SNPs. This underlying\nlearning mechanism is generalized to large-scale MHA (LS-MHA) using a single\nmatrix value representing single-head performance (SHP), analogous to\nsingle-filter performance in convolutional neural networks (CNNs). The results\nindicate that each SHP matrix comprises multiple unit clusters such that each\nlabel being explicitly recognized by a few heads with negligible noise. This\nleads to an increased signal-to-noise ratio (SNR) along the transformer blocks,\nthereby improving classification accuracy. These features give rise to several\ndistinct vision transformer (ViT) architectures that achieve the same accuracy\nbut differ in their LS-MHA structures. As a result, their soft committee yields\nsuperior accuracy, an outcome not typically observed in CNNs which rely on\nhundreds of filters. In addition, a significant reduction in latency is\nachieved without affecting the accuracy by replacing the initial transformer\nblocks with convolutional layers. This substitution accelerates early-stage\nlearning, which is then improved by subsequent transformer layers. The\nextension of this learning mechanism to natural language processing tasks,\nbased on quantitative differences between CNNs and ViT architectures, has the\npotential to yield new insights in deep learning. The findings are demonstrated\nusing compact convolutional transformer architectures trained on the CIFAR-100\ndataset.",
    "published": "2025-06-30T13:23:46Z",
    "updated": "2025-06-30T13:23:46Z",
    "authors": [
      "Ronit D. Gross",
      "Tal Halevi",
      "Ella Koresh",
      "Yarden Tzach",
      "Ido Kanter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.10581v1",
    "title": "Universal Approximation Theorem for a Single-Layer Transformer",
    "summary": "Deep learning employs multi-layer neural networks trained via the\nbackpropagation algorithm. This approach has achieved success across many\ndomains and relies on adaptive gradient methods such as the Adam optimizer.\nSequence modeling evolved from recurrent neural networks to attention-based\nmodels, culminating in the Transformer architecture. Transformers have achieved\nstate-of-the-art performance in natural language processing (for example, BERT\nand GPT-3) and have been applied in computer vision and computational biology.\nHowever, theoretical understanding of these models remains limited. In this\npaper, we examine the mathematical foundations of deep learning and\nTransformers and present a novel theoretical result. We review key concepts\nfrom linear algebra, probability, and optimization that underpin deep learning,\nand we analyze the multi-head self-attention mechanism and the backpropagation\nalgorithm in detail. Our main contribution is a universal approximation theorem\nfor Transformers: we prove that a single-layer Transformer, comprising one\nself-attention layer followed by a position-wise feed-forward network with ReLU\nactivation, can approximate any continuous sequence-to-sequence mapping on a\ncompact domain to arbitrary precision. We provide a formal statement and a\ncomplete proof. Finally, we present case studies that demonstrate the practical\nimplications of this result. Our findings advance the theoretical understanding\nof Transformer models and help bridge the gap between theory and practice.",
    "published": "2025-07-11T11:37:39Z",
    "updated": "2025-07-11T11:37:39Z",
    "authors": [
      "Esmail Gumaan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.00901v2",
    "title": "Filtering with Self-Attention and Storing with MLP: One-Layer\n  Transformers Can Provably Acquire and Extract Knowledge",
    "summary": "Modern large language models excel in knowledge-intensive tasks, yet how\ntransformers acquire (store) knowledge during pre-training and extract\n(retrieve) it during post-fine-tuning inference remains theoretically opaque.\nWhile prior theoretical work has begun to investigate these questions through\nthe analysis of training dynamics, such studies are limited to single-layer,\nattention-only architectures. However, most existing studies suggest that MLPs\nare the most contributing components for storing knowledge in transformer-based\nlanguage models. Meanwhile, our empirical investigations reveal that such\nsimplified models, when trained using standard next-token prediction\nobjectives, may be incapable of acquiring or extracting factual knowledge. To\novercome this limitation, we introduce a tractable one-layer transformer\nframework that crucially incorporates both self-attention and MLP modules. By\ntracking its gradient dynamics, we establish convergence and generalization\nguarantees that illuminate the ability of knowledge acquisition and extraction.\nWe prove that 1) Transformers can achieve near-optimal training loss during\npre-training, signifying effective knowledge acquisition; 2) With a large\nfine-tuning dataset and specific data multiplicity conditions met, transformers\ncan achieve low generalization error when tested on factual knowledge learned\nduring pre-training but not reinforced during the fine-tuning, indicating\nsuccessful knowledge extraction; 3) When the conditions are not satisfied,\ntransformers exhibit high generalization loss, resulting in hallucinations. Our\nanalysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,\nour analysis offers theoretical insights into several pertinent empirical\nphenomena, such as the role of learning rate schedules. Experiments on\nsynthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate\nour results.",
    "published": "2025-07-28T17:24:57Z",
    "updated": "2025-08-05T03:25:50Z",
    "authors": [
      "Ruichen Xu",
      "Kexin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.15213v3",
    "title": "GTNet: Graph Transformer Network for 3D Point Cloud Classification and\n  Semantic Segmentation",
    "summary": "Recently, graph-based and Transformer-based deep learning networks have\ndemonstrated excellent performances on various point cloud tasks. Most of the\nexisting graph methods are based on static graph, which take a fixed input to\nestablish graph relations. Moreover, many graph methods apply maximization and\naveraging to aggregate neighboring features, so that only a single neighboring\npoint affects the feature of centroid or different neighboring points have the\nsame influence on the centroid's feature, which ignoring the correlation and\ndifference between points. Most Transformer-based methods extract point cloud\nfeatures based on global attention and lack the feature learning on local\nneighbors. To solve the problems of these two types of models, we propose a new\nfeature extraction block named Graph Transformer and construct a 3D point point\ncloud learning network called GTNet to learn features of point clouds on local\nand global patterns. Graph Transformer integrates the advantages of graph-based\nand Transformer-based methods, and consists of Local Transformer and Global\nTransformer modules. Local Transformer uses a dynamic graph to calculate all\nneighboring point weights by intra-domain cross-attention with dynamically\nupdated graph relations, so that every neighboring point could affect the\nfeatures of centroid with different weights; Global Transformer enlarges the\nreceptive field of Local Transformer by a global self-attention. In addition,\nto avoid the disappearance of the gradient caused by the increasing depth of\nnetwork, we conduct residual connection for centroid features in GTNet; we also\nadopt the features of centroid and neighbors to generate the local geometric\ndescriptors in Local Transformer to strengthen the local information learning\ncapability of the model. Finally, we use GTNet for shape classification, part\nsegmentation and semantic segmentation tasks in this paper.",
    "published": "2023-05-24T14:51:18Z",
    "updated": "2024-08-05T06:40:52Z",
    "authors": [
      "Wei Zhou",
      "Qian Wang",
      "Weiwei Jin",
      "Xinzhe Shi",
      "Ying He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.08687v2",
    "title": "Long-term series forecasting with Query Selector -- efficient model of\n  sparse attention",
    "summary": "Various modifications of TRANSFORMER were recently used to solve time-series\nforecasting problem. We propose Query Selector - an efficient, deterministic\nalgorithm for sparse attention matrix. Experiments show it achieves\nstate-of-the art results on ETT, Helpdesk and BPI'12 datasets.",
    "published": "2021-07-19T08:46:22Z",
    "updated": "2021-08-17T11:03:15Z",
    "authors": [
      "Jacek Klimek",
      "Jakub Klimek",
      "Witold Kraskiewicz",
      "Mateusz Topolewski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17808v1",
    "title": "An Attention Infused Deep Learning System with Grad-CAM Visualization\n  for Early Screening of Glaucoma",
    "summary": "This research work reveals the eye opening wisdom of the hybrid labyrinthine\ndeep learning models synergy born out of combining a trailblazing convolutional\nneural network with a disruptive Vision Transformer, both intertwined together\nwith a radical Cross Attention module. Here, two high yielding datasets for\nartificial intelligence models in detecting glaucoma, namely ACRIMA and\nDrishti, are utilized.",
    "published": "2025-05-23T12:25:13Z",
    "updated": "2025-05-23T12:25:13Z",
    "authors": [
      "Ramanathan Swaminathan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.08457v2",
    "title": "Scratching Visual Transformer's Back with Uniform Attention",
    "summary": "The favorable performance of Vision Transformers (ViTs) is often attributed\nto the multi-head self-attention (MSA). The MSA enables global interactions at\neach layer of a ViT model, which is a contrasting feature against Convolutional\nNeural Networks (CNNs) that gradually increase the range of interaction across\nmultiple layers. We study the role of the density of the attention. Our\npreliminary analyses suggest that the spatial interactions of attention maps\nare close to dense interactions rather than sparse ones. This is a curious\nphenomenon, as dense attention maps are harder for the model to learn due to\nsteeper softmax gradients around them. We interpret this as a strong preference\nfor ViT models to include dense interaction. We thus manually insert the\nuniform attention to each layer of ViT models to supply the much needed dense\ninteractions. We call this method Context Broadcasting, CB. We observe that the\ninclusion of CB reduces the degree of density in the original attention maps\nand increases both the capacity and generalizability of the ViT models. CB\nincurs negligible costs: 1 line in your model code, no additional parameters,\nand minimal extra operations.",
    "published": "2022-10-16T06:14:27Z",
    "updated": "2024-12-26T02:16:35Z",
    "authors": [
      "Nam Hyeon-Woo",
      "Kim Yu-Ji",
      "Byeongho Heo",
      "Dongyoon Han",
      "Seong Joon Oh",
      "Tae-Hyun Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1708.01015v1",
    "title": "Sensor Transformation Attention Networks",
    "summary": "Recent work on encoder-decoder models for sequence-to-sequence mapping has\nshown that integrating both temporal and spatial attention mechanisms into\nneural networks increases the performance of the system substantially. In this\nwork, we report on the application of an attentional signal not on temporal and\nspatial regions of the input, but instead as a method of switching among inputs\nthemselves. We evaluate the particular role of attentional switching in the\npresence of dynamic noise in the sensors, and demonstrate how the attentional\nsignal responds dynamically to changing noise levels in the environment to\nachieve increased performance on both audio and visual tasks in three\ncommonly-used datasets: TIDIGITS, Wall Street Journal, and GRID. Moreover, the\nproposed sensor transformation network architecture naturally introduces a\nnumber of advantages that merit exploration, including ease of adding new\nsensors to existing architectures, attentional interpretability, and increased\nrobustness in a variety of noisy environments not seen during training.\nFinally, we demonstrate that the sensor selection attention mechanism of a\nmodel trained only on the small TIDIGITS dataset can be transferred directly to\na pre-existing larger network trained on the Wall Street Journal dataset,\nmaintaining functionality of switching between sensors to yield a dramatic\nreduction of error in the presence of noise.",
    "published": "2017-08-03T06:35:36Z",
    "updated": "2017-08-03T06:35:36Z",
    "authors": [
      "Stefan Braun",
      "Daniel Neil",
      "Enea Ceolini",
      "Jithendar Anumula",
      "Shih-Chii Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.13120v3",
    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
    "summary": "Transformer achieves promising results on various tasks. However,\nself-attention suffers from quadratic memory requirements with respect to the\nsequence length. Existing work focuses on reducing time and space complexity\nfrom an algorithm perspective. In this work, we propose sequence parallelism, a\nmemory-efficient parallelism method to help us break input sequence length\nlimitation and train with longer sequences on GPUs efficiently. Our approach is\ncompatible with most existing parallelisms (e.g. data parallelism, pipeline\nparallelism and tensor parallelism), which means our sequence parallelism makes\n4D parallelism possible. More importantly, we no longer require a single device\nto hold the whole sequence. That is, with sparse attention, our sequence\nparallelism enables us to train transformer with infinite long sequence.\nSpecifically, we split the input sequence into multiple chunks and feed each\nchunk into its corresponding device (i.e. GPU). To compute the attention\noutput, we integrated ring-style communication with self-attention calculation\nand proposed Ring Self-Attention (RSA). Experiments show that sequence\nparallelism performs well when scaling with batch size and sequence length.\nCompared with tensor parallelism, our approach achieved $13.7\\times$ and\n$3.0\\times$ maximum batch size and sequence length respectively when scaling up\nto 64 NVIDIA P100 GPUs. With sparse attention, sequence can handle sequence\nwith over 114K tokens, which is over $27\\times$ longer than existing sparse\nattention works holding the whole sequence on a single device.",
    "published": "2021-05-26T13:40:58Z",
    "updated": "2022-05-21T06:03:54Z",
    "authors": [
      "Shenggui Li",
      "Fuzhao Xue",
      "Chaitanya Baranwal",
      "Yongbin Li",
      "Yang You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.02488v2",
    "title": "ABC: Attention with Bounded-memory Control",
    "summary": "Transformer architectures have achieved state-of-the-art results on a variety\nof sequence modeling tasks. However, their attention mechanism comes with a\nquadratic complexity in sequence lengths, making the computational overhead\nprohibitive, especially for long sequences. Attention context can be seen as a\nrandom-access memory with each token taking a slot. Under this perspective, the\nmemory size grows linearly with the sequence length, and so does the overhead\nof reading from it. One way to improve the efficiency is to bound the memory\nsize. We show that disparate approaches can be subsumed into one abstraction,\nattention with bounded-memory control (ABC), and they vary in their\norganization of the memory. ABC reveals new, unexplored possibilities. First,\nit connects several efficient attention variants that would otherwise seem\napart. Second, this abstraction gives new insights--an established approach\n(Wang et al., 2020b) previously thought to be not applicable in causal\nattention, actually is. Last, we present a new instance of ABC, which draws\ninspiration from existing ABC approaches, but replaces their heuristic\nmemory-organizing functions with a learned, contextualized one. Our experiments\non language modeling, machine translation, and masked language model finetuning\nshow that our approach outperforms previous efficient attention models;\ncompared to the strong transformer baselines, it significantly improves the\ninference time and space efficiency with no or negligible accuracy loss.",
    "published": "2021-10-06T03:53:25Z",
    "updated": "2022-06-01T23:58:25Z",
    "authors": [
      "Hao Peng",
      "Jungo Kasai",
      "Nikolaos Pappas",
      "Dani Yogatama",
      "Zhaofeng Wu",
      "Lingpeng Kong",
      "Roy Schwartz",
      "Noah A. Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.04559v1",
    "title": "A Song of (Dis)agreement: Evaluating the Evaluation of Explainable\n  Artificial Intelligence in Natural Language Processing",
    "summary": "There has been significant debate in the NLP community about whether or not\nattention weights can be used as an explanation - a mechanism for interpreting\nhow important each input token is for a particular prediction. The validity of\n\"attention as explanation\" has so far been evaluated by computing the rank\ncorrelation between attention-based explanations and existing feature\nattribution explanations using LSTM-based models. In our work, we (i) compare\nthe rank correlation between five more recent feature attribution methods and\ntwo attention-based methods, on two types of NLP tasks, and (ii) extend this\nanalysis to also include transformer-based models. We find that attention-based\nexplanations do not correlate strongly with any recent feature attribution\nmethods, regardless of the model or task. Furthermore, we find that none of the\ntested explanations correlate strongly with one another for the\ntransformer-based model, leading us to question the underlying assumption that\nwe should measure the validity of attention-based explanations based on how\nwell they correlate with existing feature attribution explanation methods.\nAfter conducting experiments on five datasets using two different models, we\nargue that the community should stop using rank correlation as an evaluation\nmetric for attention-based explanations. We suggest that researchers and\npractitioners should instead test various explanation methods and employ a\nhuman-in-the-loop process to determine if the explanations align with human\nintuition for the particular use case at hand.",
    "published": "2022-05-09T21:07:39Z",
    "updated": "2022-05-09T21:07:39Z",
    "authors": [
      "Michael Neely",
      "Stefan F. Schouten",
      "Maurits Bleeker",
      "Ana Lucic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.12831v2",
    "title": "EFormer: Enhanced Transformer towards Semantic-Contour Features of\n  Foreground for Portraits Matting",
    "summary": "The portrait matting task aims to extract an alpha matte with complete\nsemantics and finely-detailed contours. In comparison to CNN-based approaches,\ntransformers with self-attention module have a better capacity to capture\nlong-range dependencies and low-frequency semantic information of a portrait.\nHowever, the recent research shows that self-attention mechanism struggles with\nmodeling high-frequency contour information and capturing fine contour details,\nwhich can lead to bias while predicting the portrait's contours. To deal with\nthis issue, we propose EFormer to enhance the model's attention towards both of\nthe low-frequency semantic and high-frequency contour features. For the\nhigh-frequency contours, our research demonstrates that cross-attention module\nbetween different resolutions can guide our model to allocate attention\nappropriately to these contour regions. Supported on this, we can successfully\nextract the high-frequency detail information around the portrait's contours,\nwhich are previously ignored by self-attention. Based on cross-attention\nmodule, we further build a semantic and contour detector (SCD) to accurately\ncapture both of the low-frequency semantic and high-frequency contour features.\nAnd we design contour-edge extraction branch and semantic extraction branch to\nextract refined high-frequency contour features and complete low-frequency\nsemantic information, respectively. Finally, we fuse the two kinds of features\nand leverage segmentation head to generate a predicted portrait matte.\nExperiments on VideoMatte240K (JPEG SD Format) and Adobe Image Matting (AIM)\ndatasets demonstrate that EFormer outperforms previous portrait matte methods.",
    "published": "2023-08-24T14:45:03Z",
    "updated": "2023-11-30T08:59:34Z",
    "authors": [
      "Zitao Wang",
      "Qiguang Miao",
      "Peipei Zhao",
      "Yue Xi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.00121v1",
    "title": "Beyond Self-Attention: Deformable Large Kernel Attention for Medical\n  Image Segmentation",
    "summary": "Medical image segmentation has seen significant improvements with transformer\nmodels, which excel in grasping far-reaching contexts and global contextual\ninformation. However, the increasing computational demands of these models,\nproportional to the squared token count, limit their depth and resolution\ncapabilities. Most current methods process D volumetric image data\nslice-by-slice (called pseudo 3D), missing crucial inter-slice information and\nthus reducing the model's overall performance. To address these challenges, we\nintroduce the concept of \\textbf{Deformable Large Kernel Attention (D-LKA\nAttention)}, a streamlined attention mechanism employing large convolution\nkernels to fully appreciate volumetric context. This mechanism operates within\na receptive field akin to self-attention while sidestepping the computational\noverhead. Additionally, our proposed attention mechanism benefits from\ndeformable convolutions to flexibly warp the sampling grid, enabling the model\nto adapt appropriately to diverse data patterns. We designed both 2D and 3D\nadaptations of the D-LKA Attention, with the latter excelling in cross-depth\ndata understanding. Together, these components shape our novel hierarchical\nVision Transformer architecture, the \\textit{D-LKA Net}. Evaluations of our\nmodel against leading methods on popular medical segmentation datasets\n(Synapse, NIH Pancreas, and Skin lesion) demonstrate its superior performance.\nOur code implementation is publicly available at the:\nhttps://github.com/mindflow-institue/deformableLKA",
    "published": "2023-08-31T20:21:12Z",
    "updated": "2023-08-31T20:21:12Z",
    "authors": [
      "Reza Azad",
      "Leon Niggemeier",
      "Michael Huttemann",
      "Amirhossein Kazerouni",
      "Ehsan Khodapanah Aghdam",
      "Yury Velichko",
      "Ulas Bagci",
      "Dorit Merhof"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.15157v1",
    "title": "Advancing Vision Transformers with Group-Mix Attention",
    "summary": "Vision Transformers (ViTs) have been shown to enhance visual recognition\nthrough modeling long-range dependencies with multi-head self-attention (MHSA),\nwhich is typically formulated as Query-Key-Value computation. However, the\nattention map generated from the Query and Key captures only token-to-token\ncorrelations at one single granularity. In this paper, we argue that\nself-attention should have a more comprehensive mechanism to capture\ncorrelations among tokens and groups (i.e., multiple adjacent tokens) for\nhigher representational capacity. Thereby, we propose Group-Mix Attention (GMA)\nas an advanced replacement for traditional self-attention, which can\nsimultaneously capture token-to-token, token-to-group, and group-to-group\ncorrelations with various group sizes. To this end, GMA splits the Query, Key,\nand Value into segments uniformly and performs different group aggregations to\ngenerate group proxies. The attention map is computed based on the mixtures of\ntokens and group proxies and used to re-combine the tokens and groups in Value.\nBased on GMA, we introduce a powerful backbone, namely GroupMixFormer, which\nachieves state-of-the-art performance in image classification, object\ndetection, and semantic segmentation with fewer parameters than existing\nmodels. For instance, GroupMixFormer-L (with 70.3M parameters and 384^2 input)\nattains 86.2% Top-1 accuracy on ImageNet-1K without external data, while\nGroupMixFormer-B (with 45.8M parameters) attains 51.2% mIoU on ADE20K.",
    "published": "2023-11-26T01:25:03Z",
    "updated": "2023-11-26T01:25:03Z",
    "authors": [
      "Chongjian Ge",
      "Xiaohan Ding",
      "Zhan Tong",
      "Li Yuan",
      "Jiangliu Wang",
      "Yibing Song",
      "Ping Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.16003v2",
    "title": "Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter\n  Estimation",
    "summary": "Dynamic parameterization of acoustic environments has drawn widespread\nattention in the field of audio processing. Precise representation of local\nroom acoustic characteristics is crucial when designing audio filters for\nvarious audio rendering applications. Key parameters in this context include\nreverberation time (RT60) and geometric room volume. In recent years, neural\nnetworks have been extensively applied in the task of blind room parameter\nestimation. However, there remains a question of whether pure attention\nmechanisms can achieve superior performance in this task. To address this\nissue, this study employs blind room parameter estimation based on monaural\nnoisy speech signals. Various model architectures are investigated, including a\nproposed attention-based model. This model is a convolution-free Audio\nSpectrogram Transformer, utilizing patch splitting, attention mechanisms, and\ncross-modality transfer learning from a pretrained Vision Transformer.\nExperimental results suggest that the proposed attention mechanism-based model,\nrelying purely on attention mechanisms without using convolution, exhibits\nsignificantly improved performance across various room parameter estimation\ntasks, especially with the help of dedicated pretraining and data augmentation\nschemes. Additionally, the model demonstrates more advantageous adaptability\nand robustness when handling variable-length audio inputs compared to existing\nmethods.",
    "published": "2024-02-25T06:32:21Z",
    "updated": "2024-04-25T14:43:57Z",
    "authors": [
      "Chunxi Wang",
      "Maoshen Jia",
      "Meiran Li",
      "Changchun Bao",
      "Wenyu Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.20021v4",
    "title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity",
    "summary": "Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we observe that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From this observation, we find that aligning\nattention maps of synthetic data helps improve the overall performance of\nquantized ViTs. Motivated by this finding, we devise MimiQ, a novel DFQ method\ndesigned for ViTs that enhances inter-head attention similarity. First, we\ngenerate synthetic data by aligning head-wise attention outputs from each\nspatial query patch. Then, we align the attention maps of the quantized network\nto those of the full-precision teacher by applying head-wise structural\nattention distillation. The experimental results show that the proposed method\nsignificantly outperforms baselines, setting a new state-of-the-art for\nViT-DFQ. This paper is an extended version of our work published in the\nproceedings of AAAI 2025, including additional supplementary material.",
    "published": "2024-07-29T13:57:40Z",
    "updated": "2025-04-14T07:42:52Z",
    "authors": [
      "Kanghyun Choi",
      "Hye Yoon Lee",
      "Dain Kwon",
      "SunJong Park",
      "Kyuyeun Kim",
      "Noseong Park",
      "Jonghyun Choi",
      "Jinho Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.16684v1",
    "title": "PartFormer: Awakening Latent Diverse Representation from Vision\n  Transformer for Object Re-Identification",
    "summary": "Extracting robust feature representation is critical for object\nre-identification to accurately identify objects across non-overlapping\ncameras. Although having a strong representation ability, the Vision\nTransformer (ViT) tends to overfit on most distinct regions of training data,\nlimiting its generalizability and attention to holistic object features.\nMeanwhile, due to the structural difference between CNN and ViT, fine-grained\nstrategies that effectively address this issue in CNN do not continue to be\nsuccessful in ViT. To address this issue, by observing the latent diverse\nrepresentation hidden behind the multi-head attention, we present PartFormer,\nan innovative adaptation of ViT designed to overcome the granularity\nlimitations in object Re-ID tasks. The PartFormer integrates a Head\nDisentangling Block (HDB) that awakens the diverse representation of multi-head\nself-attention without the typical loss of feature richness induced by\nconcatenation and FFN layers post-attention. To avoid the homogenization of\nattention heads and promote robust part-based feature learning, two head\ndiversity constraints are imposed: attention diversity constraint and\ncorrelation diversity constraint. These constraints enable the model to exploit\ndiverse and discriminative feature representations from different attention\nheads. Comprehensive experiments on various object Re-ID benchmarks demonstrate\nthe superiority of the PartFormer. Specifically, our framework significantly\noutperforms state-of-the-art by 2.4\\% mAP scores on the most challenging MSMT17\ndataset.",
    "published": "2024-08-29T16:31:05Z",
    "updated": "2024-08-29T16:31:05Z",
    "authors": [
      "Lei Tan",
      "Pingyang Dai",
      "Jie Chen",
      "Liujuan Cao",
      "Yongjian Wu",
      "Rongrong Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.01651v4",
    "title": "Efficient Length-Generalizable Attention via Causal Retrieval for\n  Long-Context Language Modeling",
    "summary": "Despite the success of Transformers, handling long contexts remains\nchallenging due to the limited length generalization and quadratic complexity\nof self-attention. Thus Transformers often require post-training with a larger\nattention window, significantly increasing computational and memory costs. In\nthis paper, we propose a novel attention mechanism based on dynamic context,\nGrouped Cross Attention (GCA), which can generalize to 1000 times the\npre-training context length while maintaining the ability to access distant\ninformation with a constant attention window size. For a given input sequence,\nwe split it into chunks and use each chunk to retrieve top-k relevant past\nchunks for subsequent text generation. Specifically, unlike most previous works\nthat use an off-the-shelf retriever, our key innovation allows the retriever to\nlearn how to retrieve past chunks that better minimize the auto-regressive loss\nof subsequent tokens in an end-to-end manner. Such a mechanism accommodates\nretrieved chunks with a fixed-size attention window to achieve long-range\ninformation access, significantly reducing computational and memory costs\nduring training and inference. Experiments show that GCA-based models achieve\nnear-perfect accuracy in passkey retrieval for 16M context lengths, which is\n1000 times the training length.",
    "published": "2024-10-02T15:18:34Z",
    "updated": "2025-06-12T02:49:51Z",
    "authors": [
      "Xiang Hu",
      "Zhihao Teng",
      "Jun Zhao",
      "Wei Wu",
      "Kewei Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06155v2",
    "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention\n  Tile",
    "summary": "Despite the promise of synthesizing high-fidelity videos, Diffusion\nTransformers (DiTs) with 3D full attention suffer from expensive inference due\nto the complexity of attention computation and numerous sampling steps. For\nexample, the popular Open-Sora-Plan model consumes more than 9 minutes for\ngenerating a single video of 29 frames. This paper addresses the inefficiency\nissue from two aspects: 1) Prune the 3D full attention based on the redundancy\nwithin video data; We identify a prevalent tile-style repetitive pattern in the\n3D attention maps for video data, and advocate a new family of sparse 3D\nattention that holds a linear complexity w.r.t. the number of video frames. 2)\nShorten the sampling process by adopting existing multi-step consistency\ndistillation; We split the entire sampling trajectory into several segments and\nperform consistency distillation within each one to activate few-step\ngeneration capacities. We further devise a three-stage training pipeline to\nconjoin the low-complexity attention and few-step generation capacities.\nNotably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into\nan efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video\ngeneration with a marginal performance trade-off in VBench. In addition, we\ndemonstrate that our approach is amenable to distributed inference, achieving\nan additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
    "published": "2025-02-10T05:00:56Z",
    "updated": "2025-02-17T07:08:23Z",
    "authors": [
      "Hangliang Ding",
      "Dacheng Li",
      "Runlong Su",
      "Peiyuan Zhang",
      "Zhijie Deng",
      "Ion Stoica",
      "Hao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.12734v2",
    "title": "In-Context Linear Regression Demystified: Training Dynamics and\n  Mechanistic Interpretability of Multi-Head Softmax Attention",
    "summary": "We study how multi-head softmax attention models are trained to perform\nin-context learning on linear data. Through extensive empirical experiments and\nrigorous theoretical analysis, we demystify the emergence of elegant attention\npatterns: a diagonal and homogeneous pattern in the key-query (KQ) weights, and\na last-entry-only and zero-sum pattern in the output-value (OV) weights.\nRemarkably, these patterns consistently appear from gradient-based training\nstarting from random initialization. Our analysis reveals that such emergent\nstructures enable multi-head attention to approximately implement a debiased\ngradient descent predictor -- one that outperforms single-head attention and\nnearly achieves Bayesian optimality up to proportional factor. Furthermore,\ncompared to linear transformers, the softmax attention readily generalizes to\nsequences longer than those seen during training. We also extend our study to\nscenarios with anisotropic covariates and multi-task linear regression. In the\nformer, multi-head attention learns to implement a form of pre-conditioned\ngradient descent. In the latter, we uncover an intriguing regime where the\ninterplay between head number and task number triggers a superposition\nphenomenon that efficiently resolves multi-task in-context learning. Our\nresults reveal that in-context learning ability emerges from the trained\ntransformer as an aggregated effect of its architecture and the underlying data\ndistribution, paving the way for deeper understanding and broader applications\nof in-context learning.",
    "published": "2025-03-17T02:00:49Z",
    "updated": "2025-05-27T18:50:18Z",
    "authors": [
      "Jianliang He",
      "Xintian Pan",
      "Siyu Chen",
      "Zhuoran Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.08166v1",
    "title": "Learning Object Focused Attention",
    "summary": "We propose an adaptation to the training of Vision Transformers (ViTs) that\nallows for an explicit modeling of objects during the attention computation.\nThis is achieved by adding a new branch to selected attention layers that\ncomputes an auxiliary loss which we call the object-focused attention (OFA)\nloss. We restrict the attention to image patches that belong to the same object\nclass, which allows ViTs to gain a better understanding of configural (or\nholistic) object shapes by focusing on intra-object patches instead of other\npatches such as those in the background. Our proposed inductive bias fits\neasily into the attention framework of transformers since it only adds an\nauxiliary loss over selected attention layers. Furthermore, our approach has no\nadditional overhead during inference. We also experiment with multiscale\nmasking to further improve the performance of our OFA model and give a path\nforward for self-supervised learning with our method. Our experimental results\ndemonstrate that ViTs with OFA achieve better classification results than their\nbase models, exhibit a stronger generalization ability to out-of-distribution\n(OOD) and adversarially corrupted images, and learn representations based on\nobject shapes rather than spurious correlations via general textures. For our\nOOD setting, we generate a novel dataset using the COCO dataset and Stable\nDiffusion inpainting which we plan to share with the community.",
    "published": "2025-04-10T23:23:26Z",
    "updated": "2025-04-10T23:23:26Z",
    "authors": [
      "Vivek Trivedy",
      "Amani Almalki",
      "Longin Jan Latecki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18698v2",
    "title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware\n  Structured Attention",
    "summary": "Transformers have achieved state-of-the-art performance across various tasks,\nbut suffer from a notable quadratic complexity in sequence length due to the\nattention mechanism. In this work, we propose MonarchAttention -- a novel\napproach to sub-quadratic attention approximation via Monarch matrices, an\nexpressive class of structured matrices. Based on the variational form of\nsoftmax, we describe an efficient optimization-based algorithm to compute an\napproximate projection of softmax attention onto the class of Monarch matrices\nwith $\\Theta(N\\sqrt{N} d)$ computational complexity and $\\Theta(Nd)$ memory/IO\ncomplexity. Unlike previous approaches, MonarchAttention is both (1)\ntransferable, yielding minimal performance loss with no additional training,\neven when replacing every attention layer of the Transformer, and (2)\nhardware-efficient, utilizing the highest-throughput tensor core units on\nmodern GPUs. With optimized kernels, MonarchAttention achieves substantial\nspeed-ups in wall-time over FlashAttention-2: $1.4\\times$ for shorter sequences\n$(N=256)$, $4.5\\times$ for medium-length sequences $(N=4K)$, and $8.2\\times$\nfor longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention\non diverse tasks and architectures in vision and language problems, showing\nthat it flexibly and accurately approximates softmax attention in a variety of\ncontexts. Our code is available at\nhttps://github.com/cjyaras/monarch-attention.",
    "published": "2025-05-24T13:44:44Z",
    "updated": "2025-10-25T17:57:42Z",
    "authors": [
      "Can Yaras",
      "Alec S. Xu",
      "Pierre Abillama",
      "Changwoo Lee",
      "Laura Balzano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15881v2",
    "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
    "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
    "published": "2025-08-21T15:25:40Z",
    "updated": "2025-08-25T02:24:20Z",
    "authors": [
      "Xiaojuan Tang",
      "Fanxu Meng",
      "Pingzhi Tang",
      "Yuxuan Wang",
      "Di Yin",
      "Xing Sun",
      "Muhan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12941v1",
    "title": "Computationally Efficient Neural Receivers via Axial Self-Attention",
    "summary": "Deep learning-based neural receivers are redefining physical-layer signal\nprocessing for next-generation wireless systems. We propose an axial\nself-attention transformer neural receiver designed for applicability to 6G and\nbeyond wireless systems, validated through 5G-compliant experimental\nconfigurations, that achieves state-of-the-art block error rate (BLER)\nperformance with significantly improved computational efficiency. By\nfactorizing attention operations along temporal and spectral axes, the proposed\narchitecture reduces the quadratic complexity of conventional multi-head\nself-attention from $O((TF)^2)$ to $O(T^2F+TF^2)$, yielding substantially fewer\ntotal floating-point operations and attention matrix multiplications per\ntransformer block compared to global self-attention. Relative to convolutional\nneural receiver baselines, the axial neural receiver achieves significantly\nlower computational cost with a fraction of the parameters. Experimental\nvalidation under 3GPP Clustered Delay Line (CDL) channels demonstrates\nconsistent performance gains across varying mobility scenarios. Under\nnon-line-of-sight CDL-C conditions, the axial neural receiver consistently\noutperforms all evaluated receiver architectures, including global\nself-attention, convolutional neural receivers, and traditional LS-LMMSE at\n10\\% BLER with reduced computational complexity per inference. At stringent\nreliability targets of 1\\% BLER, the axial receiver maintains robust symbol\ndetection at high user speeds, whereas the traditional LS-LMMSE receiver fails\nto converge, underscoring its suitability for ultra-reliable low-latency\n(URLLC) communication in dynamic 6G environments and beyond. These results\nestablish the axial neural receiver as a structured, scalable, and efficient\nframework for AI-Native 6G RAN systems, enabling deployment in\nresource-constrained edge environments.",
    "published": "2025-10-14T19:39:24Z",
    "updated": "2025-10-14T19:39:24Z",
    "authors": [
      "SaiKrishna Saketh Yellapragada",
      "Atchutaram K. Kocharlakota",
      "MÃ¡rio Costa",
      "Esa Ollila",
      "Sergiy A. Vorobyov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.06338v2",
    "title": "Transformer based Grapheme-to-Phoneme Conversion",
    "summary": "Attention mechanism is one of the most successful techniques in deep learning\nbased Natural Language Processing (NLP). The transformer network architecture\nis completely based on attention mechanisms, and it outperforms\nsequence-to-sequence models in neural machine translation without recurrent and\nconvolutional layers. Grapheme-to-phoneme (G2P) conversion is a task of\nconverting letters (grapheme sequence) to their pronunciations (phoneme\nsequence). It plays a significant role in text-to-speech (TTS) and automatic\nspeech recognition (ASR) systems. In this paper, we investigate the application\nof transformer architecture to G2P conversion and compare its performance with\nrecurrent and convolutional neural network based approaches. Phoneme and word\nerror rates are evaluated on the CMUDict dataset for US English and the NetTalk\ndataset. The results show that transformer based G2P outperforms the\nconvolutional-based approach in terms of word error rate and our results\nsignificantly exceeded previous recurrent approaches (without attention)\nregarding word and phoneme error rates on both datasets. Furthermore, the size\nof the proposed model is much smaller than the size of the previous approaches.",
    "published": "2020-04-14T07:48:15Z",
    "updated": "2020-06-26T21:09:53Z",
    "authors": [
      "Sevinj Yolchuyeva",
      "GÃ©za NÃ©meth",
      "BÃ¡lint Gyires-TÃ³th"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.04312v1",
    "title": "MATE: Multi-view Attention for Table Transformer Efficiency",
    "summary": "This work presents a sparse-attention Transformer architecture for modeling\ndocuments that contain large tables. Tables are ubiquitous on the web, and are\nrich in information. However, more than 20% of relational tables on the web\nhave 20 or more rows (Cafarella et al., 2008), and these large tables present a\nchallenge for current Transformer models, which are typically limited to 512\ntokens. Here we propose MATE, a novel Transformer architecture designed to\nmodel the structure of web tables. MATE uses sparse attention in a way that\nallows heads to efficiently attend to either rows or columns in a table. This\narchitecture scales linearly with respect to speed and memory, and can handle\ndocuments containing more than 8000 tokens with current accelerators. MATE also\nhas a more appropriate inductive bias for tabular data, and sets a new\nstate-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,\n2020b), a dataset that involves large documents containing tables, we improve\nthe best prior result by 19 points.",
    "published": "2021-09-09T14:39:30Z",
    "updated": "2021-09-09T14:39:30Z",
    "authors": [
      "Julian Martin Eisenschlos",
      "Maharshi Gor",
      "Thomas MÃ¼ller",
      "William W. Cohen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.03479v1",
    "title": "Delta Keyword Transformer: Bringing Transformers to the Edge through\n  Dynamically Pruned Multi-Head Self-Attention",
    "summary": "Multi-head self-attention forms the core of Transformer networks. However,\ntheir quadratically growing complexity with respect to the input sequence\nlength impedes their deployment on resource-constrained edge devices. We\naddress this challenge by proposing a dynamic pruning method, which exploits\nthe temporal stability of data across tokens to reduce inference cost. The\nthreshold-based method only retains significant differences between the\nsubsequent tokens, effectively reducing the number of multiply-accumulates, as\nwell as the internal tensor data sizes. The approach is evaluated on the Google\nSpeech Commands Dataset for keyword spotting, and the performance is compared\nagainst the baseline Keyword Transformer. Our experiments show that we can\nreduce ~80% of operations while maintaining the original 98.4% accuracy.\nMoreover, a reduction of ~87-94% operations can be achieved when only degrading\nthe accuracy by 1-4%, speeding up the multi-head self-attention inference by a\nfactor of ~7.5-16.",
    "published": "2022-03-20T20:59:13Z",
    "updated": "2022-03-20T20:59:13Z",
    "authors": [
      "Zuzana JelÄicovÃ¡",
      "Marian Verhelst"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.15769v1",
    "title": "Fully-attentive and interpretable: vision and video vision transformers\n  for pain detection",
    "summary": "Pain is a serious and costly issue globally, but to be treated, it must first\nbe detected. Vision transformers are a top-performing architecture in computer\nvision, with little research on their use for pain detection. In this paper, we\npropose the first fully-attentive automated pain detection pipeline that\nachieves state-of-the-art performance on binary pain detection from facial\nexpressions. The model is trained on the UNBC-McMaster dataset, after faces are\n3D-registered and rotated to the canonical frontal view. In our experiments we\nidentify important areas of the hyperparameter space and their interaction with\nvision and video vision transformers, obtaining 3 noteworthy models. We analyse\nthe attention maps of one of our models, finding reasonable interpretations for\nits predictions. We also evaluate Mixup, an augmentation technique, and\nSharpness-Aware Minimization, an optimizer, with no success. Our presented\nmodels, ViT-1 (F1 score 0.55 +- 0.15), ViViT-1 (F1 score 0.55 +- 0.13), and\nViViT-2 (F1 score 0.49 +- 0.04), all outperform earlier works, showing the\npotential of vision transformers for pain detection. Code is available at\nhttps://github.com/IPDTFE/ViT-McMaster",
    "published": "2022-10-27T21:01:40Z",
    "updated": "2022-10-27T21:01:40Z",
    "authors": [
      "Giacomo Fiorentini",
      "Itir Onal Ertugrul",
      "Albert Ali Salah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.16643v2",
    "title": "XNOR-FORMER: Learning Accurate Approximations in Long Speech\n  Transformers",
    "summary": "Transformers are among the state of the art for many tasks in speech, vision,\nand natural language processing, among others. Self-attentions, which are\ncrucial contributors to this performance have quadratic computational\ncomplexity, which makes training on longer input sequences challenging. Prior\nwork has produced state-of-the-art transformer variants with linear attention,\nhowever, current models sacrifice performance to achieve efficient\nimplementations. In this work, we develop a novel linear transformer by\nexamining the properties of the key-query product within self-attentions. Our\nmodel outperforms state of the art approaches on speech recognition and speech\nsummarization, resulting in 1 % absolute WER improvement on the Librispeech-100\nspeech recognition benchmark and a new INTERVIEW speech recognition benchmark,\nand 5 points on ROUGE for summarization with How2.",
    "published": "2022-10-29T16:21:30Z",
    "updated": "2022-12-19T20:40:58Z",
    "authors": [
      "Roshan Sharma",
      "Bhiksha Raj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.00562v1",
    "title": "ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and\n  Bottom-Up and Top-Down Attention",
    "summary": "Textbook Question Answering is a complex task in the intersection of Machine\nComprehension and Visual Question Answering that requires reasoning with\nmultimodal information from text and diagrams. For the first time, this paper\ntaps on the potential of transformer language models and bottom-up and top-down\nattention to tackle the language and visual understanding challenges this task\nentails. Rather than training a language-visual transformer from scratch we\nrely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up\nand top-down attention to identify regions of interest corresponding to diagram\nconstituents and their relationships, improving the selection of relevant\nvisual information for each question and answer options. Our system ISAAQ\nreports unprecedented success in all TQA question types, with accuracies of\n81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice\nquestions. ISAAQ also demonstrates its broad applicability, obtaining\nstate-of-the-art results in other demanding datasets.",
    "published": "2020-10-01T17:28:47Z",
    "updated": "2020-10-01T17:28:47Z",
    "authors": [
      "Jose Manuel Gomez-Perez",
      "Raul Ortega"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.01252v1",
    "title": "A Novel Transformer Network with Shifted Window Cross-Attention for\n  Spatiotemporal Weather Forecasting",
    "summary": "Earth Observatory is a growing research area that can capitalize on the\npowers of AI for short time forecasting, a Now-casting scenario. In this work,\nwe tackle the challenge of weather forecasting using a video transformer\nnetwork. Vision transformer architectures have been explored in various\napplications, with major constraints being the computational complexity of\nAttention and the data hungry training. To address these issues, we propose the\nuse of Video Swin-Transformer, coupled with a dedicated augmentation scheme.\nMoreover, we employ gradual spatial reduction on the encoder side and\ncross-attention on the decoder. The proposed approach is tested on the\nWeather4Cast2021 weather forecasting challenge data, which requires the\nprediction of 8 hours ahead future frames (4 per hour) from an hourly weather\nproduct sequence. The dataset was normalized to 0-1 to facilitate using the\nevaluation metrics across different datasets. The model results in an MSE score\nof 0.4750 when provided with training data, and 0.4420 during transfer learning\nwithout using training data, respectively.",
    "published": "2022-08-02T05:04:53Z",
    "updated": "2022-08-02T05:04:53Z",
    "authors": [
      "Alabi Bojesomo",
      "Hasan Al Marzouqi",
      "Panos Liatsis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.09567v1",
    "title": "Multiple Instance Neuroimage Transformer",
    "summary": "For the first time, we propose using a multiple instance learning based\nconvolution-free transformer model, called Multiple Instance Neuroimage\nTransformer (MINiT), for the classification of T1weighted (T1w) MRIs. We first\npresent several variants of transformer models adopted for neuroimages. These\nmodels extract non-overlapping 3D blocks from the input volume and perform\nmulti-headed self-attention on a sequence of their linear projections. MINiT,\non the other hand, treats each of the non-overlapping 3D blocks of the input\nMRI as its own instance, splitting it further into non-overlapping 3D patches,\non which multi-headed self-attention is computed. As a proof-of-concept, we\nevaluate the efficacy of our model by training it to identify sex from T1w-MRIs\nof two public datasets: Adolescent Brain Cognitive Development (ABCD) and the\nNational Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA).\nThe learned attention maps highlight voxels contributing to identifying sex\ndifferences in brain morphometry. The code is available at\nhttps://github.com/singlaayush/MINIT.",
    "published": "2022-08-19T23:42:06Z",
    "updated": "2022-08-19T23:42:06Z",
    "authors": [
      "Ayush Singla",
      "Qingyu Zhao",
      "Daniel K. Do",
      "Yuyin Zhou",
      "Kilian M. Pohl",
      "Ehsan Adeli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.17803v5",
    "title": "Rethinking Local Perception in Lightweight Vision Transformer",
    "summary": "Vision Transformers (ViTs) have been shown to be effective in various vision\ntasks. However, resizing them to a mobile-friendly size leads to significant\nperformance degradation. Therefore, developing lightweight vision transformers\nhas become a crucial area of research. This paper introduces CloFormer, a\nlightweight vision transformer that leverages context-aware local enhancement.\nCloFormer explores the relationship between globally shared weights often used\nin vanilla convolutional operators and token-specific context-aware weights\nappearing in attention, then proposes an effective and straightforward module\nto capture high-frequency local information. In CloFormer, we introduce\nAttnConv, a convolution operator in attention's style. The proposed AttnConv\nuses shared weights to aggregate local information and deploys carefully\ndesigned context-aware weights to enhance local features. The combination of\nthe AttnConv and vanilla attention which uses pooling to reduce FLOPs in\nCloFormer enables the model to perceive high-frequency and low-frequency\ninformation. Extensive experiments were conducted in image classification,\nobject detection, and semantic segmentation, demonstrating the superiority of\nCloFormer. The code is available at \\url{https://github.com/qhfan/CloFormer}.",
    "published": "2023-03-31T05:25:32Z",
    "updated": "2023-06-01T07:42:15Z",
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Jiyang Guan",
      "Ran He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1910.07204v1",
    "title": "Transformer ASR with Contextual Block Processing",
    "summary": "The Transformer self-attention network has recently shown promising\nperformance as an alternative to recurrent neural networks (RNNs) in end-to-end\n(E2E) automatic speech recognition (ASR) systems. However, the Transformer has\na drawback in that the entire input sequence is required to compute\nself-attention. In this paper, we propose a new block processing method for the\nTransformer encoder by introducing a context-aware inheritance mechanism. An\nadditional context embedding vector handed over from the previously processed\nblock helps to encode not only local acoustic information but also global\nlinguistic, channel, and speaker attributes. We introduce a novel mask\ntechnique to implement the context inheritance to train the model efficiently.\nEvaluations of the Wall Street Journal (WSJ), Librispeech, VoxForge Italian,\nand AISHELL-1 Mandarin speech recognition datasets show that our proposed\ncontextual block processing method outperforms naive block processing\nconsistently. Furthermore, the attention weight tendency of each layer is\nanalyzed to clarify how the added contextual inheritance mechanism models the\nglobal information.",
    "published": "2019-10-16T08:04:07Z",
    "updated": "2019-10-16T08:04:07Z",
    "authors": [
      "Emiru Tsunoo",
      "Yosuke Kashiwagi",
      "Toshiyuki Kumakura",
      "Shinji Watanabe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1910.12977v1",
    "title": "Transformer-Transducer: End-to-End Speech Recognition with\n  Self-Attention",
    "summary": "We explore options to use Transformer networks in neural transducer for\nend-to-end speech recognition. Transformer networks use self-attention for\nsequence modeling and comes with advantages in parallel computation and\ncapturing contexts. We propose 1) using VGGNet with causal convolution to\nincorporate positional information and reduce frame rate for efficient\ninference 2) using truncated self-attention to enable streaming for Transformer\nand reduce computational complexity. All experiments are conducted on the\npublic LibriSpeech corpus. The proposed Transformer-Transducer outperforms\nneural transducer with LSTM/BLSTM networks and achieved word error rates of\n6.37 % on the test-clean set and 15.30 % on the test-other set, while remaining\nstreamable, compact with 45.7M parameters for the entire system, and\ncomputationally efficient with complexity of O(T), where T is input sequence\nlength.",
    "published": "2019-10-28T21:29:21Z",
    "updated": "2019-10-28T21:29:21Z",
    "authors": [
      "Ching-Feng Yeh",
      "Jay Mahadeokar",
      "Kaustubh Kalgaonkar",
      "Yongqiang Wang",
      "Duc Le",
      "Mahaveer Jain",
      "Kjell Schubert",
      "Christian Fuegen",
      "Michael L. Seltzer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.03274v1",
    "title": "GMAT: Global Memory Augmentation for Transformers",
    "summary": "Transformer-based models have become ubiquitous in natural language\nprocessing thanks to their large capacity, innate parallelism and high\nperformance. The contextualizing component of a Transformer block is the\n$\\textit{pairwise dot-product}$ attention that has a large $\\Omega(L^2)$ memory\nrequirement for length $L$ sequences, limiting its ability to process long\ndocuments. This has been the subject of substantial interest recently, where\nmultiple approximations were proposed to reduce the quadratic memory\nrequirement using sparse attention matrices. In this work, we propose to\naugment sparse Transformer blocks with a dense attention-based $\\textit{global\nmemory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the\nentire input sequence to each position. Our augmentation has a manageable\n$O(M\\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior\nsparse solutions. Moreover, global memory can also be used for sequence\ncompression, by representing a long input sequence with the memory\nrepresentations only. We empirically show that our method leads to substantial\nimprovement on a range of tasks, including (a) synthetic tasks that require\nglobal reasoning, (b) masked language modeling, and (c) reading comprehension.",
    "published": "2020-06-05T07:50:40Z",
    "updated": "2020-06-05T07:50:40Z",
    "authors": [
      "Ankit Gupta",
      "Jonathan Berant"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2007.09163v1",
    "title": "Wavelet Channel Attention Module with a Fusion Network for Single Image\n  Deraining",
    "summary": "Single image deraining is a crucial problem because rain severely degenerates\nthe visibility of images and affects the performance of computer vision tasks\nlike outdoor surveillance systems and intelligent vehicles. In this paper, we\npropose the new convolutional neural network (CNN) called the wavelet channel\nattention module with a fusion network. Wavelet transform and the inverse\nwavelet transform are substituted for down-sampling and up-sampling so feature\nmaps from the wavelet transform and convolutions contain different frequencies\nand scales. Furthermore, feature maps are integrated by channel attention. Our\nproposed network learns confidence maps of four sub-band images derived from\nthe wavelet transform of the original images. Finally, the clear image can be\nwell restored via the wavelet reconstruction and fusion of the low-frequency\npart and high-frequency parts. Several experimental results on synthetic and\nreal images present that the proposed algorithm outperforms state-of-the-art\nmethods.",
    "published": "2020-07-17T18:06:13Z",
    "updated": "2020-07-17T18:06:13Z",
    "authors": [
      "Hao-Hsiang Yang",
      "Chao-Han Huck Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2009.04534v3",
    "title": "Pay Attention when Required",
    "summary": "Transformer-based models consist of interleaved feed-forward blocks - that\ncapture content meaning, and relatively more expensive self-attention blocks -\nthat capture context meaning. In this paper, we explored trade-offs and\nordering of the blocks to improve upon the current Transformer architecture and\nproposed PAR Transformer. It needs 35% lower compute time than Transformer-XL\nachieved by replacing ~63% of the self-attention blocks with feed-forward\nblocks, and retains the perplexity on WikiText-103 language modelling\nbenchmark. We further validated our results on text8 and enwiki8 datasets, as\nwell as on the BERT model.",
    "published": "2020-09-09T19:39:15Z",
    "updated": "2021-05-17T04:03:34Z",
    "authors": [
      "Swetha Mandava",
      "Szymon Migacz",
      "Alex Fit Florea"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.05163v1",
    "title": "Learning dynamic and hierarchical traffic spatiotemporal features with\n  Transformer",
    "summary": "Traffic forecasting is an indispensable part of Intelligent transportation\nsystems (ITS), and long-term network-wide accurate traffic speed forecasting is\none of the most challenging tasks. Recently, deep learning methods have become\npopular in this domain. As traffic data are physically associated with road\nnetworks, most proposed models treat it as a spatiotemporal graph modeling\nproblem and use Graph Convolution Network (GCN) based methods. These GCN-based\nmodels highly depend on a predefined and fixed adjacent matrix to reflect the\nspatial dependency. However, the predefined fixed adjacent matrix is limited in\nreflecting the actual dependence of traffic flow. This paper proposes a novel\nmodel, Traffic Transformer, for spatial-temporal graph modeling and long-term\ntraffic forecasting to overcome these limitations. Transformer is the most\npopular framework in Natural Language Processing (NLP). And by adapting it to\nthe spatiotemporal problem, Traffic Transformer hierarchically extracts\nspatiotemporal features through data dynamically by multi-head attention and\nmasked multi-head attention mechanism, and fuse these features for traffic\nforecasting. Furthermore, analyzing the attention weight matrixes can find the\ninfluential part of road networks, allowing us to learn the traffic networks\nbetter. Experimental results on the public traffic network datasets and\nreal-world traffic network datasets generated by ourselves demonstrate our\nproposed model achieves better performance than the state-of-the-art ones.",
    "published": "2021-04-12T02:29:58Z",
    "updated": "2021-04-12T02:29:58Z",
    "authors": [
      "Haoyang Yan",
      "Xiaolei Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.11746v2",
    "title": "VidTr: Video Transformer Without Convolutions",
    "summary": "We introduce Video Transformer (VidTr) with separable-attention for video\nclassification. Comparing with commonly used 3D networks, VidTr is able to\naggregate spatio-temporal information via stacked attentions and provide better\nperformance with higher efficiency. We first introduce the vanilla video\ntransformer and show that transformer module is able to perform spatio-temporal\nmodeling from raw pixels, but with heavy memory usage. We then present VidTr\nwhich reduces the memory cost by 3.3$\\times$ while keeping the same\nperformance. To further optimize the model, we propose the standard deviation\nbased topK pooling for attention ($pool_{topK\\_std}$), which reduces the\ncomputation by dropping non-informative features along temporal dimension.\nVidTr achieves state-of-the-art performance on five commonly used datasets with\nlower computational requirement, showing both the efficiency and effectiveness\nof our design. Finally, error analysis and visualization show that VidTr is\nespecially good at predicting actions that require long-term temporal\nreasoning.",
    "published": "2021-04-23T17:59:01Z",
    "updated": "2021-10-15T23:41:28Z",
    "authors": [
      "Yanyi Zhang",
      "Xinyu Li",
      "Chunhui Liu",
      "Bing Shuai",
      "Yi Zhu",
      "Biagio Brattoli",
      "Hao Chen",
      "Ivan Marsic",
      "Joseph Tighe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.12631v1",
    "title": "Head-synchronous Decoding for Transformer-based Streaming ASR",
    "summary": "Online Transformer-based automatic speech recognition (ASR) systems have been\nextensively studied due to the increasing demand for streaming applications.\nRecently proposed Decoder-end Adaptive Computation Steps (DACS) algorithm for\nonline Transformer ASR was shown to achieve state-of-the-art performance and\noutperform other existing methods. However, like any other online approach, the\nDACS-based attention heads in each of the Transformer decoder layers operate\nindependently (or asynchronously) and lead to diverged attending positions.\nSince DACS employs a truncation threshold to determine the halting position,\nsome of the attention weights are cut off untimely and might impact the\nstability and precision of decoding. To overcome these issues, here we propose\na head-synchronous (HS) version of the DACS algorithm, where the boundary of\nattention is jointly detected by all the DACS heads in each decoder layer. ASR\nexperiments on Wall Street Journal (WSJ), AIShell-1 and Librispeech show that\nthe proposed method consistently outperforms vanilla DACS and achieves\nstate-of-the-art performance. We will also demonstrate that HS-DACS has reduced\ndecoding cost when compared to vanilla DACS.",
    "published": "2021-04-26T14:57:57Z",
    "updated": "2021-04-26T14:57:57Z",
    "authors": [
      "Mohan Li",
      "Catalin Zorila",
      "Rama Doddipatla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.06195v1",
    "title": "MlTr: Multi-label Classification with Transformer",
    "summary": "The task of multi-label image classification is to recognize all the object\nlabels presented in an image. Though advancing for years, small objects,\nsimilar objects and objects with high conditional probability are still the\nmain bottlenecks of previous convolutional neural network(CNN) based models,\nlimited by convolutional kernels' representational capacity. Recent vision\ntransformer networks utilize the self-attention mechanism to extract the\nfeature of pixel granularity, which expresses richer local semantic\ninformation, while is insufficient for mining global spatial dependence. In\nthis paper, we point out the three crucial problems that CNN-based methods\nencounter and explore the possibility of conducting specific transformer\nmodules to settle them. We put forward a Multi-label Transformer\narchitecture(MlTr) constructed with windows partitioning, in-window pixel\nattention, cross-window attention, particularly improving the performance of\nmulti-label image classification tasks. The proposed MlTr shows\nstate-of-the-art results on various prevalent multi-label datasets such as\nMS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively.\nThe code will be available soon at https://github.com/starmemda/MlTr/",
    "published": "2021-06-11T06:53:09Z",
    "updated": "2021-06-11T06:53:09Z",
    "authors": [
      "Xing Cheng",
      "Hezheng Lin",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Nian Shi",
      "Honglin Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.14156v1",
    "title": "Post-Training Quantization for Vision Transformer",
    "summary": "Recently, transformer has achieved remarkable performance on a variety of\ncomputer vision applications. Compared with mainstream convolutional neural\nnetworks, vision transformers are often of sophisticated architectures for\nextracting powerful feature representations, which are more difficult to be\ndeveloped on mobile devices. In this paper, we present an effective\npost-training quantization algorithm for reducing the memory storage and\ncomputational costs of vision transformers. Basically, the quantization task\ncan be regarded as finding the optimal low-bit quantization intervals for\nweights and inputs, respectively. To preserve the functionality of the\nattention mechanism, we introduce a ranking loss into the conventional\nquantization objective that aims to keep the relative order of the\nself-attention results after quantization. Moreover, we thoroughly analyze the\nrelationship between quantization loss of different layers and the feature\ndiversity, and explore a mixed-precision quantization scheme by exploiting the\nnuclear norm of each attention map and output feature. The effectiveness of the\nproposed method is verified on several benchmark models and datasets, which\noutperforms the state-of-the-art post-training quantization algorithms. For\ninstance, we can obtain an 81.29\\% top-1 accuracy using DeiT-B model on\nImageNet dataset with about 8-bit quantization.",
    "published": "2021-06-27T06:27:22Z",
    "updated": "2021-06-27T06:27:22Z",
    "authors": [
      "Zhenhua Liu",
      "Yunhe Wang",
      "Kai Han",
      "Siwei Ma",
      "Wen Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.00645v2",
    "title": "Global Filter Networks for Image Classification",
    "summary": "Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet",
    "published": "2021-07-01T17:58:16Z",
    "updated": "2021-10-26T13:21:45Z",
    "authors": [
      "Yongming Rao",
      "Wenliang Zhao",
      "Zheng Zhu",
      "Jiwen Lu",
      "Jie Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.04152v1",
    "title": "Levi Graph AMR Parser using Heterogeneous Attention",
    "summary": "Coupled with biaffine decoders, transformers have been effectively adapted to\ntext-to-graph transduction and achieved state-of-the-art performance on AMR\nparsing. Many prior works, however, rely on the biaffine decoder for either or\nboth arc and label predictions although most features used by the decoder may\nbe learned by the transformer already. This paper presents a novel approach to\nAMR parsing by combining heterogeneous data (tokens, concepts, labels) as one\ninput to a transformer to learn attention, and use only attention matrices from\nthe transformer to predict all elements in AMR graphs (concepts, arcs, labels).\nAlthough our models use significantly fewer parameters than the previous\nstate-of-the-art graph parser, they show similar or better accuracy on AMR 2.0\nand 3.0.",
    "published": "2021-07-09T00:06:17Z",
    "updated": "2021-07-09T00:06:17Z",
    "authors": [
      "Han He",
      "Jinho D. Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.00335v1",
    "title": "Geometry Attention Transformer with Position-aware LSTMs for Image\n  Captioning",
    "summary": "In recent years, transformer structures have been widely applied in image\ncaptioning with impressive performance. For good captioning results, the\ngeometry and position relations of different visual objects are often thought\nof as crucial information. Aiming to further promote image captioning by\ntransformers, this paper proposes an improved Geometry Attention Transformer\n(GAT) model. In order to further leverage geometric information, two novel\ngeometry-aware architectures are designed respectively for the encoder and\ndecoder in our GAT. Besides, this model includes the two work modules: 1) a\ngeometry gate-controlled self-attention refiner, for explicitly incorporating\nrelative spatial information into image region representations in encoding\nsteps, and 2) a group of position-LSTMs, for precisely informing the decoder of\nrelative word position in generating caption texts. The experiment comparisons\non the datasets MS COCO and Flickr30K show that our GAT is efficient, and it\ncould often outperform current state-of-the-art image captioning models.",
    "published": "2021-10-01T11:57:50Z",
    "updated": "2021-10-01T11:57:50Z",
    "authors": [
      "Chi Wang",
      "Yulin Shen",
      "Luping Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.15225v3",
    "title": "Pruning Attention Heads of Transformer Models Using A* Search: A Novel\n  Approach to Compress Big NLP Architectures",
    "summary": "Recent years have seen a growing adoption of Transformer models such as BERT\nin Natural Language Processing and even in Computer Vision. However, due to\ntheir size, there has been limited adoption of such models within\nresource-constrained computing environments. This paper proposes novel pruning\nalgorithm to compress transformer models by eliminating redundant Attention\nHeads. We apply the A* search algorithm to obtain a pruned model with strict\naccuracy guarantees. Our results indicate that the method could eliminate as\nmuch as 40% of the attention heads in the BERT transformer model with no loss\nin accuracy.",
    "published": "2021-10-28T15:39:11Z",
    "updated": "2021-11-17T14:50:51Z",
    "authors": [
      "Archit Parnami",
      "Rahul Singh",
      "Tarun Joshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.01353v2",
    "title": "Can Vision Transformers Perform Convolution?",
    "summary": "Several recent studies have demonstrated that attention-based networks, such\nas Vision Transformer (ViT), can outperform Convolutional Neural Networks\n(CNNs) on several computer vision tasks without using convolutional layers.\nThis naturally leads to the following questions: Can a self-attention layer of\nViT express any convolution operation? In this work, we prove that a single ViT\nlayer with image patches as the input can perform any convolution operation\nconstructively, where the multi-head attention mechanism and the relative\npositional encoding play essential roles. We further provide a lower bound on\nthe number of heads for Vision Transformers to express CNNs. Corresponding with\nour analysis, experimental results show that the construction in our proof can\nhelp inject convolutional bias into Transformers and significantly improve the\nperformance of ViT in low data regimes.",
    "published": "2021-11-02T03:30:17Z",
    "updated": "2021-11-03T00:53:16Z",
    "authors": [
      "Shanda Li",
      "Xiangning Chen",
      "Di He",
      "Cho-Jui Hsieh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.15588v4",
    "title": "SimpleTRON: Simple Transformer with O(N) Complexity",
    "summary": "In this paper, we propose that the dot product pairwise matching attention\nlayer, which is widely used in Transformer-based models, is redundant for the\nmodel performance. Attention, in its original formulation, has to be seen\nrather as a human-level tool to explore and/or visualize relevancy scores in\nsequential data. However, the way how it is constructed leads to significant\ncomputational complexity. Instead, we present SimpleTRON: Simple Transformer\nwith O(N) Complexity, a simple and fast alternative without any approximation\nthat, unlike other approximation models, does not have any architecture-related\noverhead and therefore can be seen as a purely linear Transformer-like model.\nThis architecture, to the best of our knowledge, outperforms existing\nsub-quadratic attention approximation models on several tasks from the\nLong-Range Arena benchmark. Moreover, we show, that SimpleTRON can benefit from\nweight transfer from pretrained large language models, as its parameters can be\nfully transferable.",
    "published": "2021-11-23T17:06:01Z",
    "updated": "2022-06-28T13:18:03Z",
    "authors": [
      "Uladzislau Yorsh",
      "Alexander Kovalenko",
      "VojtÄch VanÄura",
      "Daniel VaÅ¡ata",
      "Pavel KordÃ­k",
      "TomÃ¡Å¡ Mikolov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.15465v2",
    "title": "Protein language models trained on multiple sequence alignments learn\n  phylogenetic relationships",
    "summary": "Self-supervised neural language models with attention have recently been\napplied to biological sequence data, advancing structure, function and\nmutational effect prediction. Some protein language models, including MSA\nTransformer and AlphaFold's EvoFormer, take multiple sequence alignments (MSAs)\nof evolutionarily related proteins as inputs. Simple combinations of MSA\nTransformer's row attentions have led to state-of-the-art unsupervised\nstructural contact prediction. We demonstrate that similarly simple, and\nuniversal, combinations of MSA Transformer's column attentions strongly\ncorrelate with Hamming distances between sequences in MSAs. Therefore,\nMSA-based language models encode detailed phylogenetic relationships. We\nfurther show that these models can separate coevolutionary signals encoding\nfunctional and structural constraints from phylogenetic correlations reflecting\nhistorical contingency. To assess this, we generate synthetic MSAs, either\nwithout or with phylogeny, from Potts models trained on natural MSAs. We find\nthat unsupervised contact prediction is substantially more resilient to\nphylogenetic noise when using MSA Transformer versus inferred Potts models.",
    "published": "2022-03-29T12:07:45Z",
    "updated": "2022-09-21T09:46:33Z",
    "authors": [
      "Umberto Lupo",
      "Damiano Sgarbossa",
      "Anne-Florence Bitbol"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.02126v1",
    "title": "Improving Semantic Segmentation in Transformers using Hierarchical\n  Inter-Level Attention",
    "summary": "Existing transformer-based image backbones typically propagate feature\ninformation in one direction from lower to higher-levels. This may not be ideal\nsince the localization ability to delineate accurate object boundaries, is most\nprominent in the lower, high-resolution feature maps, while the semantics that\ncan disambiguate image signals belonging to one object vs. another, typically\nemerges in a higher level of processing. We present Hierarchical Inter-Level\nAttention (HILA), an attention-based method that captures Bottom-Up and\nTop-Down Updates between features of different levels. HILA extends\nhierarchical vision transformer architectures by adding local connections\nbetween features of higher and lower levels to the backbone encoder. In each\niteration, we construct a hierarchy by having higher-level features compete for\nassignments to update lower-level features belonging to them, iteratively\nresolving object-part relationships. These improved lower-level features are\nthen used to re-update the higher-level features. HILA can be integrated into\nthe majority of hierarchical architectures without requiring any changes to the\nbase model. We add HILA into SegFormer and the Swin Transformer and show\nnotable improvements in accuracy in semantic segmentation with fewer parameters\nand FLOPS. Project website and code:\nhttps://www.cs.toronto.edu/~garyleung/hila/",
    "published": "2022-07-05T15:47:31Z",
    "updated": "2022-07-05T15:47:31Z",
    "authors": [
      "Gary Leung",
      "Jun Gao",
      "Xiaohui Zeng",
      "Sanja Fidler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.11022v1",
    "title": "Semantic Segmentation Enhanced Transformer Model for Human Attention\n  Prediction",
    "summary": "Saliency Prediction aims to predict the attention distribution of human eyes\ngiven an RGB image. Most of the recent state-of-the-art methods are based on\ndeep image feature representations from traditional CNNs. However, the\ntraditional convolution could not capture the global features of the image well\ndue to its small kernel size. Besides, the high-level factors which closely\ncorrelate to human visual perception, e.g., objects, color, light, etc., are\nnot considered. Inspired by these, we propose a Transformer-based method with\nsemantic segmentation as another learning objective. More global cues of the\nimage could be captured by Transformer. In addition, simultaneously learning\nthe object segmentation simulates the human visual perception, which we would\nverify in our investigation of human gaze control in cognitive science. We\nbuild an extra decoder for the subtask and the multiple tasks share the same\nTransformer encoder, forcing it to learn from multiple feature spaces. We find\nin practice simply adding the subtask might confuse the main task learning,\nhence Multi-task Attention Module is proposed to deal with the feature\ninteraction between the multiple learning targets. Our method achieves\ncompetitive performance compared to other state-of-the-art methods.",
    "published": "2023-01-26T10:27:51Z",
    "updated": "2023-01-26T10:27:51Z",
    "authors": [
      "Shuo Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.00947v2",
    "title": "RePAST: Relative Pose Attention Scene Representation Transformer",
    "summary": "The Scene Representation Transformer (SRT) is a recent method to render novel\nviews at interactive rates. Since SRT uses camera poses with respect to an\narbitrarily chosen reference camera, it is not invariant to the order of the\ninput views. As a result, SRT is not directly applicable to large-scale scenes\nwhere the reference frame would need to be changed regularly. In this work, we\npropose Relative Pose Attention SRT (RePAST): Instead of fixing a reference\nframe at the input, we inject pairwise relative camera pose information\ndirectly into the attention mechanism of the Transformers. This leads to a\nmodel that is by definition invariant to the choice of any global reference\nframe, while still retaining the full capabilities of the original method.\nEmpirical results show that adding this invariance to the model does not lead\nto a loss in quality. We believe that this is a step towards applying fully\nlatent transformer-based rendering methods to large-scale scenes.",
    "published": "2023-04-03T13:13:12Z",
    "updated": "2023-04-10T13:11:13Z",
    "authors": [
      "Aleksandr Safin",
      "Daniel Duckworth",
      "Mehdi S. M. Sajjadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.14124v1",
    "title": "Exploiting Inductive Bias in Transformer for Point Cloud Classification\n  and Segmentation",
    "summary": "Discovering inter-point connection for efficient high-dimensional feature\nextraction from point coordinate is a key challenge in processing point cloud.\nMost existing methods focus on designing efficient local feature extractors\nwhile ignoring global connection, or vice versa. In this paper, we design a new\nInductive Bias-aided Transformer (IBT) method to learn 3D inter-point\nrelations, which considers both local and global attentions. Specifically,\nconsidering local spatial coherence, local feature learning is performed\nthrough Relative Position Encoding and Attentive Feature Pooling. We\nincorporate the learned locality into the Transformer module. The local feature\naffects value component in Transformer to modulate the relationship between\nchannels of each point, which can enhance self-attention mechanism with\nlocality based channel interaction. We demonstrate its superiority\nexperimentally on classification and segmentation tasks. The code is available\nat: https://github.com/jiamang/IBT",
    "published": "2023-04-27T12:17:35Z",
    "updated": "2023-04-27T12:17:35Z",
    "authors": [
      "Zihao Li",
      "Pan Gao",
      "Hui Yuan",
      "Ran Wei",
      "Manoranjan Paul"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.01944v1",
    "title": "Dynamic Token-Pass Transformers for Semantic Segmentation",
    "summary": "Vision transformers (ViT) usually extract features via forwarding all the\ntokens in the self-attention layers from top to toe. In this paper, we\nintroduce dynamic token-pass vision transformers (DoViT) for semantic\nsegmentation, which can adaptively reduce the inference cost for images with\ndifferent complexity. DoViT gradually stops partial easy tokens from\nself-attention calculation and keeps the hard tokens forwarding until meeting\nthe stopping criteria. We employ lightweight auxiliary heads to make the\ntoken-pass decision and divide the tokens into keeping/stopping parts. With a\ntoken separate calculation, the self-attention layers are speeded up with\nsparse tokens and still work friendly with hardware. A token reconstruction\nmodule is built to collect and reset the grouped tokens to their original\nposition in the sequence, which is necessary to predict correct semantic masks.\nWe conduct extensive experiments on two common semantic segmentation tasks, and\ndemonstrate that our method greatly reduces about 40% $\\sim$ 60% FLOPs and the\ndrop of mIoU is within 0.8% for various segmentation transformers. The\nthroughput and inference speed of ViT-L/B are increased to more than 2$\\times$\non Cityscapes.",
    "published": "2023-08-03T06:14:24Z",
    "updated": "2023-08-03T06:14:24Z",
    "authors": [
      "Yuang Liu",
      "Qiang Zhou",
      "Jing Wang",
      "Fan Wang",
      "Jun Wang",
      "Wei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.05051v1",
    "title": "PAT: Position-Aware Transformer for Dense Multi-Label Action Detection",
    "summary": "We present PAT, a transformer-based network that learns complex temporal\nco-occurrence action dependencies in a video by exploiting multi-scale temporal\nfeatures. In existing methods, the self-attention mechanism in transformers\nloses the temporal positional information, which is essential for robust action\ndetection. To address this issue, we (i) embed relative positional encoding in\nthe self-attention mechanism and (ii) exploit multi-scale temporal\nrelationships by designing a novel non hierarchical network, in contrast to the\nrecent transformer-based approaches that use a hierarchical structure. We argue\nthat joining the self-attention mechanism with multiple sub-sampling processes\nin the hierarchical approaches results in increased loss of positional\ninformation. We evaluate the performance of our proposed approach on two\nchallenging dense multi-label benchmark datasets, and show that PAT improves\nthe current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and\nMultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art\nmAP at 26.5% and 44.6%, respectively. We also perform extensive ablation\nstudies to examine the impact of the different components of our proposed\nnetwork.",
    "published": "2023-08-09T16:29:31Z",
    "updated": "2023-08-09T16:29:31Z",
    "authors": [
      "Faegheh Sardari",
      "Armin Mustafa",
      "Philip J. B. Jackson",
      "Adrian Hilton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.06404v1",
    "title": "Positional Encodings for Light Curve Transformers: Playing with\n  Positions and Attention",
    "summary": "We conducted empirical experiments to assess the transferability of a light\ncurve transformer to datasets with different cadences and magnitude\ndistributions using various positional encodings (PEs). We proposed a new\napproach to incorporate the temporal information directly to the output of the\nlast attention layer. Our results indicated that using trainable PEs lead to\nsignificant improvements in the transformer performances and training times.\nOur proposed PE on attention can be trained faster than the traditional\nnon-trainable PE transformer while achieving competitive results when\ntransfered to other datasets.",
    "published": "2023-08-11T22:02:40Z",
    "updated": "2023-08-11T22:02:40Z",
    "authors": [
      "Daniel Moreno-Cartagena",
      "Guillermo Cabrera-Vives",
      "Pavlos Protopapas",
      "Cristobal Donoso-Oliva",
      "Manuel PÃ©rez-Carrasco",
      "Martina CÃ¡diz-Leyton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.16379v1",
    "title": "Multi-Objective Decision Transformers for Offline Reinforcement Learning",
    "summary": "Offline Reinforcement Learning (RL) is structured to derive policies from\nstatic trajectory data without requiring real-time environment interactions.\nRecent studies have shown the feasibility of framing offline RL as a sequence\nmodeling task, where the sole aim is to predict actions based on prior context\nusing the transformer architecture. However, the limitation of this single task\nlearning approach is its potential to undermine the transformer model's\nattention mechanism, which should ideally allocate varying attention weights\nacross different tokens in the input context for optimal prediction. To address\nthis, we reformulate offline RL as a multi-objective optimization problem,\nwhere the prediction is extended to states and returns. We also highlight a\npotential flaw in the trajectory representation used for sequence modeling,\nwhich could generate inaccuracies when modeling the state and return\ndistributions. This is due to the non-smoothness of the action distribution\nwithin the trajectory dictated by the behavioral policy. To mitigate this\nissue, we introduce action space regions to the trajectory representation. Our\nexperiments on D4RL benchmark locomotion tasks reveal that our propositions\nallow for more effective utilization of the attention mechanism in the\ntransformer model, resulting in performance that either matches or outperforms\ncurrent state-of-the art methods.",
    "published": "2023-08-31T00:47:58Z",
    "updated": "2023-08-31T00:47:58Z",
    "authors": [
      "Abdelghani Ghanem",
      "Philippe Ciblat",
      "Mounir Ghogho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.07315v2",
    "title": "Traveling Words: A Geometric Interpretation of Transformers",
    "summary": "Transformers have significantly advanced the field of natural language\nprocessing, but comprehending their internal mechanisms remains a challenge. In\nthis paper, we introduce a novel geometric perspective that elucidates the\ninner mechanisms of transformer operations. Our primary contribution is\nillustrating how layer normalization confines the latent features to a\nhyper-sphere, subsequently enabling attention to mold the semantic\nrepresentation of words on this surface. This geometric viewpoint seamlessly\nconnects established properties such as iterative refinement and contextual\nembeddings. We validate our insights by probing a pre-trained 124M parameter\nGPT-2 model. Our findings reveal clear query-key attention patterns in early\nlayers and build upon prior observations regarding the subject-specific nature\nof attention heads at deeper layers. Harnessing these geometric insights, we\npresent an intuitive understanding of transformers, depicting them as processes\nthat model the trajectory of word particles along the hyper-sphere.",
    "published": "2023-09-13T21:01:03Z",
    "updated": "2023-09-19T00:34:56Z",
    "authors": [
      "Raul Molina"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.17936v1",
    "title": "Transformers as Graph-to-Graph Models",
    "summary": "We argue that Transformers are essentially graph-to-graph models, with\nsequences just being a special case. Attention weights are functionally\nequivalent to graph edges. Our Graph-to-Graph Transformer architecture makes\nthis ability explicit, by inputting graph edges into the attention weight\ncomputations and predicting graph edges with attention-like functions, thereby\nintegrating explicit graphs into the latent graphs learned by pretrained\nTransformers. Adding iterative graph refinement provides a joint embedding of\ninput, output, and latent graphs, allowing non-autoregressive graph prediction\nto optimise the complete graph without any bespoke pipeline or decoding\nstrategy. Empirical results show that this architecture achieves\nstate-of-the-art accuracies for modelling a variety of linguistic structures,\nintegrating very effectively with the latent linguistic representations learned\nby pretraining.",
    "published": "2023-10-27T07:21:37Z",
    "updated": "2023-10-27T07:21:37Z",
    "authors": [
      "James Henderson",
      "Alireza Mohammadshahi",
      "Andrei C. Coman",
      "Lesly Miculicich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.13102v2",
    "title": "Detecting out-of-distribution text using topological features of\n  transformer-based language models",
    "summary": "To safeguard machine learning systems that operate on textual data against\nout-of-distribution (OOD) inputs that could cause unpredictable behaviour, we\nexplore the use of topological features of self-attention maps from\ntransformer-based language models to detect when input text is out of\ndistribution. Self-attention forms the core of transformer-based language\nmodels, dynamically assigning vectors to words based on context, thus in theory\nour methodology is applicable to any transformer-based language model with\nmultihead self-attention. We evaluate our approach on BERT and compare it to a\ntraditional OOD approach using CLS embeddings. Our results show that our\napproach outperforms CLS embeddings in distinguishing in-distribution samples\nfrom far-out-of-domain samples, but struggles with near or same-domain\ndatasets.",
    "published": "2023-11-22T02:04:35Z",
    "updated": "2024-07-18T05:45:45Z",
    "authors": [
      "Andres Pollano",
      "Anupam Chaudhuri",
      "Anj Simmons"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.00743v2",
    "title": "Theoretical Understanding of In-Context Learning in Shallow Transformers\n  with Unstructured Data",
    "summary": "Large language models (LLMs) are powerful models that can learn concepts at\nthe inference stage via in-context learning (ICL). While theoretical studies,\ne.g., \\cite{zhang2023trained}, attempt to explain the mechanism of ICL, they\nassume the input $x_i$ and the output $y_i$ of each demonstration example are\nin the same token (i.e., structured data). However, in real practice, the\nexamples are usually text input, and all words, regardless of their logic\nrelationship, are stored in different tokens (i.e., unstructured data\n\\cite{wibisono2023role}). To understand how LLMs learn from the unstructured\ndata in ICL, this paper studies the role of each component in the transformer\narchitecture and provides a theoretical understanding to explain the success of\nthe architecture. In particular, we consider a simple transformer with one/two\nattention layers and linear regression tasks for the ICL prediction. We observe\nthat (1) a transformer with two layers of (self-)attentions with a look-ahead\nattention mask can learn from the prompt in the unstructured data, and (2)\npositional encoding can match the $x_i$ and $y_i$ tokens to achieve a better\nICL performance.",
    "published": "2024-02-01T16:39:45Z",
    "updated": "2024-06-18T13:11:32Z",
    "authors": [
      "Yue Xing",
      "Xiaofeng Lin",
      "Chenheng Xu",
      "Namjoon Suh",
      "Qifan Song",
      "Guang Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.17656v1",
    "title": "SGHormer: An Energy-Saving Graph Transformer Driven by Spikes",
    "summary": "Graph Transformers (GTs) with powerful representation learning ability make a\nhuge success in wide range of graph tasks. However, the costs behind\noutstanding performances of GTs are higher energy consumption and computational\noverhead. The complex structure and quadratic complexity during attention\ncalculation in vanilla transformer seriously hinder its scalability on the\nlarge-scale graph data. Though existing methods have made strides in\nsimplifying combinations among blocks or attention-learning paradigm to improve\nGTs' efficiency, a series of energy-saving solutions originated from\nbiologically plausible structures are rarely taken into consideration when\nconstructing GT framework. To this end, we propose a new spiking-based graph\ntransformer (SGHormer). It turns full-precision embeddings into sparse and\nbinarized spikes to reduce memory and computational costs. The spiking graph\nself-attention and spiking rectify blocks in SGHormer explicitly capture global\nstructure information and recover the expressive power of spiking embeddings,\nrespectively. In experiments, SGHormer achieves comparable performances to\nother full-precision GTs with extremely low computational energy consumption.\nThe results show that SGHomer makes a remarkable progress in the field of\nlow-energy GTs.",
    "published": "2024-03-26T12:39:02Z",
    "updated": "2024-03-26T12:39:02Z",
    "authors": [
      "Huizhe Zhang",
      "Jintang Li",
      "Liang Chen",
      "Zibin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.09173v3",
    "title": "TransformerFAM: Feedback attention is working memory",
    "summary": "While Transformers have revolutionized deep learning, their quadratic\nattention complexity hinders their ability to process infinitely long inputs.\nWe propose Feedback Attention Memory (FAM), a novel Transformer architecture\nthat leverages a feedback loop to enable the network to attend to its own\nlatent representations. This design fosters the emergence of working memory\nwithin the Transformer, allowing it to process indefinitely long sequences.\nTransformerFAM requires no additional weights, enabling seamless integration\nwith pre-trained models. Our experiments show that TransformerFAM significantly\nimproves Transformer performance on long-context tasks across various model\nsizes (1B, 8B, and 24B). These results showcase the potential to empower Large\nLanguage Models (LLMs) to process sequences of unlimited length.",
    "published": "2024-04-14T07:43:45Z",
    "updated": "2024-05-07T13:23:46Z",
    "authors": [
      "Dongseong Hwang",
      "Weiran Wang",
      "Zhuoyuan Huo",
      "Khe Chai Sim",
      "Pedro Moreno Mengibar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.15276v1",
    "title": "SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose\n  Estimation",
    "summary": "Existing Transformers for monocular 3D human shape and pose estimation\ntypically have a quadratic computation and memory complexity with respect to\nthe feature length, which hinders the exploitation of fine-grained information\nin high-resolution features that is beneficial for accurate reconstruction. In\nthis work, we propose an SMPL-based Transformer framework (SMPLer) to address\nthis issue. SMPLer incorporates two key ingredients: a decoupled attention\noperation and an SMPL-based target representation, which allow effective\nutilization of high-resolution features in the Transformer. In addition, based\non these two designs, we also introduce several novel modules including a\nmulti-scale attention and a joint-aware attention to further boost the\nreconstruction performance. Extensive experiments demonstrate the effectiveness\nof SMPLer against existing 3D human shape and pose estimation methods both\nquantitatively and qualitatively. Notably, the proposed algorithm achieves an\nMPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by\nmore than 10% with fewer than one-third of the parameters. Code and pretrained\nmodels are available at https://github.com/xuxy09/SMPLer.",
    "published": "2024-04-23T17:59:59Z",
    "updated": "2024-04-23T17:59:59Z",
    "authors": [
      "Xiangyu Xu",
      "Lijuan Liu",
      "Shuicheng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.03303v1",
    "title": "Learning Visual Prompts for Guiding the Attention of Vision Transformers",
    "summary": "Visual prompting infuses visual information into the input image to adapt\nmodels toward specific predictions and tasks. Recently, manually crafted\nmarkers such as red circles are shown to guide the model to attend to a target\nregion on the image. However, these markers only work on models trained with\ndata containing those markers. Moreover, finding these prompts requires\nguesswork or prior knowledge of the domain on which the model is trained. This\nwork circumvents manual design constraints by proposing to learn the visual\nprompts for guiding the attention of vision transformers. The learned visual\nprompt, added to any input image would redirect the attention of the\npre-trained vision transformer to its spatial location on the image.\nSpecifically, the prompt is learned in a self-supervised manner without\nrequiring annotations and without fine-tuning the vision transformer. Our\nexperiments demonstrate the effectiveness of the proposed optimization-based\nvisual prompting strategy across various pre-trained vision encoders.",
    "published": "2024-06-05T14:13:38Z",
    "updated": "2024-06-05T14:13:38Z",
    "authors": [
      "Razieh Rezaei",
      "Masoud Jalili Sabet",
      "Jindong Gu",
      "Daniel Rueckert",
      "Philip Torr",
      "Ashkan Khakzar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.04909v1",
    "title": "Efficient Training of Transformers for Molecule Property Prediction on\n  Small-scale Datasets",
    "summary": "The blood-brain barrier (BBB) serves as a protective barrier that separates\nthe brain from the circulatory system, regulating the passage of substances\ninto the central nervous system. Assessing the BBB permeability of potential\ndrugs is crucial for effective drug targeting. However, traditional\nexperimental methods for measuring BBB permeability are challenging and\nimpractical for large-scale screening. Consequently, there is a need to develop\ncomputational approaches to predict BBB permeability. This paper proposes a GPS\nTransformer architecture augmented with Self Attention, designed to perform\nwell in the low-data regime. The proposed approach achieved a state-of-the-art\nperformance on the BBB permeability prediction task using the BBBP dataset,\nsurpassing existing models. With a ROC-AUC of 78.8%, the approach sets a\nstate-of-the-art by 5.5%. We demonstrate that standard Self Attention coupled\nwith GPS transformer performs better than other variants of attention coupled\nwith GPS Transformer.",
    "published": "2024-09-07T21:07:12Z",
    "updated": "2024-09-07T21:07:12Z",
    "authors": [
      "Shivesh Prakash"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.20317v1",
    "title": "ProtSCAPE: Mapping the landscape of protein conformations in molecular\n  dynamics",
    "summary": "Understanding the dynamic nature of protein structures is essential for\ncomprehending their biological functions. While significant progress has been\nmade in predicting static folded structures, modeling protein motions on\nmicrosecond to millisecond scales remains challenging. To address these\nchallenges, we introduce a novel deep learning architecture, Protein\nTransformer with Scattering, Attention, and Positional Embedding (ProtSCAPE),\nwhich leverages the geometric scattering transform alongside transformer-based\nattention mechanisms to capture protein dynamics from molecular dynamics (MD)\nsimulations. ProtSCAPE utilizes the multi-scale nature of the geometric\nscattering transform to extract features from protein structures conceptualized\nas graphs and integrates these features with dual attention structures that\nfocus on residues and amino acid signals, generating latent representations of\nprotein trajectories. Furthermore, ProtSCAPE incorporates a regression head to\nenforce temporally coherent latent representations.",
    "published": "2024-10-27T02:59:48Z",
    "updated": "2024-10-27T02:59:48Z",
    "authors": [
      "Siddharth Viswanath",
      "Dhananjay Bhaskar",
      "David R. Johnson",
      "Joao Felipe Rocha",
      "Egbert Castro",
      "Jackson D. Grady",
      "Alex T. Grigas",
      "Michael A. Perlmutter",
      "Corey S. O'Hern",
      "Smita Krishnaswamy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.13028v1",
    "title": "A Theory for Compressibility of Graph Transformers for Transductive\n  Learning",
    "summary": "Transductive tasks on graphs differ fundamentally from typical supervised\nmachine learning tasks, as the independent and identically distributed (i.i.d.)\nassumption does not hold among samples. Instead, all train/test/validation\nsamples are present during training, making them more akin to a semi-supervised\ntask. These differences make the analysis of the models substantially different\nfrom other models. Recently, Graph Transformers have significantly improved\nresults on these datasets by overcoming long-range dependency problems.\nHowever, the quadratic complexity of full Transformers has driven the community\nto explore more efficient variants, such as those with sparser attention\npatterns. While the attention matrix has been extensively discussed, the hidden\ndimension or width of the network has received less attention. In this work, we\nestablish some theoretical bounds on how and under what conditions the hidden\ndimension of these networks can be compressed. Our results apply to both sparse\nand dense variants of Graph Transformers.",
    "published": "2024-11-20T04:20:17Z",
    "updated": "2024-11-20T04:20:17Z",
    "authors": [
      "Hamed Shirzad",
      "Honghao Lin",
      "Ameya Velingker",
      "Balaji Venkatachalam",
      "David Woodruff",
      "Danica Sutherland"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.06061v2",
    "title": "Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail\n  to Generalize on Time Series Forecasting and Beyond",
    "summary": "The application of transformer-based models on time series forecasting (TSF)\ntasks has long been popular to study. However, many of these works fail to beat\nthe simple linear residual model, and the theoretical understanding of this\nissue is still limited. In this work, we propose the first theoretical\nexplanation of the inefficiency of transformers on TSF tasks. We attribute the\nmechanism behind it to {\\bf Asymmetric Learning} in training attention\nnetworks. When the sign of the previous step is inconsistent with the sign of\nthe current step in the next-step-prediction time series, attention fails to\nlearn the residual features. This makes it difficult to generalize on\nout-of-distribution (OOD) data, especially on the sign-inconsistent\nnext-step-prediction data, with the same representation pattern, whereas a\nlinear residual network could easily accomplish it. We hope our theoretical\ninsights provide important necessary conditions for designing the expressive\nand efficient transformer-based architecture for practitioners.",
    "published": "2024-12-08T20:29:06Z",
    "updated": "2025-02-28T20:36:37Z",
    "authors": [
      "Yekun Ke",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Chiwun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.14847v2",
    "title": "A Survey of RWKV",
    "summary": "The Receptance Weighted Key Value (RWKV) model offers a novel alternative to\nthe Transformer architecture, merging the benefits of recurrent and\nattention-based systems. Unlike conventional Transformers, which depend heavily\non self-attention, RWKV adeptly captures long-range dependencies with minimal\ncomputational demands. By utilizing a recurrent framework, RWKV addresses some\ncomputational inefficiencies found in Transformers, particularly in tasks with\nlong sequences. RWKV has recently drawn considerable attention for its robust\nperformance across multiple domains. Despite its growing popularity, no\nsystematic review of the RWKV model exists. This paper seeks to fill this gap\nas the first comprehensive review of the RWKV architecture, its core\nprinciples, and its varied applications, such as natural language generation,\nnatural language understanding, and computer vision. We assess how RWKV\ncompares to traditional Transformer models, highlighting its capability to\nmanage long sequences efficiently and lower computational costs. Furthermore,\nwe explore the challenges RWKV encounters and propose potential directions for\nfuture research and advancement. We consistently maintain the related\nopen-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.",
    "published": "2024-12-19T13:39:24Z",
    "updated": "2025-01-05T13:54:06Z",
    "authors": [
      "Zhiyuan Li",
      "Tingyu Xia",
      "Yi Chang",
      "Yuan Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.02735v1",
    "title": "Sequence Complementor: Complementing Transformers For Time Series\n  Forecasting with Learnable Sequences",
    "summary": "Since its introduction, the transformer has shifted the development\ntrajectory away from traditional models (e.g., RNN, MLP) in time series\nforecasting, which is attributed to its ability to capture global dependencies\nwithin temporal tokens. Follow-up studies have largely involved altering the\ntokenization and self-attention modules to better adapt Transformers for\naddressing special challenges like non-stationarity, channel-wise dependency,\nand variable correlation in time series. However, we found that the expressive\ncapability of sequence representation is a key factor influencing Transformer\nperformance in time forecasting after investigating several representative\nmethods, where there is an almost linear relationship between sequence\nrepresentation entropy and mean square error, with more diverse representations\nperforming better. In this paper, we propose a novel attention mechanism with\nSequence Complementors and prove feasible from an information theory\nperspective, where these learnable sequences are able to provide complementary\ninformation beyond current input to feed attention. We further enhance the\nSequence Complementors via a diversification loss that is theoretically\ncovered. The empirical evaluation of both long-term and short-term forecasting\nhas confirmed its superiority over the recent state-of-the-art methods.",
    "published": "2025-01-06T03:08:39Z",
    "updated": "2025-01-06T03:08:39Z",
    "authors": [
      "Xiwen Chen",
      "Peijie Qiu",
      "Wenhui Zhu",
      "Huayu Li",
      "Hao Wang",
      "Aristeidis Sotiras",
      "Yalin Wang",
      "Abolfazl Razi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.07825v1",
    "title": "An Efficient Sparse Hardware Accelerator for Spike-Driven Transformer",
    "summary": "Recently, large models, such as Vision Transformer and BERT, have garnered\nsignificant attention due to their exceptional performance. However, their\nextensive computational requirements lead to considerable power and hardware\nresource consumption. Brain-inspired computing, characterized by its\nspike-driven methods, has emerged as a promising approach for low-power\nhardware implementation. In this paper, we propose an efficient sparse hardware\naccelerator for Spike-driven Transformer. We first design a novel encoding\nmethod that encodes the position information of valid activations and skips\nnon-spike values. This method enables us to use encoded spikes for executing\nthe calculations of linear, maxpooling and spike-driven self-attention.\nCompared with the single spike input design of conventional SNN accelerators\nthat primarily focus on convolution-based spiking computations, the specialized\nmodule for spike-driven self-attention is unique in its ability to handle dual\nspike inputs. By exclusively utilizing activated spikes, our design fully\nexploits the sparsity of Spike-driven Transformer, which diminishes redundant\noperations, lowers power consumption, and minimizes computational latency.\nExperimental results indicate that compared to existing SNNs accelerators, our\ndesign achieves up to 13.24$\\times$ and 1.33$\\times$ improvements in terms of\nthroughput and energy efficiency, respectively.",
    "published": "2025-01-14T04:01:41Z",
    "updated": "2025-01-14T04:01:41Z",
    "authors": [
      "Zhengke Li",
      "Wendong Mao",
      "Siyu Zhang",
      "Qiwei Dong",
      "Zhongfeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07982v1",
    "title": "Deep Semantic Graph Learning via LLM based Node Enhancement",
    "summary": "Graph learning has attracted significant attention due to its widespread\nreal-world applications. Current mainstream approaches rely on text node\nfeatures and obtain initial node embeddings through shallow embedding learning\nusing GNNs, which shows limitations in capturing deep textual semantics. Recent\nadvances in Large Language Models (LLMs) have demonstrated superior\ncapabilities in understanding text semantics, transforming traditional text\nfeature processing. This paper proposes a novel framework that combines Graph\nTransformer architecture with LLM-enhanced node features. Specifically, we\nleverage LLMs to generate rich semantic representations of text nodes, which\nare then processed by a multi-head self-attention mechanism in the Graph\nTransformer to capture both local and global graph structural information. Our\nmodel utilizes the Transformer's attention mechanism to dynamically aggregate\nneighborhood information while preserving the semantic richness provided by LLM\nembeddings. Experimental results demonstrate that the LLM-enhanced node\nfeatures significantly improve the performance of graph learning models on node\nclassification tasks. This approach shows promising results across multiple\ngraph learning tasks, offering a practical direction for combining graph\nnetworks with language models.",
    "published": "2025-02-11T21:55:46Z",
    "updated": "2025-02-11T21:55:46Z",
    "authors": [
      "Chuanqi Shi",
      "Yiyi Tao",
      "Hang Zhang",
      "Lun Wang",
      "Shaoshuai Du",
      "Yixian Shen",
      "Yanxin Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16483v1",
    "title": "A Split-Window Transformer for Multi-Model Sequence Spammer Detection\n  using Multi-Model Variational Autoencoder",
    "summary": "This paper introduces a new Transformer, called MS$^2$Dformer, that can be\nused as a generalized backbone for multi-modal sequence spammer detection.\nSpammer detection is a complex multi-modal task, thus the challenges of\napplying Transformer are two-fold. Firstly, complex multi-modal noisy\ninformation about users can interfere with feature mining. Secondly, the long\nsequence of users' historical behaviors also puts a huge GPU memory pressure on\nthe attention computation. To solve these problems, we first design a user\nbehavior Tokenization algorithm based on the multi-modal variational\nautoencoder (MVAE). Subsequently, a hierarchical split-window multi-head\nattention (SW/W-MHA) mechanism is proposed. The split-window strategy\ntransforms the ultra-long sequences hierarchically into a combination of\nintra-window short-term and inter-window overall attention. Pre-trained on the\npublic datasets, MS$^2$Dformer's performance far exceeds the previous state of\nthe art. The experiments demonstrate MS$^2$Dformer's ability to act as a\nbackbone.",
    "published": "2025-02-23T07:53:08Z",
    "updated": "2025-02-23T07:53:08Z",
    "authors": [
      "Zhou Yang",
      "Yucai Pang",
      "Hongbo Yin",
      "Yunpeng Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07294v2",
    "title": "From $\\mathcal{O}(n^{2})$ to $\\mathcal{O}(n)$ Parameters: Quantum\n  Self-Attention in Vision Transformers for Biomedical Image Classification",
    "summary": "We demonstrate that quantum vision transformers (QViTs), vision transformers\n(ViTs) with self-attention (SA) mechanisms replaced by quantum self-attention\n(QSA) mechanisms, can match state-of-the-art (SOTA) biomedical image\nclassifiers while using 99.99% fewer parameters. QSAs are produced by replacing\nlinear SA layers with parameterised quantum neural networks (QNNs), producing a\nQSA mechanism and reducing parameter scaling from $\\mathcal{O}(n^2)$ to\n$\\mathcal{O}(n)$. On RetinaMNIST, our ultra parameter-efficient QViT\noutperforms 13/14 SOTA methods including CNNs and ViTs, achieving 56.5%\naccuracy, just 0.88% below the top MedMamba model while using 99.99% fewer\nparameters (1K vs 14.5M) and 89% fewer GFLOPs. We present the first\ninvestigation of knowledge distillation (KD) from classical to quantum vision\ntransformers in biomedical image classification, showing that QViTs maintain\ncomparable performance to classical ViTs across eight diverse datasets spanning\nmultiple modalities, with improved QSA parameter-efficiency. Our higher-qubit\narchitecture benefitted more from KD pre-training, suggesting a scaling\nrelationship between QSA parameters and KD effectiveness. These findings\nestablish QSA as a practical architectural choice toward parameter-efficient\nbiomedical image analysis.",
    "published": "2025-03-10T13:16:48Z",
    "updated": "2025-06-25T17:08:53Z",
    "authors": [
      "Thomas Boucher",
      "John Whittle",
      "Evangelos B. Mazomenos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16774v1",
    "title": "Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors\n  and Cross-Model Attention Mechanism",
    "summary": "The examination of chest X-ray images is a crucial component in detecting\nvarious thoracic illnesses. This study introduces a new image description\ngeneration model that integrates a Vision Transformer (ViT) encoder with\ncross-modal attention and a GPT-4-based transformer decoder. The ViT captures\nhigh-quality visual features from chest X-rays, which are fused with text data\nthrough cross-modal attention to improve the accuracy, context, and richness of\nimage descriptions. The GPT-4 decoder transforms these fused features into\naccurate and relevant captions. The model was tested on the National Institutes\nof Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU\ndataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and\n0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all\nmetrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),\nand ROUGE-L (0.705). This framework has the potential to enhance chest X-ray\nevaluation, assisting radiologists in more precise and efficient diagnosis.",
    "published": "2025-04-23T14:46:10Z",
    "updated": "2025-04-23T14:46:10Z",
    "authors": [
      "Lakshita Agarwal",
      "Bindu Verma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20802v1",
    "title": "Leaner Transformers: More Heads, Less Depth",
    "summary": "Transformers have reshaped machine learning by utilizing attention mechanisms\nto capture complex patterns in large datasets, leading to significant\nimprovements in performance. This success has contributed to the belief that\n\"bigger means better\", leading to ever-increasing model sizes. This paper\nchallenge this ideology by showing that many existing transformers might be\nunnecessarily oversized. We discover a theoretical principle that redefines the\nrole of multi-head attention. An important benefit of the multiple heads is in\nimproving the conditioning of the attention block. We exploit this theoretical\ninsight and redesign popular architectures with an increased number of heads.\nThe improvement in the conditioning proves so significant in practice that\nmodel depth can be decreased, reducing the parameter count by up to 30-50%\nwhile maintaining accuracy. We obtain consistent benefits across a variety of\ntransformer-based architectures of various scales, on tasks in computer vision\n(ImageNet-1k) as well as language and sequence modeling (GLUE benchmark,\nTinyStories, and the Long-Range Arena benchmark).",
    "published": "2025-05-27T07:06:54Z",
    "updated": "2025-05-27T07:06:54Z",
    "authors": [
      "Hemanth Saratchandran",
      "Damien Teney",
      "Simon Lucey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20904v2",
    "title": "HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal\n  Fusion for Transparent and Reflective Objects Depth Completion",
    "summary": "Transparent and reflective objects pose significant challenges for depth\nsensors, resulting in incomplete depth information that adversely affects\ndownstream robotic perception and manipulation tasks. To address this issue, we\npropose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba\narchitectures. The encoder is based on a dual-branch CNN-Transformer framework,\nthe bottleneck fusion module adopts a Transformer-Mamba architecture, and the\ndecoder is built upon a multi-scale fusion module. We introduce a novel\nmultimodal fusion module grounded in self-attention mechanisms and state space\nmodels, marking the first application of the Mamba architecture in the field of\ntransparent object depth completion and revealing its promising potential.\nAdditionally, we design an innovative multi-scale fusion module for the decoder\nthat combines channel attention, spatial attention, and multi-scale feature\nextraction techniques to effectively integrate multi-scale features through a\ndown-fusion strategy. Extensive evaluations on multiple public datasets\ndemonstrate that our model achieves state-of-the-art(SOTA) performance,\nvalidating the effectiveness of our approach.",
    "published": "2025-05-27T08:51:38Z",
    "updated": "2025-05-28T08:36:38Z",
    "authors": [
      "Guanghu Xie",
      "Yonglong Zhang",
      "Zhiduo Jiang",
      "Yang Liu",
      "Zongwu Xie",
      "Baoshi Cao",
      "Hong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07357v1",
    "title": "CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through\n  Spatially Adaptive Attention Mechanisms",
    "summary": "Object detection is vital in precision agriculture for plant monitoring,\ndisease detection, and yield estimation. However, models like YOLO struggle\nwith occlusions, irregular structures, and background noise, reducing detection\naccuracy. While Spatial Transformer Networks (STNs) improve spatial invariance\nthrough learned transformations, affine mappings are insufficient for non-rigid\ndeformations such as bent leaves and overlaps.\n  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)\ninto STNs for flexible, non-rigid spatial transformations that better align\nfeatures. Performance is further enhanced by the Convolutional Block Attention\nModule (CBAM), which suppresses background noise and emphasizes relevant\nspatial and channel-wise features.\n  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model\noutperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction\nin false positives, highlighting the benefits of improved spatial flexibility\nand attention-guided refinement. We also examine the impact of the TPS\nregularization parameter in balancing transformation smoothness and detection\nperformance.\n  This lightweight model improves spatial awareness and supports real-time edge\ndeployment, making it ideal for smart farming applications requiring accurate\nand efficient monitoring.",
    "published": "2025-06-09T02:11:46Z",
    "updated": "2025-06-09T02:11:46Z",
    "authors": [
      "Satvik Praveen",
      "Yoonsung Jung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15714v1",
    "title": "Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and\n  Scalable Replacement for Self-Attention",
    "summary": "We propose an innovative, learnable two-sided short-time Laplace transform\n(STLT) mechanism to supplant the traditional self attention in\ntransformer-based LLMs. Our STLT introduces trainable parameters for each\nLaplace node, enabling end-to-end learning of decay rates , oscillatory\nfrequencies, and window bandwidth T. This flexibility allows the model to\ndynamically adapt token relevance half lives and frequency responses during\ntraining. By selecting S learnable nodes and leveraging fast recursive\nconvolution, we achieve an effective complexity of in time and memory. We\nfurther incorporate an efficient FFT-based computation of the relevance matrix\nand an adaptive node allocation mechanism to dynamically adjust the number of\nactive Laplace nodes. Empirical results on language modeling (WikiText\\-103,\nProject Gutenberg), machine translation (WMT'14 En\\-De), and long document\nquestion answering (NarrativeQA) demonstrate that our learnable STLT achieves\nperplexities and scores on par with or better than existing efficient\ntransformers while naturally extending to context lengths exceeding 100k tokens\nor more limited only by available hardware. Ablation studies confirm the\nimportance of learnable parameters and adaptive node allocation. The proposed\napproach combines interpretability, through explicit decay and frequency\nparameters, with scalability and robustness, offering a pathway towards\nultra-long-sequence language modeling without the computational bottleneck of\nself-attention.",
    "published": "2025-06-01T00:32:24Z",
    "updated": "2025-06-01T00:32:24Z",
    "authors": [
      "Andrew Kiruluta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18999v1",
    "title": "Diffusion Transformer-to-Mamba Distillation for High-Resolution Image\n  Generation",
    "summary": "The quadratic computational complexity of self-attention in diffusion\ntransformers (DiT) introduces substantial computational costs in\nhigh-resolution image generation. While the linear-complexity Mamba model\nemerges as a potential alternative, direct Mamba training remains empirically\nchallenging. To address this issue, this paper introduces diffusion\ntransformer-to-mamba distillation (T2MD), forming an efficient training\npipeline that facilitates the transition from the self-attention-based\ntransformer to the linear complexity state-space model Mamba. We establish a\ndiffusion self-attention and Mamba hybrid model that simultaneously achieves\nefficiency and global dependencies. With the proposed layer-level teacher\nforcing and feature-based knowledge distillation, T2MD alleviates the training\ndifficulty and high cost of a state space model from scratch. Starting from the\ndistilled 512$\\times$512 resolution base model, we push the generation towards\n2048$\\times$2048 images via lightweight adaptation and high-resolution\nfine-tuning. Experiments demonstrate that our training path leads to low\noverhead but high-quality text-to-image generation. Importantly, our results\nalso justify the feasibility of using sequential and causal Mamba models for\ngenerating non-causal visual output, suggesting the potential for future\nexploration.",
    "published": "2025-06-23T18:01:19Z",
    "updated": "2025-06-23T18:01:19Z",
    "authors": [
      "Yuan Yao",
      "Yicong Hong",
      "Difan Liu",
      "Long Mai",
      "Feng Liu",
      "Jiebo Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21641v1",
    "title": "Quantum Variational Transformer Model for Enhanced Cancer Classification",
    "summary": "Accurate prediction of cancer type and primary tumor site is critical for\neffective diagnosis, personalized treatment, and improved outcomes. Traditional\nmodels struggle with the complexity of genomic and clinical data, but quantum\ncomputing offers enhanced computational capabilities. This study develops a\nhybrid quantum-classical transformer model, incorporating quantum attention\nmechanisms via variational quantum circuits (VQCs) to improve prediction\naccuracy. Using 30,000 anonymized cancer samples from the Genome Warehouse\n(GWH), data preprocessing included cleaning, encoding, and feature selection.\nClassical self-attention modules were replaced with quantum attention layers,\nwith classical data encoded into quantum states via amplitude encoding. The\nmodel, trained using hybrid backpropagation and quantum gradient calculations,\noutperformed the classical transformer model, achieving 92.8% accuracy and an\nAUC of 0.96 compared to 87.5% accuracy and an AUC of 0.89. It also demonstrated\n35% faster training and 25% fewer parameters, highlighting computational\nefficiency. These findings showcase the potential of quantum-enhanced\ntransformers to advance biomedical data analysis, enabling more accurate\ndiagnostics and personalized medicine.",
    "published": "2025-06-25T20:53:40Z",
    "updated": "2025-06-25T20:53:40Z",
    "authors": [
      "Don Roosan",
      "Rubayat Khan",
      "Md Rahatul Ashakin",
      "Tiffany Khou",
      "Saif Nirzhor",
      "Mohammad Rifat Haider"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12021v1",
    "title": "Evaluating the Explainability of Vision Transformers in Medical Imaging",
    "summary": "Understanding model decisions is crucial in medical imaging, where\ninterpretability directly impacts clinical trust and adoption. Vision\nTransformers (ViTs) have demonstrated state-of-the-art performance in\ndiagnostic imaging; however, their complex attention mechanisms pose challenges\nto explainability. This study evaluates the explainability of different Vision\nTransformer architectures and pre-training strategies - ViT, DeiT, DINO, and\nSwin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct\nboth quantitative and qualitative analyses on two medical imaging tasks:\nperipheral blood cell classification and breast ultrasound image\nclassification. Our findings indicate that DINO combined with Grad-CAM offers\nthe most faithful and localized explanations across datasets. Grad-CAM\nconsistently produces class-discriminative and spatially precise heatmaps,\nwhile Gradient Attention Rollout yields more scattered activations. Even in\nmisclassification cases, DINO with Grad-CAM highlights clinically relevant\nmorphological features that appear to have misled the model. By improving model\ntransparency, this research supports the reliable and explainable integration\nof ViTs into critical medical diagnostic workflows.",
    "published": "2025-10-13T23:53:26Z",
    "updated": "2025-10-13T23:53:26Z",
    "authors": [
      "Leili Barekatain",
      "Ben Glocker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.25013v1",
    "title": "Emergence of Minimal Circuits for Indirect Object Identification in\n  Attention-Only Transformers",
    "summary": "Mechanistic interpretability aims to reverse-engineer large language models\n(LLMs) into human-understandable computational circuits. However, the\ncomplexity of pretrained models often obscures the minimal mechanisms required\nfor specific reasoning tasks. In this work, we train small, attention-only\ntransformers from scratch on a symbolic version of the Indirect Object\nIdentification (IOI) task -- a benchmark for studying coreference -- like\nreasoning in transformers. Surprisingly, a single-layer model with only two\nattention heads achieves perfect IOI accuracy, despite lacking MLPs and\nnormalization layers. Through residual stream decomposition, spectral analysis,\nand embedding interventions, we find that the two heads specialize into\nadditive and contrastive subcircuits that jointly implement IOI resolution.\nFurthermore, we show that a two-layer, one-head model achieves similar\nperformance by composing information across layers through query-value\ninteractions. These results demonstrate that task-specific training induces\nhighly interpretable, minimal circuits, offering a controlled testbed for\nprobing the computational foundations of transformer reasoning.",
    "published": "2025-10-28T22:25:19Z",
    "updated": "2025-10-28T22:25:19Z",
    "authors": [
      "Rabin Adhikari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.14195v1",
    "title": "Rethinking Transformer for Long Contextual Histopathology Whole Slide\n  Image Analysis",
    "summary": "Histopathology Whole Slide Image (WSI) analysis serves as the gold standard\nfor clinical cancer diagnosis in the daily routines of doctors. To develop\ncomputer-aided diagnosis model for WSIs, previous methods typically employ\nMulti-Instance Learning to enable slide-level prediction given only slide-level\nlabels. Among these models, vanilla attention mechanisms without pairwise\ninteractions have traditionally been employed but are unable to model\ncontextual information. More recently, self-attention models have been utilized\nto address this issue. To alleviate the computational complexity of long\nsequences in large WSIs, methods like HIPT use region-slicing, and TransMIL\nemploys approximation of full self-attention. Both approaches suffer from\nsuboptimal performance due to the loss of key information. Moreover, their use\nof absolute positional embedding struggles to effectively handle long\ncontextual dependencies in shape-varying WSIs. In this paper, we first analyze\nhow the low-rank nature of the long-sequence attention matrix constrains the\nrepresentation ability of WSI modelling. Then, we demonstrate that the rank of\nattention matrix can be improved by focusing on local interactions via a local\nattention mask. Our analysis shows that the local mask aligns with the\nattention patterns in the lower layers of the Transformer. Furthermore, the\nlocal attention mask can be implemented during chunked attention calculation,\nreducing the quadratic computational complexity to linear with a small local\nbandwidth. Building on this, we propose a local-global hybrid Transformer for\nboth computational acceleration and local-global information interactions\nmodelling. Our method, Long-contextual MIL (LongMIL), is evaluated through\nextensive experiments on various WSI tasks to validate its superiority. Our\ncode will be available at github.com/invoker-LL/Long-MIL.",
    "published": "2024-10-18T06:12:36Z",
    "updated": "2024-10-18T06:12:36Z",
    "authors": [
      "Honglin Li",
      "Yunlong Zhang",
      "Pingyi Chen",
      "Zhongyi Shui",
      "Chenglu Zhu",
      "Lin Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.10632v3",
    "title": "Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?",
    "summary": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation that consists\nof learnable activation functions, with the potential to capture more complex\nrelationships from data. Presently, KANs are deployed by replacing multilayer\nperceptrons (MLPs) in deep networks, including advanced architectures such as\nvision Transformers (ViTs). This work asks whether KAN could learn token\ninteractions. In this paper, we design the first learnable attention called\nKolmogorov-Arnold Attention (KArAt) for ViTs that can operate on any basis,\nranging from Fourier, Wavelets, Splines, to Rational Functions. However,\nlearnable activations in the attention cause a memory explosion. To remedy\nthis, we propose a modular version of KArAt that uses a low-rank approximation.\nBy adopting the Fourier basis, Fourier-KArAt and its variants, in some cases,\noutperform their traditional softmax counterparts, or show comparable\nperformance on CIFAR-10, CIFAR-100, and ImageNet-1K. We also deploy Fourier\nKArAt to ConViT and Swin-Transformer, and use it in detection and segmentation\nwith ViT-Det. We dissect the performance of these architectures by analyzing\ntheir loss landscapes, weight distributions, optimizer paths, attention\nvisualizations, and transferability to other datasets. KArAt's learnable\nactivation yields a better attention score across all ViTs, indicating improved\ntoken-to-token interactions and contributing to enhanced inference. Still, its\ngeneralizability does not scale with larger ViTs. However, many factors,\nincluding the present computing interface, affect the relative performance of\nparameter- and memory-heavy KArAts. We note that the goal of this paper is not\nto produce efficient attention or challenge the traditional activations; by\ndesigning KArAt, we are the first to show that attention can be learned and\nencourage researchers to explore KArAt in conjunction with more advanced\narchitectures.",
    "published": "2025-03-13T17:59:52Z",
    "updated": "2025-10-21T21:01:00Z",
    "authors": [
      "Subhajit Maity",
      "Killian Hitsman",
      "Xin Li",
      "Aritra Dutta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1905.04226v2",
    "title": "Language Modeling with Deep Transformers",
    "summary": "We explore deep autoregressive Transformer models in language modeling for\nspeech recognition. We focus on two aspects. First, we revisit Transformer\nmodel configurations specifically for language modeling. We show that well\nconfigured Transformer models outperform our baseline models based on the\nshallow stack of LSTM recurrent neural network layers. We carry out experiments\non the open-source LibriSpeech 960hr task, for both 200K vocabulary word-level\nand 10K byte-pair encoding subword-level language modeling. We apply our\nword-level models to conventional hybrid speech recognition by lattice\nrescoring, and the subword-level models to attention based encoder-decoder\nmodels by shallow fusion. Second, we show that deep Transformer language models\ndo not require positional encoding. The positional encoding is an essential\naugmentation for the self-attention mechanism which is invariant to sequence\nordering. However, in autoregressive setup, as is the case for language\nmodeling, the amount of information increases along the position dimension,\nwhich is a positional signal by its own. The analysis of attention weights\nshows that deep autoregressive self-attention models can automatically make use\nof such positional information. We find that removing the positional encoding\neven slightly improves the performance of these models.",
    "published": "2019-05-10T15:50:00Z",
    "updated": "2019-07-11T15:45:32Z",
    "authors": [
      "Kazuki Irie",
      "Albert Zeyer",
      "Ralf SchlÃ¼ter",
      "Hermann Ney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.02001v4",
    "title": "TransVPR: Transformer-based place recognition with multi-level attention\n  aggregation",
    "summary": "Visual place recognition is a challenging task for applications such as\nautonomous driving navigation and mobile robot localization. Distracting\nelements presenting in complex scenes often lead to deviations in the\nperception of visual place. To address this problem, it is crucial to integrate\ninformation from only task-relevant regions into image representations. In this\npaper, we introduce a novel holistic place recognition model, TransVPR, based\non vision Transformers. It benefits from the desirable property of the\nself-attention operation in Transformers which can naturally aggregate\ntask-relevant features. Attentions from multiple levels of the Transformer,\nwhich focus on different regions of interest, are further combined to generate\na global image representation. In addition, the output tokens from Transformer\nlayers filtered by the fused attention mask are considered as key-patch\ndescriptors, which are used to perform spatial matching to re-rank the\ncandidates retrieved by the global image features. The whole model allows\nend-to-end training with a single objective and image-level supervision.\nTransVPR achieves state-of-the-art performance on several real-world benchmarks\nwhile maintaining low computational time and storage requirements.",
    "published": "2022-01-06T10:20:24Z",
    "updated": "2022-04-13T05:46:16Z",
    "authors": [
      "Ruotong Wang",
      "Yanqing Shen",
      "Weiliang Zuo",
      "Sanping Zhou",
      "Nanning Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.12257v1",
    "title": "Video Mobile-Former: Video Recognition with Efficient Global\n  Spatial-temporal Modeling",
    "summary": "Transformer-based models have achieved top performance on major video\nrecognition benchmarks. Benefiting from the self-attention mechanism, these\nmodels show stronger ability of modeling long-range dependencies compared to\nCNN-based models. However, significant computation overheads, resulted from the\nquadratic complexity of self-attention on top of a tremendous number of tokens,\nlimit the use of existing video transformers in applications with limited\nresources like mobile devices. In this paper, we extend Mobile-Former to Video\nMobile-Former, which decouples the video architecture into a lightweight\n3D-CNNs for local context modeling and a Transformer modules for global\ninteraction modeling in a parallel fashion. To avoid significant computational\ncost incurred by computing self-attention between the large number of local\npatches in videos, we propose to use very few global tokens (e.g., 6) for a\nwhole video in Transformers to exchange information with 3D-CNNs with a\ncross-attention mechanism. Through efficient global spatial-temporal modeling,\nVideo Mobile-Former significantly improves the video recognition performance of\nalternative lightweight baselines, and outperforms other efficient CNN-based\nmodels at the low FLOP regime from 500M to 6G total FLOPs on various video\nrecognition tasks. It is worth noting that Video Mobile-Former is the first\nTransformer-based video model which constrains the computational budget within\n1G FLOPs.",
    "published": "2022-08-25T17:59:00Z",
    "updated": "2022-08-25T17:59:00Z",
    "authors": [
      "Rui Wang",
      "Zuxuan Wu",
      "Dongdong Chen",
      "Yinpeng Chen",
      "Xiyang Dai",
      "Mengchen Liu",
      "Luowei Zhou",
      "Lu Yuan",
      "Yu-Gang Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.11170v1",
    "title": "Transformer-based Spatial-Temporal Feature Learning for EEG Decoding",
    "summary": "At present, people usually use some methods based on convolutional neural\nnetworks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have\nlimitations in perceiving global dependencies, which is not adequate for common\nEEG paradigms with a strong overall relationship. Regarding this issue, we\npropose a novel EEG decoding method that mainly relies on the attention\nmechanism. The EEG data is firstly preprocessed and spatially filtered. And\nthen, we apply attention transforming on the feature-channel dimension so that\nthe model can enhance more relevant spatial features. The most crucial step is\nto slice the data in the time dimension for attention transforming, and finally\nobtain a highly distinguishable representation. At this time, global averaging\npooling and a simple fully-connected layer are used to classify different\ncategories of EEG data. Experiments on two public datasets indicate that the\nstrategy of attention transforming effectively utilizes spatial and temporal\nfeatures. And we have reached the level of the state-of-the-art in\nmulti-classification of EEG, with fewer parameters. As far as we know, it is\nthe first time that a detailed and complete method based on the transformer\nidea has been proposed in this field. It has good potential to promote the\npracticality of brain-computer interface (BCI). The source code can be found\nat: \\textit{https://github.com/anranknight/EEG-Transformer}.",
    "published": "2021-06-11T00:48:18Z",
    "updated": "2021-06-11T00:48:18Z",
    "authors": [
      "Yonghao Song",
      "Xueyu Jia",
      "Lie Yang",
      "Longhan Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.06076v4",
    "title": "PVT: Point-Voxel Transformer for Point Cloud Learning",
    "summary": "The recently developed pure Transformer architectures have attained promising\naccuracy on point cloud learning benchmarks compared to convolutional neural\nnetworks. However, existing point cloud Transformers are computationally\nexpensive since they waste a significant amount of time on structuring the\nirregular data. To solve this shortcoming, we present Sparse Window Attention\n(SWA) module to gather coarse-grained local features from non-empty voxels,\nwhich not only bypasses the expensive irregular data structuring and invalid\nempty voxel computation, but also obtains linear computational complexity with\nrespect to voxel resolution. Meanwhile, to gather fine-grained features about\nthe global shape, we introduce relative attention (RA) module, a more robust\nself-attention variant for rigid transformations of objects. Equipped with the\nSWA and RA, we construct our neural architecture called PVT that integrates\nboth modules into a joint framework for point cloud learning. Compared with\nprevious Transformer-based and attention-based models, our method attains top\naccuracy of 94.0% on classification benchmark and 10x inference speedup on\naverage. Extensive experiments also valid the effectiveness of PVT on part and\nsemantic segmentation benchmarks (86.6% and 69.2% mIoU, respectively).",
    "published": "2021-08-13T06:07:57Z",
    "updated": "2022-05-25T06:34:21Z",
    "authors": [
      "Cheng Zhang",
      "Haocheng Wan",
      "Xinyi Shen",
      "Zizhao Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.02857v5",
    "title": "PTTR: Relational 3D Point Cloud Object Tracking with Transformer",
    "summary": "In a point cloud sequence, 3D object tracking aims to predict the location\nand orientation of an object in the current search point cloud given a template\npoint cloud. Motivated by the success of transformers, we propose Point\nTracking TRansformer (PTTR), which efficiently predicts high-quality 3D\ntracking results in a coarse-to-fine manner with the help of transformer\noperations. PTTR consists of three novel designs. 1) Instead of random\nsampling, we design Relation-Aware Sampling to preserve relevant points to\ngiven templates during subsampling. 2) Furthermore, we propose a Point Relation\nTransformer (PRT) consisting of a self-attention and a cross-attention module.\nThe global self-attention operation captures long-range dependencies to enhance\nencoded point features for the search area and the template, respectively.\nSubsequently, we generate the coarse tracking results by matching the two sets\nof point features via cross-attention. 3) Based on the coarse tracking results,\nwe employ a novel Prediction Refinement Module to obtain the final refined\nprediction. In addition, we create a large-scale point cloud single object\ntracking benchmark based on the Waymo Open Dataset. Extensive experiments show\nthat PTTR achieves superior point cloud tracking in both accuracy and\nefficiency.",
    "published": "2021-12-06T08:28:05Z",
    "updated": "2022-03-30T05:25:15Z",
    "authors": [
      "Changqing Zhou",
      "Zhipeng Luo",
      "Yueru Luo",
      "Tianrui Liu",
      "Liang Pan",
      "Zhongang Cai",
      "Haiyu Zhao",
      "Shijian Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.16414v1",
    "title": "Surface Vision Transformers: Attention-Based Modelling applied to\n  Cortical Analysis",
    "summary": "The extension of convolutional neural networks (CNNs) to non-Euclidean\ngeometries has led to multiple frameworks for studying manifolds. Many of those\nmethods have shown design limitations resulting in poor modelling of long-range\nassociations, as the generalisation of convolutions to irregular surfaces is\nnon-trivial. Motivated by the success of attention-modelling in computer\nvision, we translate convolution-free vision transformer approaches to surface\ndata, to introduce a domain-agnostic architecture to study any surface data\nprojected onto a spherical manifold. Here, surface patching is achieved by\nrepresenting spherical data as a sequence of triangular patches, extracted from\na subdivided icosphere. A transformer model encodes the sequence of patches via\nsuccessive multi-head self-attention layers while preserving the sequence\nresolution. We validate the performance of the proposed Surface Vision\nTransformer (SiT) on the task of phenotype regression from cortical surface\nmetrics derived from the Developing Human Connectome Project (dHCP).\nExperiments show that the SiT generally outperforms surface CNNs, while\nperforming comparably on registered and unregistered data. Analysis of\ntransformer attention maps offers strong potential to characterise subtle\ncognitive developmental patterns.",
    "published": "2022-03-30T15:56:11Z",
    "updated": "2022-03-30T15:56:11Z",
    "authors": [
      "Simon Dahan",
      "Abdulah Fawaz",
      "Logan Z. J. Williams",
      "Chunhui Yang",
      "Timothy S. Coalson",
      "Matthew F. Glasser",
      "A. David Edwards",
      "Daniel Rueckert",
      "Emma C. Robinson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.10448v1",
    "title": "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection",
    "summary": "The task of action detection aims at deducing both the action category and\nlocalization of the start and end moment for each action instance in a long,\nuntrimmed video. While vision Transformers have driven the recent advances in\nvideo understanding, it is non-trivial to design an efficient architecture for\naction detection due to the prohibitively expensive self-attentions over a long\nsequence of video clips. To this end, we present an efficient hierarchical\nSpatio-Temporal Pyramid Transformer (STPT) for action detection, building upon\nthe fact that the early self-attention layers in Transformers still focus on\nlocal patterns. Specifically, we propose to use local window attention to\nencode rich local spatio-temporal representations in the early stages while\napplying global attention modules to capture long-term space-time dependencies\nin the later stages. In this way, our STPT can encode both locality and\ndependency with largely reduced redundancy, delivering a promising trade-off\nbetween accuracy and efficiency. For example, with only RGB input, the proposed\nSTPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10%\nand performing favorably against state-of-the-art AFSD that uses additional\nflow features with 31% fewer GFLOPs, which serves as an effective and efficient\nend-to-end Transformer-based framework for action detection.",
    "published": "2022-07-21T12:38:05Z",
    "updated": "2022-07-21T12:38:05Z",
    "authors": [
      "Yuetian Weng",
      "Zizheng Pan",
      "Mingfei Han",
      "Xiaojun Chang",
      "Bohan Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.10876v2",
    "title": "An Attention Matrix for Every Decision: Faithfulness-based Arbitration\n  Among Multiple Attention-Based Interpretations of Transformers in Text\n  Classification",
    "summary": "Transformers are widely used in natural language processing, where they\nconsistently achieve state-of-the-art performance. This is mainly due to their\nattention-based architecture, which allows them to model rich linguistic\nrelations between (sub)words. However, transformers are difficult to interpret.\nBeing able to provide reasoning for its decisions is an important property for\na model in domains where human lives are affected. With transformers finding\nwide use in such fields, the need for interpretability techniques tailored to\nthem arises. We propose a new technique that selects the most faithful\nattention-based interpretation among the several ones that can be obtained by\ncombining different head, layer and matrix operations. In addition, two\nvariations are introduced towards (i) reducing the computational complexity,\nthus being faster and friendlier to the environment, and (ii) enhancing the\nperformance in multi-label data. We further propose a new faithfulness metric\nthat is more suitable for transformer models and exhibits high correlation with\nthe area under the precision-recall curve based on ground truth rationales. We\nvalidate the utility of our contributions with a series of quantitative and\nqualitative experiments on seven datasets.",
    "published": "2022-09-22T09:19:22Z",
    "updated": "2022-11-28T11:37:33Z",
    "authors": [
      "Nikolaos Mylonas",
      "Ioannis Mollas",
      "Grigorios Tsoumakas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.08543v1",
    "title": "Demystify Self-Attention in Vision Transformers from a Semantic\n  Perspective: Analysis and Application",
    "summary": "Self-attention mechanisms, especially multi-head self-attention (MSA), have\nachieved great success in many fields such as computer vision and natural\nlanguage processing. However, many existing vision transformer (ViT) works\nsimply inherent transformer designs from NLP to adapt vision tasks, while\nignoring the fundamental difference between ``how MSA works in image and\nlanguage settings''. Language naturally contains highly semantic structures\nthat are directly interpretable by humans. Its basic unit (word) is discrete\nwithout redundant information, which readily supports interpretable studies on\nMSA mechanisms of language transformer. In contrast, visual data exhibits a\nfundamentally different structure: Its basic unit (pixel) is a natural\nlow-level representation with significant redundancies in the neighbourhood,\nwhich poses obvious challenges to the interpretability of MSA mechanism in ViT.\nIn this paper, we introduce a typical image processing technique, i.e.,\nscale-invariant feature transforms (SIFTs), which maps low-level\nrepresentations into mid-level spaces, and annotates extensive discrete\nkeypoints with semantically rich information. Next, we construct a weighted\npatch interrelation analysis based on SIFT keypoints to capture the attention\npatterns hidden in patches with different semantic concentrations\nInterestingly, we find this quantitative analysis is not only an effective\ncomplement to the interpretability of MSA mechanisms in ViT, but can also be\napplied to 1) spurious correlation discovery and ``prompting'' during model\ninference, 2) and guided model pre-training acceleration. Experimental results\non both applications show significant advantages over baselines, demonstrating\nthe efficacy of our method.",
    "published": "2022-11-13T15:18:31Z",
    "updated": "2022-11-13T15:18:31Z",
    "authors": [
      "Leijie Wu",
      "Song Guo",
      "Yaohong Ding",
      "Junxiao Wang",
      "Wenchao Xu",
      "Richard Yida Xu",
      "Jie Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.02031v1",
    "title": "DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks\n  for Image Super-Resolution",
    "summary": "We propose an effective lightweight dynamic local and global self-attention\nnetwork (DLGSANet) to solve image super-resolution. Our method explores the\nproperties of Transformers while having low computational costs. Motivated by\nthe network designs of Transformers, we develop a simple yet effective\nmulti-head dynamic local self-attention (MHDLSA) module to extract local\nfeatures efficiently. In addition, we note that existing Transformers usually\nexplore all similarities of the tokens between the queries and keys for the\nfeature aggregation. However, not all the tokens from the queries are relevant\nto those in keys, using all the similarities does not effectively facilitate\nthe high-resolution image reconstruction. To overcome this problem, we develop\na sparse global self-attention (SparseGSA) module to select the most useful\nsimilarity values so that the most useful global features can be better\nutilized for the high-resolution image reconstruction. We develop a hybrid\ndynamic-Transformer block(HDTB) that integrates the MHDLSA and SparseGSA for\nboth local and global feature exploration. To ease the network training, we\nformulate the HDTBs into a residual hybrid dynamic-Transformer group (RHDTG).\nBy embedding the RHDTGs into an end-to-end trainable network, we show that our\nproposed method has fewer network parameters and lower computational costs\nwhile achieving competitive performance against state-of-the-art ones in terms\nof accuracy. More information is available at\nhttps://neonleexiang.github.io/DLGSANet/",
    "published": "2023-01-05T12:06:47Z",
    "updated": "2023-01-05T12:06:47Z",
    "authors": [
      "Xiang Li",
      "Jinshan Pan",
      "Jinhui Tang",
      "Jiangxin Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.06051v2",
    "title": "DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets",
    "summary": "Designing an efficient yet deployment-friendly 3D backbone to handle sparse\npoint clouds is a fundamental problem in 3D perception. Compared with the\ncustomized sparse convolution, the attention mechanism in Transformers is more\nappropriate for flexibly modeling long-range relationships and is easier to be\ndeployed in real-world applications. However, due to the sparse characteristics\nof point clouds, it is non-trivial to apply a standard transformer on sparse\npoints. In this paper, we present Dynamic Sparse Voxel Transformer (DSVT), a\nsingle-stride window-based voxel Transformer backbone for outdoor 3D\nperception. In order to efficiently process sparse points in parallel, we\npropose Dynamic Sparse Window Attention, which partitions a series of local\nregions in each window according to its sparsity and then computes the features\nof all regions in a fully parallel manner. To allow the cross-set connection,\nwe design a rotated set partitioning strategy that alternates between two\npartitioning configurations in consecutive self-attention layers. To support\neffective downsampling and better encode geometric information, we also propose\nan attention-style 3D pooling module on sparse points, which is powerful and\ndeployment-friendly without utilizing any customized CUDA operations. Our model\nachieves state-of-the-art performance with a broad range of 3D perception\ntasks. More importantly, DSVT can be easily deployed by TensorRT with real-time\ninference speed (27Hz). Code will be available at\n\\url{https://github.com/Haiyang-W/DSVT}.",
    "published": "2023-01-15T09:31:58Z",
    "updated": "2023-03-20T16:36:27Z",
    "authors": [
      "Haiyang Wang",
      "Chen Shi",
      "Shaoshuai Shi",
      "Meng Lei",
      "Sen Wang",
      "Di He",
      "Bernt Schiele",
      "Liwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.12689v2",
    "title": "FIT: Far-reaching Interleaved Transformers",
    "summary": "We present FIT: a transformer-based architecture with efficient\nself-attention and adaptive computation. Unlike original transformers, which\noperate on a single sequence of data tokens, we divide the data tokens into\ngroups, with each group being a shorter sequence of tokens. We employ two types\nof transformer layers: local layers operate on data tokens within each group,\nwhile global layers operate on a smaller set of introduced latent tokens. These\nlayers, comprising the same set of self-attention and feed-forward layers as\nstandard transformers, are interleaved, and cross-attention is used to\nfacilitate information exchange between data and latent tokens within the same\ngroup. The attention complexity is $O(n^2)$ locally within each group of size\n$n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The\nefficiency can be further enhanced by relying more on global layers that\nperform adaptive computation using a smaller set of latent tokens. FIT is a\nversatile architecture and can function as an encoder, diffusion decoder, or\nautoregressive decoder. We provide initial evidence demonstrating its\neffectiveness in high-resolution image understanding and generation tasks.\nNotably, FIT exhibits potential in performing end-to-end training on\ngigabit-scale data, such as 6400$\\times$6400 images, or 160K tokens (after\npatch tokenization), within a memory capacity of 16GB, without requiring\nspecific optimizations or model parallelism.",
    "published": "2023-05-22T03:56:44Z",
    "updated": "2023-05-25T16:27:30Z",
    "authors": [
      "Ting Chen",
      "Lala Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.01705v1",
    "title": "The Information Pathways Hypothesis: Transformers are Dynamic\n  Self-Ensembles",
    "summary": "Transformers use the dense self-attention mechanism which gives a lot of\nflexibility for long-range connectivity. Over multiple layers of a deep\ntransformer, the number of possible connectivity patterns increases\nexponentially. However, very few of these contribute to the performance of the\nnetwork, and even fewer are essential. We hypothesize that there are sparsely\nconnected sub-networks within a transformer, called information pathways which\ncan be trained independently. However, the dynamic (i.e., input-dependent)\nnature of these pathways makes it difficult to prune dense self-attention\nduring training. But the overall distribution of these pathways is often\npredictable. We take advantage of this fact to propose Stochastically\nSubsampled self-Attention (SSA) - a general-purpose training strategy for\ntransformers that can reduce both the memory and computational cost of\nself-attention by 4 to 8 times during training while also serving as a\nregularization method - improving generalization over dense training. We show\nthat an ensemble of sub-models can be formed from the subsampled pathways\nwithin a network, which can achieve better performance than its densely\nattended counterpart. We perform experiments on a variety of NLP, computer\nvision and graph learning tasks in both generative and discriminative settings\nto provide empirical evidence for our claims and show the effectiveness of the\nproposed method.",
    "published": "2023-06-02T17:28:46Z",
    "updated": "2023-06-02T17:28:46Z",
    "authors": [
      "Md Shamim Hussain",
      "Mohammed J. Zaki",
      "Dharmashankar Subramanian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.00752v2",
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "summary": "Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.",
    "published": "2023-12-01T18:01:34Z",
    "updated": "2024-05-31T17:55:27Z",
    "authors": [
      "Albert Gu",
      "Tri Dao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.12138v3",
    "title": "Perceiving Longer Sequences With Bi-Directional Cross-Attention\n  Transformers",
    "summary": "We present a novel bi-directional Transformer architecture (BiXT) which\nscales linearly with input size in terms of computational cost and memory\nconsumption, but does not suffer the drop in performance or limitation to only\none input modality seen with other efficient Transformer-based approaches. BiXT\nis inspired by the Perceiver architectures but replaces iterative attention\nwith an efficient bi-directional cross-attention module in which input tokens\nand latent variables attend to each other simultaneously, leveraging a\nnaturally emerging attention-symmetry between the two. This approach unlocks a\nkey bottleneck experienced by Perceiver-like architectures and enables the\nprocessing and interpretation of both semantics ('what') and location ('where')\nto develop alongside each other over multiple layers -- allowing its direct\napplication to dense and instance-based tasks alike. By combining efficiency\nwith the generality and performance of a full Transformer architecture, BiXT\ncan process longer sequences like point clouds, text or images at higher\nfeature resolutions and achieves competitive performance across a range of\ntasks like point cloud part segmentation, semantic image segmentation, image\nclassification, hierarchical sequence modeling and document retrieval. Our\nexperiments demonstrate that BiXT models outperform larger competitors by\nleveraging longer sequences more efficiently on vision tasks like\nclassification and segmentation, and perform on par with full Transformer\nvariants on sequence modeling and document retrieval -- but require $28\\%$\nfewer FLOPs and are up to $8.4\\times$ faster.",
    "published": "2024-02-19T13:38:15Z",
    "updated": "2024-10-31T06:38:27Z",
    "authors": [
      "Markus Hiller",
      "Krista A. Ehinger",
      "Tom Drummond"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.11026v2",
    "title": "EfficientMorph: Parameter-Efficient Transformer-Based Architecture for\n  3D Image Registration",
    "summary": "Transformers have emerged as the state-of-the-art architecture in medical\nimage registration, outperforming convolutional neural networks (CNNs) by\naddressing their limited receptive fields and overcoming gradient instability\nin deeper models. Despite their success, transformer-based models require\nsubstantial resources for training, including data, memory, and computational\npower, which may restrict their applicability for end users with limited\nresources. In particular, existing transformer-based 3D image registration\narchitectures face two critical gaps that challenge their efficiency and\neffectiveness. Firstly, although window-based attention mechanisms reduce the\nquadratic complexity of full attention by focusing on local regions, they often\nstruggle to effectively integrate both local and global information. Secondly,\nthe granularity of tokenization, a crucial factor in registration accuracy,\npresents a performance trade-off: smaller voxel-size tokens enhance detail\ncapture but come with increased computational complexity, higher memory usage,\nand a greater risk of overfitting. We present \\name, a transformer-based\narchitecture for unsupervised 3D image registration that balances local and\nglobal attention in 3D volumes through a plane-based attention mechanism and\nemploys a Hi-Res tokenization strategy with merging operations, thus capturing\nfiner details without compromising computational efficiency. Notably, \\name\nsets a new benchmark for performance on the OASIS dataset with 16-27x fewer\nparameters. https://github.com/MedVIC-Lab/Efficient_Morph_Registration",
    "published": "2024-03-16T22:01:55Z",
    "updated": "2024-11-28T02:47:37Z",
    "authors": [
      "Abu Zahid Bin Aziz",
      "Mokshagna Sai Teja Karanam",
      "Tushar Kataria",
      "Shireen Y. Elhabian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.02359v1",
    "title": "CVTGAD: Simplified Transformer with Cross-View Attention for\n  Unsupervised Graph-level Anomaly Detection",
    "summary": "Unsupervised graph-level anomaly detection (UGAD) has received remarkable\nperformance in various critical disciplines, such as chemistry analysis and\nbioinformatics. Existing UGAD paradigms often adopt data augmentation\ntechniques to construct multiple views, and then employ different strategies to\nobtain representations from different views for jointly conducting UGAD.\nHowever, most previous works only considered the relationship between\nnodes/graphs from a limited receptive field, resulting in some key structure\npatterns and feature information being neglected. In addition, most existing\nmethods consider different views separately in a parallel manner, which is not\nable to explore the inter-relationship across different views directly. Thus, a\nmethod with a larger receptive field that can explore the inter-relationship\nacross different views directly is in need. In this paper, we propose a novel\nSimplified Transformer with Cross-View Attention for Unsupervised Graph-level\nAnomaly Detection, namely, CVTGAD. To increase the receptive field, we\nconstruct a simplified transformer-based module, exploiting the relationship\nbetween nodes/graphs from both intra-graph and inter-graph perspectives.\nFurthermore, we design a cross-view attention mechanism to directly exploit the\nview co-occurrence between different views, bridging the inter-view gap at node\nlevel and graph level. To the best of our knowledge, this is the first work to\napply transformer and cross attention to UGAD, which realizes graph neural\nnetwork and transformer working collaboratively. Extensive experiments on 15\nreal-world datasets of 3 fields demonstrate the superiority of CVTGAD on the\nUGAD task. The code is available at\n\\url{https://github.com/jindongli-Ai/CVTGAD}.",
    "published": "2024-05-03T03:31:00Z",
    "updated": "2024-05-03T03:31:00Z",
    "authors": [
      "Jindong Li",
      "Qianli Xing",
      "Qi Wang",
      "Yi Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.00882v2",
    "title": "FullTransNet: Full Transformer with Local-Global Attention for Video\n  Summarization",
    "summary": "Video summarization aims to generate a compact, informative, and\nrepresentative synopsis of raw videos, which is crucial for browsing,\nanalyzing, and understanding video content. Dominant approaches in video\nsummarization primarily rely on recurrent or convolutional neural networks, and\nmore recently on encoder-only transformer architectures. However, these methods\ntypically suffer from several limitations in parallelism, modeling long-range\ndependencies, and providing explicit generative capabilities. To address these\nissues, we propose a transformer-like architecture named FullTransNet with\ntwo-fold ideas. First, it uses a full transformer with an encoder-decoder\nstructure as an alternative architecture for video summarization. As the full\ntransformer is specifically designed for sequence transduction tasks, its\ndirect application to video summarization is both intuitive and effective.\nSecond, it replaces the standard full attention mechanism with a combination of\nlocal and global sparse attention, enabling the model to capture long-range\ndependencies while significantly reducing computational costs. This\nlocal-global sparse attention is applied exclusively at the encoder side, where\nthe majority of computations occur, further enhancing efficiency. Extensive\nexperiments on two widely used benchmark datasets, SumMe and TVSum, demonstrate\nthat our model achieves F-scores of 54.4% and 63.9%, respectively, while\nmaintaining relatively low computational and memory requirements. These results\nsurpass the second-best performing methods by 0.1% and 0.3%, respectively,\nverifying the effectiveness and efficiency of FullTransNet.",
    "published": "2025-01-01T16:07:27Z",
    "updated": "2025-08-07T10:00:01Z",
    "authors": [
      "Libin Lan",
      "Lu Jiang",
      "Tianshu Yu",
      "Xiaojuan Liu",
      "Zhongshi He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.01182v2",
    "title": "RingFormer: A Neural Vocoder with Ring Attention and\n  Convolution-Augmented Transformer",
    "summary": "While transformers demonstrate outstanding performance across various audio\ntasks, their application to neural vocoders remains challenging. Neural\nvocoders require the generation of long audio signals at the sample level,\nwhich demands high temporal resolution. This results in significant\ncomputational costs for attention map generation and limits their ability to\nefficiently process both global and local information. Additionally, the\nsequential nature of sample generation in neural vocoders poses difficulties\nfor real-time processing, making the direct adoption of transformers\nimpractical. To address these challenges, we propose RingFormer, a neural\nvocoder that incorporates the ring attention mechanism into a lightweight\ntransformer variant, the convolution-augmented transformer (Conformer). Ring\nattention effectively captures local details while integrating global\ninformation, making it well-suited for processing long sequences and enabling\nreal-time audio generation. RingFormer is trained using adversarial training\nwith two discriminators. The proposed model is applied to the decoder of the\ntext-to-speech model VITS and compared with state-of-the-art vocoders such as\nHiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various\nobjective and subjective metrics. Experimental results show that RingFormer\nachieves comparable or superior performance to existing models, particularly\nexcelling in real-time audio generation. Our code and audio samples are\navailable on GitHub.",
    "published": "2025-01-02T10:18:57Z",
    "updated": "2025-07-19T06:41:48Z",
    "authors": [
      "Seongho Hong",
      "Yong-Hoon Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.19247v2",
    "title": "ProxyTransformation: Preshaping Point Cloud Manifold With Proxy\n  Attention For 3D Visual Grounding",
    "summary": "Embodied intelligence requires agents to interact with 3D environments in\nreal time based on language instructions. A foundational task in this domain is\nego-centric 3D visual grounding. However, the point clouds rendered from RGB-D\nimages retain a large amount of redundant background data and inherent noise,\nboth of which can interfere with the manifold structure of the target regions.\nExisting point cloud enhancement methods often require a tedious process to\nimprove the manifold, which is not suitable for real-time tasks. We propose\nProxy Transformation suitable for multimodal task to efficiently improve the\npoint cloud manifold. Our method first leverages Deformable Point Clustering to\nidentify the point cloud sub-manifolds in target regions. Then, we propose a\nProxy Attention module that utilizes multimodal proxies to guide point cloud\ntransformation. Built upon Proxy Attention, we design a submanifold\ntransformation generation module where textual information globally guides\ntranslation vectors for different submanifolds, optimizing relative spatial\nrelationships of target regions. Simultaneously, image information guides\nlinear transformations within each submanifold, refining the local point cloud\nmanifold of target regions. Extensive experiments demonstrate that Proxy\nTransformation significantly outperforms all existing methods, achieving an\nimpressive improvement of 7.49% on easy targets and 4.60% on hard targets,\nwhile reducing the computational overhead of attention blocks by 40.6%. These\nresults establish a new SOTA in ego-centric 3D visual grounding, showcasing the\neffectiveness and robustness of our approach.",
    "published": "2025-02-26T15:53:41Z",
    "updated": "2025-02-27T09:22:15Z",
    "authors": [
      "Qihang Peng",
      "Henry Zheng",
      "Gao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16744v1",
    "title": "IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular\n  Gesture Classification",
    "summary": "Hand gestures are a primary output of the human motor system, yet the\ndecoding of their neuromuscular signatures remains a bottleneck for basic\nneuroscience and assistive technologies such as prosthetics. Traditional\nhuman-machine interface pipelines rely on a single biosignal modality, but\nmultimodal fusion can exploit complementary information from sensors. We\nsystematically compare linear and attention-based fusion strategies across\nthree architectures: a Multimodal MLP, a Multimodal Transformer, and a\nHierarchical Transformer, evaluating performance on scenarios with unimodal and\nmultimodal inputs. Experiments use two publicly available datasets: NinaPro DB2\n(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).\nAcross both datasets, the Hierarchical Transformer with attention-based fusion\nconsistently achieved the highest accuracy, surpassing the multimodal and best\nsingle-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%\non HD-sEMG. To investigate how modalities interact, we introduce an Isolation\nNetwork that selectively silences unimodal or cross-modal attention pathways,\nquantifying each group of token interactions' contribution to downstream\ndecisions. Ablations reveal that cross-modal interactions contribute\napproximately 30% of the decision signal across transformer layers,\nhighlighting the importance of attention-driven fusion in harnessing\ncomplementary modality information. Together, these findings reveal when and\nhow multimodal fusion would enhance biosignal classification and also provides\nmechanistic insights of human muscle activities. The study would be beneficial\nin the design of sensor arrays for neurorobotic systems.",
    "published": "2025-06-20T04:31:32Z",
    "updated": "2025-06-20T04:31:32Z",
    "authors": [
      "Eion Tyacke",
      "Kunal Gupta",
      "Jay Patel",
      "Rui Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.01013v1",
    "title": "HyFormer-Net: A Synergistic CNN-Transformer with Interpretable\n  Multi-Scale Fusion for Breast Lesion Segmentation and Classification in\n  Ultrasound Images",
    "summary": "B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,\noperator dependency, and indistinct boundaries. Existing deep learning suffers\nfrom single-task learning, architectural constraints (CNNs lack global context,\nTransformers local features), and black-box decision-making. These gaps hinder\nclinical adoption.\n  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous\nsegmentation and classification with intrinsic interpretability. Its\ndual-branch encoder integrates EfficientNet-B3 and Swin Transformer via\nmulti-scale hierarchical fusion blocks. An attention-gated decoder provides\nprecision and explainability. We introduce dual-pipeline interpretability: (1)\nintrinsic attention validation with quantitative IoU verification (mean: 0.86),\nand (2) Grad-CAM for classification reasoning.\n  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and\naccuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant\nRecall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling\nyields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant\nRecall, eliminating false negatives. Ablation studies confirm multi-scale\nfusion contributes +16.8% Dice and attention gates add +5.9%.\n  Crucially, we conduct the first cross-dataset generalization study for hybrid\nCNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),\nconfirming domain shift. However, progressive fine-tuning with only 10%\ntarget-domain data (68 images) recovers 92.5% performance. With 50% data, our\nmodel achieves 77.3% Dice, exceeding source-domain performance (76.1%) and\ndemonstrating true generalization.",
    "published": "2025-11-02T17:06:12Z",
    "updated": "2025-11-02T17:06:12Z",
    "authors": [
      "Mohammad Amanour Rahman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.09573v3",
    "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and\n  Accelerator Co-Design",
    "summary": "Vision Transformers (ViTs) have achieved state-of-the-art performance on\nvarious vision tasks. However, ViTs' self-attention module is still arguably a\nmajor bottleneck, limiting their achievable hardware efficiency. Meanwhile,\nexisting accelerators dedicated to NLP Transformers are not optimal for ViTs.\nThis is because there is a large difference between ViTs and NLP Transformers:\nViTs have a relatively fixed number of input tokens, whose attention maps can\nbe pruned by up to 90% even with fixed sparse patterns; while NLP Transformers\nneed to handle input sequences of varying numbers of tokens and rely on\non-the-fly predictions of dynamic sparse attention patterns for each input to\nachieve a decent sparsity (e.g., >=50%). To this end, we propose a dedicated\nalgorithm and accelerator co-design framework dubbed ViTCoD for accelerating\nViTs. Specifically, on the algorithm level, ViTCoD prunes and polarizes the\nattention maps to have either denser or sparser fixed patterns for regularizing\ntwo levels of workloads without hurting the accuracy, largely reducing the\nattention computations while leaving room for alleviating the remaining\ndominant data movements; on top of that, we further integrate a lightweight and\nlearnable auto-encoder module to enable trading the dominant high-cost data\nmovements for lower-cost computations. On the hardware level, we develop a\ndedicated accelerator to simultaneously coordinate the enforced denser/sparser\nworkloads and encoder/decoder engines for boosted hardware utilization.\nExtensive experiments and ablation studies validate that ViTCoD largely reduces\nthe dominant data movement costs, achieving speedups of up to 235.3x, 142.9x,\n86.0x, 10.1x, and 6.8x over general computing platforms CPUs, EdgeGPUs, GPUs,\nand prior-art Transformer accelerators SpAtten and Sanger under an attention\nsparsity of 90%, respectively.",
    "published": "2022-10-18T04:07:23Z",
    "updated": "2025-03-03T16:58:16Z",
    "authors": [
      "Haoran You",
      "Zhanyi Sun",
      "Huihong Shi",
      "Zhongzhi Yu",
      "Yang Zhao",
      "Yongan Zhang",
      "Chaojian Li",
      "Baopu Li",
      "Yingyan Celine Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.03722v2",
    "title": "GRN-Transformer: Enhancing Motion Artifact Detection in PICU\n  Photoplethysmogram Signals",
    "summary": "This study investigates artifact detection in clinical photoplethysmogram\nsignals using Transformer-based models. Recent findings have shown that in\ndetecting artifacts from the Pediatric Critical Care Unit at CHU Sainte-Justine\n(CHUSJ), semi-supervised learning label propagation and conventional supervised\nmachine learning (K-nearest neighbors) outperform the Transformer-based\nattention mechanism, particularly in limited data scenarios. However, these\nmethods exhibit sensitivity to data volume and limited improvement with\nincreased data availability. We propose the GRN-Transformer, an innovative\nmodel that integrates the Gated Residual Network (GRN) into the Transformer\narchitecture to overcome these limitations. The GRN-Transformer demonstrates\nsuperior performance, achieving remarkable metrics of 98% accuracy, 90%\nprecision, 97% recall, and 93% F1 score, clearly surpassing the Transformer's\nresults of 95% accuracy, 85% precision, 86% recall, and 85% F1 score. By\nintegrating the GRN, which excels at feature extraction, with the Transformer's\nattention mechanism, the proposed GRN-Transformer overcomes the limitations of\nprevious methods. It achieves smoother training and validation loss,\neffectively mitigating overfitting and demonstrating enhanced performance in\nsmall datasets with imbalanced classes. The GRN-Transformer's potential impact\non artifact detection can significantly improve the reliability and accuracy of\nthe clinical decision support system at CHUSJ, ultimately leading to improved\npatient outcomes and safety. Remarkably, the proposed model stands as the\npioneer in its domain, being the first of its kind to detect artifacts from PPG\nsignals. Further research could explore its applicability to other medical\ndomains and datasets with similar constraints.",
    "published": "2023-08-07T16:54:52Z",
    "updated": "2025-05-25T16:29:17Z",
    "authors": [
      "Thanh-Dung Le",
      "Clara Macabiau",
      "KÃ©vin Albert",
      "Philippe Jouvet",
      "Rita Noumeir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.08868v2",
    "title": "B-Cos Aligned Transformers Learn Human-Interpretable Features",
    "summary": "Vision Transformers (ViTs) and Swin Transformers (Swin) are currently\nstate-of-the-art in computational pathology. However, domain experts are still\nreluctant to use these models due to their lack of interpretability. This is\nnot surprising, as critical decisions need to be transparent and\nunderstandable. The most common approach to understanding transformers is to\nvisualize their attention. However, attention maps of ViTs are often\nfragmented, leading to unsatisfactory explanations. Here, we introduce a novel\narchitecture called the B-cos Vision Transformer (BvT) that is designed to be\nmore interpretable. It replaces all linear transformations with the B-cos\ntransform to promote weight-input alignment. In a blinded study, medical\nexperts clearly ranked BvTs above ViTs, suggesting that our network is better\nat capturing biomedically relevant structures. This is also true for the B-cos\nSwin Transformer (Bwin). Compared to the Swin Transformer, it even improves the\nF1-score by up to 4.7% on two public datasets.",
    "published": "2024-01-16T22:46:29Z",
    "updated": "2024-01-18T07:14:00Z",
    "authors": [
      "Manuel Tran",
      "Amal Lahiani",
      "Yashin Dicente Cid",
      "Melanie Boxberg",
      "Peter Lienemann",
      "Christian Matek",
      "Sophia J. Wagner",
      "Fabian J. Theis",
      "Eldad Klaiman",
      "Tingying Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.15520v1",
    "title": "GTC: GNN-Transformer Co-contrastive Learning for Self-supervised\n  Heterogeneous Graph Representation",
    "summary": "Graph Neural Networks (GNNs) have emerged as the most powerful weapon for\nvarious graph tasks due to the message-passing mechanism's great local\ninformation aggregation ability. However, over-smoothing has always hindered\nGNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs,\nTransformers can model global information and multi-hop interactions via\nmulti-head self-attention and a proper Transformer structure can show more\nimmunity to the over-smoothing problem. So, can we propose a novel framework to\ncombine GNN and Transformer, integrating both GNN's local information\naggregation and Transformer's global information modeling ability to eliminate\nthe over-smoothing problem? To realize this, this paper proposes a\ncollaborative learning scheme for GNN-Transformer and constructs GTC\narchitecture. GTC leverages the GNN and Transformer branch to encode node\ninformation from different views respectively, and establishes contrastive\nlearning tasks based on the encoded cross-view information to realize\nself-supervised heterogeneous graph representation. For the Transformer branch,\nwe propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with\nGNN to attentively encode neighborhood information from different levels. As\nfar as we know, this is the first attempt in the field of graph representation\nlearning to utilize both GNN and Transformer to collaboratively capture\ndifferent view information and conduct cross-view contrastive learning. The\nexperiments on real datasets show that GTC exhibits superior performance\ncompared with state-of-the-art methods. Codes can be available at\nhttps://github.com/PHD-lanyu/GTC.",
    "published": "2024-03-22T12:22:44Z",
    "updated": "2024-03-22T12:22:44Z",
    "authors": [
      "Yundong Sun",
      "Dongjie Zhu",
      "Yansong Wang",
      "Zhaoshuo Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.03192v4",
    "title": "A Survey of Quantum Transformers: Architectures, Challenges and Outlooks",
    "summary": "Quantum Transformers integrate the representational power of classical\nTransformers with the computational advantages of quantum computing. Since\n2022, research in this area has rapidly expanded, giving rise to diverse\ntechnical paradigms and early applications. To address the growing need for\nconsolidation, this paper presents the first comprehensive, systematic, and\nin-depth survey of quantum Transformer models. First, we delineate the research\nscope, focusing on improving Transformer parts with quantum methods, and\nintroduce foundational concepts in classical Transformers and quantum machine\nlearning. Then we organize existing studies into two main paradigms: PQC-based\nand QLA-based, with PQC-based paradigm further divided into QKV-only Quantum\nMapping, Quantum Pairwise Attention, Quantum Holistic Attention. and\nQuantum-Assisted Optimization, analyzing their core mechanisms and\narchitectural traits. We also summarize empirical results that demonstrate\npreliminary quantum advantages, especially on small-scale tasks or\nresource-constrained settings. Following this, we examine key technical\nchallenges, such as complexity-resource trade-offs, scalability and\ngeneralization limitations, and trainability issues including barren plateaus,\nand provide potential solutions, including quantumizing classical transformer\nvariants with lower complexity, hybrid designs, and improved optimization\nstrategies. Finally, we propose several promising future directions, e.g.,\nscaling quantum modules into large architectures, applying quantum Transformers\nto domains with inherently quantum data (e.g., physics, chemistry), and\ndeveloping theory-driven designs grounded in quantum information science. This\nsurvey will help researchers and practitioners quickly grasp the overall\nlandscape of current quantum Transformer research and promote future\ndevelopments in this emerging field.",
    "published": "2025-04-04T05:40:18Z",
    "updated": "2025-06-30T06:02:23Z",
    "authors": [
      "Hui Zhang",
      "Qinglin Zhao",
      "Mengchu Zhou",
      "Li Feng",
      "Dusit Niyato",
      "Shenggen Zheng",
      "Lin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1709.05769v1",
    "title": "Where to Focus: Deep Attention-based Spatially Recurrent Bilinear\n  Networks for Fine-Grained Visual Recognition",
    "summary": "Fine-grained visual recognition typically depends on modeling subtle\ndifference from object parts. However, these parts often exhibit dramatic\nvisual variations such as occlusions, viewpoints, and spatial transformations,\nmaking it hard to detect. In this paper, we present a novel attention-based\nmodel to automatically, selectively and accurately focus on critical object\nregions with higher importance against appearance variations. Given an image,\ntwo different Convolutional Neural Networks (CNNs) are constructed, where the\noutputs of two CNNs are correlated through bilinear pooling to simultaneously\nfocus on discriminative regions and extract relevant features. To capture\nspatial distributions among the local regions with visual attention, soft\nattention based spatial Long-Short Term Memory units (LSTMs) are incorporated\nto realize spatially recurrent yet visually selective over local input\npatterns. All the above intuitions equip our network with the following novel\nmodel: two-stream CNN layers, bilinear pooling layer, spatial recurrent layer\nwith location attention are jointly trained via an end-to-end fashion to serve\nas the part detector and feature extractor, whereby relevant features are\nlocalized and extracted attentively. We show the significance of our network\nagainst two well-known visual recognition tasks: fine-grained image\nclassification and person re-identification.",
    "published": "2017-09-18T03:56:08Z",
    "updated": "2017-09-18T03:56:08Z",
    "authors": [
      "Lin Wu",
      "Yang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1907.04197v1",
    "title": "Attending to Emotional Narratives",
    "summary": "Attention mechanisms in deep neural networks have achieved excellent\nperformance on sequence-prediction tasks. Here, we show that these\nrecently-proposed attention-based mechanisms---in particular, the Transformer\nwith its parallelizable self-attention layers, and the Memory Fusion Network\nwith attention across modalities and time---also generalize well to multimodal\ntime-series emotion recognition. Using a recently-introduced dataset of\nemotional autobiographical narratives, we adapt and apply these two attention\nmechanisms to predict emotional valence over time. Our models perform extremely\nwell, in some cases reaching a performance comparable with human raters. We end\nwith a discussion of the implications of attention mechanisms to affective\ncomputing.",
    "published": "2019-07-08T03:50:43Z",
    "updated": "2019-07-08T03:50:43Z",
    "authors": [
      "Zhengxuan Wu",
      "Xiyu Zhang",
      "Tan Zhi-Xuan",
      "Jamil Zaki",
      "Desmond C. Ong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.08708v1",
    "title": "Adaptive Attention Span in Computer Vision",
    "summary": "Recent developments in Transformers for language modeling have opened new\nareas of research in computer vision. Results from late 2019 showed vast\nperformance increases in both object detection and recognition when\nconvolutions are replaced by local self-attention kernels. Models using local\nself-attention kernels were also shown to have less parameters and FLOPS\ncompared to equivalent architectures that only use convolutions. In this work\nwe propose a novel method for learning the local self-attention kernel size. We\nthen compare its performance to fixed-size local attention and convolution\nkernels. The code for all our experiments and models is available at\nhttps://github.com/JoeRoussy/adaptive-attention-in-cv",
    "published": "2020-04-18T21:32:47Z",
    "updated": "2020-04-18T21:32:47Z",
    "authors": [
      "Jerrod Parker",
      "Shakti Kumar",
      "Joe Roussy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.12366v1",
    "title": "Multi-Head Self-Attention with Role-Guided Masks",
    "summary": "The state of the art in learning meaningful semantic representations of words\nis the Transformer model and its attention mechanisms. Simply put, the\nattention mechanisms learn to attend to specific parts of the input dispensing\nrecurrence and convolutions. While some of the learned attention heads have\nbeen found to play linguistically interpretable roles, they can be redundant or\nprone to errors. We propose a method to guide the attention heads towards roles\nidentified in prior work as important. We do this by defining role-specific\nmasks to constrain the heads to attend to specific parts of the input, such\nthat different heads are designed to play different roles. Experiments on text\nclassification and machine translation using 7 different datasets show that our\nmethod outperforms competitive attention-based, CNN, and RNN baselines.",
    "published": "2020-12-22T21:34:02Z",
    "updated": "2020-12-22T21:34:02Z",
    "authors": [
      "Dongsheng Wang",
      "Casper Hansen",
      "Lucas Chaves Lima",
      "Christian Hansen",
      "Maria Maistro",
      "Jakob Grue Simonsen",
      "Christina Lioma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.03404v2",
    "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly\n  Exponentially with Depth",
    "summary": "Attention-based architectures have become ubiquitous in machine learning, yet\nour understanding of the reasons for their effectiveness remains limited. This\nwork proposes a new way to understand self-attention networks: we show that\ntheir output can be decomposed into a sum of smaller terms, each involving the\noperation of a sequence of attention heads across layers. Using this\ndecomposition, we prove that self-attention possesses a strong inductive bias\ntowards \"token uniformity\". Specifically, without skip connections or\nmulti-layer perceptrons (MLPs), the output converges doubly exponentially to a\nrank-1 matrix. On the other hand, skip connections and MLPs stop the output\nfrom degeneration. Our experiments verify the identified convergence phenomena\non different variants of standard transformer architectures.",
    "published": "2021-03-05T00:39:05Z",
    "updated": "2023-08-01T14:27:08Z",
    "authors": [
      "Yihe Dong",
      "Jean-Baptiste Cordonnier",
      "Andreas Loukas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1905.10650v3",
    "title": "Are Sixteen Heads Really Better than One?",
    "summary": "Attention is a powerful and ubiquitous mechanism for allowing neural models\nto focus on particular salient pieces of information by taking their weighted\naverage when making predictions. In particular, multi-headed attention is a\ndriving force behind many recent state-of-the-art NLP models such as\nTransformer-based MT models and BERT. These models apply multiple attention\nmechanisms in parallel, with each attention \"head\" potentially focusing on\ndifferent parts of the input, which makes it possible to express sophisticated\nfunctions beyond the simple weighted average. In this paper we make the\nsurprising observation that even if models have been trained using multiple\nheads, in practice, a large percentage of attention heads can be removed at\ntest time without significantly impacting performance. In fact, some layers can\neven be reduced to a single head. We further examine greedy algorithms for\npruning down models, and the potential speed, memory efficiency, and accuracy\nimprovements obtainable therefrom. Finally, we analyze the results with respect\nto which parts of the model are more reliant on having multiple heads, and\nprovide precursory evidence that training dynamics play a role in the gains\nprovided by multi-head attention.",
    "published": "2019-05-25T18:27:28Z",
    "updated": "2019-11-04T15:49:01Z",
    "authors": [
      "Paul Michel",
      "Omer Levy",
      "Graham Neubig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.02849v1",
    "title": "A Bird's-Eye Tutorial of Graph Attention Architectures",
    "summary": "Graph Neural Networks (GNNs) have shown tremendous strides in performance for\ngraph-structured problems especially in the domains of natural language\nprocessing, computer vision and recommender systems. Inspired by the success of\nthe transformer architecture, there has been an ever-growing body of work on\nattention variants of GNNs attempting to advance the state of the art in many\nof these problems. Incorporating \"attention\" into graph mining has been viewed\nas a way to overcome the noisiness, heterogenity and complexity associated with\ngraph-structured data as well as to encode soft-inductive bias. It is hence\ncrucial and advantageous to study these variants from a bird's-eye view to\nassess their strengths and weaknesses. We provide a systematic and focused\ntutorial centered around attention based GNNs in a hope to benefit researchers\ndealing with graph-structured problems. Our tutorial looks at GNN variants from\nthe point of view of the attention function and iteratively builds the reader's\nunderstanding of different graph attention variants.",
    "published": "2022-06-06T18:47:48Z",
    "updated": "2022-06-06T18:47:48Z",
    "authors": [
      "Kaustubh D. Dhole",
      "Carl Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.14550v1",
    "title": "SALO: An Efficient Spatial Accelerator Enabling Hybrid Sparse Attention\n  Mechanisms for Long Sequences",
    "summary": "The attention mechanisms of transformers effectively extract pertinent\ninformation from the input sequence. However, the quadratic complexity of\nself-attention w.r.t the sequence length incurs heavy computational and memory\nburdens, especially for tasks with long sequences. Existing accelerators face\nperformance degradation in these tasks. To this end, we propose SALO to enable\nhybrid sparse attention mechanisms for long sequences. SALO contains a data\nscheduler to map hybrid sparse attention patterns onto hardware and a spatial\naccelerator to perform the efficient attention computation. We show that SALO\nachieves 17.66x and 89.33x speedup on average compared to GPU and CPU\nimplementations, respectively, on typical workloads, i.e., Longformer and ViL.",
    "published": "2022-06-29T12:01:19Z",
    "updated": "2022-06-29T12:01:19Z",
    "authors": [
      "Guan Shen",
      "Jieru Zhao",
      "Quan Chen",
      "Jingwen Leng",
      "Chao Li",
      "Minyi Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.00941v2",
    "title": "ParaFormer: Parallel Attention Transformer for Efficient Feature\n  Matching",
    "summary": "Heavy computation is a bottleneck limiting deep-learningbased feature\nmatching algorithms to be applied in many realtime applications. However,\nexisting lightweight networks optimized for Euclidean data cannot address\nclassical feature matching tasks, since sparse keypoint based descriptors are\nexpected to be matched. This paper tackles this problem and proposes two\nconcepts: 1) a novel parallel attention model entitled ParaFormer and 2) a\ngraph based U-Net architecture with attentional pooling. First, ParaFormer\nfuses features and keypoint positions through the concept of amplitude and\nphase, and integrates self- and cross-attention in a parallel manner which\nachieves a win-win performance in terms of accuracy and efficiency. Second,\nwith U-Net architecture and proposed attentional pooling, the ParaFormer-U\nvariant significantly reduces computational complexity, and minimize\nperformance loss caused by downsampling. Sufficient experiments on various\napplications, including homography estimation, pose estimation, and image\nmatching, demonstrate that ParaFormer achieves state-of-the-art performance\nwhile maintaining high efficiency. The efficient ParaFormer-U variant achieves\ncomparable performance with less than 50% FLOPs of the existing attention-based\nmodels.",
    "published": "2023-03-02T03:29:16Z",
    "updated": "2023-03-10T02:52:47Z",
    "authors": [
      "Xiaoyong Lu",
      "Yaping Yan",
      "Bin Kang",
      "Songlin Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.10976v2",
    "title": "Attention Disturbance and Dual-Path Constraint Network for Occluded\n  Person Re-identification",
    "summary": "Occluded person re-identification (Re-ID) aims to address the potential\nocclusion problem when matching occluded or holistic pedestrians from different\ncamera views. Many methods use the background as artificial occlusion and rely\non attention networks to exclude noisy interference. However, the significant\ndiscrepancy between simple background occlusion and realistic occlusion can\nnegatively impact the generalization of the network. To address this issue, we\npropose a novel transformer-based Attention Disturbance and Dual-Path\nConstraint Network (ADP) to enhance the generalization of attention networks.\nFirstly, to imitate real-world obstacles, we introduce an Attention Disturbance\nMask (ADM) module that generates an offensive noise, which can distract\nattention like a realistic occluder, as a more complex form of occlusion.\nSecondly, to fully exploit these complex occluded images, we develop a\nDual-Path Constraint Module (DPC) that can obtain preferable supervision\ninformation from holistic images through dual-path interaction. With our\nproposed method, the network can effectively circumvent a wide variety of\nocclusions using the basic ViT baseline. Comprehensive experimental evaluations\nconducted on person re-ID benchmarks demonstrate the superiority of ADP over\nstate-of-the-art methods.",
    "published": "2023-03-20T09:56:35Z",
    "updated": "2024-02-22T08:24:45Z",
    "authors": [
      "Jiaer Xia",
      "Lei Tan",
      "Pingyang Dai",
      "Mingbo Zhao",
      "Yongjian Wu",
      "Liujuan Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1903.08788v2",
    "title": "Selective Attention for Context-aware Neural Machine Translation",
    "summary": "Despite the progress made in sentence-level NMT, current systems still fall\nshort at achieving fluent, good quality translation for a full document. Recent\nworks in context-aware NMT consider only a few previous sentences as context\nand may not scale to entire documents. To this end, we propose a novel and\nscalable top-down approach to hierarchical attention for context-aware NMT\nwhich uses sparse attention to selectively focus on relevant sentences in the\ndocument context and then attends to key words in those sentences. We also\npropose single-level attention approaches based on sentence or word-level\ninformation in the context. The document-level context representation, produced\nfrom these attention modules, is integrated into the encoder or decoder of the\nTransformer model depending on whether we use monolingual or bilingual context.\nOur experiments and evaluation on English-German datasets in different document\nMT settings show that our selective attention approach not only significantly\noutperforms context-agnostic baselines but also surpasses context-aware\nbaselines in most cases.",
    "published": "2019-03-21T01:01:22Z",
    "updated": "2019-05-24T03:47:14Z",
    "authors": [
      "Sameen Maruf",
      "AndrÃ© F. T. Martins",
      "Gholamreza Haffari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.05556v2",
    "title": "Sparse and Structured Visual Attention",
    "summary": "Visual attention mechanisms are widely used in multimodal tasks, as visual\nquestion answering (VQA). One drawback of softmax-based attention mechanisms is\nthat they assign some probability mass to all image regions, regardless of\ntheir adjacency structure and of their relevance to the text. In this paper, to\nbetter link the image structure with the text, we replace the traditional\nsoftmax attention mechanism with two alternative sparsity-promoting\ntransformations: sparsemax, which is able to select only the relevant regions\n(assigning zero weight to the rest), and a newly proposed Total-Variation\nSparse Attention (TVmax), which further encourages the joint selection of\nadjacent spatial locations. Experiments in VQA show gains in accuracy as well\nas higher similarity to human attention, which suggests better\ninterpretability.",
    "published": "2020-02-13T15:08:12Z",
    "updated": "2021-07-08T12:39:43Z",
    "authors": [
      "Pedro Henrique Martins",
      "Vlad Niculae",
      "Zita Marinho",
      "AndrÃ© Martins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.07338v1",
    "title": "Conditional Self-Attention for Query-based Summarization",
    "summary": "Self-attention mechanisms have achieved great success on a variety of NLP\ntasks due to its flexibility of capturing dependency between arbitrary\npositions in a sequence. For problems such as query-based summarization (Qsumm)\nand knowledge graph reasoning where each input sequence is associated with an\nextra query, explicitly modeling such conditional contextual dependencies can\nlead to a more accurate solution, which however cannot be captured by existing\nself-attention mechanisms. In this paper, we propose \\textit{conditional\nself-attention} (CSA), a neural network module designed for conditional\ndependency modeling. CSA works by adjusting the pairwise attention between\ninput tokens in a self-attention module with the matching score of the inputs\nto the given query. Thereby, the contextual dependencies modeled by CSA will be\nhighly relevant to the query. We further studied variants of CSA defined by\ndifferent types of attention. Experiments on Debatepedia and HotpotQA benchmark\ndatasets show CSA consistently outperforms vanilla Transformer and previous\nmodels for the Qsumm problem.",
    "published": "2020-02-18T02:22:31Z",
    "updated": "2020-02-18T02:22:31Z",
    "authors": [
      "Yujia Xie",
      "Tianyi Zhou",
      "Yi Mao",
      "Weizhu Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.04710v2",
    "title": "The Lipschitz Constant of Self-Attention",
    "summary": "Lipschitz constants of neural networks have been explored in various contexts\nin deep learning, such as provable adversarial robustness, estimating\nWasserstein distance, stabilising training of GANs, and formulating invertible\nneural networks. Such works have focused on bounding the Lipschitz constant of\nfully connected or convolutional networks, composed of linear maps and\npointwise non-linearities. In this paper, we investigate the Lipschitz constant\nof self-attention, a non-linear neural network module widely used in sequence\nmodelling. We prove that the standard dot-product self-attention is not\nLipschitz for unbounded input domain, and propose an alternative L2\nself-attention that is Lipschitz. We derive an upper bound on the Lipschitz\nconstant of L2 self-attention and provide empirical evidence for its asymptotic\ntightness. To demonstrate the practical relevance of our theoretical work, we\nformulate invertible self-attention and use it in a Transformer-based\narchitecture for a character-level language modelling task.",
    "published": "2020-06-08T16:08:38Z",
    "updated": "2021-06-09T14:13:40Z",
    "authors": [
      "Hyunjik Kim",
      "George Papamakarios",
      "Andriy Mnih"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.00943v2",
    "title": "How Far Does BERT Look At:Distance-based Clustering and Analysis of\n  BERT$'$s Attention",
    "summary": "Recent research on the multi-head attention mechanism, especially that in\npre-trained models such as BERT, has shown us heuristics and clues in analyzing\nvarious aspects of the mechanism. As most of the research focus on probing\ntasks or hidden states, previous works have found some primitive patterns of\nattention head behavior by heuristic analytical methods, but a more systematic\nanalysis specific on the attention patterns still remains primitive. In this\nwork, we clearly cluster the attention heatmaps into significantly different\npatterns through unsupervised clustering on top of a set of proposed features,\nwhich corroborates with previous observations. We further study their\ncorresponding functions through analytical study. In addition, our proposed\nfeatures can be used to explain and calibrate different attention heads in\nTransformer models.",
    "published": "2020-11-02T12:52:31Z",
    "updated": "2020-11-03T04:25:12Z",
    "authors": [
      "Yue Guan",
      "Jingwen Leng",
      "Chao Li",
      "Quan Chen",
      "Minyi Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.10277v1",
    "title": "Revisiting Linformer with a modified self-attention with linear\n  complexity",
    "summary": "Although Transformer models such as Google's BERT and OpenAI's GPT-3 are\nsuccessful in many natural language processing tasks, training and deploying\nthese models are costly and inefficient.Even if pre-trained models are used,\ndeploying these models still remained a challenge due to their large size.\nApart from deployment, these models take higher time during inference\nrestricting user-friendliness. The main bottleneck is self-attention which uses\nquadratic time and space with respect to the sequence length. In order to\nreduce the quadratic time complexity of the self-attention mechanism, Linformer\nby Facebook's AI research team was introduced where they showed that the\nself-attention mechanism can be approximated by a low-rank matrix and\nexploiting this finding, a new method for self-attention with linear time and\nspace complexity was proposed by them. In the Linformer, the time complexity\ndepends on the projection mapping dimension which acts as a hyperparameter and\naffects the performance of the model, tuning this hyperparameter can be\ntime-consuming. In this paper, I proposed an alternative method for\nself-attention with linear complexity in time and space and is independent of\nthe projection mapping dimension. Since this method works for long sequences\nthis can be used for images as well as audios.",
    "published": "2020-12-16T13:23:29Z",
    "updated": "2020-12-16T13:23:29Z",
    "authors": [
      "Madhusudan Verma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.10927v1",
    "title": "Attention Can Reflect Syntactic Structure (If You Let It)",
    "summary": "Since the popularization of the Transformer as a general-purpose feature\nencoder for NLP, many studies have attempted to decode linguistic structure\nfrom its novel multi-head attention mechanism. However, much of such work\nfocused almost exclusively on English -- a language with rigid word order and a\nlack of inflectional morphology. In this study, we present decoding experiments\nfor multilingual BERT across 18 languages in order to test the generalizability\nof the claim that dependency syntax is reflected in attention patterns. We show\nthat full trees can be decoded above baseline accuracy from single attention\nheads, and that individual relations are often tracked by the same heads across\nlanguages. Furthermore, in an attempt to address recent debates about the\nstatus of attention as an explanatory mechanism, we experiment with fine-tuning\nmBERT on a supervised parsing objective while freezing different series of\nparameters. Interestingly, in steering the objective to learn explicit\nlinguistic structure, we find much of the same structure represented in the\nresulting attention patterns, with interesting differences with respect to\nwhich parameters are frozen.",
    "published": "2021-01-26T16:49:16Z",
    "updated": "2021-01-26T16:49:16Z",
    "authors": [
      "Vinit Ravishankar",
      "Artur Kulmizev",
      "Mostafa Abdou",
      "Anders SÃ¸gaard",
      "Joakim Nivre"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.05095v4",
    "title": "Is Space-Time Attention All You Need for Video Understanding?",
    "summary": "We present a convolution-free approach to video classification built\nexclusively on self-attention over space and time. Our method, named\n\"TimeSformer,\" adapts the standard Transformer architecture to video by\nenabling spatiotemporal feature learning directly from a sequence of\nframe-level patches. Our experimental study compares different self-attention\nschemes and suggests that \"divided attention,\" where temporal attention and\nspatial attention are separately applied within each block, leads to the best\nvideo classification accuracy among the design choices considered. Despite the\nradically new design, TimeSformer achieves state-of-the-art results on several\naction recognition benchmarks, including the best reported accuracy on\nKinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks,\nour model is faster to train, it can achieve dramatically higher test\nefficiency (at a small drop in accuracy), and it can also be applied to much\nlonger video clips (over one minute long). Code and models are available at:\nhttps://github.com/facebookresearch/TimeSformer.",
    "published": "2021-02-09T19:49:33Z",
    "updated": "2021-06-09T14:48:13Z",
    "authors": [
      "Gedas Bertasius",
      "Heng Wang",
      "Lorenzo Torresani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.10406v1",
    "title": "Discrete-continuous Action Space Policy Gradient-based Attention for\n  Image-Text Matching",
    "summary": "Image-text matching is an important multi-modal task with massive\napplications. It tries to match the image and the text with similar semantic\ninformation. Existing approaches do not explicitly transform the different\nmodalities into a common space. Meanwhile, the attention mechanism which is\nwidely used in image-text matching models does not have supervision. We propose\na novel attention scheme which projects the image and text embedding into a\ncommon space and optimises the attention weights directly towards the\nevaluation metrics. The proposed attention scheme can be considered as a kind\nof supervised attention and requiring no additional annotations. It is trained\nvia a novel Discrete-continuous action space policy gradient algorithm, which\nis more effective in modelling complex action space than previous continuous\naction space policy gradient. We evaluate the proposed methods on two\nwidely-used benchmark datasets: Flickr30k and MS-COCO, outperforming the\nprevious approaches by a large margin.",
    "published": "2021-04-21T08:34:22Z",
    "updated": "2021-04-21T08:34:22Z",
    "authors": [
      "Shiyang Yan",
      "Li Yu",
      "Yuan Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14600v1",
    "title": "HIT: A Hierarchically Fused Deep Attention Network for Robust Code-mixed\n  Language Representation",
    "summary": "Understanding linguistics and morphology of resource-scarce code-mixed texts\nremains a key challenge in text processing. Although word embedding comes in\nhandy to support downstream tasks for low-resource languages, there are plenty\nof scopes in improving the quality of language representation particularly for\ncode-mixed languages. In this paper, we propose HIT, a robust representation\nlearning method for code-mixed texts. HIT is a hierarchical transformer-based\nframework that captures the semantic relationship among words and\nhierarchically learns the sentence-level semantics using a fused attention\nmechanism. HIT incorporates two attention modules, a multi-headed\nself-attention and an outer product attention module, and computes their\nweighted sum to obtain the attention weights. Our evaluation of HIT on one\nEuropean (Spanish) and five Indic (Hindi, Bengali, Tamil, Telugu, and\nMalayalam) languages across four NLP tasks on eleven datasets suggests\nsignificant performance improvement against various state-of-the-art systems.\nWe further show the adaptability of learned representation across tasks in a\ntransfer learning setup (with and without fine-tuning).",
    "published": "2021-05-30T18:53:33Z",
    "updated": "2021-05-30T18:53:33Z",
    "authors": [
      "Ayan Sengupta",
      "Sourabh Kumar Bhattacharjee",
      "Tanmoy Chakraborty",
      "Md Shad Akhtar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.05505v1",
    "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in\n  Pre-trained Language Models",
    "summary": "In this paper, we detail the relationship between convolutions and\nself-attention in natural language tasks. We show that relative position\nembeddings in self-attention layers are equivalent to recently-proposed dynamic\nlightweight convolutions, and we consider multiple new ways of integrating\nconvolutions into Transformer self-attention. Specifically, we propose\ncomposite attention, which unites previous relative position embedding methods\nunder a convolutional framework. We conduct experiments by training BERT with\ncomposite attention, finding that convolutions consistently improve performance\non multiple downstream tasks, replacing absolute position embeddings. To inform\nfuture work, we present results comparing lightweight convolutions, dynamic\nconvolutions, and depthwise-separable convolutions in language model\npre-training, considering multiple injection points for convolutions in\nself-attention layers.",
    "published": "2021-06-10T05:11:35Z",
    "updated": "2021-06-10T05:11:35Z",
    "authors": [
      "Tyler A. Chang",
      "Yifan Xu",
      "Weijian Xu",
      "Zhuowen Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.00860v1",
    "title": "Implicit and Explicit Attention for Zero-Shot Learning",
    "summary": "Most of the existing Zero-Shot Learning (ZSL) methods focus on learning a\ncompatibility function between the image representation and class attributes.\nFew others concentrate on learning image representation combining local and\nglobal features. However, the existing approaches still fail to address the\nbias issue towards the seen classes. In this paper, we propose implicit and\nexplicit attention mechanisms to address the existing bias problem in ZSL\nmodels. We formulate the implicit attention mechanism with a self-supervised\nimage angle rotation task, which focuses on specific image features aiding to\nsolve the task. The explicit attention mechanism is composed with the\nconsideration of a multi-headed self-attention mechanism via Vision Transformer\nmodel, which learns to map image features to semantic space during the training\nstage. We conduct comprehensive experiments on three popular benchmarks: AWA2,\nCUB and SUN. The performance of our proposed attention mechanisms has proved\nits effectiveness, and has achieved the state-of-the-art harmonic mean on all\nthe three datasets.",
    "published": "2021-10-02T18:06:21Z",
    "updated": "2021-10-02T18:06:21Z",
    "authors": [
      "Faisal Alamri",
      "Anjan Dutta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.08192v1",
    "title": "Attention meets Geometry: Geometry Guided Spatial-Temporal Attention for\n  Consistent Self-Supervised Monocular Depth Estimation",
    "summary": "Inferring geometrically consistent dense 3D scenes across a tuple of\ntemporally consecutive images remains challenging for self-supervised monocular\ndepth prediction pipelines. This paper explores how the increasingly popular\ntransformer architecture, together with novel regularized loss formulations,\ncan improve depth consistency while preserving accuracy. We propose a spatial\nattention module that correlates coarse depth predictions to aggregate local\ngeometric information. A novel temporal attention mechanism further processes\nthe local geometric information in a global context across consecutive images.\nAdditionally, we introduce geometric constraints between frames regularized by\nphotometric cycle consistency. By combining our proposed regularization and the\nnovel spatial-temporal-attention module we fully leverage both the geometric\nand appearance-based consistency across monocular frames. This yields\ngeometrically meaningful attention and improves temporal depth stability and\naccuracy compared to previous methods.",
    "published": "2021-10-15T16:43:31Z",
    "updated": "2021-10-15T16:43:31Z",
    "authors": [
      "Patrick Ruhkamp",
      "Daoyi Gao",
      "Hanzhi Chen",
      "Nassir Navab",
      "Benjamin Busam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.13639v1",
    "title": "Give Me Your Attention: Dot-Product Attention Considered Harmful for\n  Adversarial Patch Robustness",
    "summary": "Neural architectures based on attention such as vision transformers are\nrevolutionizing image recognition. Their main benefit is that attention allows\nreasoning about all parts of a scene jointly. In this paper, we show how the\nglobal reasoning of (scaled) dot-product attention can be the source of a major\nvulnerability when confronted with adversarial patch attacks. We provide a\ntheoretical understanding of this vulnerability and relate it to an adversary's\nability to misdirect the attention of all queries to a single key token under\nthe control of the adversarial patch. We propose novel adversarial objectives\nfor crafting adversarial patches which target this vulnerability explicitly. We\nshow the effectiveness of the proposed patch attacks on popular image\nclassification (ViTs and DeiTs) and object detection models (DETR). We find\nthat adversarial patches occupying 0.5% of the input can lead to robust\naccuracies as low as 0% for ViT on ImageNet, and reduce the mAP of DETR on MS\nCOCO to less than 3%.",
    "published": "2022-03-25T13:26:27Z",
    "updated": "2022-03-25T13:26:27Z",
    "authors": [
      "Giulio Lovisotto",
      "Nicole Finnie",
      "Mauricio Munoz",
      "Chaithanya Kumar Mummadi",
      "Jan Hendrik Metzen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.09630v2",
    "title": "Acceptability Judgements via Examining the Topology of Attention Maps",
    "summary": "The role of the attention mechanism in encoding linguistic knowledge has\nreceived special interest in NLP. However, the ability of the attention heads\nto judge the grammatical acceptability of a sentence has been underexplored.\nThis paper approaches the paradigm of acceptability judgments with topological\ndata analysis (TDA), showing that the geometric properties of the attention\ngraph can be efficiently exploited for two standard practices in linguistics:\nbinary judgments and linguistic minimal pairs. Topological features enhance the\nBERT-based acceptability classifier scores by $8$%-$24$% on CoLA in three\nlanguages (English, Italian, and Swedish). By revealing the topological\ndiscrepancy between attention maps of minimal pairs, we achieve the human-level\nperformance on the BLiMP benchmark, outperforming nine statistical and\nTransformer LM baselines. At the same time, TDA provides the foundation for\nanalyzing the linguistic functions of attention heads and interpreting the\ncorrespondence between the graph features and grammatical phenomena.",
    "published": "2022-05-19T15:45:12Z",
    "updated": "2022-10-23T18:15:54Z",
    "authors": [
      "Daniil Cherniavskii",
      "Eduard Tulchinskii",
      "Vladislav Mikhailov",
      "Irina Proskurina",
      "Laida Kushnareva",
      "Ekaterina Artemova",
      "Serguei Barannikov",
      "Irina Piontkovskaya",
      "Dmitri Piontkovski",
      "Evgeny Burnaev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.06683v2",
    "title": "Enhancing Multivariate Time Series Classifiers through Self-Attention\n  and Relative Positioning Infusion",
    "summary": "Time Series Classification (TSC) is an important and challenging task for\nmany visual computing applications. Despite the extensive range of methods\ndeveloped for TSC, relatively few utilized Deep Neural Networks (DNNs). In this\npaper, we propose two novel attention blocks (Global Temporal Attention and\nTemporal Pseudo-Gaussian augmented Self-Attention) that can enhance deep\nlearning-based TSC approaches, even when such approaches are designed and\noptimized for a specific dataset or task. We validate this claim by evaluating\nmultiple state-of-the-art deep learning-based TSC models on the University of\nEast Anglia (UEA) benchmark, a standardized collection of 30 Multivariate Time\nSeries Classification (MTSC) datasets. We show that adding the proposed\nattention blocks improves base models' average accuracy by up to 3.6%.\nAdditionally, the proposed TPS block uses a new injection module to include the\nrelative positional information in transformers. As a standalone unit with less\ncomputational complexity, it enables TPS to perform better than most of the\nstate-of-the-art DNN-based TSC methods. The source codes for our experimental\nsetups and proposed attention blocks are made publicly available.",
    "published": "2023-02-13T20:50:34Z",
    "updated": "2023-03-03T01:34:34Z",
    "authors": [
      "Mehryar Abbasi",
      "Parvaneh Saeedi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.05400v1",
    "title": "Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient\n  Vision Transformers",
    "summary": "Vector-quantized image modeling has shown great potential in synthesizing\nhigh-quality images. However, generating high-resolution images remains a\nchallenging task due to the quadratic computational overhead of the\nself-attention process. In this study, we seek to explore a more efficient\ntwo-stage framework for high-resolution image generation with improvements in\nthe following three aspects. (1) Based on the observation that the first\nquantization stage has solid local property, we employ a local attention-based\nquantization model instead of the global attention mechanism used in previous\nmethods, leading to better efficiency and reconstruction quality. (2) We\nemphasize the importance of multi-grained feature interaction during image\ngeneration and introduce an efficient attention mechanism that combines global\nattention (long-range semantic consistency within the whole image) and local\nattention (fined-grained details). This approach results in faster generation\nspeed, higher generation fidelity, and improved resolution. (3) We propose a\nnew generation pipeline incorporating autoencoding training and autoregressive\ngeneration strategy, demonstrating a better paradigm for image synthesis.\nExtensive experiments demonstrate the superiority of our approach in\nhigh-quality and high-resolution image reconstruction and generation.",
    "published": "2023-10-09T04:38:52Z",
    "updated": "2023-10-09T04:38:52Z",
    "authors": [
      "Shiyue Cao",
      "Yueqin Yin",
      "Lianghua Huang",
      "Yu Liu",
      "Xin Zhao",
      "Deli Zhao",
      "Kaiqi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.16142v1",
    "title": "A Language Model with Limited Memory Capacity Captures Interference in\n  Human Sentence Processing",
    "summary": "Two of the central factors believed to underpin human sentence processing\ndifficulty are expectations and retrieval from working memory. A recent attempt\nto create a unified cognitive model integrating these two factors relied on the\nparallels between the self-attention mechanism of transformer language models\nand cue-based retrieval theories of working memory in human sentence processing\n(Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in\nspecialized attention heads of GPT-2 are consistent with similarity-based\ninterference, a key prediction of cue-based retrieval models, their method\nrequires identifying syntactically specialized attention heads, and makes the\ncognitively implausible assumption that hundreds of memory retrieval operations\ntake place in parallel. In the present work, we develop a recurrent neural\nlanguage model with a single self-attention head, which more closely parallels\nthe memory system assumed by cognitive theories. We show that our model's\nsingle attention head captures semantic and syntactic interference effects\nobserved in human experiments.",
    "published": "2023-10-24T19:33:27Z",
    "updated": "2023-10-24T19:33:27Z",
    "authors": [
      "William Timkey",
      "Tal Linzen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.12091v1",
    "title": "DAS: A Deformable Attention to Capture Salient Information in CNNs",
    "summary": "Convolutional Neural Networks (CNNs) excel in local spatial pattern\nrecognition. For many vision tasks, such as object recognition and\nsegmentation, salient information is also present outside CNN's kernel\nboundaries. However, CNNs struggle in capturing such relevant information due\nto their confined receptive fields. Self-attention can improve a model's access\nto global information but increases computational overhead. We present a fast\nand simple fully convolutional method called DAS that helps focus attention on\nrelevant information. It uses deformable convolutions for the location of\npertinent image regions and separable convolutions for efficiency. DAS plugs\ninto existing CNNs and propagates relevant information using a gating\nmechanism. Compared to the O(n^2) computational complexity of transformer-style\nattention, DAS is O(n). Our claim is that DAS's ability to pay increased\nattention to relevant features results in performance improvements when added\nto popular CNNs for Image Classification and Object Detection. For example, DAS\nyields an improvement on Stanford Dogs (4.47%), ImageNet (1.91%), and COCO AP\n(3.3%) with base ResNet50 backbone. This outperforms other CNN attention\nmechanisms while using similar or less FLOPs. Our code will be publicly\navailable.",
    "published": "2023-11-20T18:49:58Z",
    "updated": "2023-11-20T18:49:58Z",
    "authors": [
      "Farzad Salajegheh",
      "Nader Asadi",
      "Soroush Saryazdi",
      "Sudhir Mudur"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.03485v2",
    "title": "Attention Meets Post-hoc Interpretability: A Mathematical Perspective",
    "summary": "Attention-based architectures, in particular transformers, are at the heart\nof a technological revolution. Interestingly, in addition to helping obtain\nstate-of-the-art results on a wide range of applications, the attention\nmechanism intrinsically provides meaningful insights on the internal behavior\nof the model. Can these insights be used as explanations? Debate rages on. In\nthis paper, we mathematically study a simple attention-based architecture and\npinpoint the differences between post-hoc and attention-based explanations. We\nshow that they provide quite different results, and that, despite their\nlimitations, post-hoc methods are capable of capturing more useful insights\nthan merely examining the attention weights.",
    "published": "2024-02-05T19:56:56Z",
    "updated": "2024-06-17T13:18:30Z",
    "authors": [
      "Gianluigi Lopardo",
      "Frederic Precioso",
      "Damien Garreau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.02352v1",
    "title": "ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys",
    "summary": "We propose a new attention mechanism with linear complexity, ATP, that\nfixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than\non each individual token. Particularly, ATP is driven by an important\nobservation that input sequences are typically low-rank, i.e., input sequences\ncan be represented by a few principal bases. Therefore, instead of directly\niterating over all the input tokens, ATP transforms inputs into an orthogonal\nspace and computes attention only on the top principal bases (keys). Owing to\nthe observed low-rank structure in input sequences, ATP is able to capture\nsemantic relationships in input sequences with a few principal keys.\nFurthermore, the attention complexity is reduced from \\emph{quadratic} to\n\\emph{linear} without incurring a noticeable performance drop. ATP further\nreduces complexity for other linear layers with low-rank inputs, leading to\nmore speedup compared to prior works that solely target the attention module.\nOur evaluations on various models (e.g., BERT and Llama) demonstrate that ATP\nachieves comparable accuracy with much lower computation and memory complexity\nthan the standard attention mechanism. In particular, ATP barely loses accuracy\nwith only $1/2$ principal keys, and only incurs around $2\\%$ accuracy drops\nwith $1/4$ principal keys.",
    "published": "2024-03-01T19:24:37Z",
    "updated": "2024-03-01T19:24:37Z",
    "authors": [
      "Yue Niu",
      "Saurav Prakash",
      "Salman Avestimehr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.18542v1",
    "title": "Attention-aware semantic relevance predicting Chinese sentence reading",
    "summary": "In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.",
    "published": "2024-03-27T13:22:38Z",
    "updated": "2024-03-27T13:22:38Z",
    "authors": [
      "Kun Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.17230v1",
    "title": "Improving ICD coding using Chapter based Named Entities and Attentional\n  Models",
    "summary": "Recent advancements in natural language processing (NLP) have led to\nautomation in various domains. However, clinical NLP often relies on benchmark\ndatasets that may not reflect real-world scenarios accurately. Automatic ICD\ncoding, a vital NLP task, typically uses outdated and imbalanced datasets like\nMIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4\nand 0.7 due to many false positives. Our research introduces an enhanced\napproach to ICD coding that improves F1 scores by using chapter-based named\nentities and attentional models. This method categorizes discharge summaries\ninto ICD-9 Chapters and develops attentional models with chapter-specific data,\neliminating the need to consider external data for code identification. For\ncategorization, we use Chapter-IV to de-bias and influence key entities and\nweights without neural networks, creating accurate thresholds and providing\ninterpretability for human validation. Post-validation, we develop attentional\nmodels for three frequent and three non-frequent codes from Chapter-IV using\nBidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with\nMulti-head Attention architectures. The average Micro-F1 scores of 0.79 and\n0.81 from these models demonstrate significant performance improvements in ICD\ncoding.",
    "published": "2024-07-24T12:34:23Z",
    "updated": "2024-07-24T12:34:23Z",
    "authors": [
      "Abhijith R. Beeravolu",
      "Mirjam Jonkman",
      "Sami Azam",
      "Friso De Boer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.18613v1",
    "title": "Dilated Strip Attention Network for Image Restoration",
    "summary": "Image restoration is a long-standing task that seeks to recover the latent\nsharp image from its deteriorated counterpart. Due to the robust capacity of\nself-attention to capture long-range dependencies, transformer-based methods or\nsome attention-based convolutional neural networks have demonstrated promising\nresults on many image restoration tasks in recent years. However, existing\nattention modules encounters limited receptive fields or abundant parameters.\nIn order to integrate contextual information more effectively and efficiently,\nin this paper, we propose a dilated strip attention network (DSAN) for image\nrestoration. Specifically, to gather more contextual information for each pixel\nfrom its neighboring pixels in the same row or column, a dilated strip\nattention (DSA) mechanism is elaborately proposed. By employing the DSA\noperation horizontally and vertically, each location can harvest the contextual\ninformation from a much wider region. In addition, we utilize multi-scale\nreceptive fields across different feature groups in DSA to improve\nrepresentation learning. Extensive experiments show that our DSAN outperforms\nstate-of-the-art algorithms on several image restoration tasks.",
    "published": "2024-07-26T09:12:30Z",
    "updated": "2024-07-26T09:12:30Z",
    "authors": [
      "Fangwei Hao",
      "Jiesheng Wu",
      "Ji Du",
      "Yinjie Wang",
      "Jing Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.16729v3",
    "title": "Prediction-Feedback DETR for Temporal Action Detection",
    "summary": "Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.",
    "published": "2024-08-29T17:20:59Z",
    "updated": "2024-12-19T10:53:48Z",
    "authors": [
      "Jihwan Kim",
      "Miso Lee",
      "Cheol-Ho Cho",
      "Jihyun Lee",
      "Jae-Pil Heo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.06985v2",
    "title": "Unveiling Markov Heads in Pretrained Language Models for Offline\n  Reinforcement Learning",
    "summary": "Recently, incorporating knowledge from pretrained language models (PLMs) into\ndecision transformers (DTs) has generated significant attention in offline\nreinforcement learning (RL). These PLMs perform well in RL tasks, raising an\nintriguing question: what kind of knowledge from PLMs has been transferred to\nRL to achieve such good results? This work first dives into this problem by\nanalyzing each head quantitatively and points out Markov head, a crucial\ncomponent that exists in the attention heads of PLMs. It leads to extreme\nattention on the last-input token and performs well only in short-term\nenvironments. Furthermore, we prove that this extreme attention cannot be\nchanged by re-training embedding layer or fine-tuning. Inspired by our\nanalysis, we propose a general method GPT2-DTMA, which equips a pretrained DT\nwith Mixture of Attention (MoA), to accommodate diverse attention requirements\nduring fine-tuning. Extensive experiments corroborate our theorems and\ndemonstrate the effectiveness of GPT2-DTMA: it achieves comparable performance\nin short-term environments while significantly narrowing the performance gap in\nlong-term environments.",
    "published": "2024-09-11T03:18:34Z",
    "updated": "2025-06-06T19:11:05Z",
    "authors": [
      "Wenhao Zhao",
      "Qiushui Xu",
      "Linjie Xu",
      "Lei Song",
      "Jinyu Wang",
      "Chunlai Zhou",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.17625v3",
    "title": "Benign Overfitting in Token Selection of Attention Mechanism",
    "summary": "Attention mechanism is a fundamental component of the transformer model and\nplays a significant role in its success. However, the theoretical understanding\nof how attention learns to select tokens is still an emerging area of research.\nIn this work, we study the training dynamics and generalization ability of the\nattention mechanism under classification problems with label noise. We show\nthat, with the characterization of signal-to-noise ratio (SNR), the token\nselection of attention mechanism achieves benign overfitting, i.e., maintaining\nhigh generalization performance despite fitting label noise. Our work also\ndemonstrates an interesting delayed acquisition of generalization after an\ninitial phase of overfitting. Finally, we provide experiments to support our\ntheoretical analysis using both synthetic and real-world datasets.",
    "published": "2024-09-26T08:20:05Z",
    "updated": "2025-05-17T17:16:53Z",
    "authors": [
      "Keitaro Sakamoto",
      "Issei Sato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.14829v1",
    "title": "Mention Attention for Pronoun Translation",
    "summary": "Most pronouns are referring expressions, computers need to resolve what do\nthe pronouns refer to, and there are divergences on pronoun usage across\nlanguages. Thus, dealing with these divergences and translating pronouns is a\nchallenge in machine translation. Mentions are referring candidates of pronouns\nand have closer relations with pronouns compared to general tokens. We assume\nthat extracting additional mention features can help pronoun translation.\nTherefore, we introduce an additional mention attention module in the decoder\nto pay extra attention to source mentions but not non-mention tokens. Our\nmention attention module not only extracts features from source mentions, but\nalso considers target-side context which benefits pronoun translation. In\naddition, we also introduce two mention classifiers to train models to\nrecognize mentions, whose outputs guide the mention attention. We conduct\nexperiments on the WMT17 English-German translation task, and evaluate our\nmodels on general translation and pronoun translation, using BLEU, APT, and\ncontrastive evaluation metrics. Our proposed model outperforms the baseline\nTransformer model in terms of APT and BLEU scores, this confirms our hypothesis\nthat we can improve pronoun translation by paying additional attention to\nsource mentions, and shows that our introduced additional modules do not have\nnegative effect on the general translation quality.",
    "published": "2024-12-19T13:19:19Z",
    "updated": "2024-12-19T13:19:19Z",
    "authors": [
      "Gongbo Tang",
      "Christian Hardmeier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.15630v2",
    "title": "Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum\n  Approach",
    "summary": "Recent advances in quantum computing have opened new pathways for enhancing\ndeep learning architectures, particularly in domains characterized by\nhigh-dimensional and context-rich data such as natural language processing\n(NLP). In this work, we present a hybrid classical-quantum Transformer model\nthat integrates a quantum-enhanced attention mechanism into the standard\nclassical architecture. By embedding token representations into a quantum\nHilbert space via parameterized variational circuits and exploiting\nentanglement-aware kernel similarities, the model captures complex semantic\nrelationships beyond the reach of conventional dot-product attention. We\ndemonstrate the effectiveness of this approach across diverse NLP benchmarks,\nshowing improvements in both efficiency and representational capacity. The\nresults section reveal that the quantum attention layer yields globally\ncoherent attention maps and more separable latent features, while requiring\ncomparatively fewer parameters than classical counterparts. These findings\nhighlight the potential of quantum-classical hybrid models to serve as a\npowerful and resource-efficient alternative to existing attention mechanisms in\nNLP.",
    "published": "2025-01-26T18:29:06Z",
    "updated": "2025-06-27T14:09:08Z",
    "authors": [
      "S. M. Yousuf Iqbal Tomal",
      "Abdullah Al Shafin",
      "Debojit Bhattacharjee",
      "MD. Khairul Amin",
      "Rafiad Sadat Shahir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.16790v1",
    "title": "Exponential Family Attention",
    "summary": "The self-attention mechanism is the backbone of the transformer neural\nnetwork underlying most large language models. It can capture complex word\npatterns and long-range dependencies in natural language. This paper introduces\nexponential family attention (EFA), a probabilistic generative model that\nextends self-attention to handle high-dimensional sequence, spatial, or\nspatial-temporal data of mixed data types, including both discrete and\ncontinuous observations. The key idea of EFA is to model each observation\nconditional on all other existing observations, called the context, whose\nrelevance is learned in a data-driven way via an attention-based latent factor\nmodel. In particular, unlike static latent embeddings, EFA uses the\nself-attention mechanism to capture dynamic interactions in the context, where\nthe relevance of each context observations depends on other observations. We\nestablish an identifiability result and provide a generalization guarantee on\nexcess loss for EFA. Across real-world and synthetic data sets -- including\nU.S. city temperatures, Instacart shopping baskets, and MovieLens ratings -- we\nfind that EFA consistently outperforms existing models in capturing complex\nlatent structures and reconstructing held-out data.",
    "published": "2025-01-28T08:42:58Z",
    "updated": "2025-01-28T08:42:58Z",
    "authors": [
      "Kevin Christian Wibisono",
      "Yixin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.18795v2",
    "title": "Rope to Nope and Back Again: A New Hybrid Attention Strategy",
    "summary": "Long-context large language models (LLMs) have achieved remarkable\nadvancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et\nal., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et\nal., 2023). By adjusting RoPE parameters and incorporating training data with\nextended contexts, we can train performant models with considerably longer\ninput sequences. However, existing RoPE-based methods exhibit performance\nlimitations when applied to extended context lengths. This paper presents a\ncomprehensive analysis of various attention mechanisms, including RoPE, No\nPositional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying\ntheir strengths and shortcomings in long-context modeling. Our investigation\nidentifies distinctive attention patterns in these methods and highlights their\nimpact on long-context performance, providing valuable insights for\narchitectural design. Building on these findings, we propose a novel\narchitecture featuring a hybrid attention mechanism that integrates global and\nlocal attention spans. This design not only surpasses conventional RoPE-based\ntransformer models with full attention in both long and short context tasks but\nalso delivers substantial efficiency gains during training and inference.",
    "published": "2025-01-30T23:05:57Z",
    "updated": "2025-10-22T22:43:58Z",
    "authors": [
      "Bowen Yang",
      "Bharat Venkitesh",
      "Dwarak Talupuru",
      "Hangyu Lin",
      "David Cairuz",
      "Phil Blunsom",
      "Acyr Locatelli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07962v2",
    "title": "ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport\n  Plans",
    "summary": "While self-attention has been instrumental in the success of Transformers, it\ncan lead to over-concentration on a few tokens during training, resulting in\nsuboptimal information flow. Enforcing doubly-stochastic constraints in\nattention matrices has been shown to improve structure and balance in attention\ndistributions. However, existing methods rely on iterative Sinkhorn\nnormalization, which is computationally costly. In this paper, we introduce a\nnovel, fully parallelizable doubly-stochastic attention mechanism based on\nsliced optimal transport, leveraging Expected Sliced Transport Plans (ESP).\nUnlike prior approaches, our method enforces doubly stochasticity without\niterative Sinkhorn normalization, significantly enhancing efficiency. To ensure\ndifferentiability, we incorporate a temperature-based soft sorting technique,\nenabling seamless integration into deep learning models. Experiments across\nmultiple benchmark datasets, including image classification, point cloud\nclassification, sentiment analysis, and neural machine translation, demonstrate\nthat our enhanced attention regularization consistently improves performance\nacross diverse applications. Our implementation code can be found at\nhttps://github.com/dariansal/ESPFormer.",
    "published": "2025-02-11T21:20:48Z",
    "updated": "2025-07-12T19:17:03Z",
    "authors": [
      "Ashkan Shahbazi",
      "Elaheh Akbari",
      "Darian Salehi",
      "Xinran Liu",
      "Navid Naderializadeh",
      "Soheil Kolouri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.15349v1",
    "title": "AttentionEngine: A Versatile Framework for Efficient Attention\n  Mechanisms on Diverse Hardware Platforms",
    "summary": "Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.",
    "published": "2025-02-21T10:06:41Z",
    "updated": "2025-02-21T10:06:41Z",
    "authors": [
      "Feiyang Chen",
      "Yu Cheng",
      "Lei Wang",
      "Yuqing Xia",
      "Ziming Miao",
      "Lingxiao Ma",
      "Fan Yang",
      "Jilong Xue",
      "Zhi Yang",
      "Mao Yang",
      "Haibo Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01564v2",
    "title": "Attention Condensation via Sparsity Induced Regularized Training",
    "summary": "As the context window expands, self-attention increasingly dominates the\ntransformer's inference time. Therefore, accelerating attention computation\nwhile minimizing performance degradation is essential for the efficient\ndeployment of Large Language Models (LLMs). In this study we extend a\ntheoretical framework of attention sparsity in LLMs. A customized loss function\nis designed to enforce the sparsity by restricting the number of top elements\nin the attention matrix. We perform an initial set of evaluations with GPT-2 to\nshow the effectiveness of our sparsification approach. The attention matrices\nof the models trained with the proposed loss are both sparse and effective in\ncapturing relevant input dependencies. We now continue working to demonstrate\nthe value of our approach on larger models and different architectures.",
    "published": "2025-03-03T14:09:13Z",
    "updated": "2025-03-12T18:12:59Z",
    "authors": [
      "Eli Sason",
      "Darya Frolova",
      "Boris Nazarov",
      "Felix Goldberd"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.23353v1",
    "title": "Object Isolated Attention for Consistent Story Visualization",
    "summary": "Open-ended story visualization is a challenging task that involves generating\ncoherent image sequences from a given storyline. One of the main difficulties\nis maintaining character consistency while creating natural and contextually\nfitting scenes--an area where many existing methods struggle. In this paper, we\npropose an enhanced Transformer module that uses separate self attention and\ncross attention mechanisms, leveraging prior knowledge from pre-trained\ndiffusion models to ensure logical scene creation. The isolated self attention\nmechanism improves character consistency by refining attention maps to reduce\nfocus on irrelevant areas and highlight key features of the same character.\nMeanwhile, the isolated cross attention mechanism independently processes each\ncharacter's features, avoiding feature fusion and further strengthening\nconsistency. Notably, our method is training-free, allowing the continuous\ngeneration of new characters and storylines without re-tuning. Both qualitative\nand quantitative evaluations show that our approach outperforms current\nmethods, demonstrating its effectiveness.",
    "published": "2025-03-30T08:16:52Z",
    "updated": "2025-03-30T08:16:52Z",
    "authors": [
      "Xiangyang Luo",
      "Junhao Cheng",
      "Yifan Xie",
      "Xin Zhang",
      "Tao Feng",
      "Zhou Liu",
      "Fei Ma",
      "Fei Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.23447v1",
    "title": "CA^2ST: Cross-Attention in Audio, Space, and Time for Holistic Video\n  Recognition",
    "summary": "We propose Cross-Attention in Audio, Space, and Time (CA^2ST), a\ntransformer-based method for holistic video recognition. Recognizing actions in\nvideos requires both spatial and temporal understanding, yet most existing\nmodels lack a balanced spatio-temporal understanding of videos. To address\nthis, we propose a novel two-stream architecture, called Cross-Attention in\nSpace and Time (CAST), using only RGB input. In each layer of CAST, Bottleneck\nCross-Attention (B-CA) enables spatial and temporal experts to exchange\ninformation and make synergistic predictions. For holistic video understanding,\nwe extend CAST by integrating an audio expert, forming Cross-Attention in\nVisual and Audio (CAVA). We validate the CAST on benchmarks with different\ncharacteristics, EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400,\nconsistently showing balanced performance. We also validate the CAVA on\naudio-visual action recognition benchmarks, including UCF-101, VGG-Sound,\nKineticsSound, and EPIC-SOUNDS. With a favorable performance of CAVA across\nthese datasets, we demonstrate the effective information exchange among\nmultiple experts within the B-CA module. In summary, CA^2ST combines CAST and\nCAVA by employing spatial, temporal, and audio experts through cross-attention,\nachieving balanced and holistic video understanding.",
    "published": "2025-03-30T13:57:58Z",
    "updated": "2025-03-30T13:57:58Z",
    "authors": [
      "Jongseo Lee",
      "Joohyun Chang",
      "Dongho Lee",
      "Jinwoo Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.06634v1",
    "title": "Crafting Query-Aware Selective Attention for Single Image\n  Super-Resolution",
    "summary": "Single Image Super-Resolution (SISR) reconstructs high-resolution images from\nlow-resolution inputs, enhancing image details. While Vision Transformer\n(ViT)-based models improve SISR by capturing long-range dependencies, they\nsuffer from quadratic computational costs or employ selective attention\nmechanisms that do not explicitly focus on query-relevant regions. Despite\nthese advancements, prior work has overlooked how selective attention\nmechanisms should be effectively designed for SISR. We propose SSCAN, which\ndynamically selects the most relevant key-value windows based on query\nsimilarity, ensuring focused feature extraction while maintaining efficiency.\nIn contrast to prior approaches that apply attention globally or heuristically,\nour method introduces a query-aware window selection strategy that better\naligns attention computation with important image regions. By incorporating\nfixed-sized windows, SSCAN reduces memory usage and enforces linear\ntoken-to-token complexity, making it scalable for large images. Our experiments\ndemonstrate that SSCAN outperforms existing attention-based SISR methods,\nachieving up to 0.14 dB PSNR improvement on urban datasets, guaranteeing both\ncomputational efficiency and reconstruction quality in SISR.",
    "published": "2025-04-09T07:17:29Z",
    "updated": "2025-04-09T07:17:29Z",
    "authors": [
      "Junyoung Kim",
      "Youngrok Kim",
      "Siyeol Jung",
      "Donghyun Min"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.19901v1",
    "title": "Attention Mechanism, Max-Affine Partition, and Universal Approximation",
    "summary": "We establish the universal approximation capability of single-layer,\nsingle-head self- and cross-attention mechanisms with minimal attached\nstructures. Our key insight is to interpret single-head attention as an input\ndomain-partition mechanism that assigns distinct values to subregions. This\nallows us to engineer the attention weights such that this assignment imitates\nthe target function. Building on this, we prove that a single self-attention\nlayer, preceded by sum-of-linear transformations, is capable of approximating\nany continuous function on a compact domain under the $L_\\infty$-norm.\nFurthermore, we extend this construction to approximate any Lebesgue integrable\nfunction under $L_p$-norm for $1\\leq p <\\infty$. Lastly, we also extend our\ntechniques and show that, for the first time, single-head cross-attention\nachieves the same universal approximation guarantees.",
    "published": "2025-04-28T15:31:45Z",
    "updated": "2025-04-28T15:31:45Z",
    "authors": [
      "Hude Liu",
      "Jerry Yao-Chieh Hu",
      "Zhao Song",
      "Han Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.01451v2",
    "title": "AdSight: Scalable and Accurate Quantification of User Attention in\n  Multi-Slot Sponsored Search",
    "summary": "Modern Search Engine Results Pages (SERPs) present complex layouts where\nmultiple elements compete for visibility. Attention modelling is crucial for\noptimising web design and computational advertising, whereas attention metrics\ncan inform ad placement and revenue strategies. We introduce AdSight, a method\nleveraging mouse cursor trajectories to quantify in a scalable and accurate\nmanner user attention in multi-slot environments like SERPs. AdSight uses a\nnovel Transformer-based sequence-to-sequence architecture where the encoder\nprocesses cursor trajectory embeddings, and the decoder incorporates\nslot-specific features, enabling robust attention prediction across various\nSERP layouts. We evaluate our approach on two Machine Learning tasks: (1)\nregression, to predict fixation times and counts; and (2) classification, to\ndetermine some slot types were noticed. Our findings demonstrate the model's\nability to predict attention with unprecedented precision, offering actionable\ninsights for researchers and practitioners.",
    "published": "2025-04-30T08:51:26Z",
    "updated": "2025-05-07T07:11:49Z",
    "authors": [
      "Mario VillaizÃ¡n-Vallelado",
      "Matteo Salvatori",
      "Kayhan Latifzadeh",
      "Antonio Penta",
      "Luis A. Leiva",
      "Ioannis Arapakis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06279v1",
    "title": "Interpretable Learning Dynamics in Unsupervised Reinforcement Learning",
    "summary": "We present an interpretability framework for unsupervised reinforcement\nlearning (URL) agents, aimed at understanding how intrinsic motivation shapes\nattention, behavior, and representation learning. We analyze five agents DQN,\nRND, ICM, PPO, and a Transformer-RND variant trained on procedurally generated\nenvironments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),\nexploration metrics, and latent space clustering. To capture how agents\nperceive and adapt over time, we introduce two metrics: attention diversity,\nwhich measures the spatial breadth of focus, and attention change rate, which\nquantifies temporal shifts in attention. Our findings show that\ncuriosity-driven agents display broader, more dynamic attention and exploratory\nbehavior than their extrinsically motivated counterparts. Among them,\nTransformerRND combines wide attention, high exploration coverage, and compact,\nstructured latent representations. Our results highlight the influence of\narchitectural inductive biases and training signals on internal agent dynamics.\nBeyond reward-centric evaluation, the proposed framework offers diagnostic\ntools to probe perception and abstraction in RL agents, enabling more\ninterpretable and generalizable behavior.",
    "published": "2025-05-06T19:57:09Z",
    "updated": "2025-05-06T19:57:09Z",
    "authors": [
      "Shashwat Pandey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07908v1",
    "title": "A Reproduction Study: The Kernel PCA Interpretation of Self-Attention\n  Fails Under Scrutiny",
    "summary": "In this reproduction study, we revisit recent claims that self-attention\nimplements kernel principal component analysis (KPCA) (Teo et al., 2024),\npositing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix\nof the keys, and (ii) that self-attention projects queries onto the principal\ncomponent axes of the key matrix $K$ in a feature space. Our analysis reveals\nthree critical inconsistencies: (1) No alignment exists between learned\nself-attention value vectors and what is proposed in the KPCA perspective, with\naverage similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA\n(Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating\nnegligible correspondence; (2) Reported decreases in reconstruction loss\n$J_\\text{proj}$, arguably justifying the claim that the self-attention\nminimizes the projection error of KPCA, are misinterpreted, as the quantities\ninvolved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix\neigenvalue statistics, introduced to justify that $V$ captures the eigenvector\nof the gram matrix, are irreproducible without undocumented\nimplementation-specific adjustments. Across 10 transformer architectures, we\nconclude that the KPCA interpretation of self-attention lacks empirical\nsupport.",
    "published": "2025-05-12T12:38:46Z",
    "updated": "2025-05-12T12:38:46Z",
    "authors": [
      "Karahan SarÄ±taÅ",
      "ÃaÄatay YÄ±ldÄ±z"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13544v3",
    "title": "Multi-head Temporal Latent Attention",
    "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
    "published": "2025-05-19T02:09:41Z",
    "updated": "2025-11-02T20:27:27Z",
    "authors": [
      "Keqi Deng",
      "Philip C. Woodland"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.12541v1",
    "title": "BSA: Ball Sparse Attention for Large-scale Geometries",
    "summary": "Self-attention scales quadratically with input size, limiting its use for\nlarge-scale physical systems. Although sparse attention mechanisms provide a\nviable alternative, they are primarily designed for regular structures such as\ntext or images, making them inapplicable for irregular geometries. In this\nwork, we present Ball Sparse Attention (BSA), which adapts Native Sparse\nAttention (NSA) (Yuan et al., 2025) to unordered point sets by imposing\nregularity using the Ball Tree structure from the Erwin Transformer (Zhdanov et\nal., 2025). We modify NSA's components to work with ball-based neighborhoods,\nyielding a global receptive field at sub-quadratic cost. On an airflow pressure\nprediction task, we achieve accuracy comparable to Full Attention while\nsignificantly reducing the theoretical computational complexity. Our\nimplementation is available at https://github.com/britacatalin/bsa.",
    "published": "2025-06-14T15:29:41Z",
    "updated": "2025-06-14T15:29:41Z",
    "authors": [
      "Catalin E. Brita",
      "Hieu Nguyen",
      "Lohithsai Yadala Chanchu",
      "Domonkos Nagy",
      "Maksim Zhdanov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15754v1",
    "title": "Explainable speech emotion recognition through attentive pooling:\n  insights from attention-based temporal localization",
    "summary": "State-of-the-art transformer models for Speech Emotion Recognition (SER) rely\non temporal feature aggregation, yet advanced pooling methods remain\nunderexplored. We systematically benchmark pooling strategies, including\nMulti-Query Multi-Head Attentive Statistics Pooling, which achieves a 3.5\npercentage point macro F1 gain over average pooling. Attention analysis shows\n15 percent of frames capture 80 percent of emotion cues, revealing a localized\npattern of emotional information. Analysis of high-attention frames reveals\nthat non-linguistic vocalizations and hyperarticulated phonemes are\ndisproportionately prioritized during pooling, mirroring human perceptual\nstrategies. Our findings position attentive pooling as both a performant SER\nmechanism and a biologically plausible tool for explainable emotion\nlocalization. On Interspeech 2025 Speech Emotion Recognition in Naturalistic\nConditions Challenge, our approach obtained a macro F1 score of 0.3649.",
    "published": "2025-06-18T07:22:47Z",
    "updated": "2025-06-18T07:22:47Z",
    "authors": [
      "Tahitoa Leygue",
      "Astrid Sabourin",
      "Christian Bolzmacher",
      "Sylvain Bouchigny",
      "Margarita Anastassova",
      "Quoc-Cuong Pham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17657v2",
    "title": "Attention (as Discrete-Time Markov) Chains",
    "summary": "We introduce a new interpretation of the attention matrix as a discrete-time\nMarkov chain. Our interpretation sheds light on common operations involving\nattention scores such as selection, summation, and averaging in a unified\nframework. It further extends them by considering indirect attention,\npropagated through the Markov chain, as opposed to previous studies that only\nmodel immediate effects. Our key observation is that tokens linked to\nsemantically similar regions form metastable states, i.e., regions where\nattention tends to concentrate, while noisy attention scores dissipate.\nMetastable states and their prevalence can be easily computed through simple\nmatrix multiplication and eigenanalysis, respectively. Using these lightweight\ntools, we demonstrate state-of-the-art zero-shot segmentation. Lastly, we\ndefine TokenRank -- the steady state vector of the Markov chain, which measures\nglobal token importance. We show that TokenRank enhances unconditional image\ngeneration, improving both quality (IS) and diversity (FID), and can also be\nincorporated into existing segmentation techniques to improve their performance\nover existing benchmarks. We believe our framework offers a fresh view of how\ntokens are being attended in modern visual transformers.",
    "published": "2025-07-23T16:20:47Z",
    "updated": "2025-10-20T00:56:06Z",
    "authors": [
      "Yotam Erel",
      "Olaf DÃ¼nkel",
      "Rishabh Dabral",
      "Vladislav Golyanik",
      "Christian Theobalt",
      "Amit H. Bermano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.04868v1",
    "title": "Dual-Stream Attention with Multi-Modal Queries for Object Detection in\n  Transportation Applications",
    "summary": "Transformer-based object detectors often struggle with occlusions,\nfine-grained localization, and computational inefficiency caused by fixed\nqueries and dense attention. We propose DAMM, Dual-stream Attention with\nMulti-Modal queries, a novel framework introducing both query adaptation and\nstructured cross-attention for improved accuracy and efficiency. DAMM\ncapitalizes on three types of queries: appearance-based queries from\nvision-language models, positional queries using polygonal embeddings, and\nrandom learned queries for general scene coverage. Furthermore, a dual-stream\ncross-attention module separately refines semantic and spatial features,\nboosting localization precision in cluttered scenes. We evaluated DAMM on four\nchallenging benchmarks, and it achieved state-of-the-art performance in average\nprecision (AP) and recall, demonstrating the effectiveness of multi-modal query\nadaptation and dual-stream attention. Source code is at:\n\\href{https://github.com/DET-LIP/DAMM}{GitHub}.",
    "published": "2025-08-06T20:37:24Z",
    "updated": "2025-08-06T20:37:24Z",
    "authors": [
      "Noreen Anwar",
      "Guillaume-Alexandre Bilodeau",
      "Wassim Bouachir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.11921v1",
    "title": "ENA: Efficient N-dimensional Attention",
    "summary": "Efficient modeling of long sequences of high-order data requires a more\nefficient architecture than Transformer. In this paper, we investigate two key\naspects of extending linear recurrent models, especially those originally\ndesigned for language modeling, to high-order data (1D to ND): scanning\nstrategies and attention-hybrid architectures. Empirical results suggest that\nscanning provides limited benefits, while attention-hybrid models yield\npromising results. Focusing on the latter, we further evaluate types of\nattention and find that tiled high-order sliding window attention (SWA) is\nefficient in both theory and practice. We term the resulting hybrid\narchitecture of linear recurrence and high-order SWA as Efficient N-dimensional\nAttention (ENA). We then conduct several experiments to demonstrate its\neffectiveness. The intuition behind ENA is that linear recurrence compresses\nglobal information into a state, while SWA complements it by enforcing strict\nlocal modeling. Together, they form a simple framework that offers a promising\nand practical solution for ultra-long high-order data modeling.",
    "published": "2025-08-16T05:55:51Z",
    "updated": "2025-08-16T05:55:51Z",
    "authors": [
      "Yibo Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15493v1",
    "title": "Attention-Based Explainability for Structure-Property Relationships",
    "summary": "Machine learning methods are emerging as a universal paradigm for\nconstructing correlative structure-property relationships in materials science\nbased on multimodal characterization. However, this necessitates development of\nmethods for physical interpretability of the resulting correlative models.\nHere, we demonstrate the potential of attention-based neural networks for\nrevealing structure-property relationships and the underlying physical\nmechanisms, using the ferroelectric properties of PbTiO3 thin films as a case\nstudy. Through the analysis of attention scores, we disentangle the influence\nof distinct domain patterns on the polarization switching process. The\nattention-based Transformer model is explored both as a direct interpretability\ntool and as a surrogate for explaining representations learned via unsupervised\nmachine learning, enabling the identification of physically grounded\ncorrelations. We compare attention-derived interpretability scores with\nclassical SHapley Additive exPlanations (SHAP) analysis and show that, in\ncontrast to applications in natural language processing, attention mechanisms\nin materials science exhibit high efficiency in highlighting meaningful\nstructural features.",
    "published": "2025-08-21T12:20:47Z",
    "updated": "2025-08-21T12:20:47Z",
    "authors": [
      "Boris N. Slautin",
      "Utkarsh Pratiush",
      "Yongtao Liu",
      "Hiroshi Funakubo",
      "Vladimir V. Shvartsman",
      "Doru C. Lupascu",
      "Sergei V. Kalinin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.05323v2",
    "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for\n  XAIxArts",
    "summary": "This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium.",
    "published": "2025-08-30T19:46:18Z",
    "updated": "2025-09-09T12:40:17Z",
    "authors": [
      "Adam Cole",
      "Mick Grierson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.05550v2",
    "title": "TreeGPT: Pure TreeFFN Encoder-Decoder Architecture for Structured\n  Reasoning Without Attention Mechanisms",
    "summary": "We present TreeGPT, an attention-free neural architecture that explores the\npotential of pure TreeFFN encoder-decoder design for structured reasoning\ntasks. Unlike traditional transformer approaches that rely on attention\nmechanisms, TreeGPT employs bidirectional TreeFFN components that process\nsequences through adjacent connections in parallel, aiming to achieve\ncomputational efficiency while maintaining reasoning capabilities.\n  Our approach centers on a TreeFFN Encoder-Decoder mechanism: $$\\text{Encoder\nTreeFFN (L} \\rightarrow \\text{R)} + \\text{Decoder TreeFFN (R} \\leftarrow\n\\text{L)} \\rightarrow \\text{Parallel Processing}$$ where the encoder processes\nleft-to-right dependencies while the decoder handles right-to-left patterns,\nboth using simple neighbor-to-neighbor connections. This design eliminates\nattention computation while maintaining sequence modeling capabilities.\n  We evaluate our approach on the ARC Prize 2025 dataset, where TreeGPT\nachieves 99\\% validation accuracy using 3.16M parameters. The model converges\nwithin 1500 training steps and demonstrates 100\\% token-level accuracy on\nselected evaluation samples. Our preliminary results suggest that for certain\nstructured reasoning tasks, specialized TreeFFN architectures may offer\nadvantages over attention-based approaches. While these findings are\nencouraging, we acknowledge that further investigation across diverse tasks and\ndatasets would be valuable to establish the broader applicability of\nattention-free designs.",
    "published": "2025-09-06T00:39:33Z",
    "updated": "2025-09-11T10:46:29Z",
    "authors": [
      "Zixi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.07324v1",
    "title": "Mitigating Attention Localization in Small Scale: Self-Attention\n  Refinement via One-step Belief Propagation",
    "summary": "Transformer-based self-attention mechanism serves as the core of modern\nlanguage models, yet it often suffers from localization, where attentions\ncollapse onto a limited subset of tokens and fail to capture long-range\ndependencies. To address this issue, we propose Self-Attention One-step Belief\nPropagation (SAOBP), a refinement framework that injects multi-hop\nrelationships through a belief propagation process. To interpret and quantify\nthese interactions, we introduce Global Token Dependency (GTD) that captures\nthe relative contribution of multihop connections within the attention graph.\nEmpirical results indicate that SAOBP helps prevent entropy collapse in deeper\nlayers and adaptively maintains GTD at task-appropriate levels, thereby\nsupporting improvements in model performance. Importantly, we observe\ncompetitive gains in small-scale models, highlighting its potential for\nimproving inference quality in resource-constrained scenarios.",
    "published": "2025-09-09T01:43:48Z",
    "updated": "2025-09-09T01:43:48Z",
    "authors": [
      "Nakyung Lee",
      "Yeongoon Kim",
      "Minhae Oh",
      "Suhwan Kim",
      "Jin Woo Koo",
      "Hyewon Jo",
      "Jungwoo Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18816v1",
    "title": "Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal\n  Attention in Large Audio Language Models",
    "summary": "Large Audio-Language Models (LALMs) often suffer from audio-textual attention\nimbalance, prioritizing text over acoustic information, particularly in the\nmulti-modal fusion layers of the Transformer architecture. This bias hinders\ntheir ability to fully utilize acoustic cues, causing suboptimal performance on\naudio reasoning tasks. To mitigate this, we propose \\textbf{MATA}, a novel\ntraining-free method that dynamically pushes LALMs to pay \\textbf{M}ore\n\\textbf{A}ttention \\textbf{T}o \\textbf{A}udio tokens within the self-attention\nmechanism. Specifically, MATA intervenes post raw attention scoring, targeting\nonly the last token in intermediate layers without introducing additional\nparameters or computational overhead. Experiments on the MMAU and MMAR\nbenchmarks confirm MATA's effectiveness, with consistent performance gains.\nNotably, on MMAR, MATA enables an open-source model to surpass the proprietary\nGemini 2.0 Flash for the first time. Our work provides an efficient solution to\nmitigate attention bias and opens a new research direction for enhancing the\naudio-processing capabilities of multi-modal models.",
    "published": "2025-09-23T09:02:15Z",
    "updated": "2025-09-23T09:02:15Z",
    "authors": [
      "Junyu Wang",
      "Ziyang Ma",
      "Zhengding Luo",
      "Tianrui Wang",
      "Meng Ge",
      "Xiaobao Wang",
      "Longbiao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21936v1",
    "title": "Statistical Advantage of Softmax Attention: Insights from\n  Single-Location Regression",
    "summary": "Large language models rely on attention mechanisms with a softmax activation.\nYet the dominance of softmax over alternatives (e.g., component-wise or linear)\nremains poorly understood, and many theoretical works have focused on the\neasier-to-analyze linearized attention. In this work, we address this gap\nthrough a principled study of the single-location regression task, where the\noutput depends on a linear transformation of a single input token at a random\nlocation. Building on ideas from statistical physics, we develop an analysis of\nattention-based predictors in the high-dimensional limit, where generalization\nperformance is captured by a small set of order parameters. At the population\nlevel, we show that softmax achieves the Bayes risk, whereas linear attention\nfundamentally falls short. We then examine other activation functions to\nidentify which properties are necessary for optimal performance. Finally, we\nanalyze the finite-sample regime: we provide an asymptotic characterization of\nthe test error and show that, while softmax is no longer Bayes-optimal, it\nconsistently outperforms linear attention. We discuss the connection with\noptimization by gradient-based algorithms.",
    "published": "2025-09-26T06:21:30Z",
    "updated": "2025-09-26T06:21:30Z",
    "authors": [
      "O. Duranthon",
      "P. Marion",
      "C. Boyer",
      "B. Loureiro",
      "L. ZdeborovÃ¡"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.18692v1",
    "title": "MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation",
    "summary": "Long video generation with Diffusion Transformers (DiTs) is bottlenecked by\nthe quadratic scaling of full attention with sequence length. Since attention\nis highly redundant, outputs are dominated by a small subset of query-key\npairs. Existing sparse methods rely on blockwise coarse estimation, whose\naccuracy-efficiency trade-offs are constrained by block size. This paper\nintroduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention\nthat uses a lightweight, learnable token router to precisely match tokens\nwithout blockwise estimation. Through semantic-aware routing, MoGA enables\neffective long-range interactions. As a kernel-free method, MoGA integrates\nseamlessly with modern attention stacks, including FlashAttention and sequence\nparallelism. Building on MoGA, we develop an efficient long video generation\nmodel that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps,\nwith a context length of approximately 580k. Comprehensive experiments on\nvarious video generation tasks validate the effectiveness of our approach.",
    "published": "2025-10-21T14:50:42Z",
    "updated": "2025-10-21T14:50:42Z",
    "authors": [
      "Weinan Jia",
      "Yuning Lu",
      "Mengqi Huang",
      "Hualiang Wang",
      "Binyuan Huang",
      "Nan Chen",
      "Mu Liu",
      "Jidong Jiang",
      "Zhendong Mao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27641v1",
    "title": "SpecAttn: Speculating Sparse Attention",
    "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
    "published": "2025-10-31T17:12:34Z",
    "updated": "2025-10-31T17:12:34Z",
    "authors": [
      "Harsh Shah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.00652v3",
    "title": "CSWin Transformer: A General Vision Transformer Backbone with\n  Cross-Shaped Windows",
    "summary": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a mathematical analysis\nof the effect of the stripe width and vary the stripe width for different\nlayers of the Transformer network which achieves strong modeling capability\nwhile limiting the computation cost. We also introduce Locally-enhanced\nPositional Encoding (LePE), which handles the local positional information\nbetter than existing encoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for downstream\ntasks. Incorporated with these designs and a hierarchical structure, CSWin\nTransformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4\\% Top-1 accuracy on ImageNet-1K without any\nextra training data or label, 53.9 box AP and 46.4 mask AP on the COCO\ndetection task, and 52.2 mIOU on the ADE20K semantic segmentation task,\nsurpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting. By further\npretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy\non ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models are available at\nhttps://github.com/microsoft/CSWin-Transformer.",
    "published": "2021-07-01T17:59:56Z",
    "updated": "2022-01-09T05:49:30Z",
    "authors": [
      "Xiaoyi Dong",
      "Jianmin Bao",
      "Dongdong Chen",
      "Weiming Zhang",
      "Nenghai Yu",
      "Lu Yuan",
      "Dong Chen",
      "Baining Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.00973v1",
    "title": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention\n  Mechanism for Symbolic Music Modeling",
    "summary": "Following the success of the transformer architecture in the natural language\ndomain, transformer-like architectures have been widely applied to the domain\nof symbolic music recently. Symbolic music and text, however, are two different\nmodalities. Symbolic music contains multiple attributes, both absolute\nattributes (e.g., pitch) and relative attributes (e.g., pitch interval). These\nrelative attributes shape human perception of musical motifs. These important\nrelative attributes, however, are mostly ignored in existing symbolic music\nmodeling methods with the main reason being the lack of a musically-meaningful\nembedding space where both the absolute and relative embeddings of the symbolic\nmusic tokens can be efficiently represented. In this paper, we propose the\nFundamental Music Embedding (FME) for symbolic music based on a bias-adjusted\nsinusoidal encoding within which both the absolute and the relative attributes\ncan be embedded and the fundamental musical properties (e.g., translational\ninvariance) are explicitly preserved. Taking advantage of the proposed FME, we\nfurther propose a novel attention mechanism based on the relative index, pitch\nand onset embeddings (RIPO attention) such that the musical domain knowledge\ncan be fully utilized for symbolic music modeling. Experiment results show that\nour proposed model: RIPO transformer which utilizes FME and RIPO attention\noutperforms the state-of-the-art transformers (i.e., music transformer, linear\ntransformer) in a melody completion task. Moreover, using the RIPO transformer\nin a downstream music generation task, we notice that the notorious\ndegeneration phenomenon no longer exists and the music generated by the RIPO\ntransformer outperforms the music generated by state-of-the-art transformer\nmodels in both subjective and objective evaluations.",
    "published": "2022-12-02T05:04:31Z",
    "updated": "2022-12-02T05:04:31Z",
    "authors": [
      "Z. Guo",
      "J. Kang",
      "D. Herremans"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.10178v3",
    "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving",
    "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 4.1x and 2.1x\nhigher token generation throughput, respectively.",
    "published": "2025-07-14T11:40:17Z",
    "updated": "2025-09-16T02:24:35Z",
    "authors": [
      "Wonung Kim",
      "Yubin Lee",
      "Yoonsung Kim",
      "Jinwoo Hwang",
      "Seongryong Oh",
      "Jiyong Jung",
      "Aziz Huseynov",
      "Woong Gyu Park",
      "Chang Hyun Park",
      "Divya Mahajan",
      "Jongse Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.10759v5",
    "title": "SGFormer: Simplifying and Empowering Transformers for Large-Graph\n  Representations",
    "summary": "Learning representations on large-sized graphs is a long-standing challenge\ndue to the inter-dependence nature involved in massive data points.\nTransformers, as an emerging class of foundation encoders for graph-structured\ndata, have shown promising performance on small graphs due to its global\nattention capable of capturing all-pair influence beyond neighboring nodes.\nEven so, existing approaches tend to inherit the spirit of Transformers in\nlanguage and vision tasks, and embrace complicated models by stacking deep\nmulti-head attentions. In this paper, we critically demonstrate that even using\na one-layer attention can bring up surprisingly competitive performance across\nnode property prediction benchmarks where node numbers range from\nthousand-level to billion-level. This encourages us to rethink the design\nphilosophy for Transformers on large graphs, where the global attention is a\ncomputation overhead hindering the scalability. We frame the proposed scheme as\nSimplified Graph Transformers (SGFormer), which is empowered by a simple\nattention model that can efficiently propagate information among arbitrary\nnodes in one layer. SGFormer requires none of positional encodings,\nfeature/graph pre-processing or augmented loss. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M and yields up to\n141x inference acceleration over SOTA Transformers on medium-sized graphs.\nBeyond current results, we believe the proposed methodology alone enlightens a\nnew technical path of independent interest for building Transformers on large\ngraphs.",
    "published": "2023-06-19T08:03:25Z",
    "updated": "2024-08-16T08:24:25Z",
    "authors": [
      "Qitian Wu",
      "Wentao Zhao",
      "Chenxiao Yang",
      "Hengrui Zhang",
      "Fan Nie",
      "Haitian Jiang",
      "Yatao Bian",
      "Junchi Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.06673v2",
    "title": "CTRL-F: Pairing Convolution with Transformer for Image Classification\n  via Multi-Level Feature Cross-Attention and Representation Learning Fusion",
    "summary": "Transformers have captured growing attention in computer vision, thanks to\nits large capacity and global processing capabilities. However, transformers\nare data hungry, and their ability to generalize is constrained compared to\nConvolutional Neural Networks (ConvNets), especially when trained with limited\ndata due to the absence of the built-in spatial inductive biases present in\nConvNets. In this paper, we strive to optimally combine the strengths of both\nconvolution and transformers for image classification tasks. Towards this end,\nwe present a novel lightweight hybrid network that pairs Convolution with\nTransformers via Representation Learning Fusion and Multi-Level Feature\nCross-Attention named CTRL-F. Our network comprises a convolution branch and a\nnovel transformer module named multi-level feature cross-attention (MFCA). The\nMFCA module operates on multi-level feature representations obtained at\ndifferent convolution stages. It processes small patch tokens and large patch\ntokens extracted from these multi-level feature representations via two\nseparate transformer branches, where both branches communicate and exchange\nknowledge through cross-attention mechanism. We fuse the local responses\nacquired from the convolution path with the global responses acquired from the\nMFCA module using novel representation fusion techniques dubbed adaptive\nknowledge fusion (AKF) and collaborative knowledge fusion (CKF). Experiments\ndemonstrate that our CTRL-F variants achieve state-of-the-art performance,\nwhether trained from scratch on large data or even with low-data regime. For\nInstance, CTRL-F achieves top-1 accuracy of 82.24% and 99.91% when trained from\nscratch on Oxford-102 Flowers and PlantVillage datasets respectively,\nsurpassing state-of-the-art models which showcase the robustness of our model\non image classification tasks. Code at: https://github.com/hosamsherif/CTRL-F",
    "published": "2024-07-09T08:47:13Z",
    "updated": "2025-08-23T20:26:09Z",
    "authors": [
      "Hosam S. EL-Assiouti",
      "Hadeer El-Saadawy",
      "Maryam N. Al-Berry",
      "Mohamed F. Tolba"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.10491v3",
    "title": "FuseMax: Leveraging Extended Einsums to Optimize Attention Accelerator\n  Design",
    "summary": "Attention for transformers is a critical workload that has recently received\nsignificant \"attention\" as a target for custom acceleration. Yet, while prior\nwork succeeds in reducing attention's memory-bandwidth requirements, it creates\nload imbalance between operators that comprise the attention computation\n(resulting in severe compute under-utilization) and requires on-chip memory\nthat scales with sequence length (which is expected to grow over time).\n  This paper ameliorates these issues, enabling attention with nearly 100%\ncompute utilization, no off-chip memory traffic bottlenecks, and on-chip buffer\nsize requirements that are independent of sequence length. The main conceptual\ncontribution is to use a recently proposed abstraction -- the cascade of\nEinsums -- to describe, formalize, and taxonomize the space of attention\nalgorithms that appear in the literature. In particular, we show how Einsum\ncascades can be used to infer non-trivial lower bounds on the number of passes\na kernel must take through its input data, which has implications for either\nrequired on-chip buffer capacity or memory traffic. We show how this notion can\nbe used to meaningfully divide the space of attention algorithms into several\ncategories and use these categories to inform our design process.\n  Based on the above characterization, we propose FuseMax -- a novel mapping\nand binding of attention onto a spatial array-style architecture. On attention,\nin an iso-area comparison, FuseMax achieves an average 6.7x speedup over the\nprior state-of-the-art, FLAT, while using 79\\% of the energy. Similarly, on\nfull end-to-end transformer inference, FuseMax achieves an average 5.3x speedup\nover FLAT using 83 of the energy.",
    "published": "2024-06-15T04:07:05Z",
    "updated": "2024-10-31T22:34:20Z",
    "authors": [
      "Nandeeka Nayak",
      "Xinrui Wu",
      "Toluwanimi O. Odemuyiwa",
      "Michael Pellauer",
      "Joel S. Emer",
      "Christopher W. Fletcher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.07417v2",
    "title": "Transformers in 3D Point Clouds: A Survey",
    "summary": "Transformers have been at the heart of the Natural Language Processing (NLP)\nand Computer Vision (CV) revolutions. The significant success in NLP and CV\ninspired exploring the use of Transformers in point cloud processing. However,\nhow do Transformers cope with the irregularity and unordered nature of point\nclouds? How suitable are Transformers for different 3D representations (e.g.,\npoint- or voxel-based)? How competent are Transformers for various 3D\nprocessing tasks? As of now, there is still no systematic survey of the\nresearch on these issues. For the first time, we provided a comprehensive\noverview of increasingly popular Transformers for 3D point cloud analysis. We\nstart by introducing the theory of the Transformer architecture and reviewing\nits applications in 2D/3D fields. Then, we present three different taxonomies\n(i.e., implementation-, data representation-, and task-based), which can\nclassify current Transformer-based methods from multiple perspectives.\nFurthermore, we present the results of an investigation of the variants and\nimprovements of the self-attention mechanism in 3D. To demonstrate the\nsuperiority of Transformers in point cloud analysis, we present comprehensive\ncomparisons of various Transformer-based methods for classification,\nsegmentation, and object detection. Finally, we suggest three potential\nresearch directions, providing benefit references for the development of 3D\nTransformers.",
    "published": "2022-05-16T01:32:18Z",
    "updated": "2022-09-21T15:10:21Z",
    "authors": [
      "Dening Lu",
      "Qian Xie",
      "Mingqiang Wei",
      "Kyle Gao",
      "Linlin Xu",
      "Jonathan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.00608v2",
    "title": "Near-Optimal Real-Time Personalization with Simple Transformers",
    "summary": "Real-time personalization has advanced significantly in recent years, with\nplatforms utilizing machine learning models to predict user preferences based\non rich behavioral data on each individual user. Traditional approaches usually\nrely on embedding-based machine learning models to capture user preferences,\nand then reduce the final optimization task to nearest-neighbors, which can be\nperformed extremely fast. However, these models struggle to capture complex\nuser behaviors, which are essential for making accurate recommendations.\nTransformer-based models, on the other hand, are known for their practical\nability to model sequential behaviors, and hence have been intensively used in\npersonalization recently to overcome these limitations. However, optimizing\nrecommendations under transformer-based models is challenging due to their\ncomplicated architectures. In this paper, we address this challenge by\nconsidering a specific class of transformers, showing its ability to represent\ncomplex user preferences, and developing efficient algorithms for real-time\npersonalization.\n  We focus on a particular set of transformers, called simple transformers,\nwhich contain a single self-attention layer. We show that simple transformers\nare capable of capturing complex user preferences. We then develop an algorithm\nthat enables fast optimization of recommendation tasks based on simple\ntransformers. Our algorithm achieves near-optimal performance in sub-linear\ntime. Finally, we demonstrate the effectiveness of our approach through an\nempirical study on datasets from Spotify and Trivago. Our experiment results\nshow that (1) simple transformers can model/predict user preferences\nsubstantially more accurately than non-transformer models and nearly as\naccurately as more complex transformers, and (2) our algorithm completes\nsimple-transformer-based recommendation tasks quickly and effectively.",
    "published": "2025-03-01T20:29:33Z",
    "updated": "2025-10-11T00:49:41Z",
    "authors": [
      "Lin An",
      "Andrew A. Li",
      "Vaisnavi Nemala",
      "Gabriel Visotsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.19006v1",
    "title": "Is attention truly all we need? An empirical study of asset pricing in\n  pretrained RNN sparse and global attention models",
    "summary": "This study investigates the pretrained RNN attention models with the\nmainstream attention mechanisms such as additive attention, Luong's three\nattentions, global self-attention (Self-att) and sliding window sparse\nattention (Sparse-att) for the empirical asset pricing research on top 420\nlarge-cap US stocks. This is the first paper on the large-scale\nstate-of-the-art (SOTA) attention mechanisms applied in the asset pricing\ncontext. They overcome the limitations of the traditional machine learning (ML)\nbased asset pricing, such as mis-capturing the temporal dependency and short\nmemory. Moreover, the enforced causal masks in the attention mechanisms address\nthe future data leaking issue ignored by the more advanced attention-based\nmodels, such as the classic Transformer. The proposed attention models also\nconsider the temporal sparsity characteristic of asset pricing data and\nmitigate potential overfitting issues by deploying the simplified model\nstructures. This provides some insights for future empirical economic research.\nAll models are examined in three periods, which cover pre-COVID-19 (mild\nuptrend), COVID-19 (steep uptrend with a large drawdown) and one year\npost-COVID-19 (sideways movement with high fluctuations), for testing the\nstability of these models under extreme market conditions. The study finds that\nin value-weighted portfolio back testing, Model Self-att and Model Sparse-att\nexhibit great capabilities in deriving the absolute returns and hedging\ndownside risks, while they achieve an annualized Sortino ratio of 2.0 and 1.80\nrespectively in the period with COVID-19. And Model Sparse-att performs more\nstably than Model Self-att from the perspective of absolute portfolio returns\nwith respect to the size of stocks' market capitalization.",
    "published": "2025-08-26T13:04:28Z",
    "updated": "2025-08-26T13:04:28Z",
    "authors": [
      "Shanyan Lai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1110.6015v2",
    "title": "Central factorials under the Kontorovich-Lebedev transform of\n  polynomials",
    "summary": "We show that slight modifications of the Kontorovich-Lebedev transform lead\nto an automorphism of the vector space of polynomials. This circumstance along\nwith the Mellin transformation property of the modified Bessel functions\nperform the passage of monomials to central factorial polynomials. A special\nattention is driven to the polynomial sequences whose KL-transform is the\ncanonical sequence, which will be fully characterized. Finally, new identities\nbetween the central factorials and the Euler polynomials are found.",
    "published": "2011-10-27T08:57:51Z",
    "updated": "2012-02-26T17:18:11Z",
    "authors": [
      "Ana F. Loureiro",
      "S. Yakubovich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1807.03385v1",
    "title": "Initial impacts of the transformation of a large introductory lab course\n  focused on developing experimental skills and expert epistemology",
    "summary": "Recently, there has been increased attention to improving laboratory\ninstruction at all levels. At the introductory level, research results have\nshown differing levels of success based on the nature of the desired learning\noutcomes. In response to these findings, the University of Colorado's\nintroductory physics lab course was transformed to improve students'\ndevelopment of experimental skills and experimental physics epistemology. We\ndescribe the details of the transformation process and initial self-reported\nlearning gains from the first implementation of the transformed course.",
    "published": "2018-07-09T21:00:43Z",
    "updated": "2018-07-09T21:00:43Z",
    "authors": [
      "H. J. Lewandowski",
      "Daniel R. Bolton",
      "Benjamin Pollard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1904.08378v1",
    "title": "Dynamic Evaluation of Transformer Language Models",
    "summary": "This research note combines two methods that have recently improved the state\nof the art in language modeling: Transformers and dynamic evaluation.\nTransformers use stacked layers of self-attention that allow them to capture\nlong range dependencies in sequential data. Dynamic evaluation fits models to\nthe recent sequence history, allowing them to assign higher probabilities to\nre-occurring sequential patterns. By applying dynamic evaluation to\nTransformer-XL models, we improve the state of the art on enwik8 from 0.99 to\n0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3\nto 16.4 perplexity points.",
    "published": "2019-04-17T17:26:01Z",
    "updated": "2019-04-17T17:26:01Z",
    "authors": [
      "Ben Krause",
      "Emmanuel Kahembwe",
      "Iain Murray",
      "Steve Renals"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.03443v1",
    "title": "BumbleBee: A Transformer for Music",
    "summary": "We will introduce BumbleBee, a transformer model that will generate MIDI\nmusic data . We will tackle the issue of transformers applied to long sequences\nby implementing a longformer generative model that uses dilating sliding\nwindows to compute the attention layers. We will compare our results to that of\nthe music transformer and Long-Short term memory (LSTM) to benchmark our\nresults. This analysis will be performed using piano MIDI files, in particular\n, the JSB Chorales dataset that has already been used for other research works\n(Huang et al., 2018)",
    "published": "2021-07-07T19:08:16Z",
    "updated": "2021-07-07T19:08:16Z",
    "authors": [
      "Lucas Fenaux",
      "Maria Juliana Quintero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.14627v1",
    "title": "Power Generalized DUS Transformation of Exponential Distribution",
    "summary": "DUS transformation of lifetime distributions received attention by engineers\nand researchers in recent years. The present study introduces a new class of\ndistribution using exponentiation of DUS transformation. A new distribution\nusing the Exponential distribution as the baseline distribution in this\ntransformation is proposed. The statistical properties of the proposed\ndistribution have been examined and the parameter estimation is done using the\nmethod of maximum likelihood. The fitness of the proposed model is established\nusing real data analysis.",
    "published": "2021-11-17T12:37:25Z",
    "updated": "2021-11-17T12:37:25Z",
    "authors": [
      "Beenu Thomas",
      "Chacko V M"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.11730v4",
    "title": "Stronger Graph Transformer with Regularized Attention Scores",
    "summary": "Graph Neural Networks are notorious for its memory consumption. A recent\nTransformer-based GNN called Graph Transformer is shown to obtain superior\nperformances when long range dependencies exist. However, combining graph data\nand Transformer architecture led to a combinationally worse memory issue. We\npropose a novel version of \"edge regularization technique\" that alleviates the\nneed for Positional Encoding and ultimately alleviate GT's out of memory issue.\nWe observe that it is not clear whether having an edge regularization on top of\npositional encoding is helpful. However, it seems evident that applying our\nedge regularization technique indeed stably improves GT's performance compared\nto GT without Positional Encoding.",
    "published": "2023-12-18T22:12:26Z",
    "updated": "2024-03-22T02:18:44Z",
    "authors": [
      "Eugene Ku"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.09268v1",
    "title": "Transformers, parallel computation, and logarithmic depth",
    "summary": "We show that a constant number of self-attention layers can efficiently\nsimulate, and be simulated by, a constant number of communication rounds of\nMassively Parallel Computation. As a consequence, we show that logarithmic\ndepth is sufficient for transformers to solve basic computational tasks that\ncannot be efficiently solved by several other neural sequence models and\nsub-quadratic transformer approximations. We thus establish parallelism as a\nkey distinguishing property of transformers.",
    "published": "2024-02-14T15:54:55Z",
    "updated": "2024-02-14T15:54:55Z",
    "authors": [
      "Clayton Sanford",
      "Daniel Hsu",
      "Matus Telgarsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15932v4",
    "title": "Steerable Transformers for Volumetric Data",
    "summary": "We introduce Steerable Transformers, an extension of the Vision Transformer\nmechanism that maintains equivariance to the special Euclidean group\n$\\mathrm{SE}(d)$. We propose an equivariant attention mechanism that operates\non features extracted by steerable convolutions. Operating in Fourier space,\nour network utilizes Fourier space non-linearities. Our experiments in both two\nand three dimensions show that adding steerable transformer layers to steerable\nconvolutional networks enhances performance.",
    "published": "2024-05-24T20:43:19Z",
    "updated": "2025-10-24T18:25:01Z",
    "authors": [
      "Soumyabrata Kundu",
      "Risi Kondor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.21060v1",
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms\n  Through Structured State Space Duality",
    "summary": "While Transformers have been the main architecture behind deep learning's\nsuccess in language modeling, state-space models (SSMs) such as Mamba have\nrecently been shown to match or outperform Transformers at small to medium\nscale. We show that these families of models are actually quite closely\nrelated, and develop a rich framework of theoretical connections between SSMs\nand variants of attention, connected through various decompositions of a\nwell-studied class of structured semiseparable matrices. Our state space\nduality (SSD) framework allows us to design a new architecture (Mamba-2) whose\ncore layer is an a refinement of Mamba's selective SSM that is 2-8X faster,\nwhile continuing to be competitive with Transformers on language modeling.",
    "published": "2024-05-31T17:50:01Z",
    "updated": "2024-05-31T17:50:01Z",
    "authors": [
      "Tri Dao",
      "Albert Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.05207v1",
    "title": "Low Latency Transformer Inference on FPGAs for Physics Applications with\n  hls4ml",
    "summary": "This study presents an efficient implementation of transformer architectures\nin Field-Programmable Gate Arrays(FPGAs) using hls4ml. We demonstrate the\nstrategy for implementing the multi-head attention, softmax, and normalization\nlayer and evaluate three distinct models. Their deployment on VU13P FPGA chip\nachieved latency less than 2us, demonstrating the potential for real-time\napplications. HLS4ML compatibility with any TensorFlow-built transformer model\nfurther enhances the scalability and applicability of this work. Index Terms:\nFPGAs, machine learning, transformers, high energy physics, LIGO",
    "published": "2024-09-08T19:50:25Z",
    "updated": "2024-09-08T19:50:25Z",
    "authors": [
      "Zhixing Jiang",
      "Dennis Yin",
      "Yihui Chen",
      "Elham E Khoda",
      "Scott Hauck",
      "Shih-Chieh Hsu",
      "Ekaterina Govorkova",
      "Philip Harris",
      "Vladimir Loncar",
      "Eric A. Moreno"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.09684v2",
    "title": "Exploring Transformers for Large-Scale Speech Recognition",
    "summary": "While recurrent neural networks still largely define state-of-the-art speech\nrecognition systems, the Transformer network has been proven to be a\ncompetitive alternative, especially in the offline condition. Most studies with\nTransformers have been constrained in a relatively small scale setting, and\nsome forms of data argumentation approaches are usually applied to combat the\ndata sparsity issue. In this paper, we aim at understanding the behaviors of\nTransformers in the large-scale speech recognition setting, where we have used\naround 65,000 hours of training data. We investigated various aspects on\nscaling up Transformers, including model initialization, warmup training as\nwell as different Layer Normalization strategies. In the streaming condition,\nwe compared the widely used attention mask based future context lookahead\napproach to the Transformer-XL network. From our experiments, we show that\nTransformers can achieve around 6% relative word error rate (WER) reduction\ncompared to the BLSTM baseline in the offline fashion, while in the streaming\nfashion, Transformer-XL is comparable to LC-BLSTM with 800 millisecond latency\nconstraint.",
    "published": "2020-05-19T18:07:14Z",
    "updated": "2020-08-11T18:51:37Z",
    "authors": [
      "Liang Lu",
      "Changliang Liu",
      "Jinyu Li",
      "Yifan Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.11681v2",
    "title": "Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual\n  Tracking",
    "summary": "In video object tracking, there exist rich temporal contexts among successive\nframes, which have been largely overlooked in existing trackers. In this work,\nwe bridge the individual video frames and explore the temporal contexts across\nthem via a transformer architecture for robust object tracking. Different from\nclassic usage of the transformer in natural language processing tasks, we\nseparate its encoder and decoder into two parallel branches and carefully\ndesign them within the Siamese-like tracking pipelines. The transformer encoder\npromotes the target templates via attention-based feature reinforcement, which\nbenefits the high-quality tracking model generation. The transformer decoder\npropagates the tracking cues from previous templates to the current frame,\nwhich facilitates the object searching process. Our transformer-assisted\ntracking framework is neat and trained in an end-to-end manner. With the\nproposed transformer, a simple Siamese matching approach is able to outperform\nthe current top-performing trackers. By combining our transformer with the\nrecent discriminative tracking pipeline, our method sets several new\nstate-of-the-art records on prevalent tracking benchmarks.",
    "published": "2021-03-22T09:20:05Z",
    "updated": "2021-03-24T09:23:57Z",
    "authors": [
      "Ning Wang",
      "Wengang Zhou",
      "Jie Wang",
      "Houqaing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.12165v3",
    "title": "Transformers in Medical Image Analysis: A Review",
    "summary": "Transformers have dominated the field of natural language processing, and\nrecently impacted the computer vision area. In the field of medical image\nanalysis, Transformers have also been successfully applied to full-stack\nclinical applications, including image synthesis/reconstruction, registration,\nsegmentation, detection, and diagnosis. Our paper aims to promote awareness and\napplication of Transformers in the field of medical image analysis.\nSpecifically, we first overview the core concepts of the attention mechanism\nbuilt into Transformers and other basic components. Second, we review various\nTransformer architectures tailored for medical image applications and discuss\ntheir limitations. Within this review, we investigate key challenges revolving\naround the use of Transformers in different learning paradigms, improving the\nmodel efficiency, and their coupling with other techniques. We hope this review\ncan give a comprehensive picture of Transformers to the readers in the field of\nmedical image analysis.",
    "published": "2022-02-24T16:04:03Z",
    "updated": "2022-08-19T08:03:50Z",
    "authors": [
      "Kelei He",
      "Chen Gan",
      "Zhuoyuan Li",
      "Islem Rekik",
      "Zihao Yin",
      "Wen Ji",
      "Yang Gao",
      "Qian Wang",
      "Junfeng Zhang",
      "Dinggang Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.00086v3",
    "title": "Applying Plain Transformers to Real-World Point Clouds",
    "summary": "To apply transformer-based models to point cloud understanding, many previous\nworks modify the architecture of transformers by using, e.g., local attention\nand down-sampling. Although they have achieved promising results, earlier works\non transformers for point clouds have two issues. First, the power of plain\ntransformers is still under-explored. Second, they focus on simple and small\npoint clouds instead of complex real-world ones. This work revisits the plain\ntransformers in real-world point cloud understanding. We first take a closer\nlook at some fundamental components of plain transformers, e.g., patchifier and\npositional embedding, for both efficiency and performance. To close the\nperformance gap due to the lack of inductive bias and annotated data, we\ninvestigate self-supervised pre-training with masked autoencoder (MAE).\nSpecifically, we propose drop patch, which prevents information leakage and\nsignificantly improves the effectiveness of MAE. Our models achieve SOTA\nresults in semantic segmentation on the S3DIS dataset and object detection on\nthe ScanNet dataset with lower computational costs. Our work provides a new\nbaseline for future research on transformers for point clouds.",
    "published": "2023-02-28T21:06:36Z",
    "updated": "2023-08-06T13:37:00Z",
    "authors": [
      "Lanxiao Li",
      "Michael Heizmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2009.02070v2",
    "title": "AutoTrans: Automating Transformer Design via Reinforced Architecture\n  Search",
    "summary": "Though the transformer architectures have shown dominance in many natural\nlanguage understanding tasks, there are still unsolved issues for the training\nof transformer models, especially the need for a principled way of warm-up\nwhich has shown importance for stable training of a transformer, as well as\nwhether the task at hand prefer to scale the attention product or not. In this\npaper, we empirically explore automating the design choices in the transformer\nmodel, i.e., how to set layer-norm, whether to scale, number of layers, number\nof heads, activation function, etc, so that one can obtain a transformer\narchitecture that better suits the tasks at hand. RL is employed to navigate\nalong search space, and special parameter sharing strategies are designed to\naccelerate the search. It is shown that sampling a proportion of training data\nper epoch during search help to improve the search quality. Experiments on the\nCoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer\nmodel can outperform the standard transformers. In particular, we show that our\nlearned model can be trained more robustly with large learning rates without\nwarm-up.",
    "published": "2020-09-04T08:46:22Z",
    "updated": "2021-05-30T12:45:31Z",
    "authors": [
      "Wei Zhu",
      "Xiaoling Wang",
      "Xipeng Qiu",
      "Yuan Ni",
      "Guotong Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.02598v1",
    "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for\n  Intent Classification",
    "summary": "End-to-end intent classification using speech has numerous advantages\ncompared to the conventional pipeline approach using automatic speech\nrecognition (ASR), followed by natural language processing modules. It attempts\nto predict intent from speech without using an intermediate ASR module.\nHowever, such end-to-end framework suffers from the unavailability of large\nspeech resources with higher acoustic variation in spoken language\nunderstanding. In this work, we exploit the scope of the transformer\ndistillation method that is specifically designed for knowledge distillation\nfrom a transformer based language model to a transformer based speech model. In\nthis regard, we leverage the reliable and widely used bidirectional encoder\nrepresentations from transformers (BERT) model as a language model and transfer\nthe knowledge to build an acoustic model for intent classification using the\nspeech. In particular, a multilevel transformer based teacher-student model is\ndesigned, and knowledge distillation is performed across attention and hidden\nsub-layers of different transformer layers of the student and teacher models.\nWe achieve an intent classification accuracy of 99.10% and 88.79% for Fluent\nspeech corpus and ATIS database, respectively. Further, the proposed method\ndemonstrates better performance and robustness in acoustically degraded\ncondition compared to the baseline method.",
    "published": "2021-08-05T13:08:13Z",
    "updated": "2021-08-05T13:08:13Z",
    "authors": [
      "Yidi Jiang",
      "Bidisha Sharma",
      "Maulik Madhavi",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.11870v1",
    "title": "DBIA: Data-free Backdoor Injection Attack against Transformer Networks",
    "summary": "Recently, transformer architecture has demonstrated its significance in both\nNatural Language Processing (NLP) and Computer Vision (CV) tasks. Though other\nnetwork models are known to be vulnerable to the backdoor attack, which embeds\ntriggers in the model and controls the model behavior when the triggers are\npresented, little is known whether such an attack is still valid on the\ntransformer models and if so, whether it can be done in a more cost-efficient\nmanner. In this paper, we propose DBIA, a novel data-free backdoor attack\nagainst the CV-oriented transformer networks, leveraging the inherent attention\nmechanism of transformers to generate triggers and injecting the backdoor using\nthe poisoned surrogate dataset. We conducted extensive experiments based on\nthree benchmark transformers, i.e., ViT, DeiT and Swin Transformer, on two\nmainstream image classification tasks, i.e., CIFAR10 and ImageNet. The\nevaluation results demonstrate that, consuming fewer resources, our approach\ncan embed backdoors with a high success rate and a low impact on the\nperformance of the victim transformers. Our code is available at\nhttps://anonymous.4open.science/r/DBIA-825D.",
    "published": "2021-11-22T08:13:51Z",
    "updated": "2021-11-22T08:13:51Z",
    "authors": [
      "Peizhuo Lv",
      "Hualong Ma",
      "Jiachen Zhou",
      "Ruigang Liang",
      "Kai Chen",
      "Shengzhi Zhang",
      "Yunfei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.13204v1",
    "title": "Geometric algebra generation of molecular surfaces",
    "summary": "Geometric algebra is a powerful framework that unifies mathematics and\nphysics. Since its revival in the middle of the 1960s by David Hestenes, it\nattracts great attention and has been exploited in many fields such as physics,\ncomputer science, and engineering. This work introduces a geometric algebra\nmethod for the molecular surface generation that utilizes the Clifford-Fourier\ntransform which is a generalization of the classical Fourier transform.\nNotably, the classical Fourier transform and Clifford-Fourier transform differ\nin the derivative property in $R_k$ for k even. This distinction is due to the\nnoncommutativity of geometric product of pseudoscalars with multivectors and\nhas significant consequences in applications. We use the Clifford-Fourier\ntransform in $R_3$ to benefit from the derivative property in solving partial\ndifferential equations (PDEs). The Clifford-Fourier transform is used to solve\nthe mode decomposition process in PDE transform. Two different initial cases\nare proposed to make the initial shapes used in the present method. The\nproposed method is applied first to small molecules and proteins. To validate\nthe method, the molecular surfaces generated are compared to surfaces of other\ndefinitions. Applications are considered to protein electrostatic analysis.\nThis work opens the door for further applications of geometric algebra and\nClifford-Fourier transform in biological sciences.",
    "published": "2021-12-25T08:02:46Z",
    "updated": "2021-12-25T08:02:46Z",
    "authors": [
      "Azzam Alfarraj",
      "Guo-Wei Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.12708v1",
    "title": "Faith: An Efficient Framework for Transformer Verification on GPUs",
    "summary": "Transformer verification draws increasing attention in machine learning\nresearch and industry. It formally verifies the robustness of transformers\nagainst adversarial attacks such as exchanging words in a sentence with\nsynonyms. However, the performance of transformer verification is still not\nsatisfactory due to bound-centric computation which is significantly different\nfrom standard neural networks. In this paper, we propose Faith, an efficient\nframework for transformer verification on GPUs. We first propose a\nsemantic-aware computation graph transformation to identify semantic\ninformation such as bound computation in transformer verification. We exploit\nsuch semantic information to enable efficient kernel fusion at the computation\ngraph level. Second, we propose a verification-specialized kernel crafter to\nefficiently map transformer verification to modern GPUs. This crafter exploits\na set of GPU hardware supports to accelerate verification specialized\noperations which are usually memory-intensive. Third, we propose an\nexpert-guided autotuning to incorporate expert knowledge on GPU backends to\nfacilitate large search space exploration. Extensive evaluations show that\nFaith achieves $2.1\\times$ to $3.4\\times$ ($2.6\\times$ on average) speedup over\nstate-of-the-art frameworks.",
    "published": "2022-09-23T15:07:22Z",
    "updated": "2022-09-23T15:07:22Z",
    "authors": [
      "Boyuan Feng",
      "Tianqi Tang",
      "Yuke Wang",
      "Zhaodong Chen",
      "Zheng Wang",
      "Shu Yang",
      "Yuan Xie",
      "Yufei Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.12506v1",
    "title": "Exploring the Enablers of Digital Transformation in Small and\n  Medium-Sized Enterprise",
    "summary": "Recently, digital transformation has caught much attention of both academics\nand practitioners. With the advent of digital technologies,\nsmall-and-medium-sized enterprises (SMEs) have obtained the capacity to\ninitiate digital transformation initiatives in a similar fashion to large-sized\norganizations. The innate characteristics of digital technologies also favor\nSMEs in promoting initiation of digital transformation. However, the process\ndigital transformation in SMEs remains a black box and the existing findings of\ndigital transformation in SMEs are limited and remain fragmented. Considering\nthe important contribution SMEs can offer to nations and economies; it is\ntimely and relevant to conduct a profound analysis on digital transformation in\nSMEs. By conducting a thorough review of existing related literature in\nmanagement, information systems, and business disciplines, this book chapter\naims to understand both internal and external enablers of the digital\ntransformation in SMEs.",
    "published": "2023-02-24T08:30:25Z",
    "updated": "2023-02-24T08:30:25Z",
    "authors": [
      "Sachithra Lokuge",
      "Sophia Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.01082v2",
    "title": "Linear attention is (maybe) all you need (to understand transformer\n  optimization)",
    "summary": "Transformer training is notoriously difficult, requiring a careful design of\noptimizers and use of various heuristics. We make progress towards\nunderstanding the subtleties of training Transformers by carefully studying a\nsimple yet canonical linearized shallow Transformer model. Specifically, we\ntrain linear Transformers to solve regression tasks, inspired by J.~von Oswald\net al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we\nobserve that our proposed linearized models can reproduce several prominent\naspects of Transformer training dynamics. Consequently, the results obtained in\nthis paper suggest that a simple linearized Transformer model could actually be\na valuable, realistic abstraction for understanding Transformer optimization.",
    "published": "2023-10-02T10:48:42Z",
    "updated": "2024-03-13T16:48:27Z",
    "authors": [
      "Kwangjun Ahn",
      "Xiang Cheng",
      "Minhak Song",
      "Chulhee Yun",
      "Ali Jadbabaie",
      "Suvrit Sra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.10930v1",
    "title": "Enhanced Transformer Architecture for Natural Language Processing",
    "summary": "Transformer is a state-of-the-art model in the field of natural language\nprocessing (NLP). Current NLP models primarily increase the number of\ntransformers to improve processing performance. However, this technique\nrequires a lot of training resources such as computing capacity. In this paper,\na novel structure of Transformer is proposed. It is featured by full layer\nnormalization, weighted residual connection, positional encoding exploiting\nreinforcement learning, and zero masked self-attention. The proposed\nTransformer model, which is called Enhanced Transformer, is validated by the\nbilingual evaluation understudy (BLEU) score obtained with the Multi30k\ntranslation dataset. As a result, the Enhanced Transformer achieves 202.96%\nhigher BLEU score as compared to the original transformer with the translation\ndataset.",
    "published": "2023-10-17T01:59:07Z",
    "updated": "2023-10-17T01:59:07Z",
    "authors": [
      "Woohyeon Moon",
      "Taeyoung Kim",
      "Bumgeun Park",
      "Dongsoo Har"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.01429v1",
    "title": "Efficient Vision Transformer for Accurate Traffic Sign Detection",
    "summary": "This research paper addresses the challenges associated with traffic sign\ndetection in self-driving vehicles and driver assistance systems. The\ndevelopment of reliable and highly accurate algorithms is crucial for the\nwidespread adoption of traffic sign recognition and detection (TSRD) in diverse\nreal-life scenarios. However, this task is complicated by suboptimal traffic\nimages affected by factors such as camera movement, adverse weather conditions,\nand inadequate lighting. This study specifically focuses on traffic sign\ndetection methods and introduces the application of the Transformer model,\nparticularly the Vision Transformer variants, to tackle this task. The\nTransformer's attention mechanism, originally designed for natural language\nprocessing, offers improved parallel efficiency. Vision Transformers have\ndemonstrated success in various domains, including autonomous driving, object\ndetection, healthcare, and defense-related applications. To enhance the\nefficiency of the Transformer model, the research proposes a novel strategy\nthat integrates a locality inductive bias and a transformer module. This\nincludes the introduction of the Efficient Convolution Block and the Local\nTransformer Block, which effectively capture short-term and long-term\ndependency information, thereby improving both detection speed and accuracy.\nExperimental evaluations demonstrate the significant advancements achieved by\nthis approach, particularly when applied to the GTSDB dataset.",
    "published": "2023-11-02T17:44:32Z",
    "updated": "2023-11-02T17:44:32Z",
    "authors": [
      "Javad Mirzapour Kaleybar",
      "Hooman Khaloo",
      "Avaz Naghipour"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.02005v2",
    "title": "Topology-Informed Graph Transformer",
    "summary": "Transformers have revolutionized performance in Natural Language Processing\nand Vision, paving the way for their integration with Graph Neural Networks\n(GNNs). One key challenge in enhancing graph transformers is strengthening the\ndiscriminative power of distinguishing isomorphisms of graphs, which plays a\ncrucial role in boosting their predictive performances. To address this\nchallenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel\ntransformer enhancing both discriminative power in detecting graph isomorphisms\nand the overall performance of Graph Transformers. TIGT consists of four\ncomponents: A topological positional embedding layer using non-isomorphic\nuniversal covers based on cyclic subgraphs of graphs to ensure unique graph\nrepresentation: A dual-path message-passing layer to explicitly encode\ntopological characteristics throughout the encoder layers: A global attention\nmechanism: And a graph information layer to recalibrate channel-wise graph\nfeatures for better feature representation. TIGT outperforms previous Graph\nTransformers in classifying synthetic dataset aimed at distinguishing\nisomorphism classes of graphs. Additionally, mathematical analysis and\nempirical evaluations highlight our model's competitive edge over\nstate-of-the-art Graph Transformers across various benchmark datasets.",
    "published": "2024-02-03T03:17:44Z",
    "updated": "2025-03-01T13:45:42Z",
    "authors": [
      "Yun Young Choi",
      "Sun Woo Park",
      "Minho Lee",
      "Youngho Woo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.11311v1",
    "title": "Discrete Quaternion Quadratic Phase Fourier Transform",
    "summary": "A novel addition to the family of integral transforms, the quadratic phase\nFourier transform (QPFT) embodies a variety of signal processing tools,\nincluding the Fourier transform (FT), fractional Fourier transform (FRFT),\nlinear canonical transform (LCT), and special affine Fourier transforms. Due to\nits additional degrees of freedom, QPFT performs better in applications than\nother time-frequency analysis methods. Recently, quaternion quadratic phase\nFourier (QQPFT), an extension of the QPFT in quaternion algebra, has been\nderived and since received noticeable attention because of its expressiveness\nand grace in the analysis of multidimensional quaternion-valued signals and\nvisuals. To the best of our knowledge, the discrete form of the QQPFT is\nundefined, making it impossible to compute the QQPFT using digital techniques\nat this time. It initiated us to introduce the two-dimensional (2D) discrete\nquaternion quadratic phase Fourier (DQQPFT) that is analogous to the 2D\ndiscrete quaternion Fourier transform (DQFT). Some fundamental properties\nincluding Modulation, the reconstruction formula and the Plancherel theorem of\nthe 2D DQQPFT are obtained. Crucially, the fast computation algorithm and\nconvolution theorem of 2D DQQPFT which are essential for engineering\napplications are also taken into account. Finally, we present an application of\nthe DQQPFT to study the two-dimensional discrete linear time-varying systems.",
    "published": "2024-02-17T15:50:05Z",
    "updated": "2024-02-17T15:50:05Z",
    "authors": [
      "Aamir Hamid Dar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.12691v2",
    "title": "Tree-Planted Transformers: Unidirectional Transformer Language Models\n  with Implicit Syntactic Supervision",
    "summary": "Syntactic Language Models (SLMs) can be trained efficiently to reach\nrelatively high performance; however, they have trouble with inference\nefficiency due to the explicit generation of syntactic structures. In this\npaper, we propose a new method dubbed tree-planting: instead of explicitly\ngenerating syntactic structures, we \"plant\" trees into attention weights of\nunidirectional Transformer LMs to implicitly reflect syntactic structures of\nnatural language. Specifically, unidirectional Transformer LMs trained with\ntree-planting will be called Tree-Planted Transformers (TPT), which inherit the\ntraining efficiency from SLMs without changing the inference efficiency of\ntheir underlying Transformer LMs. Targeted syntactic evaluations on the\nSyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit\ngeneration of syntactic structures, significantly outperformed not only vanilla\nTransformer LMs but also various SLMs that generate hundreds of syntactic\nstructures in parallel. This result suggests that TPTs can learn human-like\nsyntactic knowledge as data-efficiently as SLMs while maintaining the modeling\nspace of Transformer LMs unchanged.",
    "published": "2024-02-20T03:37:24Z",
    "updated": "2024-06-06T13:16:16Z",
    "authors": [
      "Ryo Yoshida",
      "Taiga Someya",
      "Yohei Oseki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.15451v2",
    "title": "CFPFormer: Feature-pyramid like Transformer Decoder for Segmentation and\n  Detection",
    "summary": "Feature pyramids have been widely adopted in convolutional neural networks\nand transformers for tasks in medical image segmentation. However, existing\nmodels generally focus on the Encoder-side Transformer for feature extraction.\nWe further explore the potential in improving the feature decoder with a\nwell-designed architecture. We propose Cross Feature Pyramid Transformer\ndecoder (CFPFormer), a novel decoder block that integrates feature pyramids and\ntransformers. Even though transformer-like architecture impress with\noutstanding performance in segmentation, the concerns to reduce the redundancy\nand training costs still exist. Specifically, by leveraging patch embedding,\ncross-layer feature concatenation mechanisms, CFPFormer enhances feature\nextraction capabilities while complexity issue is mitigated by our Gaussian\nAttention. Benefiting from Transformer structure and U-shaped connections, our\nwork is capable of capturing long-range dependencies and effectively up-sample\nfeature maps. Experimental results are provided to evaluate CFPFormer on\nmedical image segmentation datasets, demonstrating the efficacy and\neffectiveness. With a ResNet50 backbone, our method achieves 92.02\\% Dice\nScore, highlighting the efficacy of our methods. Notably, our VGG-based model\noutperformed baselines with more complex ViT and Swin Transformer backbone.",
    "published": "2024-04-23T18:46:07Z",
    "updated": "2025-04-05T23:18:49Z",
    "authors": [
      "Hongyi Cai",
      "Mohammad Mahdinur Rahman",
      "Wenzhen Dong",
      "Jingyu Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.12638v1",
    "title": "ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer\n  Neural Networks",
    "summary": "Transformers have emerged as a powerful tool for natural language processing\n(NLP) and computer vision. Through the attention mechanism, these models have\nexhibited remarkable performance gains when compared to conventional approaches\nlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\nNevertheless, transformers typically demand substantial execution time due to\ntheir extensive computations and large memory footprint. Processing in-memory\n(PIM) and near-memory computing (NMC) are promising solutions to accelerating\ntransformers as they offer high compute parallelism and memory bandwidth.\nHowever, designing PIM/NMC architectures to support the complex operations and\nmassive amounts of data that need to be moved between layers in transformer\nneural networks remains a challenge. We propose ARTEMIS, a mixed\nanalog-stochastic in-DRAM accelerator for transformer models. Through employing\nminimal changes to the conventional DRAM arrays, ARTEMIS efficiently alleviates\nthe costs associated with transformer model execution by supporting stochastic\ncomputing for multiplications and temporal analog accumulations using a novel\nin-DRAM metal-on-metal capacitor. Our analysis indicates that ARTEMIS exhibits\nat least 3.0x speedup, 1.8x lower energy, and 1.9x better energy efficiency\ncompared to GPU, TPU, CPU, and state-of-the-art PIM transformer hardware\naccelerators.",
    "published": "2024-07-17T15:08:14Z",
    "updated": "2024-07-17T15:08:14Z",
    "authors": [
      "Salma Afifi",
      "Ishan Thakkar",
      "Sudeep Pasricha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.19931v2",
    "title": "Provable optimal transport with transformers: The essence of depth and\n  prompt engineering",
    "summary": "Can we establish provable performance guarantees for transformers?\nEstablishing such theoretical guarantees is a milestone in developing\ntrustworthy generative AI. In this paper, we take a step toward addressing this\nquestion by focusing on optimal transport, a fundamental problem at the\nintersection of combinatorial and continuous optimization. Leveraging the\ncomputational power of attention layers, we prove that a transformer with fixed\nparameters can effectively solve the optimal transport problem in Wasserstein-2\nwith entropic regularization for an arbitrary number of points. Consequently,\nthe transformer can sort lists of arbitrary sizes up to an approximation\nfactor. Our results rely on an engineered prompt that enables the transformer\nto implement gradient descent with adaptive stepsizes on the dual optimal\ntransport. Combining the convergence analysis of gradient descent with Sinkhorn\ndynamics, we establish an explicit approximation bound for optimal transport\nwith transformers, which improves as depth increases. Our findings provide\nnovel insights into the essence of prompt engineering and depth for solving\noptimal transport. In particular, prompt engineering boosts the algorithmic\nexpressivity of transformers, allowing them implement an optimization method.\nWith increasing depth, transformers can simulate several iterations of gradient\ndescent.",
    "published": "2024-10-25T19:07:29Z",
    "updated": "2024-11-01T16:54:46Z",
    "authors": [
      "Hadi Daneshmand"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.16993v1",
    "title": "Tree Transformers are an Ineffective Model of Syntactic Constituency",
    "summary": "Linguists have long held that a key aspect of natural language syntax is the\nrecursive organization of language units into constituent structures, and\nresearch has suggested that current state-of-the-art language models lack an\ninherent bias towards this feature. A number of alternative models have been\nproposed to provide inductive biases towards constituency, including the Tree\nTransformer, which utilizes a modified attention mechanism to organize tokens\ninto constituents.\n  We investigate Tree Transformers to study whether they utilize meaningful\nand/or useful constituent structures. We pretrain a large Tree Transformer on\nlanguage modeling in order to investigate the learned constituent tree\nrepresentations of sentences, finding little evidence for meaningful\nstructures. Next, we evaluate Tree Transformers with similar transformer models\non error detection tasks requiring constituent structure. We find that while\nthe Tree Transformer models may slightly outperform at these tasks, there is\nlittle evidence to suggest a meaningful improvement. In general, we conclude\nthat there is little evidence to support Tree Transformer as an effective model\nof syntactic constituency.",
    "published": "2024-11-25T23:53:46Z",
    "updated": "2024-11-25T23:53:46Z",
    "authors": [
      "Michael Ginn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.10599v1",
    "title": "Advances in Transformers for Robotic Applications: A Review",
    "summary": "The introduction of Transformers architecture has brought about significant\nbreakthroughs in Deep Learning (DL), particularly within Natural Language\nProcessing (NLP). Since their inception, Transformers have outperformed many\ntraditional neural network architectures due to their \"self-attention\"\nmechanism and their scalability across various applications. In this paper, we\ncover the use of Transformers in Robotics. We go through recent advances and\ntrends in Transformer architectures and examine their integration into robotic\nperception, planning, and control for autonomous systems. Furthermore, we\nreview past work and recent research on use of Transformers in Robotics as\npre-trained foundation models and integration of Transformers with Deep\nReinforcement Learning (DRL) for autonomous systems. We discuss how different\nTransformer variants are being adapted in robotics for reliable planning and\nperception, increasing human-robot interaction, long-horizon decision-making,\nand generalization. Finally, we address limitations and challenges, offering\ninsight and suggestions for future research directions.",
    "published": "2024-12-13T23:02:15Z",
    "updated": "2024-12-13T23:02:15Z",
    "authors": [
      "Nikunj Sanghai",
      "Nik Bear Brown"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16533v2",
    "title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
    "summary": "Graph Transformers (GTs) have demonstrated a strong capability in modeling\ngraph structures by addressing the intrinsic limitations of graph neural\nnetworks (GNNs), such as over-smoothing and over-squashing. Recent studies have\nproposed diverse architectures, enhanced explainability, and practical\napplications for Graph Transformers. In light of these rapid developments, we\nconduct a comprehensive review of Graph Transformers, covering aspects such as\ntheir architectures, theoretical foundations, and applications within this\nsurvey. We categorize the architecture of Graph Transformers according to their\nstrategies for processing structural information, including graph tokenization,\npositional encoding, structure-aware attention and model ensemble. Furthermore,\nfrom the theoretical perspective, we examine the expressivity of Graph\nTransformers in various discussed architectures and contrast them with other\nadvanced graph learning algorithms to discover the connections. Furthermore, we\nprovide a summary of the practical applications where Graph Transformers have\nbeen utilized, such as molecule, protein, language, vision, traffic, brain and\nmaterial data. At the end of this survey, we will discuss the current\nchallenges and prospective directions in Graph Transformers for potential\nfuture research.",
    "published": "2025-02-23T10:55:19Z",
    "updated": "2025-02-27T06:17:29Z",
    "authors": [
      "Chaohao Yuan",
      "Kangfei Zhao",
      "Ercan Engin Kuruoglu",
      "Liang Wang",
      "Tingyang Xu",
      "Wenbing Huang",
      "Deli Zhao",
      "Hong Cheng",
      "Yu Rong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01835v1",
    "title": "Primus: Enforcing Attention Usage for 3D Medical Image Segmentation",
    "summary": "Transformers have achieved remarkable success across multiple fields, yet\ntheir impact on 3D medical image segmentation remains limited with\nconvolutional networks still dominating major benchmarks. In this work, we a)\nanalyze current Transformer-based segmentation models and identify critical\nshortcomings, particularly their over-reliance on convolutional blocks.\nFurther, we demonstrate that in some architectures, performance is unaffected\nby the absence of the Transformer, thereby demonstrating their limited\neffectiveness. To address these challenges, we move away from hybrid\narchitectures and b) introduce a fully Transformer-based segmentation\narchitecture, termed Primus. Primus leverages high-resolution tokens, combined\nwith advances in positional embeddings and block design, to maximally leverage\nits Transformer blocks. Through these adaptations Primus surpasses current\nTransformer-based methods and competes with state-of-the-art convolutional\nmodels on multiple public datasets. By doing so, we create the first pure\nTransformer architecture and take a significant step towards making\nTransformers state-of-the-art for 3D medical image segmentation.",
    "published": "2025-03-03T18:56:29Z",
    "updated": "2025-03-03T18:56:29Z",
    "authors": [
      "Tassilo Wald",
      "Saikat Roy",
      "Fabian Isensee",
      "Constantin Ulrich",
      "Sebastian Ziegler",
      "Dasha Trofimova",
      "Raphael Stock",
      "Michael Baumgartner",
      "Gregor KÃ¶hler",
      "Klaus Maier-Hein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16055v2",
    "title": "Knee-Deep in C-RASP: A Transformer Depth Hierarchy",
    "summary": "It has been observed that transformers with greater depth (that is, more\nlayers) have more capabilities, but can we establish formally which\ncapabilities are gained? We answer this question with a theoretical proof\nfollowed by an empirical study. First, we consider transformers that round to\nfixed precision except inside attention. We show that this subclass of\ntransformers is expressively equivalent to the programming language C-RASP and\nthis equivalence preserves depth. Second, we prove that deeper C-RASP programs\nare more expressive than shallower C-RASP programs, implying that deeper\ntransformers are more expressive than shallower transformers (within the\nsubclass mentioned above). The same is also proven for transformers with\npositional encodings (like RoPE and ALiBi). These results are established by\nstudying a temporal logic with counting operators equivalent to C-RASP.\nFinally, we provide empirical evidence that our theory predicts the depth\nrequired for transformers without positional encodings to length-generalize on\na family of sequential dependency tasks.",
    "published": "2025-06-19T06:27:54Z",
    "updated": "2025-10-24T17:50:04Z",
    "authors": [
      "Andy Yang",
      "MichaÃ«l Cadilhac",
      "David Chiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.04125v1",
    "title": "Graph Neural Networks as a Substitute for Transformers in Single-Cell\n  Transcriptomics",
    "summary": "Graph Neural Networks (GNNs) and Transformers share significant similarities\nin their encoding strategies for interacting with features from nodes of\ninterest, where Transformers use query-key scores and GNNs use edges. Compared\nto GNNs, which are unable to encode relative positions, Transformers leverage\ndynamic attention capabilities to better represent relative relationships,\nthereby becoming the standard backbones in large-scale sequential pre-training.\nHowever, the subtle difference prompts us to consider: if positions are no\nlonger crucial, could we substitute Transformers with Graph Neural Networks in\nsome fields such as Single-Cell Transcriptomics? In this paper, we first\nexplore the similarities and differences between GNNs and Transformers,\nspecifically in terms of relative positions. Additionally, we design a\nsynthetic example to illustrate their equivalence where there are no relative\npositions between tokens in the sample. Finally, we conduct extensive\nexperiments on a large-scale position-agnostic dataset-single-cell\ntranscriptomics-finding that GNNs achieve competitive performance compared to\nTransformers while consuming fewer computation resources. These findings\nprovide novel insights for researchers in the field of single-cell\ntranscriptomics, challenging the prevailing notion that the Transformer is\nalways the optimum choice.",
    "published": "2025-07-05T18:37:16Z",
    "updated": "2025-07-05T18:37:16Z",
    "authors": [
      "Jiaxin Qi",
      "Yan Cui",
      "Jinli Ou",
      "Jianqiang Huang",
      "Gaogang Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15782v1",
    "title": "Learning in Focus: Detecting Behavioral and Collaborative Engagement\n  Using Vision Transformers",
    "summary": "In early childhood education, accurately detecting behavioral and\ncollaborative engagement is essential for fostering meaningful learning\nexperiences. This paper presents an AI-driven approach that leverages Vision\nTransformers (ViTs) to automatically classify children's engagement using\nvisual cues such as gaze direction, interaction, and peer collaboration.\nUtilizing the Child-Play gaze dataset, our method is trained on annotated video\nsegments to classify behavioral and collaborative engagement states (e.g.,\nengaged, not engaged, collaborative, not collaborative). We evaluated three\nstate-of-the-art transformer models: Vision Transformer (ViT), Data-efficient\nImage Transformer (DeiT), and Swin Transformer. Among these, the Swin\nTransformer achieved the highest classification performance with an accuracy of\n97.58%, demonstrating its effectiveness in modeling local and global attention.\nOur results highlight the potential of transformer-based architectures for\nscalable, automated engagement analysis in real-world educational settings.",
    "published": "2025-08-05T22:26:07Z",
    "updated": "2025-08-05T22:26:07Z",
    "authors": [
      "Sindhuja Penchala",
      "Saketh Reddy Kontham",
      "Prachi Bhattacharjee",
      "Sareh Karami",
      "Mehdi Ghahremani",
      "Noorbakhsh Amiri Golilarz",
      "Shahram Rahimi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.12155v3",
    "title": "Transformer-based End-to-End Speech Recognition with Local Dense\n  Synthesizer Attention",
    "summary": "Recently, several studies reported that dot-product selfattention (SA) may\nnot be indispensable to the state-of-theart Transformer models. Motivated by\nthe fact that dense synthesizer attention (DSA), which dispenses with dot\nproducts and pairwise interactions, achieved competitive results in many\nlanguage processing tasks, in this paper, we first propose a DSA-based speech\nrecognition, as an alternative to SA. To reduce the computational complexity\nand improve the performance, we further propose local DSA (LDSA) to restrict\nthe attention scope of DSA to a local range around the current central frame\nfor speech recognition. Finally, we combine LDSA with SA to extract the local\nand global information simultaneously. Experimental results on the Ai-shell1\nMandarine speech recognition corpus show that the proposed LDSA-Transformer\nachieves a character error rate (CER) of 6.49%, which is slightly better than\nthat of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less\ncomputation than the SATransformer. The proposed combination method not only\nachieves a CER of 6.18%, which significantly outperforms the SA-Transformer,\nbut also has roughly the same number of parameters and computational complexity\nas the latter. The implementation of the multi-head LDSA is available at\nhttps://github.com/mlxu995/multihead-LDSA.",
    "published": "2020-10-23T04:13:44Z",
    "updated": "2021-07-24T03:52:37Z",
    "authors": [
      "Menglong Xu",
      "Shengqiang Li",
      "Xiao-Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.15553v3",
    "title": "MoViT: Memorizing Vision Transformers for Medical Image Analysis",
    "summary": "The synergy of long-range dependencies from transformers and local\nrepresentations of image content from convolutional neural networks (CNNs) has\nled to advanced architectures and increased performance for various medical\nimage analysis tasks due to their complementary benefits. However, compared\nwith CNNs, transformers require considerably more training data, due to a\nlarger number of parameters and an absence of inductive bias. The need for\nincreasingly large datasets continues to be problematic, particularly in the\ncontext of medical imaging, where both annotation efforts and data protection\nresult in limited data availability. In this work, inspired by the human\ndecision-making process of correlating new evidence with previously memorized\nexperience, we propose a Memorizing Vision Transformer (MoViT) to alleviate the\nneed for large-scale datasets to successfully train and deploy\ntransformer-based architectures. MoViT leverages an external memory structure\nto cache history attention snapshots during the training stage. To prevent\noverfitting, we incorporate an innovative memory update scheme, attention\ntemporal moving average, to update the stored external memories with the\nhistorical moving average. For inference speedup, we design a prototypical\nattention learning method to distill the external memory into smaller\nrepresentative subsets. We evaluate our method on a public histology image\ndataset and an in-house MRI dataset, demonstrating that MoViT applied to varied\nmedical image analysis tasks, can outperform vanilla transformer models across\nvaried data regimes, especially in cases where only a small amount of annotated\ndata is available. More importantly, MoViT can reach a competitive performance\nof ViT with only 3.0% of the training data.",
    "published": "2023-03-27T19:12:02Z",
    "updated": "2023-09-29T20:14:37Z",
    "authors": [
      "Yiqing Shen",
      "Pengfei Guo",
      "Jingpu Wu",
      "Qianqi Huang",
      "Nhat Le",
      "Jinyuan Zhou",
      "Shanshan Jiang",
      "Mathias Unberath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.08077v4",
    "title": "Scene Text Recognition via Transformer",
    "summary": "Scene text recognition with arbitrary shape is very challenging due to large\nvariations in text shapes, fonts, colors, backgrounds, etc. Most\nstate-of-the-art algorithms rectify the input image into the normalized image,\nthen treat the recognition as a sequence prediction task. The bottleneck of\nsuch methods is the rectification, which will cause errors due to distortion\nperspective. In this paper, we find that the rectification is completely\nunnecessary. What all we need is the spatial attention. We therefore propose a\nsimple but extremely effective scene text recognition method based on\ntransformer [50]. Different from previous transformer based models [56,34],\nwhich just use the decoder of the transformer to decode the convolutional\nattention, the proposed method use a convolutional feature maps as word\nembedding input into transformer. In such a way, our method is able to make\nfull use of the powerful attention mechanism of the transformer. Extensive\nexperimental results show that the proposed method significantly outperforms\nstate-of-the-art methods by a very large margin on both regular and irregular\ntext datasets. On one of the most challenging CUTE dataset whose\nstate-of-the-art prediction accuracy is 89.6%, our method achieves 99.3%, which\nis a pretty surprising result. We will release our source code and believe that\nour method will be a new benchmark of scene text recognition with arbitrary\nshapes.",
    "published": "2020-03-18T07:38:02Z",
    "updated": "2020-04-29T02:56:28Z",
    "authors": [
      "Xinjie Feng",
      "Hongxun Yao",
      "Yuankai Qi",
      "Jun Zhang",
      "Shengping Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.04734v2",
    "title": "Mixed Transformer U-Net For Medical Image Segmentation",
    "summary": "Though U-Net has achieved tremendous success in medical image segmentation\ntasks, it lacks the ability to explicitly model long-range dependencies.\nTherefore, Vision Transformers have emerged as alternative segmentation\nstructures recently, for their innate ability of capturing long-range\ncorrelations through Self-Attention (SA). However, Transformers usually rely on\nlarge-scale pre-training and have high computational complexity. Furthermore,\nSA can only model self-affinities within a single sample, ignoring the\npotential correlations of the overall dataset. To address these problems, we\npropose a novel Transformer module named Mixed Transformer Module (MTM) for\nsimultaneous inter- and intra- affinities learning. MTM first calculates\nself-affinities efficiently through our well-designed Local-Global\nGaussian-Weighted Self-Attention (LGG-SA). Then, it mines inter-connections\nbetween data samples through External Attention (EA). By using MTM, we\nconstruct a U-shaped model named Mixed Transformer U-Net (MT-UNet) for accurate\nmedical image segmentation. We test our method on two different public\ndatasets, and the experimental results show that the proposed method achieves\nbetter performance over other state-of-the-art methods. The code is available\nat: https://github.com/Dootmaan/MT-UNet.",
    "published": "2021-11-08T09:03:46Z",
    "updated": "2021-11-11T05:51:20Z",
    "authors": [
      "Hongyi Wang",
      "Shiao Xie",
      "Lanfen Lin",
      "Yutaro Iwamoto",
      "Xian-Hua Han",
      "Yen-Wei Chen",
      "Ruofeng Tong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.09856v1",
    "title": "LipsFormer: Introducing Lipschitz Continuity to Vision Transformers",
    "summary": "We present a Lipschitz continuous Transformer, called LipsFormer, to pursue\ntraining stability both theoretically and empirically for Transformer-based\nmodels. In contrast to previous practical tricks that address training\ninstability by learning rate warmup, layer normalization, attention\nformulation, and weight initialization, we show that Lipschitz continuity is a\nmore essential property to ensure training stability. In LipsFormer, we replace\nunstable Transformer component modules with Lipschitz continuous counterparts:\nCenterNorm instead of LayerNorm, spectral initialization instead of Xavier\ninitialization, scaled cosine similarity attention instead of dot-product\nattention, and weighted residual shortcut. We prove that these introduced\nmodules are Lipschitz continuous and derive an upper bound on the Lipschitz\nconstant of LipsFormer. Our experiments show that LipsFormer allows stable\ntraining of deep Transformer architectures without the need of careful learning\nrate tuning such as warmup, yielding a faster convergence and better\ngeneralization. As a result, on the ImageNet 1K dataset, LipsFormer-Swin-Tiny\nbased on Swin Transformer training for 300 epochs can obtain 82.7\\% without any\nlearning rate warmup. Moreover, LipsFormer-CSwin-Tiny, based on CSwin, training\nfor 300 epochs achieves a top-1 accuracy of 83.5\\% with 4.7G FLOPs and 24M\nparameters. The code will be released at\n\\url{https://github.com/IDEA-Research/LipsFormer}.",
    "published": "2023-04-19T17:59:39Z",
    "updated": "2023-04-19T17:59:39Z",
    "authors": [
      "Xianbiao Qi",
      "Jianan Wang",
      "Yihao Chen",
      "Yukai Shi",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.04236v2",
    "title": "RFR-WWANet: Weighted Window Attention-Based Recovery Feature Resolution\n  Network for Unsupervised Image Registration",
    "summary": "The Swin transformer has recently attracted attention in medical image\nanalysis due to its computational efficiency and long-range modeling\ncapability. Owing to these properties, the Swin Transformer is suitable for\nestablishing more distant relationships between corresponding voxels in\ndifferent positions in complex abdominal image registration tasks. However, the\nregistration models based on transformers combine multiple voxels into a single\nsemantic token. This merging process limits the transformers to model and\ngenerate coarse-grained spatial information. To address this issue, we propose\nRecovery Feature Resolution Network (RFRNet), which allows the transformer to\ncontribute fine-grained spatial information and rich semantic correspondences\nto higher resolution levels. Furthermore, shifted window partitioning\noperations are inflexible, indicating that they cannot perceive the semantic\ninformation over uncertain distances and automatically bridge the global\nconnections between windows. Therefore, we present a Weighted Window Attention\n(WWA) to build global interactions between windows automatically. It is\nimplemented after the regular and cyclic shift window partitioning operations\nwithin the Swin transformer block. The proposed unsupervised deformable image\nregistration model, named RFR-WWANet, detects the long-range correlations, and\nfacilitates meaningful semantic relevance of anatomical structures. Qualitative\nand quantitative results show that RFR-WWANet achieves significant improvements\nover the current state-of-the-art methods. Ablation experiments demonstrate the\neffectiveness of the RFRNet and WWA designs. Our code is available at\n\\url{https://github.com/MingR-Ma/RFR-WWANet}.",
    "published": "2023-05-07T09:57:29Z",
    "updated": "2023-05-22T02:41:32Z",
    "authors": [
      "Mingrui Ma",
      "Tao Wang",
      "Lei Song",
      "Weijie Wang",
      "Guixia Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.03005v1",
    "title": "MCTformer+: Multi-Class Token Transformer for Weakly Supervised Semantic\n  Segmentation",
    "summary": "This paper proposes a novel transformer-based framework that aims to enhance\nweakly supervised semantic segmentation (WSSS) by generating accurate\nclass-specific object localization maps as pseudo labels. Building upon the\nobservation that the attended regions of the one-class token in the standard\nvision transformer can contribute to a class-agnostic localization map, we\nexplore the potential of the transformer model to capture class-specific\nattention for class-discriminative object localization by learning multiple\nclass tokens. We introduce a Multi-Class Token transformer, which incorporates\nmultiple class tokens to enable class-aware interactions with the patch tokens.\nTo achieve this, we devise a class-aware training strategy that establishes a\none-to-one correspondence between the output class tokens and the ground-truth\nclass labels. Moreover, a Contrastive-Class-Token (CCT) module is proposed to\nenhance the learning of discriminative class tokens, enabling the model to\nbetter capture the unique characteristics and properties of each class. As a\nresult, class-discriminative object localization maps can be effectively\ngenerated by leveraging the class-to-patch attentions associated with different\nclass tokens. To further refine these localization maps, we propose the\nutilization of patch-level pairwise affinity derived from the patch-to-patch\ntransformer attention. Furthermore, the proposed framework seamlessly\ncomplements the Class Activation Mapping (CAM) method, resulting in\nsignificantly improved WSSS performance on the PASCAL VOC 2012 and MS COCO 2014\ndatasets. These results underline the importance of the class token for WSSS.",
    "published": "2023-08-06T03:30:20Z",
    "updated": "2023-08-06T03:30:20Z",
    "authors": [
      "Lian Xu",
      "Mohammed Bennamoun",
      "Farid Boussaid",
      "Hamid Laga",
      "Wanli Ouyang",
      "Dan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.04366v1",
    "title": "CNN Injected Transformer for Image Exposure Correction",
    "summary": "Capturing images with incorrect exposure settings fails to deliver a\nsatisfactory visual experience. Only when the exposure is properly set, can the\ncolor and details of the images be appropriately preserved. Previous exposure\ncorrection methods based on convolutions often produce exposure deviation in\nimages as a consequence of the restricted receptive field of convolutional\nkernels. This issue arises because convolutions are not capable of capturing\nlong-range dependencies in images accurately. To overcome this challenge, we\ncan apply the Transformer to address the exposure correction problem,\nleveraging its capability in modeling long-range dependencies to capture global\nrepresentation. However, solely relying on the window-based Transformer leads\nto visually disturbing blocking artifacts due to the application of\nself-attention in small patches. In this paper, we propose a CNN Injected\nTransformer (CIT) to harness the individual strengths of CNN and Transformer\nsimultaneously. Specifically, we construct the CIT by utilizing a window-based\nTransformer to exploit the long-range interactions among different regions in\nthe entire image. Within each CIT block, we incorporate a channel attention\nblock (CAB) and a half-instance normalization block (HINB) to assist the\nwindow-based self-attention to acquire the global statistics and refine local\nfeatures. In addition to the hybrid architecture design for exposure\ncorrection, we apply a set of carefully formulated loss functions to improve\nthe spatial coherence and rectify potential color deviations. Extensive\nexperiments demonstrate that our image exposure correction method outperforms\nstate-of-the-art approaches in terms of both quantitative and qualitative\nmetrics.",
    "published": "2023-09-08T14:53:00Z",
    "updated": "2023-09-08T14:53:00Z",
    "authors": [
      "Shuning Xu",
      "Xiangyu Chen",
      "Binbin Song",
      "Jiantao Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.05249v1",
    "title": "In-Context Convergence of Transformers",
    "summary": "Transformers have recently revolutionized many domains in modern machine\nlearning and one salient discovery is their remarkable in-context learning\ncapability, where models can solve an unseen task by utilizing task-specific\nprompts without further parameters fine-tuning. This also inspired recent\ntheoretical studies aiming to understand the in-context learning mechanism of\ntransformers, which however focused only on linear transformers. In this work,\nwe take the first step toward studying the learning dynamics of a one-layer\ntransformer with softmax attention trained via gradient descent in order to\nin-context learn linear function classes. We consider a structured data model,\nwhere each token is randomly sampled from a set of feature vectors in either\nbalanced or imbalanced fashion. For data with balanced features, we establish\nthe finite-time convergence guarantee with near-zero prediction error by\nnavigating our analysis over two phases of the training dynamics of the\nattention map. More notably, for data with imbalanced features, we show that\nthe learning dynamics take a stage-wise convergence process, where the\ntransformer first converges to a near-zero prediction error for the query\ntokens of dominant features, and then converges later to a near-zero prediction\nerror for the query tokens of under-represented features, respectively via one\nand four training phases. Our proof features new techniques for analyzing the\ncompeting strengths of two types of attention weights, the change of which\ndetermines different training phases.",
    "published": "2023-10-08T17:55:33Z",
    "updated": "2023-10-08T17:55:33Z",
    "authors": [
      "Yu Huang",
      "Yuan Cheng",
      "Yingbin Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.15719v2",
    "title": "AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement\n  Learning",
    "summary": "In this paper we investigate transformer architectures designed for partially\nobservable online reinforcement learning. The self-attention mechanism in the\ntransformer architecture is capable of capturing long-range dependencies and it\nis the main reason behind its effectiveness in processing sequential data.\nNevertheless, despite their success, transformers have two significant\ndrawbacks that still limit their applicability in online reinforcement\nlearning: (1) in order to remember all past information, the self-attention\nmechanism requires access to the whole history to be provided as context. (2)\nThe inference cost in transformers is expensive. In this paper, we introduce\nrecurrent alternatives to the transformer self-attention mechanism that offer\ncontext-independent inference cost, leverage long-range dependencies\neffectively, and performs well in online reinforcement learning task. We\nquantify the impact of the different components of our architecture in a\ndiagnostic environment and assess performance gains in 2D and 3D pixel-based\npartially-observable environments (e.g. T-Maze, Mystery Path, Craftax, and\nMemory Maze). Compared with a state-of-the-art architecture, GTrXL, inference\nin our approach is at least 40% cheaper while reducing memory use more than\n50%. Our approach either performs similarly or better than GTrXL, improving\nmore than 37% upon GTrXL performance in harder tasks.",
    "published": "2023-10-24T10:51:50Z",
    "updated": "2024-10-15T17:14:26Z",
    "authors": [
      "Subhojeet Pramanik",
      "Esraa Elelimy",
      "Marlos C. Machado",
      "Adam White"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.06272v1",
    "title": "U-MixFormer: UNet-like Transformer with Mix-Attention for Efficient\n  Semantic Segmentation",
    "summary": "Semantic segmentation has witnessed remarkable advancements with the\nadaptation of the Transformer architecture. Parallel to the strides made by the\nTransformer, CNN-based U-Net has seen significant progress, especially in\nhigh-resolution medical imaging and remote sensing. This dual success inspired\nus to merge the strengths of both, leading to the inception of a U-Net-based\nvision transformer decoder tailored for efficient contextual encoding. Here, we\npropose a novel transformer decoder, U-MixFormer, built upon the U-Net\nstructure, designed for efficient semantic segmentation. Our approach\ndistinguishes itself from the previous transformer methods by leveraging\nlateral connections between the encoder and decoder stages as feature queries\nfor the attention modules, apart from the traditional reliance on skip\nconnections. Moreover, we innovatively mix hierarchical feature maps from\nvarious encoder and decoder stages to form a unified representation for keys\nand values, giving rise to our unique mix-attention module. Our approach\ndemonstrates state-of-the-art performance across various configurations.\nExtensive experiments show that U-MixFormer outperforms SegFormer, FeedFormer,\nand SegNeXt by a large margin. For example, U-MixFormer-B0 surpasses\nSegFormer-B0 and FeedFormer-B0 with 3.8% and 2.0% higher mIoU and 27.3% and\n21.8% less computation and outperforms SegNext with 3.3% higher mIoU with\nMSCAN-T encoder on ADE20K. Code available at\nhttps://github.com/julian-klitzing/u-mixformer.",
    "published": "2023-12-11T10:19:42Z",
    "updated": "2023-12-11T10:19:42Z",
    "authors": [
      "Seul-Ki Yeom",
      "Julian von Klitzing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.11859v2",
    "title": "LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution",
    "summary": "Given the broad application of infrared technology across diverse fields,\nthere is an increasing emphasis on investigating super-resolution techniques\nfor infrared images within the realm of deep learning. Despite the impressive\nresults of current Transformer-based methods in image super-resolution tasks,\ntheir reliance on the self-attentive mechanism intrinsic to the Transformer\narchitecture results in images being treated as one-dimensional sequences,\nthereby neglecting their inherent two-dimensional structure. Moreover, infrared\nimages exhibit a uniform pixel distribution and a limited gradient range,\nposing challenges for the model to capture effective feature information.\nConsequently, we suggest a potent Transformer model, termed Large Kernel\nTransformer (LKFormer), to address this issue. Specifically, we have designed a\nLarge Kernel Residual Attention (LKRA) module with linear complexity. This\nmainly employs depth-wise convolution with large kernels to execute non-local\nfeature modeling, thereby substituting the standard self-attentive layer.\nAdditionally, we have devised a novel feed-forward network structure called\nGated-Pixel Feed-Forward Network (GPFN) to augment the LKFormer's capacity to\nmanage the information flow within the network. Comprehensive experimental\nresults reveal that our method surpasses the most advanced techniques\navailable, using fewer parameters and yielding considerably superior\nperformance.The source code will be available at\nhttps://github.com/sad192/large-kernel-Transformer.",
    "published": "2024-01-22T11:28:24Z",
    "updated": "2024-01-24T11:24:40Z",
    "authors": [
      "Feiwei Qin",
      "Kang Yan",
      "Changmiao Wang",
      "Ruiquan Ge",
      "Yong Peng",
      "Kai Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.18228v1",
    "title": "Fourier or Wavelet bases as counterpart self-attention in spikformer for\n  efficient visual classification",
    "summary": "Energy-efficient spikformer has been proposed by integrating the biologically\nplausible spiking neural network (SNN) and artificial Transformer, whereby the\nSpiking Self-Attention (SSA) is used to achieve both higher accuracy and lower\ncomputational cost. However, it seems that self-attention is not always\nnecessary, especially in sparse spike-form calculation manners. In this paper,\nwe innovatively replace vanilla SSA (using dynamic bases calculating from Query\nand Key) with spike-form Fourier Transform, Wavelet Transform, and their\ncombinations (using fixed triangular or wavelets bases), based on a key\nhypothesis that both of them use a set of basis functions for information\ntransformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is\nproposed and verified in visual classification tasks, including both static\nimage and event-based video datasets. The FWformer can achieve comparable or\neven higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$\nfor training and $19\\%$-$70\\%$ for inference), reduced theoretical energy\nconsumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$),\ncompared to the standard spikformer. Our result indicates the continuous\nrefinement of new Transformers, that are inspired either by biological\ndiscovery (spike-form), or information theory (Fourier or Wavelet Transform),\nis promising.",
    "published": "2024-03-27T03:31:16Z",
    "updated": "2024-03-27T03:31:16Z",
    "authors": [
      "Qingyu Wang",
      "Duzhen Zhang",
      "Tilelin Zhang",
      "Bo Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.05508v2",
    "title": "PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer\n  Architecture",
    "summary": "In recent years, point cloud analysis methods based on the Transformer\narchitecture have made significant progress, particularly in the context of\nmultimedia applications such as 3D modeling, virtual reality, and autonomous\nsystems. However, the high computational resource demands of the Transformer\narchitecture hinder its scalability, real-time processing capabilities, and\ndeployment on mobile devices and other platforms with limited computational\nresources. This limitation remains a significant obstacle to its practical\napplication in scenarios requiring on-device intelligence and multimedia\nprocessing. To address this challenge, we propose an efficient point cloud\nanalysis architecture, \\textbf{Point} \\textbf{M}LP-\\textbf{T}ransformer\n(PointMT). This study tackles the quadratic complexity of the self-attention\nmechanism by introducing a linear complexity local attention mechanism for\neffective feature aggregation. Additionally, to counter the Transformer's focus\non token differences while neglecting channel differences, we introduce a\nparameter-free channel temperature adaptation mechanism that adaptively adjusts\nthe attention weight distribution in each channel, enhancing the precision of\nfeature aggregation. To improve the Transformer's slow convergence speed due to\nthe limited scale of point cloud datasets, we propose an MLP-Transformer hybrid\nmodule, which significantly enhances the model's convergence speed.\nFurthermore, to boost the feature representation capability of point tokens, we\nrefine the classification head, enabling point tokens to directly participate\nin prediction. Experimental results on multiple evaluation benchmarks\ndemonstrate that PointMT achieves performance comparable to state-of-the-art\nmethods while maintaining an optimal balance between performance and accuracy.",
    "published": "2024-08-10T10:16:03Z",
    "updated": "2024-09-16T16:44:58Z",
    "authors": [
      "Qiang Zheng",
      "Chao Zhang",
      "Jian Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.01482v4",
    "title": "Masked Mixers for Language Generation and Retrieval",
    "summary": "Attention mechanisms that confer selective focus on a strict subset of input\nelements are nearly ubiquitous in language models today. We posit there to be\ndownside to the use of attention: most input information is lost. In support of\nthis idea we observe poor input representation accuracy in transformers and\nmore accurate representation in what we term masked mixers, which replace\nself-attention with masked convolutions. The masked mixer learns causal\nlanguage modeling more efficiently than early transformer implementations and\neven outperforms optimized, current transformers when training on small\n($n_{ctx}<512$) but not larger context windows. Evidence is presented for the\nhypothesis that differences in transformer and masked mixer training\nefficiencies for various tasks are best predicted by input representation\naccuracy, or equivalently global invertibility. We hypothesize that the\ninformation loss exhibited by transformers would be more detrimental to\nretrieval than generation, as the former is more closely approximated by a\nbijective and thus invertible function. We find that masked mixers are more\neffective retrieval models both when the pretrained embedding model is\nunchanged as well as when the embedding model is modified via cosine\nsimilarity-based InfoNCE loss minimization. A small masked mixer is shown to\noutperform a large and near state-of-the-art transformer-based retrieval model,\ndespite the latter being trained with many orders of magnitude more data and\ncompute.",
    "published": "2024-09-02T22:17:18Z",
    "updated": "2025-03-20T17:39:10Z",
    "authors": [
      "Benjamin L. Badger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.11358v1",
    "title": "SeaDATE: Remedy Dual-Attention Transformer with Semantic Alignment via\n  Contrast Learning for Multimodal Object Detection",
    "summary": "Multimodal object detection leverages diverse modal information to enhance\nthe accuracy and robustness of detectors. By learning long-term dependencies,\nTransformer can effectively integrate multimodal features in the feature\nextraction stage, which greatly improves the performance of multimodal object\ndetection. However, current methods merely stack Transformer-guided fusion\ntechniques without exploring their capability to extract features at various\ndepth layers of network, thus limiting the improvements in detection\nperformance. In this paper, we introduce an accurate and efficient object\ndetection method named SeaDATE. Initially, we propose a novel dual attention\nFeature Fusion (DTF) module that, under Transformer's guidance, integrates\nlocal and global information through a dual attention mechanism, strengthening\nthe fusion of modal features from orthogonal perspectives using spatial and\nchannel tokens. Meanwhile, our theoretical analysis and empirical validation\ndemonstrate that the Transformer-guided fusion method, treating images as\nsequences of pixels for fusion, performs better on shallow features' detail\ninformation compared to deep semantic information. To address this, we designed\na contrastive learning (CL) module aimed at learning features of multimodal\nsamples, remedying the shortcomings of Transformer-guided fusion in extracting\ndeep semantic features, and effectively utilizing cross-modal information.\nExtensive experiments and ablation studies on the FLIR, LLVIP, and M3FD\ndatasets have proven our method to be effective, achieving state-of-the-art\ndetection performance.",
    "published": "2024-10-15T07:26:39Z",
    "updated": "2024-10-15T07:26:39Z",
    "authors": [
      "Shuhan Dong",
      "Yunsong Li",
      "Weiying Xie",
      "Jiaqing Zhang",
      "Jiayuan Tian",
      "Danian Yang",
      "Jie Lei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06474v1",
    "title": "UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths",
    "summary": "Unified multimodal transformers, which handle both generation and\nunderstanding tasks within a shared parameter space, have received increasing\nattention in recent research. Although various unified transformers have been\nproposed, training these models is costly due to redundant tokens and heavy\nattention computation. In the past, studies on large language models have\ndemonstrated that token pruning methods, such as Mixture of Depths (MoD), can\nsignificantly improve computational efficiency. MoD employs a router to select\nthe most important ones for processing within a transformer layer. However,\ndirectly applying MoD-based token pruning to unified transformers will result\nin suboptimal performance because different tasks exhibit varying levels of\ntoken redundancy. In our work, we analyze the unified transformers by (1)\nexamining attention weight patterns, (2) evaluating the layer importance and\ntoken redundancy, and (3) analyzing task interactions. Our findings reveal that\ntoken redundancy is primarily influenced by different tasks and layers.\nBuilding on these findings, we introduce UniMoD, a task-aware token pruning\nmethod that employs a separate router for each task to determine which tokens\nshould be pruned. We apply our method to Show-o and Emu3, reducing training\nFLOPs by approximately 15% in Show-o and 40% in Emu3, while maintaining or\nimproving performance on several benchmarks. Code will be released at\nhttps://github.com/showlab/UniMoD.",
    "published": "2025-02-10T13:52:52Z",
    "updated": "2025-02-10T13:52:52Z",
    "authors": [
      "Weijia Mao",
      "Zhenheng Yang",
      "Mike Zheng Shou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.04996v1",
    "title": "Salient Positions based Attention Network for Image Classification",
    "summary": "The self-attention mechanism has attracted wide publicity for its most\nimportant advantage of modeling long dependency, and its variations in computer\nvision tasks, the non-local block tries to model the global dependency of the\ninput feature maps. Gathering global contextual information will inevitably\nneed a tremendous amount of memory and computing resources, which has been\nextensively studied in the past several years. However, there is a further\nproblem with the self-attention scheme: is all information gathered from the\nglobal scope helpful for the contextual modelling? To our knowledge, few\nstudies have focused on the problem. Aimed at both questions this paper\nproposes the salient positions-based attention scheme SPANet, which is inspired\nby some interesting observations on the attention maps and affinity matrices\ngenerated in self-attention scheme. We believe these observations are\nbeneficial for better understanding of the self-attention. SPANet uses the\nsalient positions selection algorithm to select only a limited amount of\nsalient points to attend in the attention map computing. This approach will not\nonly spare a lot of memory and computing resources, but also try to distill the\npositive information from the transformation of the input feature maps. In the\nimplementation, considering the feature maps with channel high dimensions,\nwhich are completely different from the general visual image, we take the\nsquared power of the feature maps along the channel dimension as the saliency\nmetric of the positions. In general, different from the non-local block method,\nSPANet models the contextual information using only the selected positions\ninstead of all, along the channel dimension instead of space dimension. Our\nsource code is available at https://github.com/likyoo/SPANet.",
    "published": "2021-06-09T11:32:29Z",
    "updated": "2021-06-09T11:32:29Z",
    "authors": [
      "Sheng Fang",
      "Kaiyu Li",
      "Zhe Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.08782v1",
    "title": "Causal Attention for Unbiased Visual Recognition",
    "summary": "Attention module does not always help deep models learn causal features that\nare robust in any confounding context, e.g., a foreground object feature is\ninvariant to different backgrounds. This is because the confounders trick the\nattention to capture spurious correlations that benefit the prediction when the\ntraining and testing data are IID (identical & independent distribution); while\nharm the prediction when the data are OOD (out-of-distribution). The sole\nfundamental solution to learn causal attention is by causal intervention, which\nrequires additional annotations of the confounders, e.g., a \"dog\" model is\nlearned within \"grass+dog\" and \"road+dog\" respectively, so the \"grass\" and\n\"road\" contexts will no longer confound the \"dog\" recognition. However, such\nannotation is not only prohibitively expensive, but also inherently\nproblematic, as the confounders are elusive in nature. In this paper, we\npropose a causal attention module (CaaM) that self-annotates the confounders in\nunsupervised fashion. In particular, multiple CaaMs can be stacked and\nintegrated in conventional attention CNN and self-attention Vision Transformer.\nIn OOD settings, deep models with CaaM outperform those without it\nsignificantly; even in IID settings, the attention localization is also\nimproved by CaaM, showing a great potential in applications that require robust\nvisual saliency. Codes are available at \\url{https://github.com/Wangt-CN/CaaM}.",
    "published": "2021-08-19T16:45:51Z",
    "updated": "2021-08-19T16:45:51Z",
    "authors": [
      "Tan Wang",
      "Chang Zhou",
      "Qianru Sun",
      "Hanwang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.11353v1",
    "title": "What can a Single Attention Layer Learn? A Study Through the Random\n  Features Lens",
    "summary": "Attention layers -- which map a sequence of inputs to a sequence of outputs\n-- are core building blocks of the Transformer architecture which has achieved\nsignificant breakthroughs in modern artificial intelligence. This paper\npresents a rigorous theoretical study on the learning and generalization of a\nsingle multi-head attention layer, with a sequence of key vectors and a\nseparate query vector as input. We consider the random feature setting where\nthe attention layer has a large number of heads, with randomly sampled frozen\nquery and key matrices, and trainable value matrices. We show that such a\nrandom-feature attention layer can express a broad class of target functions\nthat are permutation invariant to the key vectors. We further provide\nquantitative excess risk bounds for learning these target functions from finite\nsamples, using random feature attention with finitely many heads.\n  Our results feature several implications unique to the attention structure\ncompared with existing random features theory for neural networks, such as (1)\nAdvantages in the sample complexity over standard two-layer random-feature\nnetworks; (2) Concrete and natural classes of functions that can be learned\nefficiently by a random-feature attention layer; and (3) The effect of the\nsampling distribution of the query-key weight matrix (the product of the query\nand key matrix), where Gaussian random weights with a non-zero mean result in\nbetter sample complexities over the zero-mean counterpart for learning certain\nnatural target functions. Experiments on simulated data corroborate our\ntheoretical findings and further illustrate the interplay between the sample\nsize and the complexity of the target function.",
    "published": "2023-07-21T05:05:55Z",
    "updated": "2023-07-21T05:05:55Z",
    "authors": [
      "Hengyu Fu",
      "Tianyu Guo",
      "Yu Bai",
      "Song Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.14777v2",
    "title": "pCTFusion: Point Convolution-Transformer Fusion with Semantic Aware Loss\n  for Outdoor LiDAR Point Cloud Segmentation",
    "summary": "LiDAR-generated point clouds are crucial for perceiving outdoor environments.\nThe segmentation of point clouds is also essential for many applications.\nPrevious research has focused on using self-attention and convolution (local\nattention) mechanisms individually in semantic segmentation architectures.\nHowever, there is limited work on combining the learned representations of\nthese attention mechanisms to improve performance. Additionally, existing\nresearch that combines convolution with self-attention relies on global\nattention, which is not practical for processing large point clouds. To address\nthese challenges, this study proposes a new architecture, pCTFusion, which\ncombines kernel-based convolutions and self-attention mechanisms for better\nfeature learning and capturing local and global dependencies in segmentation.\nThe proposed architecture employs two types of self-attention mechanisms, local\nand global, based on the hierarchical positions of the encoder blocks.\nFurthermore, the existing loss functions do not consider the semantic and\nposition-wise importance of the points, resulting in reduced accuracy,\nparticularly at sharp class boundaries. To overcome this, the study models a\nnovel attention-based loss function called Pointwise Geometric Anisotropy\n(PGA), which assigns weights based on the semantic distribution of points in a\nneighborhood. The proposed architecture is evaluated on SemanticKITTI outdoor\ndataset and showed a 5-7% improvement in performance compared to the\nstate-of-the-art architectures. The results are particularly encouraging for\nminor classes, often misclassified due to class imbalance, lack of space, and\nneighbor-aware feature encoding. These developed methods can be leveraged for\nthe segmentation of complex datasets and can drive real-world applications of\nLiDAR point cloud.",
    "published": "2023-07-27T11:12:48Z",
    "updated": "2023-07-31T04:26:02Z",
    "authors": [
      "Abhishek Kuriyal",
      "Vaibhav Kumar",
      "Bharat Lohani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.03429v1",
    "title": "RCMHA: Relative Convolutional Multi-Head Attention for Natural Language\n  Modelling",
    "summary": "The Attention module finds common usage in language modeling, presenting\ndistinct challenges within the broader scope of Natural Language Processing.\nMulti-Head Attention (MHA) employs an absolute positional encoding, which\nimposes limitations on token length and entails substantial memory consumption\nduring the processing of embedded inputs. The current remedy proposed by\nresearchers involves the utilization of relative positional encoding, similar\nto the approach adopted in Transformer-XL or Relative Multi-Head Attention\n(RMHA), albeit the employed architecture consumes considerable memory\nresources. To address these challenges, this study endeavors to refine MHA,\nleveraging relative positional encoding in conjunction with the Depth-Wise\nConvolutional Layer architecture, which promises heightened accuracy coupled\nwith minimized memory usage. The proposed RCMHA framework entails the\nmodification of two integral components: firstly, the application of the\nDepth-Wise Convolutional Layer to the input embedding, encompassing Query, Key,\nand Value parameters; secondly, the incorporation of Relative Positional\nEncoding into the attention scoring phase, harmoniously integrated with Scaled\nDot-Product Attention. Empirical experiments underscore the advantages of\nRCMHA, wherein it exhibits superior accuracy, boasting a score of 0.572 in\ncomparison to alternative attention modules such as MHA, Multi-DConv-Head\nAttention (MDHA), and RMHA. Concerning memory utilization, RMHA emerges as the\nmost frugal, demonstrating an average consumption of 2.98 GB, surpassing RMHA\nwhich necessitates 3.5 GB.",
    "published": "2023-08-07T09:24:24Z",
    "updated": "2023-08-07T09:24:24Z",
    "authors": [
      "Herman Sugiharto",
      " Aradea",
      "Husni Mubarok"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.16661v3",
    "title": "SA2-Net: Scale-aware Attention Network for Microscopic Image\n  Segmentation",
    "summary": "Microscopic image segmentation is a challenging task, wherein the objective\nis to assign semantic labels to each pixel in a given microscopic image. While\nconvolutional neural networks (CNNs) form the foundation of many existing\nframeworks, they often struggle to explicitly capture long-range dependencies.\nAlthough transformers were initially devised to address this issue using\nself-attention, it has been proven that both local and global features are\ncrucial for addressing diverse challenges in microscopic images, including\nvariations in shape, size, appearance, and target region density. In this\npaper, we introduce SA2-Net, an attention-guided method that leverages\nmulti-scale feature learning to effectively handle diverse structures within\nmicroscopic images. Specifically, we propose scale-aware attention (SA2) module\ndesigned to capture inherent variations in scales and shapes of microscopic\nregions, such as cells, for accurate segmentation. This module incorporates\nlocal attention at each level of multi-stage features, as well as global\nattention across multiple resolutions. Furthermore, we address the issue of\nblurred region boundaries (e.g., cell boundaries) by introducing a novel\nupsampling strategy called the Adaptive Up-Attention (AuA) module. This module\nenhances the discriminative ability for improved localization of microscopic\nregions using an explicit attention mechanism. Extensive experiments on five\nchallenging datasets demonstrate the benefits of our SA2-Net model. Our source\ncode is publicly available at \\url{https://github.com/mustansarfiaz/SA2-Net}.",
    "published": "2023-09-28T17:58:05Z",
    "updated": "2023-11-19T17:03:33Z",
    "authors": [
      "Mustansar Fiaz",
      "Moein Heidari",
      "Rao Muhammad Anwer",
      "Hisham Cholakkal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.20051v1",
    "title": "The Expressibility of Polynomial based Attention Scheme",
    "summary": "Large language models (LLMs) have significantly improved various aspects of\nour daily lives. These models have impacted numerous domains, from healthcare\nto education, enhancing productivity, decision-making processes, and\naccessibility. As a result, they have influenced and, to some extent, reshaped\npeople's lifestyles. However, the quadratic complexity of attention in\ntransformer architectures poses a challenge when scaling up these models for\nprocessing long textual contexts. This issue makes it impractical to train very\nlarge models on lengthy texts or use them efficiently during inference. While a\nrecent study by [KMZ23] introduced a technique that replaces the softmax with a\npolynomial function and polynomial sketching to speed up attention mechanisms,\nthe theoretical understandings of this new approach are not yet well\nunderstood.\n  In this paper, we offer a theoretical analysis of the expressive capabilities\nof polynomial attention. Our study reveals a disparity in the ability of\nhigh-degree and low-degree polynomial attention. Specifically, we construct two\ncarefully designed datasets, namely $\\mathcal{D}_0$ and $\\mathcal{D}_1$, where\n$\\mathcal{D}_1$ includes a feature with a significantly larger value compared\nto $\\mathcal{D}_0$. We demonstrate that with a sufficiently high degree\n$\\beta$, a single-layer polynomial attention network can distinguish between\n$\\mathcal{D}_0$ and $\\mathcal{D}_1$. However, with a low degree $\\beta$, the\nnetwork cannot effectively separate the two datasets. This analysis underscores\nthe greater effectiveness of high-degree polynomials in amplifying large values\nand distinguishing between datasets. Our analysis offers insight into the\nrepresentational capacity of polynomial attention and provides a rationale for\nincorporating higher-degree polynomials in attention mechanisms to capture\nintricate linguistic correlations.",
    "published": "2023-10-30T22:16:18Z",
    "updated": "2023-10-30T22:16:18Z",
    "authors": [
      "Zhao Song",
      "Guangyi Xu",
      "Junze Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.05738v3",
    "title": "Interpreting and Improving Attention From the Perspective of Large\n  Kernel Convolution",
    "summary": "Attention mechanisms have significantly advanced visual models by capturing\nglobal context effectively. However, their reliance on large-scale datasets and\nsubstantial computational resources poses challenges in data-scarce and\nresource-constrained scenarios. Moreover, traditional self-attention mechanisms\nlack inherent spatial inductive biases, making them suboptimal for modeling\nlocal features critical to tasks involving smaller datasets. In this work, we\nintroduce Large Kernel Convolutional Attention (LKCA), a novel formulation that\nreinterprets attention operations as a single large-kernel convolution. This\ndesign unifies the strengths of convolutional architectures locality and\ntranslation invariance with the global context modeling capabilities of\nself-attention. By embedding these properties into a computationally efficient\nframework, LKCA addresses key limitations of traditional attention mechanisms.\nThe proposed LKCA achieves competitive performance across various visual tasks,\nparticularly in data-constrained settings. Experimental results on CIFAR-10,\nCIFAR-100, SVHN, and Tiny-ImageNet demonstrate its ability to excel in image\nclassification, outperforming conventional attention mechanisms and vision\ntransformers in compact model settings. These findings highlight the\neffectiveness of LKCA in bridging local and global feature modeling, offering a\npractical and robust solution for real-world applications with limited data and\nresources.",
    "published": "2024-01-11T08:40:35Z",
    "updated": "2024-12-02T00:04:23Z",
    "authors": [
      "Chenghao Li",
      "Chaoning Zhang",
      "Boheng Zeng",
      "Yi Lu",
      "Pengbo Shi",
      "Qingzi Chen",
      "Jirui Liu",
      "Lingyun Zhu",
      "Yang Yang",
      "Heng Tao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.01476v2",
    "title": "Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian\n  Processes",
    "summary": "While the great capability of Transformers significantly boosts prediction\naccuracy, it could also yield overconfident predictions and require calibrated\nuncertainty estimation, which can be commonly tackled by Gaussian processes\n(GPs). Existing works apply GPs with symmetric kernels under variational\ninference to the attention kernel; however, omitting the fact that attention\nkernels are in essence asymmetric. Moreover, the complexity of deriving the GP\nposteriors remains high for large-scale data. In this work, we propose\nKernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building\nuncertainty-aware self-attention where the asymmetry of attention kernels is\ntackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through\nKEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from\nKSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using\nonly a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP\nposteriors can be based on the inversion of a diagonal matrix containing\nsingular values, contributing to a reduction in time complexity; iii) an\nevidence lower bound is derived so that variational parameters and network\nweights can be optimized with it. Experiments verify our excellent performances\nand efficiency on in-distribution, distribution-shift and out-of-distribution\nbenchmarks.",
    "published": "2024-02-02T15:05:13Z",
    "updated": "2024-05-28T09:13:19Z",
    "authors": [
      "Yingyi Chen",
      "Qinghua Tao",
      "Francesco Tonin",
      "Johan A. K. Suykens"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.13456v4",
    "title": "KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural\n  Networks",
    "summary": "Graph neural networks (GNNs) with attention mechanisms, often referred to as\nattentive GNNs, have emerged as a prominent paradigm in advanced GNN models in\nrecent years. However, our understanding of the critical process of scoring\nneighbor nodes remains limited, leading to the underperformance of many\nexisting attentive GNNs. In this paper, we unify the scoring functions of\ncurrent attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which\nintegrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring\nprocess. KAA enhances the performance of scoring functions across the board and\ncan be applied to nearly all existing attentive GNNs. To compare the expressive\npower of KAA with other scoring functions, we introduce Maximum Ranking\nDistance (MRD) to quantitatively estimate their upper bounds in ranking errors\nfor node importance. Our analysis reveals that, under limited parameters and\nconstraints on width and depth, both linear transformation-based and MLP-based\nscoring functions exhibit finite expressive power. In contrast, our proposed\nKAA, even with a single-layer KAN parameterized by zero-order B-spline\nfunctions, demonstrates nearly infinite expressive power. Extensive experiments\non both node-level and graph-level tasks using various backbone models show\nthat KAA-enhanced scoring functions consistently outperform their original\ncounterparts, achieving performance improvements of over 20% in some cases.",
    "published": "2025-01-23T08:14:55Z",
    "updated": "2025-03-11T08:59:12Z",
    "authors": [
      "Taoran Fang",
      "Tianhong Gao",
      "Chunping Wang",
      "Yihao Shang",
      "Wei Chow",
      "Lei Chen",
      "Yang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06811v2",
    "title": "Aligning Human and Machine Attention for Enhanced Supervised Learning",
    "summary": "Attention, or prioritization of certain information items over others, is a\ncritical element of any learning process, for both humans and machines. Given\nthat humans continue to outperform machines in certain learning tasks, it seems\nplausible that machine performance could be enriched by aligning machine\nattention with human attention mechanisms -- yet research on this topic is\nsparse and has achieved only limited success. This paper proposes a new\napproach to address this gap, called Human-Machine Attention Learning (HuMAL).\nThis approach involves reliance on data annotated by humans to reflect their\nself-perceived attention during specific tasks. We evaluate several alternative\nstrategies for integrating such human attention data into machine learning (ML)\nalgorithms, using a sentiment analysis task (review data from Yelp) and a\npersonality-type classification task (data from myPersonality). The\nbest-performing HuMAL strategy significantly enhances the task performance of\nfine-tuned transformer models (BERT, as well as GPT-2 and XLNET), and the\nbenefit is particularly pronounced under challenging conditions of imbalanced\nor sparse labeled data. This research contributes to a deeper understanding of\nstrategies for integrating human attention into ML models and highlights the\npotential of leveraging human cognition to augment ML in real-world\napplications.",
    "published": "2025-02-04T20:44:38Z",
    "updated": "2025-02-19T20:57:37Z",
    "authors": [
      "Avihay Chriqui",
      "Inbal Yahav",
      "Dov Teeni",
      "Ahmed Abbasi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.03776v1",
    "title": "PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel\n  Pickup Route Prediction",
    "summary": "Optimization of the last-mile delivery and first-mile pickup of parcels is an\nintegral part of the broader logistics optimization pipeline as it entails both\ncost and resource efficiency as well as a heightened service quality. Such\noptimization requires accurate route and time prediction systems to adapt to\ndifferent scenarios in advance. This work tackles the first building block,\nnamely route prediction. This is done by introducing a novel Proximity\nAttention mechanism in an encoder-decoder architecture utilizing a Pointer\nNetwork in the decoding process (Proximity Attention Encoder and Pointer\nNetwork decoder: PAPN) to leverage the underlying connections between the\ndifferent visitable pickup positions at each timestep. To this local attention\nprocess is coupled global context computing via a multi-head attention\ntransformer encoder. The obtained global context is then mixed to an aggregated\nversion of the local embedding thus achieving a mix of global and local\nattention for complete modeling of the problems. Proximity attention is also\nused in the decoding process to skew predictions towards the locations with the\nhighest attention scores and thus using inter-connectivity of locations as a\nbase for next-location prediction. This method is trained, validated and tested\non a large industry-level dataset of real-world, large-scale last-mile delivery\nand first-mile pickup named LaDE[1]. This approach shows noticeable promise,\noutperforming all state-of-the-art supervised systems in terms of most metrics\nused for benchmarking methods on this dataset while still being competitive\nwith the best-performing reinforcement learning method named DRL4Route[2].",
    "published": "2025-04-30T08:24:41Z",
    "updated": "2025-04-30T08:24:41Z",
    "authors": [
      "Hansi Denis",
      "Siegfried Mercelis",
      "Ngoc-Quang Luong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15545v1",
    "title": "RATTENTION: Towards the Minimal Sliding Window Size in Local-Global\n  Attention Models",
    "summary": "Local-global attention models have recently emerged as compelling\nalternatives to standard Transformers, promising improvements in both training\nand inference efficiency. However, the crucial choice of window size presents a\nPareto tradeoff: larger windows maintain performance akin to full attention but\noffer minimal efficiency gains in short-context scenarios, while smaller\nwindows can lead to performance degradation. Current models, such as Gemma2 and\nMistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining\nlength) to preserve performance. This work investigates strategies to shift\nthis Pareto frontier, enabling local-global models to achieve efficiency gains\neven in short-context regimes. Our core motivation is to address the intrinsic\nlimitation of local attention -- its complete disregard for tokens outside the\ndefined window. We explore RATTENTION, a variant of local attention integrated\nwith a specialized linear attention mechanism designed to capture information\nfrom these out-of-window tokens. Pretraining experiments at the 3B and 12B\nscales demonstrate that RATTENTION achieves a superior Pareto tradeoff between\nperformance and efficiency. As a sweet spot, RATTENTION with a window size of\njust 512 consistently matches the performance of full-attention models across\ndiverse settings. Furthermore, the recurrent nature inherent in the linear\nattention component of RATTENTION contributes to enhanced long-context\nperformance, as validated on the RULER benchmark. Crucially, these improvements\ndo not compromise training efficiency; thanks to a specialized kernel\nimplementation and the reduced window size, RATTENTION maintains training\nspeeds comparable to existing state-of-the-art approaches.",
    "published": "2025-06-18T15:18:07Z",
    "updated": "2025-06-18T15:18:07Z",
    "authors": [
      "Bailin Wang",
      "Chang Lan",
      "Chong Wang",
      "Ruoming Pang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.01085v3",
    "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
    "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
    "published": "2025-09-01T03:16:52Z",
    "updated": "2025-09-11T06:16:31Z",
    "authors": [
      "Chenlu Zhan",
      "Wen Li",
      "Chuyu Shen",
      "Jun Zhang",
      "Suhui Wu",
      "Hao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.16816v1",
    "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural\n  Operator",
    "summary": "Neural operators offer a powerful data-driven framework for learning mappings\nbetween function spaces, in which the transformer-based neural operator\narchitecture faces a fundamental scalability-accuracy trade-off: softmax\nattention provides excellent fidelity but incurs quadratic complexity\n$\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,\nwhile linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often\nsuffer significant accuracy degradation. To address the aforementioned\nchallenge, in this paper, we present a novel type of neural operators, Linear\nAttention Neural Operator (LANO), which achieves both scalability and high\naccuracy by reformulating attention through an agent-based mechanism. LANO\nresolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll\nN)$ that mediate global interactions among $N$ tokens. This agent attention\nmechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$\nwhile preserving the expressive power of softmax attention. Theoretically, we\ndemonstrate the universal approximation property, thereby demonstrating\nimproved conditioning and stability properties. Empirically, LANO surpasses\ncurrent state-of-the-art neural PDE solvers, including Transolver with\nslice-based softmax attention, achieving average $19.5\\%$ accuracy improvement\nacross standard benchmarks. By bridging the gap between linear complexity and\nsoftmax-level performance, LANO establishes a scalable, high-accuracy\nfoundation for scientific machine learning applications.",
    "published": "2025-10-19T13:03:09Z",
    "updated": "2025-10-19T13:03:09Z",
    "authors": [
      "Ming Zhong",
      "Zhenya Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.17896v1",
    "title": "Long-Context Attention Benchmark: From Kernel Efficiency to Distributed\n  Context Parallelism",
    "summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess, yet their standard attention mechanism incurs quadratic computation\nand memory costs with respect to sequence length, posing a major bottleneck for\nlong-context training. Prior work tackles this challenge along two directions:\n(1) kernel-level optimizations, which accelerate dense and sparse attention\noperators; and (2) module-level strategies, often referred to as distributed\nattention or context parallel training, which scale attention across multiple\ndevices. However, systematic evaluation still remains limited: operator-level\ncomparisons are often incomplete, while context parallel strategies are\ntypically framework-specific, with unclear performance analysis across\ncontexts. To address these gaps, we propose a unified benchmark that integrates\nrepresentative attention kernels and context parallel mechanisms with a modular\nand extensible interface for evaluation. The benchmark evaluates methods along\ntwo critical dimensions: (1) attention mask patterns, which strongly affect\nefficiency, scalability, and usability, and (2) sequence length and distributed\nscale, which determine performance under extreme long-context training. Through\ncomprehensive experiments on the cluster of up to 96 GPUs, our benchmark\nenables reproducible comparisons, highlights method-specific trade-offs, and\nprovides practical guidance for designing and deploying attention mechanisms in\nlong-context LLM training.",
    "published": "2025-10-19T07:07:37Z",
    "updated": "2025-10-19T07:07:37Z",
    "authors": [
      "Tao Bu",
      "Qiangang Wang",
      "Bowen Zeng",
      "Hanwen Sun",
      "Yunpeng Huang",
      "Chun Cao",
      "Jingwei Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.18775v1",
    "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
    "summary": "Recent advances in video generation have made it possible to produce visually\ncompelling videos, with wide-ranging applications in content creation,\nentertainment, and virtual reality. However, most existing diffusion\ntransformer based video generation models are limited to low-resolution outputs\n(<=720P) due to the quadratic computational complexity of the attention\nmechanism with respect to the output width and height. This computational\nbottleneck makes native high-resolution video generation (1080P/2K/4K)\nimpractical for both training and inference. To address this challenge, we\npresent UltraGen, a novel video generation framework that enables i) efficient\nand ii) end-to-end native high-resolution video synthesis. Specifically,\nUltraGen features a hierarchical dual-branch attention architecture based on\nglobal-local attention decomposition, which decouples full attention into a\nlocal attention branch for high-fidelity regional content and a global\nattention branch for overall semantic consistency. We further propose a\nspatially compressed global modeling strategy to efficiently learn global\ndependencies, and a hierarchical cross-window local attention mechanism to\nreduce computational costs while enhancing information flow across different\nlocal windows. Extensive experiments demonstrate that UltraGen can effectively\nscale pre-trained low-resolution video models to 1080P and even 4K resolution\nfor the first time, outperforming existing state-of-the-art methods and\nsuper-resolution based two-stage pipelines in both qualitative and quantitative\nevaluations.",
    "published": "2025-10-21T16:23:21Z",
    "updated": "2025-10-21T16:23:21Z",
    "authors": [
      "Teng Hu",
      "Jiangning Zhang",
      "Zihan Su",
      "Ran Yi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1907.11065v2",
    "title": "DropAttention: A Regularization Method for Fully-Connected\n  Self-Attention Networks",
    "summary": "Variants dropout methods have been designed for the fully-connected layer,\nconvolutional layer and recurrent layer in neural networks, and shown to be\neffective to avoid overfitting. As an appealing alternative to recurrent and\nconvolutional layers, the fully-connected self-attention layer surprisingly\nlacks a specific dropout method. This paper explores the possibility of\nregularizing the attention weights in Transformers to prevent different\ncontextualized feature vectors from co-adaption. Experiments on a wide range of\ntasks show that DropAttention can improve performance and reduce overfitting.",
    "published": "2019-07-25T14:03:06Z",
    "updated": "2019-07-26T01:49:33Z",
    "authors": [
      "Lin Zehui",
      "Pengfei Liu",
      "Luyao Huang",
      "Junkun Chen",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2001.04589v2",
    "title": "Faster Transformer Decoding: N-gram Masked Self-Attention",
    "summary": "Motivated by the fact that most of the information relevant to the prediction\nof target tokens is drawn from the source sentence $S=s_1, \\ldots, s_S$, we\npropose truncating the target-side window used for computing self-attention by\nmaking an $N$-gram assumption. Experiments on WMT EnDe and EnFr data sets show\nthat the $N$-gram masked self-attention model loses very little in BLEU score\nfor $N$ values in the range $4, \\ldots, 8$, depending on the task.",
    "published": "2020-01-14T02:14:09Z",
    "updated": "2024-12-18T18:59:53Z",
    "authors": [
      "Ciprian Chelba",
      "Mia Chen",
      "Ankur Bapna",
      "Noam Shazeer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.01884v1",
    "title": "Research on Patch Attentive Neural Process",
    "summary": "Attentive Neural Process (ANP) improves the fitting ability of Neural Process\n(NP) and improves its prediction accuracy, but the higher time complexity of\nthe model imposes a limitation on the length of the input sequence. Inspired by\nmodels such as Vision Transformer (ViT) and Masked Auto-Encoder (MAE), we\npropose Patch Attentive Neural Process (PANP) using image patches as input and\nimprove the structure of deterministic paths based on ANP, which allows the\nmodel to extract image features more accurately and efficiently reconstruction.",
    "published": "2022-01-29T03:38:20Z",
    "updated": "2022-01-29T03:38:20Z",
    "authors": [
      "Xiaohan Yu",
      "Shaochen Mao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.12160v2",
    "title": "Self-Attention for Incomplete Utterance Rewriting",
    "summary": "Incomplete utterance rewriting (IUR) has recently become an essential task in\nNLP, aiming to complement the incomplete utterance with sufficient context\ninformation for comprehension. In this paper, we propose a novel method by\ndirectly extracting the coreference and omission relationship from the\nself-attention weight matrix of the transformer instead of word embeddings and\nedit the original text accordingly to generate the complete utterance.\nBenefiting from the rich information in the self-attention weight matrix, our\nmethod achieved competitive results on public IUR datasets.",
    "published": "2022-02-24T15:55:16Z",
    "updated": "2022-02-26T16:30:20Z",
    "authors": [
      "Yong Zhang",
      "Zhitao Li",
      "Jianzong Wang",
      "Ning Cheng",
      "Jing Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1909.03149v3",
    "title": "Enhancing Machine Translation with Dependency-Aware Self-Attention",
    "summary": "Most neural machine translation models only rely on pairs of parallel\nsentences, assuming syntactic information is automatically learned by an\nattention mechanism. In this work, we investigate different approaches to\nincorporate syntactic knowledge in the Transformer model and also propose a\nnovel, parameter-free, dependency-aware self-attention mechanism that improves\nits translation quality, especially for long sentences and in low-resource\nscenarios. We show the efficacy of each approach on WMT English-German and\nEnglish-Turkish, and WAT English-Japanese translation tasks.",
    "published": "2019-09-06T23:29:57Z",
    "updated": "2020-04-21T09:20:31Z",
    "authors": [
      "Emanuele Bugliarello",
      "Naoaki Okazaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.13561v1",
    "title": "Differentiable Window for Dynamic Local Attention",
    "summary": "We propose Differentiable Window, a new neural module and general purpose\ncomponent for dynamic window selection. While universally applicable, we\ndemonstrate a compelling use case of utilizing Differentiable Window to improve\nstandard attention modules by enabling more focused attentions over the input\nregions. We propose two variants of Differentiable Window, and integrate them\nwithin the Transformer architecture in two novel ways. We evaluate our proposed\napproach on a myriad of NLP tasks, including machine translation, sentiment\nanalysis, subject-verb agreement and language modeling. Our experimental\nresults demonstrate consistent and sizable improvements across all tasks.",
    "published": "2020-06-24T08:47:26Z",
    "updated": "2020-06-24T08:47:26Z",
    "authors": [
      "Thanh-Tung Nguyen",
      "Xuan-Phi Nguyen",
      "Shafiq Joty",
      "Xiaoli Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2009.09672v2",
    "title": "Alleviating the Inequality of Attention Heads for Neural Machine\n  Translation",
    "summary": "Recent studies show that the attention heads in Transformer are not equal. We\nrelate this phenomenon to the imbalance training of multi-head attention and\nthe model dependence on specific heads. To tackle this problem, we propose a\nsimple masking method: HeadMask, in two specific ways. Experiments show that\ntranslation improvements are achieved on multiple language pairs. Subsequent\nempirical analyses also support our assumption and confirm the effectiveness of\nthe method.",
    "published": "2020-09-21T08:14:30Z",
    "updated": "2022-08-31T11:50:22Z",
    "authors": [
      "Zewei Sun",
      "Shujian Huang",
      "Xin-Yu Dai",
      "Jiajun Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.11735v1",
    "title": "Large Scale Multimodal Classification Using an Ensemble of Transformer\n  Models and Co-Attention",
    "summary": "Accurate and efficient product classification is significant for E-commerce\napplications, as it enables various downstream tasks such as recommendation,\nretrieval, and pricing. Items often contain textual and visual information, and\nutilizing both modalities usually outperforms classification utilizing either\nmode alone. In this paper we describe our methodology and results for the SIGIR\neCom Rakuten Data Challenge. We employ a dual attention technique to model\nimage-text relationships using pretrained language and image embeddings. While\ndual attention has been widely used for Visual Question Answering(VQA) tasks,\nours is the first attempt to apply the concept for multimodal classification.",
    "published": "2020-11-23T21:22:54Z",
    "updated": "2020-11-23T21:22:54Z",
    "authors": [
      "Varnith Chordia",
      "Vijay Kumar BG"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.10986v1",
    "title": "Attention-based Part Assembly for 3D Volumetric Shape Modeling",
    "summary": "Modeling a 3D volumetric shape as an assembly of decomposed shape parts is\nmuch more challenging, but semantically more valuable than direct\nreconstruction from a full shape representation. The neural network needs to\nimplicitly learn part relations coherently, which is typically performed by\ndedicated network layers that can generate transformation matrices for each\npart. In this paper, we propose a VoxAttention network architecture for\nattention-based part assembly. We further propose a variant of using\nchannel-wise part attention and show the advantages of this approach.\nExperimental results show that our method outperforms most state-of-the-art\nmethods for the part relation-aware 3D shape modeling task.",
    "published": "2023-04-17T16:53:27Z",
    "updated": "2023-04-17T16:53:27Z",
    "authors": [
      "Chengzhi Wu",
      "Junwei Zheng",
      "Julius Pfrommer",
      "JÃ¼rgen Beyerer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.05362v1",
    "title": "Multi-head Attention-based Deep Multiple Instance Learning",
    "summary": "This paper introduces MAD-MIL, a Multi-head Attention-based Deep Multiple\nInstance Learning model, designed for weakly supervised Whole Slide Images\n(WSIs) classification in digital pathology. Inspired by the multi-head\nattention mechanism of the Transformer, MAD-MIL simplifies model complexity\nwhile achieving competitive results against advanced models like CLAM and\nDS-MIL. Evaluated on the MNIST-BAGS and public datasets, including TUPAC16,\nTCGA BRCA, TCGA LUNG, and TCGA KIDNEY, MAD-MIL consistently outperforms ABMIL.\nThis demonstrates enhanced information diversity, interpretability, and\nefficiency in slide representation. The model's effectiveness, coupled with\nfewer trainable parameters and lower computational complexity makes it a\npromising solution for automated pathology workflows. Our code is available at\nhttps://github.com/tueimage/MAD-MIL.",
    "published": "2024-04-08T09:54:28Z",
    "updated": "2024-04-08T09:54:28Z",
    "authors": [
      "Hassan Keshvarikhojasteh",
      "Josien Pluim",
      "Mitko Veta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.03025v2",
    "title": "Matten: Video Generation with Mamba-Attention",
    "summary": "In this paper, we introduce Matten, a cutting-edge latent diffusion model\nwith Mamba-Attention architecture for video generation. With minimal\ncomputational cost, Matten employs spatial-temporal attention for local video\ncontent modeling and bidirectional Mamba for global video content modeling. Our\ncomprehensive experimental evaluation demonstrates that Matten has competitive\nperformance with the current Transformer-based and GAN-based models in\nbenchmark performance, achieving superior FVD scores and efficiency.\nAdditionally, we observe a direct positive correlation between the complexity\nof our designed model and the improvement in video quality, indicating the\nexcellent scalability of Matten.",
    "published": "2024-05-05T18:36:45Z",
    "updated": "2024-05-10T08:30:07Z",
    "authors": [
      "Yu Gao",
      "Jiancheng Huang",
      "Xiaopeng Sun",
      "Zequn Jie",
      "Yujie Zhong",
      "Lin Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.10028v2",
    "title": "AttnMod: Attention-Based New Art Styles",
    "summary": "We introduce AttnMod, a training-free technique that modulates\ncross-attention in pre-trained diffusion models to generate novel, unpromptable\nart styles. The method is inspired by how a human artist might reinterpret a\ngenerated image, for example by emphasizing certain features, dispersing color,\ntwisting silhouettes, or materializing unseen elements. AttnMod simulates this\nintent by altering how the text prompt conditions the image through attention\nduring denoising. These targeted modulations enable diverse stylistic\ntransformations without changing the prompt or retraining the model, and they\nexpand the expressive capacity of text-to-image generation.",
    "published": "2024-09-16T06:38:25Z",
    "updated": "2025-08-01T03:49:12Z",
    "authors": [
      "Shih-Chieh Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.20222v1",
    "title": "QKCV Attention: Enhancing Time Series Forecasting with Static\n  Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models",
    "summary": "In real-world time series forecasting tasks, category information plays a\npivotal role in capturing inherent data patterns. This paper introduces QKCV\n(Query-Key-Category-Value) attention, an extension of the traditional QKV\nframework that incorporates a static categorical embedding C to emphasize\ncategory-specific information. As a versatile plug-in module, QKCV enhances the\nforecasting accuracy of attention-based models (e.g., Vanilla Transformer,\nInformer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV\ndemonstrates remarkable adaptability in fine-tuning univariate time series\nfoundation model by solely updating the static embedding C while preserving\npretrained weights, thereby reducing computational overhead and achieving\nsuperior fine-tuning performance.",
    "published": "2025-10-21T18:15:21Z",
    "updated": "2025-10-21T18:15:21Z",
    "authors": [
      "Hao Wang",
      "Baojun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.13575v1",
    "title": "Symmetric Transformer-based Network for Unsupervised Image Registration",
    "summary": "Medical image registration is a fundamental and critical task in medical\nimage analysis. With the rapid development of deep learning, convolutional\nneural networks (CNN) have dominated the medical image registration field. Due\nto the disadvantage of the local receptive field of CNN, some recent\nregistration methods have focused on using transformers for non-local\nregistration. However, the standard Transformer has a vast number of parameters\nand high computational complexity, which causes Transformer can only be applied\nat the bottom of the registration models. As a result, only coarse information\nis available at the lowest resolution, limiting the contribution of Transformer\nin their models. To address these challenges, we propose a convolution-based\nefficient multi-head self-attention (CEMSA) block, which reduces the parameters\nof the traditional Transformer and captures local spatial context information\nfor reducing semantic ambiguity in the attention mechanism. Based on the\nproposed CEMSA, we present a novel Symmetric Transformer-based model\n(SymTrans). SymTrans employs the Transformer blocks in the encoder and the\ndecoder respectively to model the long-range spatial cross-image relevance. We\napply SymTrans to the displacement field and diffeomorphic registration.\nExperimental results show that our proposed method achieves state-of-the-art\nperformance in image registration. Our code is publicly available at\n\\url{https://github.com/MingR-Ma/SymTrans}.",
    "published": "2022-04-28T15:45:09Z",
    "updated": "2022-04-28T15:45:09Z",
    "authors": [
      "Mingrui Ma",
      "Lei Song",
      "Yuanbo Xu",
      "Guixia Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2008.05750v1",
    "title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable\n  End-to-End Speech Recognition",
    "summary": "Transformer has achieved competitive performance against state-of-the-art\nend-to-end models in automatic speech recognition (ASR), and requires\nsignificantly less training time than RNN-based models. The original\nTransformer, with encoder-decoder architecture, is only suitable for offline\nASR. It relies on an attention mechanism to learn alignments, and encodes input\naudio bidirectionally. The high computation cost of Transformer decoding also\nlimits its use in production streaming systems. To make Transformer suitable\nfor streaming ASR, we explore Transducer framework as a streamable way to learn\nalignments. For audio encoding, we apply unidirectional Transformer with\ninterleaved convolution layers. The interleaved convolution layers are used for\nmodeling future context which is important to performance. To reduce\ncomputation cost, we gradually downsample acoustic input, also with the\ninterleaved convolution layers. Moreover, we limit the length of history\ncontext in self-attention to maintain constant computation cost for each\ndecoding step. We show that this architecture, named Conv-Transformer\nTransducer, achieves competitive performance on LibriSpeech dataset (3.6\\% WER\non test-clean) without external language models. The performance is comparable\nto previously published streamable Transformer Transducer and strong hybrid\nstreaming ASR systems, and is achieved with smaller look-ahead window (140~ms),\nfewer parameters and lower frame rate.",
    "published": "2020-08-13T08:20:02Z",
    "updated": "2020-08-13T08:20:02Z",
    "authors": [
      "Wenyong Huang",
      "Wenchao Hu",
      "Yu Ting Yeung",
      "Xiao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.01506v1",
    "title": "Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel\n  Machines",
    "summary": "Despite their ubiquity in core AI fields like natural language processing,\nthe mechanics of deep attention-based neural networks like the Transformer\nmodel are not fully understood. In this article, we present a new perspective\ntowards understanding how Transformers work. In particular, we show that the\n\"dot-product attention\" that is the core of the Transformer's operation can be\ncharacterized as a kernel learning method on a pair of Banach spaces. In\nparticular, the Transformer's kernel is characterized as having an infinite\nfeature dimension. Along the way we consider an extension of the standard\nkernel learning problem to a binary setting, where data come from two input\ndomains and a response is defined for every cross-domain pair. We prove a new\nrepresenter theorem for these binary kernel machines with non-Mercer\n(indefinite, asymmetric) kernels (implying that the functions learned are\nelements of reproducing kernel Banach spaces rather than Hilbert spaces), and\nalso prove a new universal approximation theorem showing that the Transformer\ncalculation can learn any binary non-Mercer reproducing kernel Banach space\npair. We experiment with new kernels in Transformers, and obtain results that\nsuggest the infinite dimensionality of the standard Transformer kernel is\npartially responsible for its performance. This paper's results provide a new\ntheoretical understanding of a very important but poorly understood model in\nmodern machine~learning.",
    "published": "2021-06-02T23:24:06Z",
    "updated": "2021-06-02T23:24:06Z",
    "authors": [
      "Matthew A. Wright",
      "Joseph E. Gonzalez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.11575v5",
    "title": "Shifted Chunk Transformer for Spatio-Temporal Representational Learning",
    "summary": "Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.",
    "published": "2021-08-26T04:34:33Z",
    "updated": "2021-10-29T03:02:26Z",
    "authors": [
      "Xuefan Zha",
      "Wentao Zhu",
      "Tingxun Lv",
      "Sen Yang",
      "Ji Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.10403v1",
    "title": "AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation",
    "summary": "Recent advances in transformer-based models have drawn attention to exploring\nthese techniques in medical image segmentation, especially in conjunction with\nthe U-Net model (or its variants), which has shown great success in medical\nimage segmentation, under both 2D and 3D settings. Current 2D based methods\neither directly replace convolutional layers with pure transformers or consider\na transformer as an additional intermediate encoder between the encoder and\ndecoder of U-Net. However, these approaches only consider the attention\nencoding within one single slice and do not utilize the axial-axis information\nnaturally provided by a 3D volume. In the 3D setting, convolution on volumetric\ndata and transformers both consume large GPU memory. One has to either\ndownsample the image or use cropped local patches to reduce GPU memory usage,\nwhich limits its performance. In this paper, we propose Axial Fusion\nTransformer UNet (AFTer-UNet), which takes both advantages of convolutional\nlayers' capability of extracting detailed features and transformers' strength\non long sequence modeling. It considers both intra-slice and inter-slice\nlong-range cues to guide the segmentation. Meanwhile, it has fewer parameters\nand takes less GPU memory to train than the previous transformer-based models.\nExtensive experiments on three multi-organ segmentation datasets demonstrate\nthat our method outperforms current state-of-the-art methods.",
    "published": "2021-10-20T06:47:28Z",
    "updated": "2021-10-20T06:47:28Z",
    "authors": [
      "Xiangyi Yan",
      "Hao Tang",
      "Shanlin Sun",
      "Haoyu Ma",
      "Deying Kong",
      "Xiaohui Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.01587v3",
    "title": "Multi-Tailed Vision Transformer for Efficient Inference",
    "summary": "Recently, Vision Transformer (ViT) has achieved promising performance in\nimage recognition and gradually serves as a powerful backbone in various vision\ntasks. To satisfy the sequential input of Transformer, the tail of ViT first\nsplits each image into a sequence of visual tokens with a fixed length. Then\nthe following self-attention layers constructs the global relationship between\ntokens to produce useful representation for the downstream tasks. Empirically,\nrepresenting the image with more tokens leads to better performance, yet the\nquadratic computational complexity of self-attention layer to the number of\ntokens could seriously influence the efficiency of ViT's inference. For\ncomputational reduction, a few pruning methods progressively prune\nuninformative tokens in the Transformer encoder, while leaving the number of\ntokens before the Transformer untouched. In fact, fewer tokens as the input for\nthe Transformer encoder can directly reduce the following computational cost.\nIn this spirit, we propose a Multi-Tailed Vision Transformer (MT-ViT) in the\npaper. MT-ViT adopts multiple tails to produce visual sequences of different\nlengths for the following Transformer encoder. A tail predictor is introduced\nto decide which tail is the most efficient for the image to produce accurate\nprediction. Both modules are optimized in an end-to-end fashion, with the\nGumbel-Softmax trick. Experiments on ImageNet-1K demonstrate that MT-ViT can\nachieve a significant reduction on FLOPs with no degradation of the accuracy\nand outperform other compared methods in both accuracy and FLOPs.",
    "published": "2022-03-03T09:30:55Z",
    "updated": "2024-03-18T14:32:54Z",
    "authors": [
      "Yunke Wang",
      "Bo Du",
      "Wenyuan Wang",
      "Chang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.12635v2",
    "title": "MoCoViT: Mobile Convolutional Vision Transformer",
    "summary": "Recently, Transformer networks have achieved impressive results on a variety\nof vision tasks. However, most of them are computationally expensive and not\nsuitable for real-world mobile applications. In this work, we present Mobile\nConvolutional Vision Transformer (MoCoViT), which improves in performance and\nefficiency by introducing transformer into mobile convolutional networks to\nleverage the benefits of both architectures. Different from recent works on\nvision transformer, the mobile transformer block in MoCoViT is carefully\ndesigned for mobile devices and is very lightweight, accomplished through two\nprimary modifications: the Mobile Self-Attention (MoSA) module and the Mobile\nFeed Forward Network (MoFFN). MoSA simplifies the calculation of the attention\nmap through Branch Sharing scheme while MoFFN serves as a mobile version of MLP\nin the transformer, further reducing the computation by a large margin.\nComprehensive experiments verify that our proposed MoCoViT family outperform\nstate-of-the-art portable CNNs and transformer neural architectures on various\nvision tasks. On ImageNet classification, it achieves 74.5% top-1 accuracy at\n147M FLOPs, gaining 1.2% over MobileNetV3 with less computations. And on the\nCOCO object detection task, MoCoViT outperforms GhostNet by 2.1 AP in RetinaNet\nframework.",
    "published": "2022-05-25T10:21:57Z",
    "updated": "2022-05-26T13:40:26Z",
    "authors": [
      "Hailong Ma",
      "Xin Xia",
      "Xing Wang",
      "Xuefeng Xiao",
      "Jiashi Li",
      "Min Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.14705v2",
    "title": "AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with\n  Transformers",
    "summary": "Self-attention-based transformer models have achieved tremendous success in\nthe domain of natural language processing. Despite their efficacy, accelerating\nthe transformer is challenging due to its quadratic computational complexity\nand large activation sizes. Existing transformer accelerators attempt to prune\nits tokens to reduce memory access, albeit with high compute overheads.\nMoreover, previous works directly operate on large matrices involved in the\nattention operation, which limits hardware utilization. In order to address\nthese challenges, this work proposes a novel dynamic inference scheme,\nDynaTran, which prunes activations at runtime with low overhead, substantially\nreducing the number of ineffectual operations. This improves the throughput of\ntransformer inference. We further propose tiling the matrices in transformer\noperations along with diverse dataflows to improve data reuse, thus enabling\nhigher energy efficiency. To effectively implement these methods, we propose\nAccelTran, a novel accelerator architecture for transformers. Extensive\nexperiments with different models and benchmarks demonstrate that DynaTran\nachieves higher accuracy than the state-of-the-art top-k hardware-aware pruning\nstrategy while attaining up to 1.2$\\times$ higher sparsity. One of our proposed\naccelerators, AccelTran-Edge, achieves 330K$\\times$ higher throughput with\n93K$\\times$ lower energy requirement when compared to a Raspberry Pi device. On\nthe other hand, AccelTran-Server achieves 5.73$\\times$ higher throughput and\n3.69$\\times$ lower energy consumption compared to the state-of-the-art\ntransformer co-processor, Energon. The simulation source code is available at\nhttps://github.com/jha-lab/acceltran.",
    "published": "2023-02-28T16:17:23Z",
    "updated": "2023-05-01T16:21:21Z",
    "authors": [
      "Shikhar Tuli",
      "Niraj K. Jha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.15099v2",
    "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence\n  Redundancy with FFT Operator",
    "summary": "The transformer model is known to be computationally demanding, and\nprohibitively costly for long sequences, as the self-attention module uses a\nquadratic time and space complexity with respect to sequence length. Many\nresearchers have focused on designing new forms of self-attention or\nintroducing new parameters to overcome this limitation, however a large portion\nof them prohibits the model to inherit weights from large pretrained models. In\nthis work, the transformer's inefficiency has been taken care of from another\nperspective. We propose Fourier Transformer, a simple yet effective approach by\nprogressively removing redundancies in hidden sequence using the ready-made\nFast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation\n(DCT). Fourier Transformer is able to significantly reduce computational costs\nwhile retain the ability to inherit from various large pretrained models.\nExperiments show that our model achieves state-of-the-art performances among\nall transformer-based models on the long-range modeling benchmark LRA with\nsignificant improvement in both speed and space. For generative seq-to-seq\ntasks including CNN/DailyMail and ELI5, by inheriting the BART weights our\nmodel outperforms the standard BART and other efficient models. Our code is\npublicly available at https://github.com/LUMIA-Group/FourierTransformer",
    "published": "2023-05-24T12:33:06Z",
    "updated": "2025-05-16T12:42:33Z",
    "authors": [
      "Ziwei He",
      "Meng Yang",
      "Minwei Feng",
      "Jingcheng Yin",
      "Xinbing Wang",
      "Jingwen Leng",
      "Zhouhan Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.05695v2",
    "title": "The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel\n  Size might be All You Need",
    "summary": "Vision Transformers have been rapidly uprising in computer vision thanks to\ntheir outstanding scaling trends, and gradually replacing convolutional neural\nnetworks (CNNs). Recent works on self-supervised learning (SSL) introduce\nsiamese pre-training tasks, on which Transformer backbones continue to\ndemonstrate ever stronger results than CNNs. People come to believe that\nTransformers or self-attention modules are inherently more suitable than CNNs\nin the context of SSL. However, it is noteworthy that most if not all prior\narts of SSL with CNNs chose the standard ResNets as their backbones, whose\narchitecture effectiveness is known to already lag behind advanced Vision\nTransformers. Therefore, it remains unclear whether the self-attention\noperation is crucial for the recent advances in SSL - or CNNs can deliver the\nsame excellence with more advanced designs, too? Can we close the SSL\nperformance gap between Transformers and CNNs? To answer these intriguing\nquestions, we apply self-supervised pre-training to the recently proposed,\nstronger lager-kernel CNN architecture and conduct an apple-to-apple comparison\nwith Transformers, in their SSL performance. Our results show that we are able\nto build pure CNN SSL architectures that perform on par with or better than the\nbest SSL-trained Transformers, by just scaling up convolutional kernel sizes\nbesides other small tweaks. Impressively, when transferring to the downstream\ntasks \\texttt{MS COCO} detection and segmentation, our SSL pre-trained CNN\nmodel (trained in 100 epochs) achieves the same good performance as the\n300-epoch pre-trained Transformer counterpart. We hope this work can help to\nbetter understand what is essential (or not) for self-supervised learning\nbackbones.",
    "published": "2023-12-09T22:23:57Z",
    "updated": "2023-12-12T18:23:42Z",
    "authors": [
      "Tianjin Huang",
      "Tianlong Chen",
      "Zhangyang Wang",
      "Shiwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.03364v4",
    "title": "Unified Error Correction Code Transformer with Low Complexity",
    "summary": "Channel coding is vital for reliable sixth-generation (6G) data transmission,\nemploying diverse error correction codes for various application scenarios.\nTraditional decoders require dedicated hardware for each code, leading to high\nhardware costs. Recently, artificial intelligence (AI)-driven approaches, such\nas the error correction code Transformer (ECCT) and its enhanced version, the\nfoundation error correction code Transformer (FECCT), have been proposed to\nreduce the hardware cost by leveraging the Transformer to decode multiple\ncodes. However, their excessively high computational complexity of\n$\\mathcal{O}(N^2)$ due to the self-attention mechanism in the Transformer\nlimits scalability, where $N$ represents the sequence length. To reduce\ncomputational complexity, we propose a unified Transformer-based decoder that\nhandles multiple linear block codes within a single framework. Specifically, a\nstandardized unit is employed to align code length and code rate across\ndifferent code types, while a redesigned low-rank unified attention module,\nwith computational complexity of $\\mathcal{O}(N)$, is shared across various\nheads in the Transformer. Additionally, a sparse mask, derived from the\nparity-check matrix's sparsity, is introduced to enhance the decoder's ability\nto capture inherent constraints between information and parity-check bits,\nimproving decoding accuracy and further reducing computational complexity by\n$86\\%$. Extensive experimental results demonstrate that the proposed unified\nTransformer-based decoder outperforms existing methods and provides a\nhigh-performance, low-complexity solution for next-generation wireless\ncommunication systems.",
    "published": "2024-10-04T12:30:42Z",
    "updated": "2025-10-30T08:00:06Z",
    "authors": [
      "Yongli Yan",
      "Jieao Zhu",
      "Tianyue Zheng",
      "Zhuo Xu",
      "Chao Jiang",
      "Linglong Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.10448v1",
    "title": "Towards Lightweight Time Series Forecasting: a Patch-wise Transformer\n  with Weak Data Enriching",
    "summary": "Patch-wise Transformer based time series forecasting achieves superior\naccuracy. However, this superiority relies heavily on intricate model design\nwith massive parameters, rendering both training and inference expensive, thus\npreventing their deployments on edge devices with limited resources and low\nlatency requirements. In addition, existing methods often work in an\nautoregressive manner, which take into account only historical values, but\nignore valuable, easy-to-obtain context information, such as weather forecasts,\ndate and time of day. To contend with the two limitations, we propose\nLiPFormer, a novel Lightweight Patch-wise Transformer with weak data enriching.\nFirst, to simplify the Transformer backbone, LiPFormer employs a novel\nlightweight cross-patch attention and a linear transformation-based attention\nto eliminate Layer Normalization and Feed Forward Network, two heavy components\nin existing Transformers. Second, we propose a lightweight, weak data enriching\nmodule to provide additional, valuable weak supervision to the training. It\nenhances forecasting accuracy without significantly increasing model complexity\nas it does not involve expensive, human-labeling but using easily accessible\ncontext information. This facilitates the weak data enriching to plug-and-play\non existing models. Extensive experiments on nine benchmark time series\ndatasets demonstrate that LiPFormer outperforms state-of-the-art methods in\naccuracy, while significantly reducing parameter scale, training duration, and\nGPU memory usage. Deployment on an edge device reveals that LiPFormer takes\nonly 1/3 inference time compared to classic Transformers. In addition, we\ndemonstrate that the weak data enriching can integrate seamlessly into various\nTransformer based models to enhance their accuracy, suggesting its generality.",
    "published": "2025-01-14T13:35:03Z",
    "updated": "2025-01-14T13:35:03Z",
    "authors": [
      "Meng Wang",
      "Jintao Yang",
      "Bin Yang",
      "Hui Li",
      "Tongxin Gong",
      "Bo Yang",
      "Jiangtao Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09029v2",
    "title": "MTDP: A Modulated Transformer based Diffusion Policy Model",
    "summary": "Recent research on robot manipulation based on Behavior Cloning (BC) has made\nsignificant progress. By combining diffusion models with BC, diffusion policiy\nhas been proposed, enabling robots to quickly learn manipulation tasks with\nhigh success rates. However, integrating diffusion policy with high-capacity\nTransformer presents challenges, traditional Transformer architectures struggle\nto effectively integrate guiding conditions, resulting in poor performance in\nmanipulation tasks when using Transformer-based models. In this paper, we\ninvestigate key architectural designs of Transformers and improve the\ntraditional Transformer architecture by proposing the Modulated Transformer\nDiffusion Policy (MTDP) model for diffusion policy. The core of this model is\nthe Modulated Attention module we proposed, which more effectively integrates\nthe guiding conditions with the main input, improving the generative model's\noutput quality and, consequently, increasing the robot's task success rate. In\nsix experimental tasks, MTDP outperformed existing Transformer model\narchitectures, particularly in the Toolhang experiment, where the success rate\nincreased by 12\\%. To verify the generality of Modulated Attention, we applied\nit to the UNet architecture to construct Modulated UNet Diffusion Policy model\n(MUDP), which also achieved higher success rates than existing UNet\narchitectures across all six experiments. The Diffusion Policy uses Denoising\nDiffusion Probabilistic Models (DDPM) as the diffusion model. Building on this,\nwe also explored Denoising Diffusion Implicit Models (DDIM) as the diffusion\nmodel, constructing the MTDP-I and MUDP-I model, which nearly doubled the\ngeneration speed while maintaining performance.",
    "published": "2025-02-13T07:35:03Z",
    "updated": "2025-03-17T02:11:44Z",
    "authors": [
      "Qianhao Wang",
      "Yinqian Sun",
      "Enmeng Lu",
      "Qian Zhang",
      "Yi Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08184v1",
    "title": "Selective Induction Heads: How Transformers Select Causal Structures In\n  Context",
    "summary": "Transformers have exhibited exceptional capabilities in sequence modeling\ntasks, leveraging self-attention and in-context learning. Critical to this\nsuccess are induction heads, attention circuits that enable copying tokens\nbased on their previous occurrences. In this work, we introduce a novel\nframework that showcases transformers' ability to dynamically handle causal\nstructures. Existing works rely on Markov Chains to study the formation of\ninduction heads, revealing how transformers capture causal dependencies and\nlearn transition probabilities in-context. However, they rely on a fixed causal\nstructure that fails to capture the complexity of natural languages, where the\nrelationship between tokens dynamically changes with context. To this end, our\nframework varies the causal structure through interleaved Markov chains with\ndifferent lags while keeping the transition probabilities fixed. This setting\nunveils the formation of Selective Induction Heads, a new circuit that endows\ntransformers with the ability to select the correct causal structure\nin-context. We empirically demonstrate that transformers learn this mechanism\nto predict the next token by identifying the correct lag and copying the\ncorresponding token from the past. We provide a detailed construction of a\n3-layer transformer to implement the selective induction head, and a\ntheoretical analysis proving that this mechanism asymptotically converges to\nthe maximum likelihood solution. Our findings advance the understanding of how\ntransformers select causal structures, providing new insights into their\nfunctioning and interpretability.",
    "published": "2025-09-09T23:13:41Z",
    "updated": "2025-09-09T23:13:41Z",
    "authors": [
      "Francesco D'Angelo",
      "Francesco Croce",
      "Nicolas Flammarion"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27015v1",
    "title": "Quantitative Bounds for Length Generalization in Transformers",
    "summary": "We study the problem of length generalization (LG) in transformers: the\nability of a model trained on shorter sequences to maintain performance when\nevaluated on much longer, previously unseen inputs. Prior work by Huang et al.\n(2025) established that transformers eventually achieve length generalization\nonce the training sequence length exceeds some finite threshold, but left open\nthe question of how large it must be. In this work, we provide the first\nquantitative bounds on the required training length for length generalization\nto occur. Motivated by previous empirical and theoretical work, we analyze LG\nin several distinct problem settings: $\\ell_\\infty$ error control vs. average\nerror control over an input distribution, infinite-precision softmax attention\nvs. finite-precision attention (which reduces to an argmax) in the transformer,\nand one- vs. two-layer transformers. In all scenarios, we prove that LG occurs\nwhen the internal behavior of the transformer on longer sequences can be\n\"simulated\" by its behavior on shorter sequences seen during training. Our\nbounds give qualitative estimates for the length of training data required for\na transformer to generalize, and we verify these insights empirically. These\nresults sharpen our theoretical understanding of the mechanisms underlying\nextrapolation in transformers, and formalize the intuition that richer training\ndata is required for generalization on more complex tasks.",
    "published": "2025-10-30T21:31:36Z",
    "updated": "2025-10-30T21:31:36Z",
    "authors": [
      "Zachary Izzo",
      "Eshaan Nichani",
      "Jason D. Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1812.02825v5",
    "title": "Attending to Mathematical Language with Transformers",
    "summary": "Mathematical expressions were generated, evaluated and used to train neural\nnetwork models based on the transformer architecture. The expressions and their\ntargets were analyzed as a character-level sequence transduction task in which\nthe encoder and decoder are built on attention mechanisms. Three models were\ntrained to understand and evaluate symbolic variables and expressions in\nmathematics: (1) the self-attentive and feed-forward transformer without\nrecurrence or convolution, (2) the universal transformer with recurrence, and\n(3) the adaptive universal transformer with recurrence and adaptive computation\ntime. The models respectively achieved test accuracies as high as 76.1%, 78.8%\nand 84.9% in evaluating the expressions to match the target values. For the\ncases inferred incorrectly, the results differed from the targets by only one\nor two characters. The models notably learned to add, subtract and multiply\nboth positive and negative decimal numbers of variable digits assigned to\nsymbolic variables.",
    "published": "2018-12-05T03:05:08Z",
    "updated": "2019-09-14T20:03:50Z",
    "authors": [
      "Artit Wangperawong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.04903v1",
    "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained\n  Language Model Positional Encoding",
    "summary": "In recent years, pre-trained Transformers have dominated the majority of NLP\nbenchmark tasks. Many variants of pre-trained Transformers have kept breaking\nout, and most focus on designing different pre-training objectives or variants\nof self-attention. Embedding the position information in the self-attention\nmechanism is also an indispensable factor in Transformers however is often\ndiscussed at will. Therefore, this paper carries out an empirical study on\nposition embeddings of mainstream pre-trained Transformers, which mainly\nfocuses on two questions: 1) Do position embeddings really learn the meaning of\npositions? 2) How do these different learned position embeddings affect\nTransformers for NLP tasks? This paper focuses on providing a new insight of\npre-trained position embeddings through feature-level analysis and empirical\nexperiments on most of iconic NLP tasks. It is believed that our experimental\nresults can guide the future work to choose the suitable positional encoding\nfunction for specific tasks given the application property.",
    "published": "2020-10-10T05:03:14Z",
    "updated": "2020-10-10T05:03:14Z",
    "authors": [
      "Yu-An Wang",
      "Yun-Nung Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.06268v3",
    "title": "Continual Transformers: Redundancy-Free Attention for Online Inference",
    "summary": "Transformers in their common form are inherently limited to operate on whole\ntoken sequences rather than on one token at a time. Consequently, their use\nduring online inference on time-series data entails considerable redundancy due\nto the overlap in successive token sequences. In this work, we propose novel\nformulations of the Scaled Dot-Product Attention, which enable Transformers to\nperform efficient online token-by-token inference on a continual input stream.\nImportantly, our modifications are purely to the order of computations, while\nthe outputs and learned weights are identical to those of the original\nTransformer Encoder. We validate our Continual Transformer Encoder with\nexperiments on the THUMOS14, TVSeries and GTZAN datasets with remarkable\nresults: Our Continual one- and two-block architectures reduce the floating\npoint operations per prediction by up to 63x and 2.6x, respectively, while\nretaining predictive performance.",
    "published": "2022-01-17T08:20:09Z",
    "updated": "2023-01-24T07:42:08Z",
    "authors": [
      "Lukas Hedegaard",
      "Arian Bakhtiarnia",
      "Alexandros Iosifidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.12903v1",
    "title": "Aggregating Global Features into Local Vision Transformer",
    "summary": "Local Transformer-based classification models have recently achieved\npromising results with relatively low computational costs. However, the effect\nof aggregating spatial global information of local Transformer-based\narchitecture is not clear. This work investigates the outcome of applying a\nglobal attention-based module named multi-resolution overlapped attention (MOA)\nin the local window-based transformer after each stage. The proposed MOA\nemploys slightly larger and overlapped patches in the key to enable\nneighborhood pixel information transmission, which leads to significant\nperformance gain. In addition, we thoroughly investigate the effect of the\ndimension of essential architecture components through extensive experiments\nand discover an optimum architecture design. Extensive experimental results\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets demonstrate that the proposed\napproach outperforms previous vision Transformers with a comparatively fewer\nnumber of parameters.",
    "published": "2022-01-30T19:57:35Z",
    "updated": "2022-01-30T19:57:35Z",
    "authors": [
      "Krushi Patel",
      "Andres M. Bur",
      "Fengjun Li",
      "Guanghui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.11356v1",
    "title": "Preformer: Predictive Transformer with Multi-Scale Segment-wise\n  Correlations for Long-Term Time Series Forecasting",
    "summary": "Transformer-based methods have shown great potential in long-term time series\nforecasting. However, most of these methods adopt the standard point-wise\nself-attention mechanism, which not only becomes intractable for long-term\nforecasting since its complexity increases quadratically with the length of\ntime series, but also cannot explicitly capture the predictive dependencies\nfrom contexts since the corresponding key and value are transformed from the\nsame point. This paper proposes a predictive Transformer-based model called\n{\\em Preformer}. Preformer introduces a novel efficient {\\em Multi-Scale\nSegment-Correlation} mechanism that divides time series into segments and\nutilizes segment-wise correlation-based attention for encoding time series. A\nmulti-scale structure is developed to aggregate dependencies at different\ntemporal scales and facilitate the selection of segment length. Preformer\nfurther designs a predictive paradigm for decoding, where the key and value\ncome from two successive segments rather than the same segment. In this way, if\na key segment has a high correlation score with the query segment, its\nsuccessive segment contributes more to the prediction of the query segment.\nExtensive experiments demonstrate that our Preformer outperforms other\nTransformer-based methods.",
    "published": "2022-02-23T08:49:35Z",
    "updated": "2022-02-23T08:49:35Z",
    "authors": [
      "Dazhao Du",
      "Bing Su",
      "Zhewei Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.06252v1",
    "title": "Transformer Lesion Tracker",
    "summary": "Evaluating lesion progression and treatment response via longitudinal lesion\ntracking plays a critical role in clinical practice. Automated approaches for\nthis task are motivated by prohibitive labor costs and time consumption when\nlesion matching is done manually. Previous methods typically lack the\nintegration of local and global information. In this work, we propose a\ntransformer-based approach, termed Transformer Lesion Tracker (TLT).\nSpecifically, we design a Cross Attention-based Transformer (CAT) to capture\nand combine both global and local information to enhance feature extraction. We\nalso develop a Registration-based Anatomical Attention Module (RAAM) to\nintroduce anatomical information to CAT so that it can focus on useful feature\nknowledge. A Sparse Selection Strategy (SSS) is presented for selecting\nfeatures and reducing memory footprint in Transformer training. In addition, we\nuse a global regression to further improve model performance. We conduct\nexperiments on a public dataset to show the superiority of our method and find\nthat our model performance has improved the average Euclidean center error by\nat least 14.3% (6mm vs. 7mm) compared with the state-of-the-art (SOTA). Code is\navailable at https://github.com/TangWen920812/TLT.",
    "published": "2022-06-13T15:35:24Z",
    "updated": "2022-06-13T15:35:24Z",
    "authors": [
      "Wen Tang",
      "Han Kang",
      "Haoyue Zhang",
      "Pengxin Yu",
      "Corey W. Arnold",
      "Rongguo Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.01753v1",
    "title": "Two-Stream Transformer Architecture for Long Video Understanding",
    "summary": "Pure vision transformer architectures are highly effective for short video\nclassification and action recognition tasks. However, due to the quadratic\ncomplexity of self attention and lack of inductive bias, transformers are\nresource intensive and suffer from data inefficiencies. Long form video\nunderstanding tasks amplify data and memory efficiency problems in transformers\nmaking current approaches unfeasible to implement on data or memory restricted\ndomains. This paper introduces an efficient Spatio-Temporal Attention Network\n(STAN) which uses a two-stream transformer architecture to model dependencies\nbetween static image features and temporal contextual features. Our proposed\napproach can classify videos up to two minutes in length on a single GPU, is\ndata efficient, and achieves SOTA performance on several long video\nunderstanding tasks.",
    "published": "2022-08-02T21:03:48Z",
    "updated": "2022-08-02T21:03:48Z",
    "authors": [
      "Edward Fish",
      "Jon Weinbren",
      "Andrew Gilbert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.02444v4",
    "title": "Calibrating Transformers via Sparse Gaussian Processes",
    "summary": "Transformer models have achieved profound success in prediction tasks in a\nwide range of applications in natural language processing, speech recognition\nand computer vision. Extending Transformer's success to safety-critical domains\nrequires calibrated uncertainty estimation which remains under-explored. To\naddress this, we propose Sparse Gaussian Process attention (SGPA), which\nperforms Bayesian inference directly in the output space of multi-head\nattention blocks (MHAs) in transformer to calibrate its uncertainty. It\nreplaces the scaled dot-product operation with a valid symmetric kernel and\nuses sparse Gaussian processes (SGP) techniques to approximate the posterior\nprocesses of MHA outputs. Empirically, on a suite of prediction tasks on text,\nimages and graphs, SGPA-based Transformers achieve competitive predictive\naccuracy, while noticeably improving both in-distribution calibration and\nout-of-distribution robustness and detection.",
    "published": "2023-03-04T16:04:17Z",
    "updated": "2025-09-10T14:37:12Z",
    "authors": [
      "Wenlong Chen",
      "Yingzhen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.07626v1",
    "title": "CAT: Causal Audio Transformer for Audio Classification",
    "summary": "The attention-based Transformers have been increasingly applied to audio\nclassification because of their global receptive field and ability to handle\nlong-term dependency. However, the existing frameworks which are mainly\nextended from the Vision Transformers are not perfectly compatible with audio\nsignals. In this paper, we introduce a Causal Audio Transformer (CAT)\nconsisting of a Multi-Resolution Multi-Feature (MRMF) feature extraction with\nan acoustic attention block for more optimized audio modeling. In addition, we\npropose a causal module that alleviates over-fitting, helps with knowledge\ntransfer, and improves interpretability. CAT obtains higher or comparable\nstate-of-the-art classification performance on ESC50, AudioSet and UrbanSound8K\ndatasets, and can be easily generalized to other Transformer-based models.",
    "published": "2023-03-14T04:50:52Z",
    "updated": "2023-03-14T04:50:52Z",
    "authors": [
      "Xiaoyu Liu",
      "Hanlin Lu",
      "Jianbo Yuan",
      "Xinyu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.14978v1",
    "title": "Learned Image Compression with Mixed Transformer-CNN Architectures",
    "summary": "Learned image compression (LIC) methods have exhibited promising progress and\nsuperior rate-distortion performance compared with classical image compression\nstandards. Most existing LIC methods are Convolutional Neural Networks-based\n(CNN-based) or Transformer-based, which have different advantages. Exploiting\nboth advantages is a point worth exploring, which has two challenges: 1) how to\neffectively fuse the two methods? 2) how to achieve higher performance with a\nsuitable complexity? In this paper, we propose an efficient parallel\nTransformer-CNN Mixture (TCM) block with a controllable complexity to\nincorporate the local modeling ability of CNN and the non-local modeling\nability of transformers to improve the overall architecture of image\ncompression models. Besides, inspired by the recent progress of entropy\nestimation models and attention modules, we propose a channel-wise entropy\nmodel with parameter-efficient swin-transformer-based attention (SWAtten)\nmodules by using channel squeezing. Experimental results demonstrate our\nproposed method achieves state-of-the-art rate-distortion performances on three\ndifferent resolution datasets (i.e., Kodak, Tecnick, CLIC Professional\nValidation) compared to existing LIC methods. The code is at\nhttps://github.com/jmliu206/LIC_TCM.",
    "published": "2023-03-27T08:19:01Z",
    "updated": "2023-03-27T08:19:01Z",
    "authors": [
      "Jinming Liu",
      "Heming Sun",
      "Jiro Katto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1910.13215v3",
    "title": "Transformer-based Cascaded Multimodal Speech Translation",
    "summary": "This paper describes the cascaded multimodal speech translation systems\ndeveloped by Imperial College London for the IWSLT 2019 evaluation campaign.\nThe architecture consists of an automatic speech recognition (ASR) system\nfollowed by a Transformer-based multimodal machine translation (MMT) system.\nWhile the ASR component is identical across the experiments, the MMT model\nvaries in terms of the way of integrating the visual context (simple\nconditioning vs. attention), the type of visual features exploited (pooled,\nconvolutional, action categories) and the underlying architecture. For the\nlatter, we explore both the canonical transformer and its deliberation version\nwith additive and cascade variants which differ in how they integrate the\ntextual attention. Upon conducting extensive experiments, we found that (i) the\nexplored visual integration schemes often harm the translation performance for\nthe transformer and additive deliberation, but considerably improve the cascade\ndeliberation; (ii) the transformer and cascade deliberation integrate the\nvisual modality better than the additive deliberation, as shown by the\nincongruence analysis.",
    "published": "2019-10-29T11:56:12Z",
    "updated": "2019-11-08T20:04:10Z",
    "authors": [
      "Zixiu Wu",
      "Ozan Caglayan",
      "Julia Ive",
      "Josiah Wang",
      "Lucia Specia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.03864v2",
    "title": "Improving Transformer Models by Reordering their Sublayers",
    "summary": "Multilayer transformer networks consist of interleaved self-attention and\nfeedforward sublayers. Could ordering the sublayers in a different pattern lead\nto better performance? We generate randomly ordered transformers and train them\nwith the language modeling objective. We observe that some of these models are\nable to achieve better performance than the interleaved baseline, and that\nthose successful variants tend to have more self-attention at the bottom and\nmore feedforward sublayers at the top. We propose a new transformer pattern\nthat adheres to this property, the sandwich transformer, and show that it\nimproves perplexity on multiple word-level and character-level language\nmodeling benchmarks, at no cost in parameters, memory, or training time.\nHowever, the sandwich reordering pattern does not guarantee performance gains\nacross every task, as we demonstrate on machine translation models. Instead, we\nsuggest that further exploration of task-specific sublayer reorderings is\nneeded in order to unlock additional gains.",
    "published": "2019-11-10T06:14:15Z",
    "updated": "2020-04-23T10:16:33Z",
    "authors": [
      "Ofir Press",
      "Noah A. Smith",
      "Omer Levy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.04474v3",
    "title": "TENER: Adapting Transformer Encoder for Named Entity Recognition",
    "summary": "The Bidirectional long short-term memory networks (BiLSTM) have been widely\nused as an encoder in models solving the named entity recognition (NER) task.\nRecently, the Transformer is broadly adopted in various Natural Language\nProcessing (NLP) tasks owing to its parallelism and advantageous performance.\nNevertheless, the performance of the Transformer in NER is not as good as it is\nin other NLP tasks. In this paper, we propose TENER, a NER architecture\nadopting adapted Transformer Encoder to model the character-level features and\nword-level features. By incorporating the direction and relative distance aware\nattention and the un-scaled attention, we prove the Transformer-like encoder is\njust as effective for NER as other NLP tasks.",
    "published": "2019-11-10T15:05:48Z",
    "updated": "2019-12-10T07:01:25Z",
    "authors": [
      "Hang Yan",
      "Bocao Deng",
      "Xiaonan Li",
      "Xipeng Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.00699v1",
    "title": "Transformer-based Arabic Dialect Identification",
    "summary": "This paper presents a dialect identification (DID) system based on the\ntransformer neural network architecture. The conventional convolutional neural\nnetwork (CNN)-based systems use the shorter receptive fields. We believe that\nlong range information is equally important for language and DID, and\nself-attention mechanism in transformer captures the long range dependencies.\nIn addition, to reduce the computational complexity, self-attention with\ndownsampling is used to process the acoustic features. This process extracts\nsparse, yet informative features. Our experimental results show that\ntransformer outperforms CNN-based networks on the Arabic dialect identification\n(ADI) dataset. We also report that the score-level fusion of CNN and\ntransformer-based systems obtains an overall accuracy of 86.29% on the ADI17\ndatabase.",
    "published": "2020-11-02T03:10:34Z",
    "updated": "2020-11-02T03:10:34Z",
    "authors": [
      "Wanqiu Lin",
      "Maulik Madhavi",
      "Rohan Kumar Das",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.09315v2",
    "title": "End-to-End Object Detection with Adaptive Clustering Transformer",
    "summary": "End-to-end Object Detection with Transformer (DETR)proposes to perform object\ndetection with Transformer and achieve comparable performance with two-stage\nobject detection like Faster-RCNN. However, DETR needs huge computational\nresources for training and inference due to the high-resolution spatial input.\nIn this paper, a novel variant of transformer named Adaptive Clustering\nTransformer(ACT) has been proposed to reduce the computation cost for\nhigh-resolution input. ACT cluster the query features adaptively using Locality\nSensitive Hashing (LSH) and ap-proximate the query-key interaction using the\nprototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside\nself-attention into O(NK) where K is the number of prototypes in each layer.\nACT can be a drop-in module replacing the original self-attention module\nwithout any training. ACT achieves a good balance between accuracy and\ncomputation cost (FLOPs). The code is available as supplementary for the ease\nof experiment replication and verification. Code is released at\n\\url{https://github.com/gaopengcuhk/SMCA-DETR/}",
    "published": "2020-11-18T14:36:37Z",
    "updated": "2021-10-18T07:15:55Z",
    "authors": [
      "Minghang Zheng",
      "Peng Gao",
      "Renrui Zhang",
      "Kunchang Li",
      "Xiaogang Wang",
      "Hongsheng Li",
      "Hao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.11472v2",
    "title": "Spatial-Channel Transformer Network for Trajectory Prediction on the\n  Traffic Scenes",
    "summary": "Predicting motion of surrounding agents is critical to real-world\napplications of tactical path planning for autonomous driving. Due to the\ncomplex temporal dependencies and social interactions of agents, on-line\ntrajectory prediction is a challenging task. With the development of attention\nmechanism in recent years, transformer model has been applied in natural\nlanguage sequence processing first and then image processing. In this paper, we\npresent a Spatial-Channel Transformer Network for trajectory prediction with\nattention functions. Instead of RNN models, we employ transformer model to\ncapture the spatial-temporal features of agents. A channel-wise module is\ninserted to measure the social interaction between agents. We find that the\nSpatial-Channel Transformer Network achieves promising results on real-world\ntrajectory prediction datasets on the traffic scenes.",
    "published": "2021-01-27T15:03:42Z",
    "updated": "2021-02-05T03:40:32Z",
    "authors": [
      "Jingwen Zhao",
      "Xuanpeng Li",
      "Qifan Xue",
      "Weigong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.08036v1",
    "title": "Exploring Transformers in Natural Language Generation: GPT, BERT, and\n  XLNet",
    "summary": "Recent years have seen a proliferation of attention mechanisms and the rise\nof Transformers in Natural Language Generation (NLG). Previously,\nstate-of-the-art NLG architectures such as RNN and LSTM ran into vanishing\ngradient problems; as sentences grew larger, distance between positions\nremained linear, and sequential computation hindered parallelization since\nsentences were processed word by word. Transformers usher in a new era. In this\npaper, we explore three major Transformer-based models, namely GPT, BERT, and\nXLNet, that carry significant implications for the field. NLG is a burgeoning\narea that is now bolstered with rapid developments in attention mechanisms.\nFrom poetry generation to summarization, text generation derives benefit as\nTransformer-based language models achieve groundbreaking results.",
    "published": "2021-02-16T09:18:16Z",
    "updated": "2021-02-16T09:18:16Z",
    "authors": [
      "M. Onat Topal",
      "Anil Bas",
      "Imke van Heerden"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.08698v2",
    "title": "A Simple and Effective Positional Encoding for Transformers",
    "summary": "Transformer models are permutation equivariant. To supply the order and type\ninformation of the input tokens, position and segment embeddings are usually\nadded to the input. Recent works proposed variations of positional encodings\nwith relative position encodings achieving better performance. Our analysis\nshows that the gain actually comes from moving positional information to\nattention layer from the input. Motivated by this, we introduce Decoupled\nPositional Attention for Transformers (DIET), a simple yet effective mechanism\nto encode position and segment information into the Transformer models. The\nproposed method has faster training and inference time, while achieving\ncompetitive performance on GLUE, XTREME and WMT benchmarks. We further\ngeneralize our method to long-range transformers and show performance gain.",
    "published": "2021-04-18T03:44:57Z",
    "updated": "2021-11-03T04:36:53Z",
    "authors": [
      "Pu-Chin Chen",
      "Henry Tsai",
      "Srinadh Bhojanapalli",
      "Hyung Won Chung",
      "Yin-Wen Chang",
      "Chun-Sung Ferng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.12335v3",
    "title": "Diverse Image Inpainting with Bidirectional and Autoregressive\n  Transformers",
    "summary": "Image inpainting is an underdetermined inverse problem, which naturally\nallows diverse contents to fill up the missing or corrupted regions\nrealistically. Prevalent approaches using convolutional neural networks (CNNs)\ncan synthesize visually pleasant contents, but CNNs suffer from limited\nperception fields for capturing global features. With image-level attention,\ntransformers enable to model long-range dependencies and generate diverse\ncontents with autoregressive modeling of pixel-sequence distributions. However,\nthe unidirectional attention in autoregressive transformers is suboptimal as\ncorrupted image regions may have arbitrary shapes with contexts from any\ndirection. We propose BAT-Fill, an innovative image inpainting framework that\nintroduces a novel bidirectional autoregressive transformer (BAT) for image\ninpainting. BAT utilizes the transformers to learn autoregressive\ndistributions, which naturally allows the diverse generation of missing\ncontents. In addition, it incorporates the masked language model like BERT,\nwhich enables bidirectionally modeling of contextual information of missing\nregions for better image completion. Extensive experiments over multiple\ndatasets show that BAT-Fill achieves superior diversity and fidelity in image\ninpainting qualitatively and quantitatively.",
    "published": "2021-04-26T03:52:27Z",
    "updated": "2021-06-01T04:10:48Z",
    "authors": [
      "Yingchen Yu",
      "Fangneng Zhan",
      "Rongliang Wu",
      "Jianxiong Pan",
      "Kaiwen Cui",
      "Shijian Lu",
      "Feiying Ma",
      "Xuansong Xie",
      "Chunyan Miao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.09142v2",
    "title": "Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?",
    "summary": "The automatic detection of humor poses a grand challenge for natural language\nprocessing. Transformer-based systems have recently achieved remarkable results\non this task, but they usually (1)~were evaluated in setups where serious vs\nhumorous texts came from entirely different sources, and (2)~focused on\nbenchmarking performance without providing insights into how the models work.\nWe make progress in both respects by training and analyzing transformer-based\nhumor recognition models on a recently introduced dataset consisting of minimal\npairs of aligned sentences, one serious, the other humorous. We find that,\nalthough our aligned dataset is much harder than previous datasets,\ntransformer-based models recognize the humorous sentence in an aligned pair\nwith high accuracy (78%). In a careful error analysis, we characterize easy vs\nhard instances. Finally, by analyzing attention weights, we obtain important\ninsights into the mechanisms by which transformers recognize humor. Most\nremarkably, we find clear evidence that one single attention head learns to\nrecognize the words that make a test sentence humorous, even without access to\nthis information at training time.",
    "published": "2021-05-19T14:02:25Z",
    "updated": "2021-08-25T08:50:51Z",
    "authors": [
      "Maxime Peyrard",
      "Beatriz Borges",
      "Kristina GligoriÄ",
      "Robert West"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.07350v1",
    "title": "THG: Transformer with Hyperbolic Geometry",
    "summary": "Transformer model architectures have become an indispensable staple in deep\nlearning lately for their effectiveness across a range of tasks. Recently, a\nsurge of \"X-former\" models have been proposed which improve upon the original\nTransformer architecture. However, most of these variants make changes only\naround the quadratic time and memory complexity of self-attention, i.e. the dot\nproduct between the query and the key. What's more, they are calculate solely\nin Euclidean space. In this work, we propose a novel Transformer with\nHyperbolic Geometry (THG) model, which take the advantage of both Euclidean\nspace and Hyperbolic space. THG makes improvements in linear transformations of\nself-attention, which are applied on the input sequence to get the query and\nthe key, with the proposed hyperbolic linear. Extensive experiments on sequence\nlabeling task, machine reading comprehension task and classification task\ndemonstrate the effectiveness and generalizability of our model. It also\ndemonstrates THG could alleviate overfitting.",
    "published": "2021-06-01T14:09:33Z",
    "updated": "2021-06-01T14:09:33Z",
    "authors": [
      "Zhe Liu",
      "Yibin Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.09322v2",
    "title": "MM-ViT: Multi-Modal Video Transformer for Compressed Video Action\n  Recognition",
    "summary": "This paper presents a pure transformer-based approach, dubbed the Multi-Modal\nVideo Transformer (MM-ViT), for video action recognition. Different from other\nschemes which solely utilize the decoded RGB frames, MM-ViT operates\nexclusively in the compressed video domain and exploits all readily available\nmodalities, i.e., I-frames, motion vectors, residuals and audio waveform. In\norder to handle the large number of spatiotemporal tokens extracted from\nmultiple modalities, we develop several scalable model variants which factorize\nself-attention across the space, time and modality dimensions. In addition, to\nfurther explore the rich inter-modal interactions and their effects, we develop\nand compare three distinct cross-modal attention mechanisms that can be\nseamlessly integrated into the transformer building block. Extensive\nexperiments on three public action recognition benchmarks (UCF-101,\nSomething-Something-v2, Kinetics-600) demonstrate that MM-ViT outperforms the\nstate-of-the-art video transformers in both efficiency and accuracy, and\nperforms better or equally well to the state-of-the-art CNN counterparts with\ncomputationally-heavy optical flow.",
    "published": "2021-08-20T18:05:39Z",
    "updated": "2021-11-12T23:40:37Z",
    "authors": [
      "Jiawei Chen",
      "Chiu Man Ho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.08822v1",
    "title": "Siamese Transformer Pyramid Networks for Real-Time UAV Tracking",
    "summary": "Recent object tracking methods depend upon deep networks or convoluted\narchitectures. Most of those trackers can hardly meet real-time processing\nrequirements on mobile platforms with limited computing resources. In this\nwork, we introduce the Siamese Transformer Pyramid Network (SiamTPN), which\ninherits the advantages from both CNN and Transformer architectures.\nSpecifically, we exploit the inherent feature pyramid of a lightweight network\n(ShuffleNetV2) and reinforce it with a Transformer to construct a robust\ntarget-specific appearance model. A centralized architecture with lateral cross\nattention is developed for building augmented high-level feature maps. To avoid\nthe computation and memory intensity while fusing pyramid representations with\nthe Transformer, we further introduce the pooling attention module, which\nsignificantly reduces memory and time complexity while improving the\nrobustness. Comprehensive experiments on both aerial and prevalent tracking\nbenchmarks achieve competitive results while operating at high speed,\ndemonstrating the effectiveness of SiamTPN. Moreover, our fastest variant\ntracker operates over 30 Hz on a single CPU-core and obtaining an AUC score of\n58.1% on the LaSOT dataset. Source codes are available at\nhttps://github.com/RISCNYUAD/SiamTPNTracker",
    "published": "2021-10-17T13:48:31Z",
    "updated": "2021-10-17T13:48:31Z",
    "authors": [
      "Daitao Xing",
      "Nikolaos Evangeliou",
      "Athanasios Tsoukalas",
      "Anthony Tzes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.09408v3",
    "title": "HRFormer: High-Resolution Transformer for Dense Prediction",
    "summary": "We present a High-Resolution Transformer (HRFormer) that learns\nhigh-resolution representations for dense prediction tasks, in contrast to the\noriginal Vision Transformer that produces low-resolution representations and\nhas high memory and computational cost. We take advantage of the\nmulti-resolution parallel design introduced in high-resolution convolutional\nnetworks (HRNet), along with local-window self-attention that performs\nself-attention over small non-overlapping image windows, for improving the\nmemory and computation efficiency. In addition, we introduce a convolution into\nthe FFN to exchange information across the disconnected image windows. We\ndemonstrate the effectiveness of the High-Resolution Transformer on both human\npose estimation and semantic segmentation tasks, e.g., HRFormer outperforms\nSwin transformer by $1.3$ AP on COCO pose estimation with $50\\%$ fewer\nparameters and $30\\%$ fewer FLOPs. Code is available at:\nhttps://github.com/HRNet/HRFormer.",
    "published": "2021-10-18T15:37:58Z",
    "updated": "2021-11-07T14:39:41Z",
    "authors": [
      "Yuhui Yuan",
      "Rao Fu",
      "Lang Huang",
      "Weihong Lin",
      "Chao Zhang",
      "Xilin Chen",
      "Jingdong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.02841v2",
    "title": "GETAM: Gradient-weighted Element-wise Transformer Attention Map for\n  Weakly-supervised Semantic segmentation",
    "summary": "Weakly Supervised Semantic Segmentation (WSSS) is challenging, particularly\nwhen image-level labels are used to supervise pixel level prediction. To bridge\ntheir gap, a Class Activation Map (CAM) is usually generated to provide pixel\nlevel pseudo labels. CAMs in Convolutional Neural Networks suffer from partial\nactivation ie, only the most discriminative regions are activated. Transformer\nbased methods, on the other hand, are highly effective at exploring global\ncontext with long range dependency modeling, potentially alleviating the\n\"partial activation\" issue. In this paper, we propose the first transformer\nbased WSSS approach, and introduce the Gradient weighted Element wise\nTransformer Attention Map (GETAM). GETAM shows fine scale activation for all\nfeature map elements, revealing different parts of the object across\ntransformer layers. Further, we propose an activation aware label completion\nmodule to generate high quality pseudo labels. Finally, we incorporate our\nmethods into an end to end framework for WSSS using double backward\npropagation. Extensive experiments on PASCAL VOC and COCO demonstrate that our\nresults beat the state-of-the-art end-to-end approaches by a significant\nmargin, and outperform most multi-stage methods.m most multi-stage methods.",
    "published": "2021-12-06T08:02:32Z",
    "updated": "2022-05-10T12:13:22Z",
    "authors": [
      "Weixuan Sun",
      "Jing Zhang",
      "Zheyuan Liu",
      "Yiran Zhong",
      "Nick Barnes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.05291v1",
    "title": "LCTR: On Awakening the Local Continuity of Transformer for Weakly\n  Supervised Object Localization",
    "summary": "Weakly supervised object localization (WSOL) aims to learn object localizer\nsolely by using image-level labels. The convolution neural network (CNN) based\ntechniques often result in highlighting the most discriminative part of objects\nwhile ignoring the entire object extent. Recently, the transformer architecture\nhas been deployed to WSOL to capture the long-range feature dependencies with\nself-attention mechanism and multilayer perceptron structure. Nevertheless,\ntransformers lack the locality inductive bias inherent to CNNs and therefore\nmay deteriorate local feature details in WSOL. In this paper, we propose a\nnovel framework built upon the transformer, termed LCTR (Local Continuity\nTRansformer), which targets at enhancing the local perception capability of\nglobal features among long-range feature dependencies. To this end, we propose\na relational patch-attention module (RPAM), which considers cross-patch\ninformation on a global basis. We further design a cue digging module (CDM),\nwhich utilizes local features to guide the learning trend of the model for\nhighlighting the weak local responses. Finally, comprehensive experiments are\ncarried out on two widely used datasets, ie, CUB-200-2011 and ILSVRC, to verify\nthe effectiveness of our method.",
    "published": "2021-12-10T01:48:40Z",
    "updated": "2021-12-10T01:48:40Z",
    "authors": [
      "Zhiwei Chen",
      "Changan Wang",
      "Yabiao Wang",
      "Guannan Jiang",
      "Yunhang Shen",
      "Ying Tai",
      "Chengjie Wang",
      "Wei Zhang",
      "Liujuan Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.09426v1",
    "title": "SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-Trained\n  Siamese Transformers",
    "summary": "We propose a novel zero-shot multi-frame image restoration method for\nremoving unwanted obstruction elements (such as rains, snow, and moire\npatterns) that vary in successive frames. It has three stages: transformer\npre-training, zero-shot restoration, and hard patch refinement. Using the\npre-trained transformers, our model is able to tell the motion difference\nbetween the true image information and the obstructing elements. For zero-shot\nimage restoration, we design a novel model, termed SiamTrans, which is\nconstructed by Siamese transformers, encoders, and decoders. Each transformer\nhas a temporal attention layer and several self-attention layers, to capture\nboth temporal and spatial information of multiple frames. Only pre-trained\n(self-supervised) on the denoising task, SiamTrans is tested on three different\nlow-level vision tasks (deraining, demoireing, and desnowing). Compared with\nrelated methods, ours achieves the best performances, even outperforming those\nwith supervised learning.",
    "published": "2021-12-17T10:42:39Z",
    "updated": "2021-12-17T10:42:39Z",
    "authors": [
      "Lin Liu",
      "Shanxin Yuan",
      "Jianzhuang Liu",
      "Xin Guo",
      "Youliang Yan",
      "Qi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.13085v1",
    "title": "SimViT: Exploring a Simple Vision Transformer with sliding windows",
    "summary": "Although vision Transformers have achieved excellent performance as backbone\nmodels in many vision tasks, most of them intend to capture global relations of\nall tokens in an image or a window, which disrupts the inherent spatial and\nlocal correlations between patches in 2D structure. In this paper, we introduce\na simple vision Transformer named SimViT, to incorporate spatial structure and\nlocal information into the vision Transformers. Specifically, we introduce\nMulti-head Central Self-Attention(MCSA) instead of conventional Multi-head\nSelf-Attention to capture highly local relations. The introduction of sliding\nwindows facilitates the capture of spatial structure. Meanwhile, SimViT\nextracts multi-scale hierarchical features from different layers for dense\nprediction tasks. Extensive experiments show the SimViT is effective and\nefficient as a general-purpose backbone model for various image processing\ntasks. Especially, our SimViT-Micro only needs 3.3M parameters to achieve 71.1%\ntop-1 accuracy on ImageNet-1k dataset, which is the smallest size vision\nTransformer model by now. Our code will be available in\nhttps://github.com/ucasligang/SimViT.",
    "published": "2021-12-24T15:18:20Z",
    "updated": "2021-12-24T15:18:20Z",
    "authors": [
      "Gang Li",
      "Di Xu",
      "Xing Cheng",
      "Lingyu Si",
      "Changwen Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.07682v3",
    "title": "Enriched CNN-Transformer Feature Aggregation Networks for\n  Super-Resolution",
    "summary": "Recent transformer-based super-resolution (SR) methods have achieved\npromising results against conventional CNN-based methods. However, these\napproaches suffer from essential shortsightedness created by only utilizing the\nstandard self-attention-based reasoning. In this paper, we introduce an\neffective hybrid SR network to aggregate enriched features, including local\nfeatures from CNNs and long-range multi-scale dependencies captured by\ntransformers. Specifically, our network comprises transformer and convolutional\nbranches, which synergetically complement each representation during the\nrestoration procedure. Furthermore, we propose a cross-scale token attention\nmodule, allowing the transformer branch to exploit the informative\nrelationships among tokens across different scales efficiently. Our proposed\nmethod achieves state-of-the-art SR results on numerous benchmark datasets.",
    "published": "2022-03-15T06:52:25Z",
    "updated": "2022-10-20T06:29:16Z",
    "authors": [
      "Jinsu Yoo",
      "Taehoon Kim",
      "Sihaeng Lee",
      "Seung Hwan Kim",
      "Honglak Lee",
      "Tae Hyun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.14557v3",
    "title": "Visual Mechanisms Inspired Efficient Transformers for Image and Video\n  Quality Assessment",
    "summary": "Visual (image, video) quality assessments can be modelled by visual features\nin different domains, e.g., spatial, frequency, and temporal domains.\nPerceptual mechanisms in the human visual system (HVS) play a crucial role in\ngeneration of quality perception. This paper proposes a general framework for\nno-reference visual quality assessment using efficient windowed transformer\narchitectures. A lightweight module for multi-stage channel attention is\nintegrated into Swin (shifted window) Transformer. Such module can represent\nappropriate perceptual mechanisms in image quality assessment (IQA) to build an\naccurate IQA model. Meanwhile, representative features for image quality\nperception in the spatial and frequency domains can also be derived from the\nIQA model, which are then fed into another windowed transformer architecture\nfor video quality assessment (VQA). The VQA model efficiently reuses attention\ninformation across local windows to tackle the issue of expensive time and\nmemory complexities of original transformer. Experimental results on both\nlarge-scale IQA and VQA databases demonstrate that the proposed quality\nassessment models outperform other state-of-the-art models by large margins.\nThe complete source code will be published on Github.",
    "published": "2022-03-28T07:55:11Z",
    "updated": "2022-08-19T19:53:06Z",
    "authors": [
      "Junyong You",
      "Zheng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.15270v3",
    "title": "MAT: Mask-Aware Transformer for Large Hole Image Inpainting",
    "summary": "Recent studies have shown the importance of modeling long-range interactions\nin the inpainting problem. To achieve this goal, existing approaches exploit\neither standalone attention techniques or transformers, but usually under a low\nresolution in consideration of computational cost. In this paper, we present a\nnovel transformer-based model for large hole inpainting, which unifies the\nmerits of transformers and convolutions to efficiently process high-resolution\nimages. We carefully design each component of our framework to guarantee the\nhigh fidelity and diversity of recovered images. Specifically, we customize an\ninpainting-oriented transformer block, where the attention module aggregates\nnon-local information only from partial valid tokens, indicated by a dynamic\nmask. Extensive experiments demonstrate the state-of-the-art performance of the\nnew model on multiple benchmark datasets. Code is released at\nhttps://github.com/fenglinglwb/MAT.",
    "published": "2022-03-29T06:36:17Z",
    "updated": "2022-06-27T03:28:26Z",
    "authors": [
      "Wenbo Li",
      "Zhe Lin",
      "Kun Zhou",
      "Lu Qi",
      "Yi Wang",
      "Jiaya Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.15836v1",
    "title": "VPTR: Efficient Transformers for Video Prediction",
    "summary": "In this paper, we propose a new Transformer block for video future frames\nprediction based on an efficient local spatial-temporal separation attention\nmechanism. Based on this new Transformer block, a fully autoregressive video\nfuture frames prediction Transformer is proposed. In addition, a\nnon-autoregressive video prediction Transformer is also proposed to increase\nthe inference speed and reduce the accumulated inference errors of its\nautoregressive counterpart. In order to avoid the prediction of very similar\nfuture frames, a contrastive feature loss is applied to maximize the mutual\ninformation between predicted and ground-truth future frame features. This work\nis the first that makes a formal comparison of the two types of attention-based\nvideo future frames prediction models over different scenarios. The proposed\nmodels reach a performance competitive with more complex state-of-the-art\nmodels. The source code is available at \\emph{https://github.com/XiYe20/VPTR}.",
    "published": "2022-03-29T18:09:09Z",
    "updated": "2022-03-29T18:09:09Z",
    "authors": [
      "Xi Ye",
      "Guillaume-Alexandre Bilodeau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.08303v1",
    "title": "MulT: An End-to-End Multitask Learning Transformer",
    "summary": "We propose an end-to-end Multitask Learning Transformer framework, named\nMulT, to simultaneously learn multiple high-level vision tasks, including depth\nestimation, semantic segmentation, reshading, surface normal estimation, 2D\nkeypoint detection, and edge detection. Based on the Swin transformer model,\nour framework encodes the input image into a shared representation and makes\npredictions for each vision task using task-specific transformer-based decoder\nheads. At the heart of our approach is a shared attention mechanism modeling\nthe dependencies across the tasks. We evaluate our model on several multitask\nbenchmarks, showing that our MulT framework outperforms both the state-of-the\nart multitask convolutional neural network models and all the respective single\ntask transformer models. Our experiments further highlight the benefits of\nsharing attention across all the tasks, and demonstrate that our MulT model is\nrobust and generalizes well to new domains. Our project website is at\nhttps://ivrl.github.io/MulT/.",
    "published": "2022-05-17T13:03:18Z",
    "updated": "2022-05-17T13:03:18Z",
    "authors": [
      "Deblina Bhattacharjee",
      "Tong Zhang",
      "Sabine SÃ¼sstrunk",
      "Mathieu Salzmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.14014v5",
    "title": "What Dense Graph Do You Need for Self-Attention?",
    "summary": "Transformers have made progress in miscellaneous tasks, but suffer from\nquadratic computational and memory complexities. Recent works propose sparse\nTransformers with attention on sparse graphs to reduce complexity and remain\nstrong performance. While effective, the crucial parts of how dense a graph\nneeds to be to perform well are not fully explored. In this paper, we propose\nNormalized Information Payload (NIP), a graph scoring function measuring\ninformation transfer on graph, which provides an analysis tool for trade-offs\nbetween performance and complexity. Guided by this theoretical analysis, we\npresent Hypercube Transformer, a sparse Transformer that models token\ninteractions in a hypercube and shows comparable or even better results with\nvanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length\n$N$. Experiments on tasks requiring various sequence lengths lay validation for\nour graph function well.",
    "published": "2022-05-27T14:36:55Z",
    "updated": "2022-10-12T15:39:38Z",
    "authors": [
      "Yuxin Wang",
      "Chu-Tak Lee",
      "Qipeng Guo",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.01076v1",
    "title": "Divert More Attention to Vision-Language Tracking",
    "summary": "Relying on Transformer for complex visual feature learning, object tracking\nhas witnessed the new standard for state-of-the-arts (SOTAs). However, this\nadvancement accompanies by larger training data and longer training period,\nmaking tracking increasingly expensive. In this paper, we demonstrate that the\nTransformer-reliance is not necessary and the pure ConvNets are still\ncompetitive and even better yet more economical and friendly in achieving SOTA\ntracking. Our solution is to unleash the power of multimodal vision-language\n(VL) tracking, simply using ConvNets. The essence lies in learning novel\nunified-adaptive VL representations with our modality mixer (ModaMixer) and\nasymmetrical ConvNet search. We show that our unified-adaptive VL\nrepresentation, learned purely with the ConvNets, is a simple yet strong\nalternative to Transformer visual features, by unbelievably improving a\nCNN-based Siamese tracker by 14.5% in SUC on challenging LaSOT (50.7% > 65.2%),\neven outperforming several Transformer-based SOTA trackers. Besides empirical\nresults, we theoretically analyze our approach to evidence its effectiveness.\nBy revealing the potential of VL representation, we expect the community to\ndivert more attention to VL tracking and hope to open more possibilities for\nfuture tracking beyond Transformer. Code and models will be released at\nhttps://github.com/JudasDie/SOTS.",
    "published": "2022-07-03T16:38:24Z",
    "updated": "2022-07-03T16:38:24Z",
    "authors": [
      "Mingzhe Guo",
      "Zhipeng Zhang",
      "Heng Fan",
      "Liping Jing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.09236v1",
    "title": "Real-time Online Video Detection with Temporal Smoothing Transformers",
    "summary": "Streaming video recognition reasons about objects and their actions in every\nframe of a video. A good streaming recognition model captures both long-term\ndynamics and short-term changes of video. Unfortunately, in most existing\nmethods, the computational complexity grows linearly or quadratically with the\nlength of the considered dynamics. This issue is particularly pronounced in\ntransformer-based architectures. To address this issue, we reformulate the\ncross-attention in a video transformer through the lens of kernel and apply two\nkinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The\nresulting streaming attention reuses much of the computation from frame to\nframe, and only requires a constant time update each frame. Based on this idea,\nwe build TeSTra, a Temporal Smoothing Transformer, that takes in arbitrarily\nlong inputs with constant caching and computing overhead. Specifically, it runs\n$6\\times$ faster than equivalent sliding-window based transformers with 2,048\nframes in a streaming setting. Furthermore, thanks to the increased temporal\nspan, TeSTra achieves state-of-the-art results on THUMOS'14 and\nEPIC-Kitchen-100, two standard online action detection and action anticipation\ndatasets. A real-time version of TeSTra outperforms all but one prior\napproaches on the THUMOS'14 dataset.",
    "published": "2022-09-19T17:59:02Z",
    "updated": "2022-09-19T17:59:02Z",
    "authors": [
      "Yue Zhao",
      "Philipp KrÃ¤henbÃ¼hl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.09545v1",
    "title": "Graph Reasoning Transformer for Image Parsing",
    "summary": "Capturing the long-range dependencies has empirically proven to be effective\non a wide range of computer vision tasks. The progressive advances on this\ntopic have been made through the employment of the transformer framework with\nthe help of the multi-head attention mechanism. However, the attention-based\nimage patch interaction potentially suffers from problems of redundant\ninteractions of intra-class patches and unoriented interactions of inter-class\npatches. In this paper, we propose a novel Graph Reasoning Transformer (GReaT)\nfor image parsing to enable image patches to interact following a relation\nreasoning pattern. Specifically, the linearly embedded image patches are first\nprojected into the graph space, where each node represents the implicit visual\ncenter for a cluster of image patches and each edge reflects the relation\nweight between two adjacent nodes. After that, global relation reasoning is\nperformed on this graph accordingly. Finally, all nodes including the relation\ninformation are mapped back into the original space for subsequent processes.\nCompared to the conventional transformer, GReaT has higher interaction\nefficiency and a more purposeful interaction pattern. Experiments are carried\nout on the challenging Cityscapes and ADE20K datasets. Results show that GReaT\nachieves consistent performance gains with slight computational overheads on\nthe state-of-the-art transformer baselines.",
    "published": "2022-09-20T08:21:37Z",
    "updated": "2022-09-20T08:21:37Z",
    "authors": [
      "Dong Zhang",
      "Jinhui Tang",
      "Kwang-Ting Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.03029v3",
    "title": "AbHE: All Attention-based Homography Estimation",
    "summary": "Homography estimation is a basic computer vision task, which aims to obtain\nthe transformation from multi-view images for image alignment. Unsupervised\nlearning homography estimation trains a convolution neural network for feature\nextraction and transformation matrix regression. While the state-of-theart\nhomography method is based on convolution neural networks, few work focuses on\ntransformer which shows superiority in highlevel vision tasks. In this paper,\nwe propose a strong-baseline model based on the Swin Transformer, which\ncombines convolution neural network for local features and transformer module\nfor global features. Moreover, a cross non-local layer is introduced to search\nthe matched features within the feature maps coarsely. In the homography\nregression stage, we adopt an attention layer for the channels of correlation\nvolume, which can drop out some weak correlation feature points. The experiment\nshows that in 8 Degree-of-Freedoms(DOFs) homography estimation our method\noverperforms the state-of-the-art method.",
    "published": "2022-12-06T15:00:00Z",
    "updated": "2023-02-05T18:41:36Z",
    "authors": [
      "Mingxiao Huo",
      "Zhihao Zhang",
      "Xinyang Ren",
      "Xianqiang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.06011v1",
    "title": "A Neural ODE Interpretation of Transformer Layers",
    "summary": "Transformer layers, which use an alternating pattern of multi-head attention\nand multi-layer perceptron (MLP) layers, provide an effective tool for a\nvariety of machine learning problems. As the transformer layers use residual\nconnections to avoid the problem of vanishing gradients, they can be viewed as\nthe numerical integration of a differential equation. In this extended\nabstract, we build upon this connection and propose a modification of the\ninternal architecture of a transformer layer. The proposed model places the\nmulti-head attention sublayer and the MLP sublayer parallel to each other. Our\nexperiments show that this simple modification improves the performance of\ntransformer networks in multiple tasks. Moreover, for the image classification\ntask, we show that using neural ODE solvers with a sophisticated integration\nscheme further improves performance.",
    "published": "2022-12-12T16:18:58Z",
    "updated": "2022-12-12T16:18:58Z",
    "authors": [
      "Yaofeng Desmond Zhong",
      "Tongtao Zhang",
      "Amit Chakraborty",
      "Biswadip Dey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.09416v1",
    "title": "Towards Robust Video Instance Segmentation with Temporal-Aware\n  Transformer",
    "summary": "Most existing transformer based video instance segmentation methods extract\nper frame features independently, hence it is challenging to solve the\nappearance deformation problem. In this paper, we observe the temporal\ninformation is important as well and we propose TAFormer to aggregate\nspatio-temporal features both in transformer encoder and decoder. Specifically,\nin transformer encoder, we propose a novel spatio-temporal joint multi-scale\ndeformable attention module which dynamically integrates the spatial and\ntemporal information to obtain enriched spatio-temporal features. In\ntransformer decoder, we introduce a temporal self-attention module to enhance\nthe frame level box queries with the temporal relation. Moreover, TAFormer\nadopts an instance level contrastive loss to increase the discriminability of\ninstance query embeddings. Therefore the tracking error caused by visually\nsimilar instances can be decreased. Experimental results show that TAFormer\neffectively leverages the spatial and temporal information to obtain\ncontext-aware feature representation and outperforms state-of-the-art methods.",
    "published": "2023-01-20T05:22:16Z",
    "updated": "2023-01-20T05:22:16Z",
    "authors": [
      "Zhenghao Zhang",
      "Fangtao Shao",
      "Zuozhuo Dai",
      "Siyu Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.08052v1",
    "title": "Hierarchical Cross-modal Transformer for RGB-D Salient Object Detection",
    "summary": "Most of existing RGB-D salient object detection (SOD) methods follow the\nCNN-based paradigm, which is unable to model long-range dependencies across\nspace and modalities due to the natural locality of CNNs. Here we propose the\nHierarchical Cross-modal Transformer (HCT), a new multi-modal transformer, to\ntackle this problem. Unlike previous multi-modal transformers that directly\nconnecting all patches from two modalities, we explore the cross-modal\ncomplementarity hierarchically to respect the modality gap and spatial\ndiscrepancy in unaligned regions. Specifically, we propose to use intra-modal\nself-attention to explore complementary global contexts, and measure\nspatial-aligned inter-modal attention locally to capture cross-modal\ncorrelations. In addition, we present a Feature Pyramid module for Transformer\n(FPT) to boost informative cross-scale integration as well as a\nconsistency-complementarity module to disentangle the multi-modal integration\npath and improve the fusion adaptivity. Comprehensive experiments on a large\nvariety of public datasets verify the efficacy of our designs and the\nconsistent improvement over state-of-the-art models.",
    "published": "2023-02-16T03:23:23Z",
    "updated": "2023-02-16T03:23:23Z",
    "authors": [
      "Hao Chen",
      "Feihong Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.13004v1",
    "title": "TBFormer: Two-Branch Transformer for Image Forgery Localization",
    "summary": "Image forgery localization aims to identify forged regions by capturing\nsubtle traces from high-quality discriminative features. In this paper, we\npropose a Transformer-style network with two feature extraction branches for\nimage forgery localization, and it is named as Two-Branch Transformer\n(TBFormer). Firstly, two feature extraction branches are elaborately designed,\ntaking advantage of the discriminative stacked Transformer layers, for both RGB\nand noise domain features. Secondly, an Attention-aware Hierarchical-feature\nFusion Module (AHFM) is proposed to effectively fuse hierarchical features from\ntwo different domains. Although the two feature extraction branches have the\nsame architecture, their features have significant differences since they are\nextracted from different domains. We adopt position attention to embed them\ninto a unified feature domain for hierarchical feature investigation. Finally,\na Transformer decoder is constructed for feature reconstruction to generate the\npredicted mask. Extensive experiments on publicly available datasets\ndemonstrate the effectiveness of the proposed model.",
    "published": "2023-02-25T06:45:01Z",
    "updated": "2023-02-25T06:45:01Z",
    "authors": [
      "Yaqi Liu",
      "Binbin Lv",
      "Xin Jin",
      "Xiaoyu Chen",
      "Xiaokun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.08809v1",
    "title": "SViTT: Temporal Learning of Sparse Video-Text Transformers",
    "summary": "Do video-text transformers learn to model temporal relationships across\nframes? Despite their immense capacity and the abundance of multimodal training\ndata, recent work has revealed the strong tendency of video-text models towards\nframe-based spatial representations, while temporal reasoning remains largely\nunsolved. In this work, we identify several key challenges in temporal learning\nof video-text transformers: the spatiotemporal trade-off from limited network\nsize; the curse of dimensionality for multi-frame modeling; and the diminishing\nreturns of semantic information by extending clip length. Guided by these\nfindings, we propose SViTT, a sparse video-text architecture that performs\nmulti-frame reasoning with significantly lower cost than naive transformers\nwith dense attention. Analogous to graph-based networks, SViTT employs two\nforms of sparsity: edge sparsity that limits the query-key communications\nbetween tokens in self-attention, and node sparsity that discards uninformative\nvisual tokens. Trained with a curriculum which increases model sparsity with\nthe clip length, SViTT outperforms dense transformer baselines on multiple\nvideo-text retrieval and question answering benchmarks, with a fraction of\ncomputational cost. Project page: http://svcl.ucsd.edu/projects/svitt.",
    "published": "2023-04-18T08:17:58Z",
    "updated": "2023-04-18T08:17:58Z",
    "authors": [
      "Yi Li",
      "Kyle Min",
      "Subarna Tripathi",
      "Nuno Vasconcelos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.14919v1",
    "title": "ScatterFormer: Locally-Invariant Scattering Transformer for\n  Patient-Independent Multispectral Detection of Epileptiform Discharges",
    "summary": "Patient-independent detection of epileptic activities based on visual\nspectral representation of continuous EEG (cEEG) has been widely used for\ndiagnosing epilepsy. However, precise detection remains a considerable\nchallenge due to subtle variabilities across subjects, channels and time\npoints. Thus, capturing fine-grained, discriminative features of EEG patterns,\nwhich is associated with high-frequency textural information, is yet to be\nresolved. In this work, we propose Scattering Transformer (ScatterFormer), an\ninvariant scattering transform-based hierarchical Transformer that specifically\npays attention to subtle features. In particular, the disentangled\nfrequency-aware attention (FAA) enables the Transformer to capture clinically\ninformative high-frequency components, offering a novel clinical explainability\nbased on visual encoding of multichannel EEG signals. Evaluations on two\ndistinct tasks of epileptiform detection demonstrate the effectiveness our\nmethod. Our proposed model achieves median AUCROC and accuracy of 98.14%,\n96.39% in patients with Rolandic epilepsy. On a neonatal seizure detection\nbenchmark, it outperforms the state-of-the-art by 9% in terms of average\nAUCROC.",
    "published": "2023-04-26T10:10:58Z",
    "updated": "2023-04-26T10:10:58Z",
    "authors": [
      "Ruizhe Zheng",
      "Jun Li",
      "Yi Wang",
      "Tian Luo",
      "Yuguo Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.00397v1",
    "title": "TransCAR: Transformer-based Camera-And-Radar Fusion for 3D Object\n  Detection",
    "summary": "Despite radar's popularity in the automotive industry, for fusion-based 3D\nobject detection, most existing works focus on LiDAR and camera fusion. In this\npaper, we propose TransCAR, a Transformer-based Camera-And-Radar fusion\nsolution for 3D object detection. Our TransCAR consists of two modules. The\nfirst module learns 2D features from surround-view camera images and then uses\na sparse set of 3D object queries to index into these 2D features. The\nvision-updated queries then interact with each other via transformer\nself-attention layer. The second module learns radar features from multiple\nradar scans and then applies transformer decoder to learn the interactions\nbetween radar features and vision-updated queries. The cross-attention layer\nwithin the transformer decoder can adaptively learn the soft-association\nbetween the radar features and vision-updated queries instead of\nhard-association based on sensor calibration only. Finally, our model estimates\na bounding box per query using set-to-set Hungarian loss, which enables the\nmethod to avoid non-maximum suppression. TransCAR improves the velocity\nestimation using the radar scans without temporal information. The superior\nexperimental results of our TransCAR on the challenging nuScenes datasets\nillustrate that our TransCAR outperforms state-of-the-art Camera-Radar\nfusion-based 3D object detection approaches.",
    "published": "2023-04-30T05:35:03Z",
    "updated": "2023-04-30T05:35:03Z",
    "authors": [
      "Su Pang",
      "Daniel Morris",
      "Hayder Radha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.13571v1",
    "title": "Latent Positional Information is in the Self-Attention Variance of\n  Transformer Language Models Without Positional Embeddings",
    "summary": "The use of positional embeddings in transformer language models is widely\naccepted. However, recent research has called into question the necessity of\nsuch embeddings. We further extend this inquiry by demonstrating that a\nrandomly initialized and frozen transformer language model, devoid of\npositional embeddings, inherently encodes strong positional information through\nthe shrinkage of self-attention variance. To quantify this variance, we derive\nthe underlying distribution of each step within a transformer layer. Through\nempirical validation using a fully pretrained model, we show that the variance\nshrinkage effect still persists after extensive gradient updates. Our findings\nserve to justify the decision to discard positional embeddings and thus\nfacilitate more efficient pretraining of transformer language models.",
    "published": "2023-05-23T01:03:40Z",
    "updated": "2023-05-23T01:03:40Z",
    "authors": [
      "Ta-Chung Chi",
      "Ting-Han Fan",
      "Li-Wei Chen",
      "Alexander I. Rudnicky",
      "Peter J. Ramadge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.17863v2",
    "title": "GridFormer: Residual Dense Transformer with Grid Structure for Image\n  Restoration in Adverse Weather Conditions",
    "summary": "Image restoration in adverse weather conditions is a difficult task in\ncomputer vision. In this paper, we propose a novel transformer-based framework\ncalled GridFormer which serves as a backbone for image restoration under\nadverse weather conditions. GridFormer is designed in a grid structure using a\nresidual dense transformer block, and it introduces two core designs. First, it\nuses an enhanced attention mechanism in the transformer layer. The mechanism\nincludes stages of the sampler and compact self-attention to improve\nefficiency, and a local enhancement stage to strengthen local information.\nSecond, we introduce a residual dense transformer block (RDTB) as the final\nGridFormer layer. This design further improves the network's ability to learn\neffective features from both preceding and current local features. The\nGridFormer framework achieves state-of-the-art results on five diverse image\nrestoration tasks in adverse weather conditions, including image deraining,\ndehazing, deraining \\& dehazing, desnowing, and multi-weather restoration. The\nsource code and pre-trained models are available at\nhttps://github.com/TaoWangzj/GridFormer.",
    "published": "2023-05-29T03:03:53Z",
    "updated": "2024-06-21T07:46:10Z",
    "authors": [
      "Tao Wang",
      "Kaihao Zhang",
      "Ziqian Shao",
      "Wenhan Luo",
      "Bjorn Stenger",
      "Tong Lu",
      "Tae-Kyun Kim",
      "Wei Liu",
      "Hongdong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.19370v3",
    "title": "Blockwise Parallel Transformer for Large Context Models",
    "summary": "Transformers have emerged as the cornerstone of state-of-the-art natural\nlanguage processing models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands posed by the\nself-attention mechanism and the large feedforward network in Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving multiple long sequences or long-term dependencies. We present a\ndistinct approach, Blockwise Parallel Transformer (BPT), that leverages\nblockwise computation of self-attention and feedforward network fusion to\nminimize memory costs. By processing longer input sequences while maintaining\nmemory efficiency, BPT enables training sequences 32 times longer than vanilla\nTransformers and up to 4 times longer than previous memory-efficient methods.\nExtensive experiments on language modeling and reinforcement learning tasks\ndemonstrate the effectiveness of BPT in reducing memory requirements and\nimproving performance.",
    "published": "2023-05-30T19:25:51Z",
    "updated": "2023-08-28T20:13:33Z",
    "authors": [
      "Hao Liu",
      "Pieter Abbeel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.02531v1",
    "title": "Choir Transformer: Generating Polyphonic Music with Relative Attention\n  on Transformer",
    "summary": "Polyphonic music generation is still a challenge direction due to its correct\nbetween generating melody and harmony. Most of the previous studies used\nRNN-based models. However, the RNN-based models are hard to establish the\nrelationship between long-distance notes. In this paper, we propose a\npolyphonic music generation neural network named Choir Transformer[\nhttps://github.com/Zjy0401/choir-transformer], with relative positional\nattention to better model the structure of music. We also proposed a music\nrepresentation suitable for polyphonic music generation. The performance of\nChoir Transformer surpasses the previous state-of-the-art accuracy of 4.06%. We\nalso measures the harmony metrics of polyphonic music. Experiments show that\nthe harmony metrics are close to the music of Bach. In practical application,\nthe generated melody and rhythm can be adjusted according to the specified\ninput, with different styles of music like folk music or pop music and so on.",
    "published": "2023-08-01T06:44:15Z",
    "updated": "2023-08-01T06:44:15Z",
    "authors": [
      "Jiuyang Zhou",
      "Hong Zhu",
      "Xingping Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.12372v1",
    "title": "Vision Transformer Adapters for Generalizable Multitask Learning",
    "summary": "We introduce the first multitasking vision transformer adapters that learn\ngeneralizable task affinities which can be applied to novel tasks and domains.\nIntegrated into an off-the-shelf vision transformer backbone, our adapters can\nsimultaneously solve multiple dense vision tasks in a parameter-efficient\nmanner, unlike existing multitasking transformers that are parametrically\nexpensive. In contrast to concurrent methods, we do not require retraining or\nfine-tuning whenever a new task or domain is added. We introduce a task-adapted\nattention mechanism within our adapter framework that combines gradient-based\ntask similarities with attention-based ones. The learned task affinities\ngeneralize to the following settings: zero-shot task transfer, unsupervised\ndomain adaptation, and generalization without fine-tuning to novel domains. We\ndemonstrate that our approach outperforms not only the existing convolutional\nneural network-based multitasking methods but also the vision transformer-based\nones. Our project page is at \\url{https://ivrl.github.io/VTAGML}.",
    "published": "2023-08-23T18:40:48Z",
    "updated": "2023-08-23T18:40:48Z",
    "authors": [
      "Deblina Bhattacharjee",
      "Sabine SÃ¼sstrunk",
      "Mathieu Salzmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.09611v1",
    "title": "Collaborative Three-Stream Transformers for Video Captioning",
    "summary": "As the most critical components in a sentence, subject, predicate and object\nrequire special attention in the video captioning task. To implement this idea,\nwe design a novel framework, named COllaborative three-Stream Transformers\n(COST), to model the three parts separately and complement each other for\nbetter representation. Specifically, COST is formed by three branches of\ntransformers to exploit the visual-linguistic interactions of different\ngranularities in spatial-temporal domain between videos and text, detected\nobjects and text, and actions and text. Meanwhile, we propose a\ncross-granularity attention module to align the interactions modeled by the\nthree branches of transformers, then the three branches of transformers can\nsupport each other to exploit the most discriminative semantic information of\ndifferent granularities for accurate predictions of captions. The whole model\nis trained in an end-to-end fashion. Extensive experiments conducted on three\nlarge-scale challenging datasets, i.e., YouCookII, ActivityNet Captions and\nMSVD, demonstrate that the proposed method performs favorably against the\nstate-of-the-art methods.",
    "published": "2023-09-18T09:33:25Z",
    "updated": "2023-09-18T09:33:25Z",
    "authors": [
      "Hao Wang",
      "Libo Zhang",
      "Heng Fan",
      "Tiejian Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.01704v1",
    "title": "Transformers are efficient hierarchical chemical graph learners",
    "summary": "Transformers, adapted from natural language processing, are emerging as a\nleading approach for graph representation learning. Contemporary graph\ntransformers often treat nodes or edges as separate tokens. This approach leads\nto computational challenges for even moderately-sized graphs due to the\nquadratic scaling of self-attention complexity with token count. In this paper,\nwe introduce SubFormer, a graph transformer that operates on subgraphs that\naggregate information by a message-passing mechanism. This approach reduces the\nnumber of tokens and enhances learning long-range interactions. We demonstrate\nSubFormer on benchmarks for predicting molecular properties from chemical\nstructures and show that it is competitive with state-of-the-art graph\ntransformers at a fraction of the computational cost, with training times on\nthe order of minutes on a consumer-grade graphics card. We interpret the\nattention weights in terms of chemical structures. We show that SubFormer\nexhibits limited over-smoothing and avoids over-squashing, which is prevalent\nin traditional graph neural networks.",
    "published": "2023-10-02T23:57:04Z",
    "updated": "2023-10-02T23:57:04Z",
    "authors": [
      "Zihan Pengmei",
      "Zimu Li",
      "Chih-chan Tien",
      "Risi Kondor",
      "Aaron R. Dinner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.15971v1",
    "title": "Graph Context Transformation Learning for Progressive Correspondence\n  Pruning",
    "summary": "Most of existing correspondence pruning methods only concentrate on gathering\nthe context information as much as possible while neglecting effective ways to\nutilize such information. In order to tackle this dilemma, in this paper we\npropose Graph Context Transformation Network (GCT-Net) enhancing context\ninformation to conduct consensus guidance for progressive correspondence\npruning. Specifically, we design the Graph Context Enhance Transformer which\nfirst generates the graph network and then transforms it into multi-branch\ngraph contexts. Moreover, it employs self-attention and cross-attention to\nmagnify characteristics of each graph context for emphasizing the unique as\nwell as shared essential information. To further apply the recalibrated graph\ncontexts to the global domain, we propose the Graph Context Guidance\nTransformer. This module adopts a confident-based sampling strategy to\ntemporarily screen high-confidence vertices for guiding accurate classification\nby searching global consensus between screened vertices and remaining ones. The\nextensive experimental results on outlier removal and relative pose estimation\nclearly demonstrate the superior performance of GCT-Net compared to\nstate-of-the-art methods across outdoor and indoor datasets. The source code\nwill be available at: https://github.com/guobaoxiao/GCT-Net/.",
    "published": "2023-12-26T09:43:30Z",
    "updated": "2023-12-26T09:43:30Z",
    "authors": [
      "Junwen Guo",
      "Guobao Xiao",
      "Shiping Wang",
      "Jun Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.17791v3",
    "title": "Graph Transformers without Positional Encodings",
    "summary": "Recently, Transformers for graph representation learning have become\nincreasingly popular, achieving state-of-the-art performance on a wide-variety\nof graph datasets, either alone or in combination with message-passing graph\nneural networks (MP-GNNs). Infusing graph inductive-biases in the innately\nstructure-agnostic transformer architecture in the form of structural or\npositional encodings (PEs) is key to achieving these impressive results.\nHowever, designing such encodings is tricky and disparate attempts have been\nmade to engineer such encodings including Laplacian eigenvectors, relative\nrandom-walk probabilities (RRWP), spatial encodings, centrality encodings, edge\nencodings etc. In this work, we argue that such encodings may not be required\nat all, provided the attention mechanism itself incorporates information about\nthe graph structure. We introduce Eigenformer, a Graph Transformer employing a\nnovel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of\nthe graph, and empirically show that it achieves performance competetive with\nSOTA Graph Transformers on a number of standard GNN benchmarks. Additionally,\nwe theoretically prove that Eigenformer can express various graph structural\nconnectivity matrices, which is particularly essential when learning over\nsmaller graphs.",
    "published": "2024-01-31T12:33:31Z",
    "updated": "2024-05-06T13:12:05Z",
    "authors": [
      "Ayush Garg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.05602v2",
    "title": "AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for\n  Transformers",
    "summary": "Large Language Models are prone to biased predictions and hallucinations,\nunderlining the paramount importance of understanding their model-internal\nreasoning process. However, achieving faithful attributions for the entirety of\na black-box transformer model and maintaining computational efficiency is an\nunsolved challenge. By extending the Layer-wise Relevance Propagation\nattribution method to handle attention layers, we address these challenges\neffectively. While partial solutions exist, our method is the first to\nfaithfully and holistically attribute not only input but also latent\nrepresentations of transformer models with the computational efficiency similar\nto a single backward pass. Through extensive evaluations against existing\nmethods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures,\nwe demonstrate that our proposed approach surpasses alternative methods in\nterms of faithfulness and enables the understanding of latent representations,\nopening up the door for concept-based explanations. We provide an LRP library\nat https://github.com/rachtibat/LRP-eXplains-Transformers.",
    "published": "2024-02-08T12:01:24Z",
    "updated": "2024-06-10T09:58:55Z",
    "authors": [
      "Reduan Achtibat",
      "Sayed Mohammad Vakilzadeh Hatefi",
      "Maximilian Dreyer",
      "Aakriti Jain",
      "Thomas Wiegand",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.05969v2",
    "title": "Breaking Symmetry When Training Transformers",
    "summary": "As we show in this paper, the prediction for output token $n+1$ of\nTransformer architectures without one of the mechanisms of positional encodings\nand causal attention is invariant to permutations of input tokens $1, 2, ...,\nn-1$. Usually, both mechanisms are employed and the symmetry with respect to\nthe input tokens is broken. Recently, it has been shown that one can train\nTransformers without positional encodings. This must be enabled by the causal\nattention mechanism. In this paper, we elaborate on the argument that the\ncausal connection mechanism must be responsible for the fact that Transformers\nare able to model input sequences where the order is important. Vertical\n\"slices\" of Transformers are all encouraged to represent the same location $k$\nin the input sequence. We hypothesize that residual connections contribute to\nthis phenomenon, and demonstrate evidence for this.",
    "published": "2024-02-06T00:32:28Z",
    "updated": "2024-06-16T22:18:36Z",
    "authors": [
      "Chunsheng Zuo",
      "Michael Guerzhoy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.10198v3",
    "title": "SAMformer: Unlocking the Potential of Transformers in Time Series\n  Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
    "summary": "Transformer-based architectures achieved breakthrough performance in natural\nlanguage processing and computer vision, yet they remain inferior to simpler\nlinear baselines in multivariate long-term forecasting. To better understand\nthis phenomenon, we start by studying a toy linear forecasting problem for\nwhich we show that transformers are incapable of converging to their true\nsolution despite their high expressive power. We further identify the attention\nof transformers as being responsible for this low generalization capacity.\nBuilding upon this insight, we propose a shallow lightweight transformer model\nthat successfully escapes bad local minima when optimized with sharpness-aware\noptimization. We empirically demonstrate that this result extends to all\ncommonly used real-world multivariate time series datasets. In particular,\nSAMformer surpasses current state-of-the-art methods and is on par with the\nbiggest foundation model MOIRAI while having significantly fewer parameters.\nThe code is available at https://github.com/romilbert/samformer.",
    "published": "2024-02-15T18:55:05Z",
    "updated": "2024-06-03T07:34:37Z",
    "authors": [
      "Romain Ilbert",
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Aladin Virmaux",
      "Giuseppe Paolo",
      "Themis Palpanas",
      "Ievgen Redko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.17966v3",
    "title": "STC-ViT: Spatio Temporal Continuous Vision Transformer for Weather\n  Forecasting",
    "summary": "Operational weather forecasting system relies on computationally expensive\nphysics-based models. Recently, transformer based models have shown remarkable\npotential in weather forecasting achieving state-of-the-art results. However,\ntransformers are discrete and physics-agnostic models which limit their ability\nto learn the continuous spatio-temporal features of the dynamical weather\nsystem. We address this issue with STC-ViT, a Spatio-Temporal Continuous Vision\nTransformer for weather forecasting. STC-ViT incorporates the continuous time\nNeural ODE layers with multi-head attention mechanism to learn the continuous\nweather evolution over time. The attention mechanism is encoded as a\ndifferentiable function in the transformer architecture to model the complex\nweather dynamics. Further, we define a customised physics informed loss for\nSTC-ViT which penalize the model's predictions for deviating away from physical\nlaws. We evaluate STC-ViT against operational Numerical Weather Prediction\n(NWP) model and several deep learning based weather forecasting models.\nSTC-ViT, trained on 1.5-degree 6-hourly data, demonstrates computational\nefficiency and competitive performance compared to state-of-the-art data-driven\nmodels trained on higher-resolution data for global forecasting.",
    "published": "2024-02-28T01:15:30Z",
    "updated": "2024-10-31T00:05:41Z",
    "authors": [
      "Hira Saleem",
      "Flora Salim",
      "Cormac Purcell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.07542v1",
    "title": "A Survey of Vision Transformers in Autonomous Driving: Current Trends\n  and Future Directions",
    "summary": "This survey explores the adaptation of visual transformer models in\nAutonomous Driving, a transition inspired by their success in Natural Language\nProcessing. Surpassing traditional Recurrent Neural Networks in tasks like\nsequential image processing and outperforming Convolutional Neural Networks in\nglobal context capture, as evidenced in complex scene recognition, Transformers\nare gaining traction in computer vision. These capabilities are crucial in\nAutonomous Driving for real-time, dynamic visual scene processing. Our survey\nprovides a comprehensive overview of Vision Transformer applications in\nAutonomous Driving, focusing on foundational concepts such as self-attention,\nmulti-head attention, and encoder-decoder architecture. We cover applications\nin object detection, segmentation, pedestrian detection, lane detection, and\nmore, comparing their architectural merits and limitations. The survey\nconcludes with future research directions, highlighting the growing role of\nVision Transformers in Autonomous Driving.",
    "published": "2024-03-12T11:29:40Z",
    "updated": "2024-03-12T11:29:40Z",
    "authors": [
      "Quoc-Vinh Lai-Dang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.15693v1",
    "title": "Technical Report: Masked Skeleton Sequence Modeling for Learning Larval\n  Zebrafish Behavior Latent Embeddings",
    "summary": "In this report, we introduce a novel self-supervised learning method for\nextracting latent embeddings from behaviors of larval zebrafish. Drawing\ninspiration from Masked Modeling techniquesutilized in image processing with\nMasked Autoencoders (MAE) \\cite{he2022masked} and in natural language\nprocessing with Generative Pre-trained Transformer (GPT)\n\\cite{radford2018improving}, we treat behavior sequences as a blend of images\nand language. For the skeletal sequences of swimming zebrafish, we propose a\npioneering Transformer-CNN architecture, the Sequence Spatial-Temporal\nTransformer (SSTFormer), designed to capture the inter-frame correlation of\ndifferent joints. This correlation is particularly valuable, as it reflects the\ncoordinated movement of various parts of the fish body across adjacent frames.\nTo handle the high frame rate, we segment the skeleton sequence into distinct\ntime slices, analogous to \"words\" in a sentence, and employ self-attention\ntransformer layers to encode the consecutive frames within each slice,\ncapturing the spatial correlation among different joints. Furthermore, we\nincorporate a CNN-based attention module to enhance the representations\noutputted by the transformer layers. Lastly, we introduce a temporal feature\naggregation operation between time slices to improve the discrimination of\nsimilar behaviors.",
    "published": "2024-03-23T02:58:10Z",
    "updated": "2024-03-23T02:58:10Z",
    "authors": [
      "Lanxin Xu",
      "Shuo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.19928v2",
    "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
    "summary": "In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.",
    "published": "2024-03-29T02:32:15Z",
    "updated": "2024-04-01T09:17:01Z",
    "authors": [
      "Hanting Chen",
      "Zhicheng Liu",
      "Xutao Wang",
      "Yuchuan Tian",
      "Yunhe Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.02842v1",
    "title": "IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs",
    "summary": "One limitation of existing Transformer-based models is that they cannot\nhandle very long sequences as input since their self-attention operations\nexhibit quadratic time and space complexity. This problem becomes especially\nacute when Transformers are deployed on hardware platforms equipped only with\nCPUs. To address this issue, we propose a novel method for accelerating\nself-attention at inference time that works with pretrained Transformer models\nout-of-the-box without requiring retraining. We experiment using our method to\naccelerate various long-sequence Transformers, including a leading LLaMA\n2-based LLM, on various benchmarks and demonstrate a greater speedup of 2.73x -\n7.63x while retaining 98.6% - 99.6% of the accuracy of the original pretrained\nmodels. The code is available on our project website at\nhttps://yuzhenmao.github.io/IceFormer/.",
    "published": "2024-05-05T08:18:42Z",
    "updated": "2024-05-05T08:18:42Z",
    "authors": [
      "Yuzhen Mao",
      "Martin Ester",
      "Ke Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.06640v1",
    "title": "Linearizing Large Language Models",
    "summary": "Linear transformers have emerged as a subquadratic-time alternative to\nsoftmax attention and have garnered significant interest due to their\nfixed-size recurrent state that lowers inference cost. However, their original\nformulation suffers from poor scaling and underperforms compute-matched\ntransformers. Recent linear models such as RWKV and Mamba have attempted to\naddress these shortcomings by proposing novel time-mixing and gating\narchitectures, but pre-training large language models requires significant data\nand compute investments. Thus, the search for subquadratic architectures is\nlimited by the availability of compute and quality pre-training datasets. As a\ncost-effective alternative to pre-training linear transformers, we propose\nScalable UPtraining for Recurrent Attention (SUPRA). We present a method to\nuptrain existing large pre-trained transformers into Recurrent Neural Networks\n(RNNs) with a modest compute budget. This allows us to leverage the strong\npre-training data and performance of existing transformer LLMs, while requiring\n5% of the training cost. We find that our linearization technique leads to\ncompetitive performance on standard benchmarks, but we identify persistent\nin-context learning and long-context modeling shortfalls for even the largest\nlinear models. Our code and models can be found at\nhttps://github.com/TRI-ML/linear_open_lm.",
    "published": "2024-05-10T17:59:08Z",
    "updated": "2024-05-10T17:59:08Z",
    "authors": [
      "Jean Mercat",
      "Igor Vasiljevic",
      "Sedrick Keh",
      "Kushal Arora",
      "Achal Dave",
      "Adrien Gaidon",
      "Thomas Kollar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.02486v2",
    "title": "A Temporal Kolmogorov-Arnold Transformer for Time Series Forecasting",
    "summary": "Capturing complex temporal patterns and relationships within multivariate\ndata streams is a difficult task. We propose the Temporal Kolmogorov-Arnold\nTransformer (TKAT), a novel attention-based architecture designed to address\nthis task using Temporal Kolmogorov-Arnold Networks (TKANs). Inspired by the\nTemporal Fusion Transformer (TFT), TKAT emerges as a powerful encoder-decoder\nmodel tailored to handle tasks in which the observed part of the features is\nmore important than the a priori known part. This new architecture combined the\ntheoretical foundation of the Kolmogorov-Arnold representation with the power\nof transformers. TKAT aims to simplify the complex dependencies inherent in\ntime series, making them more \"interpretable\". The use of transformer\narchitecture in this framework allows us to capture long-range dependencies\nthrough self-attention mechanisms.",
    "published": "2024-06-04T16:55:42Z",
    "updated": "2024-06-05T16:32:16Z",
    "authors": [
      "Remi Genet",
      "Hugo Inzirillo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.04589v2",
    "title": "MUSE: Flexible Voiceprint Receptive Fields and Multi-Path Fusion\n  Enhanced Taylor Transformer for U-Net-based Speech Enhancement",
    "summary": "Achieving a balance between lightweight design and high performance remains a\nchallenging task for speech enhancement. In this paper, we introduce Multi-path\nEnhanced Taylor (MET) Transformer based U-net for Speech Enhancement (MUSE), a\nlightweight speech enhancement network built upon the Unet architecture. Our\napproach incorporates a novel Multi-path Enhanced Taylor (MET) Transformer\nblock, which integrates Deformable Embedding (DE) to enable flexible receptive\nfields for voiceprints. The MET Transformer is uniquely designed to fuse\nChannel and Spatial Attention (CSA) branches, facilitating channel information\nexchange and addressing spatial attention deficits within the\nTaylor-Transformer framework. Through extensive experiments conducted on the\nVoiceBank+DEMAND dataset, we demonstrate that MUSE achieves competitive\nperformance while significantly reducing both training and deployment costs,\nboasting a mere 0.51M parameters.",
    "published": "2024-06-07T02:41:14Z",
    "updated": "2024-06-19T06:18:31Z",
    "authors": [
      "Zizhen Lin",
      "Xiaoting Chen",
      "Junyu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.09804v1",
    "title": "Optimizing Layer-Fused Scheduling of Transformer Networks on\n  Multi-accelerator Platforms",
    "summary": "The impact of transformer networks is booming, yet, they come with\nsignificant computational complexity. It is therefore essential to understand\nhow to optimally map and execute these networks on modern neural processor\nhardware. So far, literature on transformer scheduling optimization has been\nfocusing on deployment on GPU and specific ASICs. This work enables extensive\nhardware/mapping exploration by extending the DSE framework Stream towards\nsupport for transformers across a wide variety of hardware architectures and\ndifferent execution schedules. After validation, we explore the optimal\nschedule for transformer layers/attention heads and investigate whether layer\nfusion is beneficial to improve latency, energy or memory requirements. Our\nstudy shows that the memory requirements for active feature data can be\ndrastically reduced, by adapting the execution schedule based on the size of\nthe input of the attention head.",
    "published": "2024-06-14T07:56:37Z",
    "updated": "2024-06-14T07:56:37Z",
    "authors": [
      "Steven Colleman",
      "Arne Symons",
      "Victor J. B. Jung",
      "Marian Verhelst"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.17986v2",
    "title": "Supra-Laplacian Encoding for Transformer on Dynamic Graphs",
    "summary": "Fully connected Graph Transformers (GT) have rapidly become prominent in the\nstatic graph community as an alternative to Message-Passing models, which\nsuffer from a lack of expressivity, oversquashing, and under-reaching. However,\nin a dynamic context, by interconnecting all nodes at multiple snapshots with\nself-attention, GT loose both structural and temporal information. In this\nwork, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs\n(SLATE), a new spatio-temporal encoding to leverage the GT architecture while\nkeeping spatio-temporal information. Specifically, we transform Discrete Time\nDynamic Graphs into multi-layer graphs and take advantage of the spectral\nproperties of their associated supra-Laplacian matrix. Our second contribution\nexplicitly model nodes' pairwise relationships with a cross-attention\nmechanism, providing an accurate edge representation for dynamic link\nprediction. SLATE outperforms numerous state-of-the-art methods based on\nMessage-Passing Graph Neural Networks combined with recurrent models (e.g\nLSTM), and Dynamic Graph Transformers, on 9 datasets. Code is available at:\ngithub.com/ykrmm/SLATE.",
    "published": "2024-09-26T15:56:40Z",
    "updated": "2024-11-15T14:22:00Z",
    "authors": [
      "Yannis Karmim",
      "Marc Lafon",
      "Raphael Fournier S'niehotta",
      "Nicolas Thome"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.00619v1",
    "title": "A Study on Context Length and Efficient Transformers for Biomedical\n  Image Analysis",
    "summary": "Biomedical imaging modalities often produce high-resolution,\nmulti-dimensional images that pose computational challenges for deep neural\nnetworks. These computational challenges are compounded when training\ntransformers due to the self-attention operator, which scales quadratically\nwith context length. Recent developments in long-context models have potential\nto alleviate these difficulties and enable more efficient application of\ntransformers to large biomedical images, although a systematic evaluation on\nthis topic is lacking. In this study, we investigate the impact of context\nlength on biomedical image analysis and we evaluate the performance of recently\nproposed long-context models. We first curate a suite of biomedical imaging\ndatasets, including 2D and 3D data for segmentation, denoising, and\nclassification tasks. We then analyze the impact of context length on network\nperformance using the Vision Transformer and Swin Transformer by varying patch\nsize and attention window size. Our findings reveal a strong relationship\nbetween context length and performance, particularly for pixel-level prediction\ntasks. Finally, we show that recent long-context models demonstrate significant\nimprovements in efficiency while maintaining comparable performance, though we\nhighlight where gaps remain. This work underscores the potential and challenges\nof using long-context models in biomedical imaging.",
    "published": "2024-12-31T19:38:38Z",
    "updated": "2024-12-31T19:38:38Z",
    "authors": [
      "Sarah M. Hooper",
      "Hui Xue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01124v1",
    "title": "ViKANformer: Embedding Kolmogorov Arnold Networks in Vision Transformers\n  for Pattern-Based Learning",
    "summary": "Vision Transformers (ViTs) have significantly advanced image classification\nby applying self-attention on patch embeddings. However, the standard MLP\nblocks in each Transformer layer may not capture complex nonlinear dependencies\noptimally. In this paper, we propose ViKANformer, a Vision Transformer where we\nreplace the MLP sub-layers with Kolmogorov-Arnold Network (KAN) expansions,\nincluding Vanilla KAN, Efficient-KAN, Fast-KAN, SineKAN, and FourierKAN, while\nalso examining a Flash Attention variant. By leveraging the Kolmogorov-Arnold\ntheorem, which guarantees that multivariate continuous functions can be\nexpressed via sums of univariate continuous functions, we aim to boost\nrepresentational power. Experimental results on MNIST demonstrate that SineKAN,\nFast-KAN, and a well-tuned Vanilla KAN can achieve over 97% accuracy, albeit\nwith increased training overhead. This trade-off highlights that KAN expansions\nmay be beneficial if computational cost is acceptable. We detail the\nexpansions, present training/test accuracy and F1/ROC metrics, and provide\npseudocode and hyperparameters for reproducibility. Finally, we compare\nViKANformer to a simple MLP and a small CNN baseline on MNIST, illustrating the\nefficiency of Transformer-based methods even on a small-scale dataset.",
    "published": "2025-03-03T03:10:26Z",
    "updated": "2025-03-03T03:10:26Z",
    "authors": [
      "Shreyas S",
      "Akshath M"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.17658v1",
    "title": "Sentinel: Multi-Patch Transformer with Temporal and Channel Attention\n  for Time Series Forecasting",
    "summary": "Transformer-based time series forecasting has recently gained strong interest\ndue to the ability of transformers to model sequential data. Most of the\nstate-of-the-art architectures exploit either temporal or inter-channel\ndependencies, limiting their effectiveness in multivariate time-series\nforecasting where both types of dependencies are crucial. We propose Sentinel,\na full transformer-based architecture composed of an encoder able to extract\ncontextual information from the channel dimension, and a decoder designed to\ncapture causal relations and dependencies across the temporal dimension.\nAdditionally, we introduce a multi-patch attention mechanism, which leverages\nthe patching process to structure the input sequence in a way that can be\nnaturally integrated into the transformer architecture, replacing the\nmulti-head splitting process. Extensive experiments on standard benchmarks\ndemonstrate that Sentinel, because of its ability to \"monitor\" both the\ntemporal and the inter-channel dimension, achieves better or comparable\nperformance with respect to state-of-the-art approaches.",
    "published": "2025-03-22T06:01:50Z",
    "updated": "2025-03-22T06:01:50Z",
    "authors": [
      "Davide Villaboni",
      "Alberto Castellini",
      "Ivan Luciano Danesi",
      "Alessandro Farinelli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06633v1",
    "title": "Attention Is Not All You Need: The Importance of Feedforward Networks in\n  Transformer Models",
    "summary": "Decoder-only transformer networks have become incredibly popular for language\nmodeling tasks. State-of-the-art models can have over a hundred transformer\nblocks, containing billions of trainable parameters, and are trained on\ntrillions of tokens of text. Each transformer block typically consists of a\nmulti-head attention (MHA) mechanism and a two-layer fully connected\nfeedforward network (FFN). In this paper, we examine the importance of the FFN\nduring the model pre-training process through a series of experiments,\nconfirming that the FFN is important to model performance. Furthermore, we show\nthat models using a transformer block configuration with three-layer FFNs with\nfewer such blocks outperform the standard two-layer configuration delivering\nlower training loss with fewer total parameters in less time.",
    "published": "2025-05-10T12:54:21Z",
    "updated": "2025-05-10T12:54:21Z",
    "authors": [
      "Isaac Gerber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17054v1",
    "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery",
    "summary": "Recent advances in transformer architectures have revolutionised natural\nlanguage processing, but their application to healthcare domains presents\nunique challenges. Patient timelines are characterised by irregular sampling,\nvariable temporal dependencies, and complex contextual relationships that\ndiffer substantially from traditional language tasks. This paper introduces\n\\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel\ntransformer architecture specifically designed to address the challenges of\nclinical sequence modelling in electronic health records. \\METHOD~integrates\nthree key innovations: (1) a patient-aware attention mechanism that prevents\ninformation leakage whilst enabling efficient batch processing; (2) an adaptive\nsliding window attention scheme that captures multi-scale temporal\ndependencies; and (3) a U-Net inspired architecture with dynamic skip\nconnections for effective long sequence processing. Evaluations on the MIMIC-IV\ndatabase demonstrate that \\METHOD~consistently outperforms the state-of-the-art\n\\ETHOS~model, particularly in predicting high-severity cases that require\nurgent clinical intervention. \\METHOD~exhibits stable performance across\nvarying inference lengths, a crucial feature for clinical deployment where\npatient histories vary significantly in length. Analysis of learned embeddings\nreveals that \\METHOD~better preserves clinical hierarchies and relationships\nbetween medical concepts. These results suggest that \\METHOD~represents a\nsignificant advancement in transformer architectures optimised for healthcare\napplications, providing more accurate and clinically relevant predictions\nwhilst maintaining computational efficiency.",
    "published": "2025-05-16T15:52:56Z",
    "updated": "2025-05-16T15:52:56Z",
    "authors": [
      "Linglong Qian",
      "Zina Ibrahim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24531v1",
    "title": "Transformers Are Universally Consistent",
    "summary": "Despite their central role in the success of foundational models and\nlarge-scale language modeling, the theoretical foundations governing the\noperation of Transformers remain only partially understood. Contemporary\nresearch has largely focused on their representational capacity for language\ncomprehension and their prowess in in-context learning, frequently under\nidealized assumptions such as linearized attention mechanisms. Initially\nconceived to model sequence-to-sequence transformations, a fundamental and\nunresolved question is whether Transformers can robustly perform functional\nregression over sequences of input tokens. This question assumes heightened\nimportance given the inherently non-Euclidean geometry underlying real-world\ndata distributions. In this work, we establish that Transformers equipped with\nsoftmax-based nonlinear attention are uniformly consistent when tasked with\nexecuting Ordinary Least Squares (OLS) regression, provided both the inputs and\noutputs are embedded in hyperbolic space. We derive deterministic upper bounds\non the empirical error which, in the asymptotic regime, decay at a provable\nrate of $\\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens\nand $d$ the embedding dimensionality. Notably, our analysis subsumes the\nEuclidean setting as a special case, recovering analogous convergence\nguarantees parameterized by the intrinsic dimensionality of the data manifold.\nThese theoretical insights are corroborated through empirical evaluations on\nreal-world datasets involving both continuous and categorical response\nvariables.",
    "published": "2025-05-30T12:39:26Z",
    "updated": "2025-05-30T12:39:26Z",
    "authors": [
      "Sagar Ghosh",
      "Kushal Bose",
      "Swagatam Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18437v1",
    "title": "Frequency-Domain Fusion Transformer for Image Inpainting",
    "summary": "Image inpainting plays a vital role in restoring missing image regions and\nsupporting high-level vision tasks, but traditional methods struggle with\ncomplex textures and large occlusions. Although Transformer-based approaches\nhave demonstrated strong global modeling capabilities, they often fail to\npreserve high-frequency details due to the low-pass nature of self-attention\nand suffer from high computational costs. To address these challenges, this\npaper proposes a Transformer-based image inpainting method incorporating\nfrequency-domain fusion. Specifically, an attention mechanism combining wavelet\ntransform and Gabor filtering is introduced to enhance multi-scale structural\nmodeling and detail preservation. Additionally, a learnable frequency-domain\nfilter based on the fast Fourier transform is designed to replace the\nfeedforward network, enabling adaptive noise suppression and detail retention.\nThe model adopts a four-level encoder-decoder structure and is guided by a\nnovel loss strategy to balance global semantics and fine details. Experimental\nresults demonstrate that the proposed method effectively improves the quality\nof image inpainting by preserving more high-frequency information.",
    "published": "2025-06-23T09:19:04Z",
    "updated": "2025-06-23T09:19:04Z",
    "authors": [
      "Sijin He",
      "Guangfeng Lin",
      "Tao Li",
      "Yajun Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.08396v5",
    "title": "MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation",
    "summary": "Since their emergence, Convolutional Neural Networks (CNNs) have made\nsignificant strides in medical image analysis. However, the local nature of the\nconvolution operator may pose a limitation for capturing global and long-range\ninteractions in CNNs. Recently, Transformers have gained popularity in the\ncomputer vision community and also in medical image segmentation due to their\nability to process global features effectively. The scalability issues of the\nself-attention mechanism and lack of the CNN-like inductive bias may have\nlimited their adoption. Therefore, hybrid Vision transformers\n(CNN-Transformer), exploiting the advantages of both Convolution and\nSelf-attention Mechanisms, have gained importance. In this work, we present\nMaxViT-UNet, a new Encoder-Decoder based UNet type hybrid vision transformer\n(CNN-Transformer) for medical image segmentation. The proposed Hybrid Decoder\nis designed to harness the power of both the convolution and self-attention\nmechanisms at each decoding stage with a nominal memory and computational\nburden. The inclusion of multi-axis self-attention, within each decoder stage,\nsignificantly enhances the discriminating capacity between the object and\nbackground regions, thereby helping in improving the segmentation efficiency.\nIn the Hybrid Decoder, a new block is also proposed. The fusion process\ncommences by integrating the upsampled lower-level decoder features, obtained\nthrough transpose convolution, with the skip-connection features derived from\nthe hybrid encoder. Subsequently, the fused features undergo refinement through\nthe utilization of a multi-axis attention mechanism. The proposed decoder block\nis repeated multiple times to segment the nuclei regions progressively.\nExperimental results on MoNuSeg18 and MoNuSAC20 datasets demonstrate the\neffectiveness of the proposed technique.",
    "published": "2023-05-15T07:23:54Z",
    "updated": "2024-03-29T12:50:38Z",
    "authors": [
      "Abdul Rehman Khan",
      "Asifullah Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.20348v1",
    "title": "UTSRMorph: A Unified Transformer and Superresolution Network for\n  Unsupervised Medical Image Registration",
    "summary": "Complicated image registration is a key issue in medical image analysis, and\ndeep learning-based methods have achieved better results than traditional\nmethods. The methods include ConvNet-based and Transformer-based methods.\nAlthough ConvNets can effectively utilize local information to reduce\nredundancy via small neighborhood convolution, the limited receptive field\nresults in the inability to capture global dependencies. Transformers can\nestablish long-distance dependencies via a self-attention mechanism; however,\nthe intense calculation of the relationships among all tokens leads to high\nredundancy. We propose a novel unsupervised image registration method named the\nunified Transformer and superresolution (UTSRMorph) network, which can enhance\nfeature representation learning in the encoder and generate detailed\ndisplacement fields in the decoder to overcome these problems. We first propose\na fusion attention block to integrate the advantages of ConvNets and\nTransformers, which inserts a ConvNet-based channel attention module into a\nmultihead self-attention module. The overlapping attention block, a novel\ncross-attention method, uses overlapping windows to obtain abundant\ncorrelations with match information of a pair of images. Then, the blocks are\nflexibly stacked into a new powerful encoder. The decoder generation process of\na high-resolution deformation displacement field from low-resolution features\nis considered as a superresolution process. Specifically, the superresolution\nmodule was employed to replace interpolation upsampling, which can overcome\nfeature degradation. UTSRMorph was compared to state-of-the-art registration\nmethods in the 3D brain MR (OASIS, IXI) and MR-CT datasets. The qualitative and\nquantitative results indicate that UTSRMorph achieves relatively better\nperformance. The code and datasets are publicly available at\nhttps://github.com/Runshi-Zhang/UTSRMorph.",
    "published": "2024-10-27T06:28:43Z",
    "updated": "2024-10-27T06:28:43Z",
    "authors": [
      "Runshi Zhang",
      "Hao Mo",
      "Junchen Wang",
      "Bimeng Jie",
      "Yang He",
      "Nenghao Jin",
      "Liang Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.03032v1",
    "title": "A Systematic Analysis of Morphological Content in BERT Models for\n  Multiple Languages",
    "summary": "This work describes experiments which probe the hidden representations of\nseveral BERT-style models for morphological content. The goal is to examine the\nextent to which discrete linguistic structure, in the form of morphological\nfeatures and feature values, presents itself in the vector representations and\nattention distributions of pre-trained language models for five European\nlanguages. The experiments contained herein show that (i) Transformer\narchitectures largely partition their embedding space into convex sub-regions\nhighly correlated with morphological feature value, (ii) the contextualized\nnature of transformer embeddings allows models to distinguish ambiguous\nmorphological forms in many, but not all cases, and (iii) very specific\nattention head/layer combinations appear to hone in on subject-verb agreement.",
    "published": "2020-04-06T22:50:27Z",
    "updated": "2020-04-06T22:50:27Z",
    "authors": [
      "Daniel Edmiston"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.07486v3",
    "title": "Adaptive Transformers for Learning Multimodal Representations",
    "summary": "The usage of transformers has grown from learning about language semantics to\nforming meaningful visiolinguistic representations. These architectures are\noften over-parametrized, requiring large amounts of computation. In this work,\nwe extend adaptive approaches to learn more about model interpretability and\ncomputational efficiency. Specifically, we study attention spans, sparse, and\nstructured dropout methods to help understand how their attention mechanism\nextends for vision and language tasks. We further show that these approaches\ncan help us learn more about how the network perceives the complexity of input\nsequences, sparsity preferences for different modalities, and other related\nphenomena.",
    "published": "2020-05-15T12:12:57Z",
    "updated": "2020-07-08T12:26:12Z",
    "authors": [
      "Prajjwal Bhargava"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.00367v1",
    "title": "Natural Language to Code Using Transformers",
    "summary": "We tackle the problem of generating code snippets from natural language\ndescriptions using the CoNaLa dataset. We use the self-attention based\ntransformer architecture and show that it performs better than recurrent\nattention-based encoder decoder. Furthermore, we develop a modified form of\nback translation and use cycle consistent losses to train the model in an\nend-to-end fashion. We achieve a BLEU score of 16.99 beating the previously\nreported baseline of the CoNaLa challenge.",
    "published": "2022-02-01T12:17:52Z",
    "updated": "2022-02-01T12:17:52Z",
    "authors": [
      "Uday Kusupati",
      "Venkata Ravi Teja Ailavarapu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.03529v1",
    "title": "How to Dissect a Muppet: The Structure of Transformer Embedding Spaces",
    "summary": "Pretrained embeddings based on the Transformer architecture have taken the\nNLP community by storm. We show that they can mathematically be reframed as a\nsum of vector factors and showcase how to use this reframing to study the\nimpact of each component. We provide evidence that multi-head attentions and\nfeed-forwards are not equally useful in all downstream applications, as well as\na quantitative overview of the effects of finetuning on the overall embedding\nspace. This approach allows us to draw connections to a wide range of previous\nstudies, from vector space anisotropy to attention weights.",
    "published": "2022-06-07T18:24:46Z",
    "updated": "2022-06-07T18:24:46Z",
    "authors": [
      "Timothee Mickus",
      "Denis Paperno",
      "Mathieu Constant"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.08288v1",
    "title": "Attention-likelihood relationship in transformers",
    "summary": "We analyze how large language models (LLMs) represent out-of-context words,\ninvestigating their reliance on the given context to capture their semantics.\nOur likelihood-guided text perturbations reveal a correlation between token\nlikelihood and attention values in transformer-based language models. Extensive\nexperiments reveal that unexpected tokens cause the model to attend less to the\ninformation coming from themselves to compute their representations,\nparticularly at higher layers. These findings have valuable implications for\nassessing the robustness of LLMs in real-world scenarios. Fully reproducible\ncodebase at https://github.com/Flegyas/AttentionLikelihood.",
    "published": "2023-03-15T00:23:49Z",
    "updated": "2023-03-15T00:23:49Z",
    "authors": [
      "Valeria Ruscio",
      "Valentino Maiorca",
      "Fabrizio Silvestri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1906.05963v2",
    "title": "Image Captioning: Transforming Objects into Words",
    "summary": "Image captioning models typically follow an encoder-decoder architecture\nwhich uses abstract image feature vectors as input to the encoder. One of the\nmost successful algorithms uses feature vectors extracted from the region\nproposals obtained from an object detector. In this work we introduce the\nObject Relation Transformer, that builds upon this approach by explicitly\nincorporating information about the spatial relationship between input detected\nobjects through geometric attention. Quantitative and qualitative results\ndemonstrate the importance of such geometric attention for image captioning,\nleading to improvements on all common captioning metrics on the MS-COCO\ndataset.",
    "published": "2019-06-14T00:00:29Z",
    "updated": "2020-01-11T11:03:05Z",
    "authors": [
      "Simao Herdade",
      "Armin Kappeler",
      "Kofi Boakye",
      "Joao Soares"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.12290v1",
    "title": "Attention-based Dual-stream Vision Transformer for Radar Gait\n  Recognition",
    "summary": "Radar gait recognition is robust to light variations and less infringement on\nprivacy. Previous studies often utilize either spectrograms or cadence velocity\ndiagrams. While the former shows the time-frequency patterns, the latter\nencodes the repetitive frequency patterns. In this work, a dual-stream neural\nnetwork with attention-based fusion is proposed to fully aggregate the\ndiscriminant information from these two representations. The both streams are\ndesigned based on the Vision Transformer, which well captures the gait\ncharacteristics embedded in these representations. The proposed method is\nvalidated on a large benchmark dataset for radar gait recognition, which shows\nthat it significantly outperforms state-of-the-art solutions.",
    "published": "2021-11-24T06:16:53Z",
    "updated": "2021-11-24T06:16:53Z",
    "authors": [
      "Shiliang Chen",
      "Wentao He",
      "Jianfeng Ren",
      "Xudong Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.06953v1",
    "title": "On the interplay of adversarial robustness and architecture components:\n  patches, convolution and attention",
    "summary": "In recent years novel architecture components for image classification have\nbeen developed, starting with attention and patches used in transformers. While\nprior works have analyzed the influence of some aspects of architecture\ncomponents on the robustness to adversarial attacks, in particular for vision\ntransformers, the understanding of the main factors is still limited. We\ncompare several (non)-robust classifiers with different architectures and study\ntheir properties, including the effect of adversarial training on the\ninterpretability of the learnt features and robustness to unseen threat models.\nAn ablation from ResNet to ConvNeXt reveals key architectural changes leading\nto almost $10\\%$ higher $\\ell_\\infty$-robustness.",
    "published": "2022-09-14T22:02:32Z",
    "updated": "2022-09-14T22:02:32Z",
    "authors": [
      "Francesco Croce",
      "Matthias Hein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.10093v1",
    "title": "Visual Transformers for Primates Classification and Covid Detection",
    "summary": "We apply the vision transformer, a deep machine learning model build around\nthe attention mechanism, on mel-spectrogram representations of raw audio\nrecordings. When adding mel-based data augmentation techniques and\nsample-weighting, we achieve comparable performance on both (PRS and CCS\nchallenge) tasks of ComParE21, outperforming most single model baselines. We\nfurther introduce overlapping vertical patching and evaluate the influence of\nparameter configurations. Index Terms: audio classification, attention,\nmel-spectrogram, unbalanced data-sets, computational paralinguistics",
    "published": "2022-12-20T09:10:25Z",
    "updated": "2022-12-20T09:10:25Z",
    "authors": [
      "Steffen Illium",
      "Robert MÃ¼ller",
      "Andreas Sedlmeier",
      "Claudia-Linnhoff Popien"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.09406v1",
    "title": "Alternatives to the Scaled Dot Product for Attention in the Transformer\n  Neural Network Architecture",
    "summary": "The transformer neural network architecture uses a form of attention in which\nthe dot product of query and key is divided by the square root of the key\ndimension before applying softmax. This scaling of the dot product is designed\nto avoid the absolute value of the dot products becoming so large that applying\nsoftmax leads to vanishing gradients. In this paper, we propose some\nalternative scalings, including dividing the dot product instead by the sum of\nthe key lengths before applying softmax. We use simulated keys and queries to\nshow that in many situations this appears to be more effective at avoiding\nregions where applying softmax leads to vanishing gradients.",
    "published": "2023-11-15T22:10:42Z",
    "updated": "2023-11-15T22:10:42Z",
    "authors": [
      "James Bernhard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.17783v1",
    "title": "How Lightweight Can A Vision Transformer Be",
    "summary": "In this paper, we explore a strategy that uses Mixture-of-Experts (MoE) to\nstreamline, rather than augment, vision transformers. Each expert in an MoE\nlayer is a SwiGLU feedforward network, where V and W2 are shared across the\nlayer. No complex attention or convolutional mechanisms are employed.\nDepth-wise scaling is applied to progressively reduce the size of the hidden\nlayer and the number of experts is increased in stages. Grouped query attention\nis used. We studied the proposed approach with and without pre-training on\nsmall datasets and investigated whether transfer learning works at this scale.\nWe found that the architecture is competitive even at a size of 0.67M\nparameters.",
    "published": "2024-07-25T05:23:20Z",
    "updated": "2024-07-25T05:23:20Z",
    "authors": [
      "Jen Hong Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07373v1",
    "title": "ChronoFormer: Time-Aware Transformer Architectures for Structured\n  Clinical Event Modeling",
    "summary": "The temporal complexity of electronic health record (EHR) data presents\nsignificant challenges for predicting clinical outcomes using machine learning.\nThis paper proposes ChronoFormer, an innovative transformer based architecture\nspecifically designed to encode and leverage temporal dependencies in\nlongitudinal patient data. ChronoFormer integrates temporal embeddings,\nhierarchical attention mechanisms, and domain specific masking techniques.\nExtensive experiments conducted on three benchmark tasks mortality prediction,\nreadmission prediction, and long term comorbidity onset demonstrate substantial\nimprovements over current state of the art methods. Furthermore, detailed\nanalyses of attention patterns underscore ChronoFormer's capability to capture\nclinically meaningful long range temporal relationships.",
    "published": "2025-04-10T01:25:41Z",
    "updated": "2025-04-10T01:25:41Z",
    "authors": [
      "Yuanyun Zhang",
      "Shi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.14685v2",
    "title": "Scaled Signed Averaging Improves In-Context and Early Learning Benchmark\n  Performance in Small Transformers",
    "summary": "While Large Language models' abilities for in-context learning (ICL) have\ndrawn much attention, we examine some of its limitations on semantic tasks\ninvolving quantifiers like \"all\" and \"some\", as well as on tasks with linear\nfunctions. We identify Softmax, the scoring function in attention mechanism, as\na contributing factor to these limitations. We propose scaled signed averaging\n(SSA), a novel alternative to Softmax to mitigate these problems. We show that\nSSA significantly improves performance on our ICL tasks. In addition, SSA\noutperforms transformer models with Softmax on several early learning NLP\nbenchmarks and linguistic probing tasks on zero and few-shot settings.",
    "published": "2025-08-20T13:01:34Z",
    "updated": "2025-10-07T18:34:48Z",
    "authors": [
      "Omar Naim",
      "Swarnadeep Bhar",
      "JÃ©rÃ´me Bolte",
      "Nicholas Asher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.20363v1",
    "title": "A Transformer Inspired AI-based MIMO receiver",
    "summary": "We present AttDet, a Transformer-inspired MIMO (Multiple Input Multiple\nOutput) detection method that treats each transmit layer as a token and learns\ninter-stream interference via a lightweight self-attention mechanism. Queries\nand keys are derived directly from the estimated channel matrix, so attention\nscores quantify channel correlation. Values are initialized by matched-filter\noutputs and iteratively refined. The AttDet design combines model-based\ninterpretability with data-driven flexibility. We demonstrate through\nlink-level simulations under realistic 5G channel models and high-order, mixed\nQAM modulation and coding schemes, that AttDet can approach near-optimal\nBER/BLER (Bit Error Rate/Block Error Rate) performance while maintaining\npredictable, polynomial complexity.",
    "published": "2025-10-23T09:05:10Z",
    "updated": "2025-10-23T09:05:10Z",
    "authors": [
      "AndrÃ¡s RÃ¡cz",
      "TamÃ¡s Borsos",
      "AndrÃ¡s Veres",
      "Benedek Csala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.05109v1",
    "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision\n  Transformer Acceleration with a Linear Taylor Attention",
    "summary": "Vision Transformer (ViT) has emerged as a competitive alternative to\nconvolutional neural networks for various computer vision applications.\nSpecifically, ViT multi-head attention layers make it possible to embed\ninformation globally across the overall image. Nevertheless, computing and\nstoring such attention matrices incurs a quadratic cost dependency on the\nnumber of patches, limiting its achievable efficiency and scalability and\nprohibiting more extensive real-world ViT applications on resource-constrained\ndevices. Sparse attention has been shown to be a promising direction for\nimproving hardware acceleration efficiency for NLP models. However, a\nsystematic counterpart approach is still missing for accelerating ViT models.\nTo close the above gap, we propose a first-of-its-kind algorithm-hardware\ncodesigned framework, dubbed ViTALiTy, for boosting the inference efficiency of\nViTs. Unlike sparsity-based Transformer accelerators for NLP, ViTALiTy unifies\nboth low-rank and sparse components of the attention in ViTs. At the algorithm\nlevel, we approximate the dot-product softmax operation via first-order Taylor\nattention with row-mean centering as the low-rank component to linearize the\ncost of attention blocks and further boost the accuracy by incorporating a\nsparsity-based regularization. At the hardware level, we develop a dedicated\naccelerator to better leverage the resulting workload and pipeline from\nViTALiTy's linear Taylor attention which requires the execution of only the\nlow-rank component, to further boost the hardware efficiency. Extensive\nexperiments and ablation studies validate that ViTALiTy offers boosted\nend-to-end efficiency (e.g., $3\\times$ faster and $3\\times$ energy-efficient)\nunder comparable accuracy, with respect to the state-of-the-art solution.",
    "published": "2022-11-09T18:58:21Z",
    "updated": "2022-11-09T18:58:21Z",
    "authors": [
      "Jyotikrishna Dass",
      "Shang Wu",
      "Huihong Shi",
      "Chaojian Li",
      "Zhifan Ye",
      "Zhongfeng Wang",
      "Yingyan Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.07799v1",
    "title": "Label-Attention Transformer with Geometrically Coherent Objects for\n  Image Captioning",
    "summary": "Automatic transcription of scene understanding in images and videos is a step\ntowards artificial general intelligence. Image captioning is a nomenclature for\ndescribing meaningful information in an image using computer vision techniques.\nAutomated image captioning techniques utilize encoder and decoder architecture,\nwhere the encoder extracts features from an image and the decoder generates a\ntranscript. In this work, we investigate two unexplored ideas for image\ncaptioning using transformers: First, we demonstrate the enforcement of using\nobjects' relevance in the surrounding environment. Second, learning an explicit\nassociation between labels and language constructs. We propose label-attention\nTransformer with geometrically coherent objects (LATGeO). The proposed\ntechnique acquires a proposal of geometrically coherent objects using a deep\nneural network (DNN) and generates captions by investigating their\nrelationships using a label-attention module. Object coherence is defined using\nthe localized ratio of the geometrical properties of the proposals. The\nlabel-attention module associates the extracted objects classes to the\navailable dictionary using self-attention layers. The experimentation results\nshow that objects' relevance in surroundings and binding of their visual\nfeature with their geometrically localized ratios combined with its associated\nlabels help in defining meaningful captions. The proposed framework is tested\non the MSCOCO dataset, and a thorough evaluation resulting in overall better\nquantitative scores pronounces its superiority.",
    "published": "2021-09-16T08:43:46Z",
    "updated": "2021-09-16T08:43:46Z",
    "authors": [
      "Shikha Dubey",
      "Farrukh Olimov",
      "Muhammad Aasim Rafique",
      "Joonmo Kim",
      "Moongu Jeon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.03227v3",
    "title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning",
    "summary": "Self-attention is a key enabler of state-of-art accuracy for various\ntransformer-based Natural Language Processing models. This attention mechanism\ncalculates a correlation score for each word with respect to the other words in\na sentence. Commonly, only a small subset of words highly correlates with the\nword under attention, which is only determined at runtime. As such, a\nsignificant amount of computation is inconsequential due to low attention\nscores and can potentially be pruned. The main challenge is finding the\nthreshold for the scores below which subsequent computation will be\ninconsequential. Although such a threshold is discrete, this paper formulates\nits search through a soft differentiable regularizer integrated into the loss\nfunction of the training. This formulation piggy backs on the back-propagation\ntraining to analytically co-optimize the threshold and the weights\nsimultaneously, striking a formally optimal balance between accuracy and\ncomputation pruning. To best utilize this mathematical innovation, we devise a\nbit-serial architecture, dubbed LeOPArd, for transformer language models with\nbit-level early termination microarchitectural mechanism. We evaluate our\ndesign across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision\ntransformer models. Post-layout results show that, on average, LeOPArd yields\n1.9x and 3.9x speedup and energy reduction, respectively, while keeping the\naverage accuracy virtually intact (<0.2% degradation)",
    "published": "2022-04-07T05:31:13Z",
    "updated": "2022-04-15T02:47:47Z",
    "authors": [
      "Zheng Li",
      "Soroush Ghodrati",
      "Amir Yazdanbakhsh",
      "Hadi Esmaeilzadeh",
      "Mingu Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1905.07841v1",
    "title": "Multimodal Transformer with Multi-View Visual Representation for Image\n  Captioning",
    "summary": "Image captioning aims to automatically generate a natural language\ndescription of a given image, and most state-of-the-art models have adopted an\nencoder-decoder framework. The framework consists of a convolution neural\nnetwork (CNN)-based image encoder that extracts region-based visual features\nfrom the input image, and an recurrent neural network (RNN)-based caption\ndecoder that generates the output caption words based on the visual features\nwith the attention mechanism. Despite the success of existing studies, current\nmethods only model the co-attention that characterizes the inter-modal\ninteractions while neglecting the self-attention that characterizes the\nintra-modal interactions. Inspired by the success of the Transformer model in\nmachine translation, here we extend it to a Multimodal Transformer (MT) model\nfor image captioning. Compared to existing image captioning approaches, the MT\nmodel simultaneously captures intra- and inter-modal interactions in a unified\nattention block. Due to the in-depth modular composition of such attention\nblocks, the MT model can perform complex multimodal reasoning and output\naccurate captions. Moreover, to further improve the image captioning\nperformance, multi-view visual features are seamlessly introduced into the MT\nmodel. We quantitatively and qualitatively evaluate our approach using the\nbenchmark MSCOCO image captioning dataset and conduct extensive ablation\nstudies to investigate the reasons behind its effectiveness. The experimental\nresults show that our method significantly outperforms the previous\nstate-of-the-art methods. With an ensemble of seven models, our solution ranks\nthe 1st place on the real-time leaderboard of the MSCOCO image captioning\nchallenge at the time of the writing of this paper.",
    "published": "2019-05-20T01:56:06Z",
    "updated": "2019-05-20T01:56:06Z",
    "authors": [
      "Jun Yu",
      "Jing Li",
      "Zhou Yu",
      "Qingming Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.14778v2",
    "title": "RGBT Tracking via Progressive Fusion Transformer with Dynamically Guided\n  Learning",
    "summary": "Existing Transformer-based RGBT tracking methods either use cross-attention\nto fuse the two modalities, or use self-attention and cross-attention to model\nboth modality-specific and modality-sharing information. However, the\nsignificant appearance gap between modalities limits the feature representation\nability of certain modalities during the fusion process. To address this\nproblem, we propose a novel Progressive Fusion Transformer called ProFormer,\nwhich progressively integrates single-modality information into the multimodal\nrepresentation for robust RGBT tracking. In particular, ProFormer first uses a\nself-attention module to collaboratively extract the multimodal representation,\nand then uses two cross-attention modules to interact it with the features of\nthe dual modalities respectively. In this way, the modality-specific\ninformation can well be activated in the multimodal representation. Finally, a\nfeed-forward network is used to fuse two interacted multimodal representations\nfor the further enhancement of the final multimodal representation. In\naddition, existing learning methods of RGBT trackers either fuse multimodal\nfeatures into one for final classification, or exploit the relationship between\nunimodal branches and fused branch through a competitive learning strategy.\nHowever, they either ignore the learning of single-modality branches or result\nin one branch failing to be well optimized. To solve these problems, we propose\na dynamically guided learning algorithm that adaptively uses well-performing\nbranches to guide the learning of other branches, for enhancing the\nrepresentation ability of each branch. Extensive experiments demonstrate that\nour proposed ProFormer sets a new state-of-the-art performance on RGBT210,\nRGBT234, LasHeR, and VTUAV datasets.",
    "published": "2023-03-26T16:55:58Z",
    "updated": "2023-04-22T08:55:55Z",
    "authors": [
      "Yabin Zhu",
      "Chenglong Li",
      "Xiao Wang",
      "Jin Tang",
      "Zhixiang Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.03633v4",
    "title": "Augmented Equivariant Attention Networks for Microscopy Image\n  Reconstruction",
    "summary": "It is time-consuming and expensive to take high-quality or high-resolution\nelectron microscopy (EM) and fluorescence microscopy (FM) images. Taking these\nimages could be even invasive to samples and may damage certain subtleties in\nthe samples after long or intense exposures, often necessary for achieving\nhigh-quality or high resolution in the first place. Advances in deep learning\nenable us to perform image-to-image transformation tasks for various types of\nmicroscopy image reconstruction, computationally producing high-quality images\nfrom the physically acquired low-quality ones. When training image-to-image\ntransformation models on pairs of experimentally acquired microscopy images,\nprior models suffer from performance loss due to their inability to capture\ninter-image dependencies and common features shared among images. Existing\nmethods that take advantage of shared features in image classification tasks\ncannot be properly applied to image reconstruction tasks because they fail to\npreserve the equivariance property under spatial permutations, something\nessential in image-to-image transformation. To address these limitations, we\npropose the augmented equivariant attention networks (AEANets) with better\ncapability to capture inter-image dependencies, while preserving the\nequivariance property. The proposed AEANets captures inter-image dependencies\nand shared features via two augmentations on the attention mechanism, which are\nthe shared references and the batch-aware attention during training. We\ntheoretically derive the equivariance property of the proposed augmented\nattention model and experimentally demonstrate its consistent superiority in\nboth quantitative and visual results over the baseline methods.",
    "published": "2020-11-06T23:37:49Z",
    "updated": "2022-06-02T23:47:01Z",
    "authors": [
      "Yaochen Xie",
      "Yu Ding",
      "Shuiwang Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.02174v2",
    "title": "What Makes for Hierarchical Vision Transformer?",
    "summary": "Recent studies indicate that hierarchical Vision Transformer with a macro\narchitecture of interleaved non-overlapped window-based self-attention \\&\nshifted-window operation is able to achieve state-of-the-art performance in\nvarious visual recognition tasks, and challenges the ubiquitous convolutional\nneural networks (CNNs) using densely slid kernels. Most follow-up works attempt\nto replace the shifted-window operation with other kinds of cross-window\ncommunication paradigms, while treating self-attention as the de-facto standard\nfor window-based information aggregation. In this manuscript, we question\nwhether self-attention is the only choice for hierarchical Vision Transformer\nto attain strong performance, and the effects of different kinds of\ncross-window communication. To this end, we replace self-attention layers with\nembarrassingly simple linear mapping layers, and the resulting proof-of-concept\narchitecture termed as LinMapper can achieve very strong performance in\nImageNet-1k image recognition. Moreover, we find that LinMapper is able to\nbetter leverage the pre-trained representations from image recognition and\ndemonstrates excellent transfer learning properties on downstream dense\nprediction tasks such as object detection and instance segmentation. We also\nexperiment with other alternatives to self-attention for content aggregation\ninside each non-overlapped window under different cross-window communication\napproaches, which all give similar competitive results. Our study reveals that\nthe \\textbf{macro architecture} of Swin model families, other than specific\naggregation layers or specific means of cross-window communication, may be more\nresponsible for its strong performance and is the real challenger to the\nubiquitous CNN's dense sliding window paradigm. Code and models will be\npublicly available to facilitate future research.",
    "published": "2021-07-05T17:59:35Z",
    "updated": "2021-09-10T03:04:13Z",
    "authors": [
      "Yuxin Fang",
      "Xinggang Wang",
      "Rui Wu",
      "Wenyu Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.15343v1",
    "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation",
    "summary": "Recent advances in efficient Transformers have exploited either the sparsity\nor low-rank properties of attention matrices to reduce the computational and\nmemory bottlenecks of modeling long sequences. However, it is still challenging\nto balance the trade-off between model quality and efficiency to perform a\none-size-fits-all approximation for different tasks. To better understand this\ntrade-off, we observe that sparse and low-rank approximations excel in\ndifferent regimes, determined by the softmax temperature in attention, and\nsparse + low-rank can outperform each individually. Inspired by the classical\nrobust-PCA algorithm for sparse and low-rank decomposition, we propose\nScatterbrain, a novel way to unify sparse (via locality sensitive hashing) and\nlow-rank (via kernel feature map) attention for accurate and efficient\napproximation. The estimation is unbiased with provably low error. We\nempirically show that Scatterbrain can achieve 2.1x lower error than baselines\nwhen serving as a drop-in replacement in BigGAN image generation and\npre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without\nfine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of\nonly 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training\nwith up to 4 points better perplexity and 5 points better average accuracy than\nsparse or low-rank efficient transformers on language modeling and\nlong-range-arena tasks.",
    "published": "2021-10-28T17:52:17Z",
    "updated": "2021-10-28T17:52:17Z",
    "authors": [
      "Beidi Chen",
      "Tri Dao",
      "Eric Winsor",
      "Zhao Song",
      "Atri Rudra",
      "Christopher RÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.04674v3",
    "title": "DualFormer: Local-Global Stratified Transformer for Efficient Video\n  Recognition",
    "summary": "While transformers have shown great potential on video recognition with their\nstrong capability of capturing long-range dependencies, they often suffer high\ncomputational costs induced by the self-attention to the huge number of 3D\ntokens. In this paper, we present a new transformer architecture termed\nDualFormer, which can efficiently perform space-time attention for video\nrecognition. Concretely, DualFormer stratifies the full space-time attention\ninto dual cascaded levels, i.e., to first learn fine-grained local interactions\namong nearby 3D tokens, and then to capture coarse-grained global dependencies\nbetween the query token and global pyramid contexts. Different from existing\nmethods that apply space-time factorization or restrict attention computations\nwithin local windows for improving efficiency, our local-global stratification\nstrategy can well capture both short- and long-range spatiotemporal\ndependencies, and meanwhile greatly reduces the number of keys and values in\nattention computation to boost efficiency. Experimental results verify the\nsuperiority of DualFormer on five video benchmarks against existing methods. In\nparticular, DualFormer achieves 82.9%/85.2% top-1 accuracy on Kinetics-400/600\nwith ~1000G inference FLOPs which is at least 3.2x fewer than existing methods\nwith similar performance. We have released the source code at\nhttps://github.com/sail-sg/dualformer.",
    "published": "2021-12-09T03:05:19Z",
    "updated": "2022-11-22T09:41:50Z",
    "authors": [
      "Yuxuan Liang",
      "Pan Zhou",
      "Roger Zimmermann",
      "Shuicheng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.13533v2",
    "title": "High-Performance Transformer Tracking",
    "summary": "Correlation has a critical role in the tracking field, especially in recent\npopular Siamese-based trackers. The correlation operation is a simple fusion\nmethod that considers the similarity between the template and the search\nregion. However, the correlation operation is a local linear matching process,\nlosing semantic information and easily falling into a local optimum, which may\nbe the bottleneck in designing high-accuracy tracking algorithms. In this work,\nto determine whether a better feature fusion method exists than correlation, a\nnovel attention-based feature fusion network, inspired by the transformer, is\npresented. This network effectively combines the template and search region\nfeatures using attention. Specifically, the proposed method includes an\nego-context augment module based on self-attention and a cross-feature augment\nmodule based on cross-attention. First, we present a transformer tracking\n(named TransT) method based on the Siamese-like feature extraction backbone,\nthe designed attention-based fusion mechanism, and the classification and\nregression head. Based on the TransT baseline, we further design a segmentation\nbranch to generate an accurate mask. Finally, we propose a stronger version of\nTransT by extending TransT with a multi-template scheme and an IoU prediction\nhead, named TransT-M. Experiments show that our TransT and TransT-M methods\nachieve promising results on seven popular datasets. Code and models are\navailable at https://github.com/chenxin-dlut/TransT-M.",
    "published": "2022-03-25T09:33:29Z",
    "updated": "2022-11-23T08:45:18Z",
    "authors": [
      "Xin Chen",
      "Bin Yan",
      "Jiawen Zhu",
      "Huchuan Lu",
      "Xiang Ruan",
      "Dong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.07100v1",
    "title": "Multiformer: A Head-Configurable Transformer-Based Model for Direct\n  Speech Translation",
    "summary": "Transformer-based models have been achieving state-of-the-art results in\nseveral fields of Natural Language Processing. However, its direct application\nto speech tasks is not trivial. The nature of this sequences carries problems\nsuch as long sequence lengths and redundancy between adjacent tokens.\nTherefore, we believe that regular self-attention mechanism might not be well\nsuited for it.\n  Different approaches have been proposed to overcome these problems, such as\nthe use of efficient attention mechanisms. However, the use of these methods\nusually comes with a cost, which is a performance reduction caused by\ninformation loss. In this study, we present the Multiformer, a\nTransformer-based model which allows the use of different attention mechanisms\non each head. By doing this, the model is able to bias the self-attention\ntowards the extraction of more diverse token interactions, and the information\nloss is reduced. Finally, we perform an analysis of the head contributions, and\nwe observe that those architectures where all heads relevance is uniformly\ndistributed obtain better results. Our results show that mixing attention\npatterns along the different heads and layers outperforms our baseline by up to\n0.7 BLEU.",
    "published": "2022-05-14T17:37:47Z",
    "updated": "2022-05-14T17:37:47Z",
    "authors": [
      "Gerard Sant",
      "Gerard I. GÃ¡llego",
      "Belen Alastruey",
      "Marta R. Costa-JussÃ "
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.14741v3",
    "title": "End-to-end View Synthesis via NeRF Attention",
    "summary": "In this paper, we present a simple seq2seq formulation for view synthesis\nwhere we take a set of ray points as input and output colors corresponding to\nthe rays. Directly applying a standard transformer on this seq2seq formulation\nhas two limitations. First, the standard attention cannot successfully fit the\nvolumetric rendering procedure, and therefore high-frequency components are\nmissing in the synthesized views. Second, applying global attention to all rays\nand pixels is extremely inefficient. Inspired by the neural radiance field\n(NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On\nthe one hand, NeRFA considers the volumetric rendering equation as a soft\nfeature modulation procedure. In this way, the feature modulation enhances the\ntransformers with the NeRF-like inductive bias. On the other hand, NeRFA\nperforms multi-stage attention to reduce the computational overhead.\nFurthermore, the NeRFA model adopts the ray and pixel transformers to learn the\ninteractions between rays and pixels. NeRFA demonstrates superior performance\nover NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D.\nBesides, NeRFA establishes a new state-of-the-art under two settings: the\nsingle-scene view synthesis and the category-centric novel view synthesis.",
    "published": "2022-07-29T15:26:16Z",
    "updated": "2022-09-15T03:04:10Z",
    "authors": [
      "Zelin Zhao",
      "Jiaya Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.11750v2",
    "title": "Reconstructing high-order sequence features of dynamic functional\n  connectivity networks based on diversified covert attention patterns for\n  Alzheimer's disease classification",
    "summary": "Recent studies have applied deep learning methods such as convolutional\nrecurrent neural networks (CRNs) and Transformers to brain disease\nclassification based on dynamic functional connectivity networks (dFCNs), such\nas Alzheimer's disease (AD), achieving better performance than traditional\nmachine learning methods. However, in CRNs, the continuous convolution\noperations used to obtain high-order aggregation features may overlook the\nnon-linear correlation between different brain regions due to the essence of\nconvolution being the linear weighted sum of local elements. Inspired by modern\nneuroscience on the research of covert attention in the nervous system, we\nintroduce the self-attention mechanism, a core module of Transformers, to model\ndiversified covert attention patterns and apply these patterns to reconstruct\nhigh-order sequence features of dFCNs in order to learn complex dynamic changes\nin brain information flow. Therefore, we propose a novel CRN method based on\ndiversified covert attention patterns, DCA-CRN, which combines the advantages\nof CRNs in capturing local spatio-temporal features and sequence change\npatterns, as well as Transformers in learning global and high-order correlation\nfeatures. Experimental results on the ADNI and ADHD-200 datasets demonstrate\nthe prediction performance and generalization ability of our proposed method.",
    "published": "2022-11-19T02:13:21Z",
    "updated": "2023-09-04T12:05:36Z",
    "authors": [
      "Zhixiang Zhang",
      "Biao Jie",
      "Zhengdong Wang",
      "Jie Zhou",
      "Yang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.12292v3",
    "title": "Exemplar-free Continual Learning of Vision Transformers via Gated\n  Class-Attention and Cascaded Feature Drift Compensation",
    "summary": "We propose a new method for exemplar-free class incremental training of ViTs.\nThe main challenge of exemplar-free continual learning is maintaining\nplasticity of the learner without causing catastrophic forgetting of previously\nlearned tasks. This is often achieved via exemplar replay which can help\nrecalibrate previous task classifiers to the feature drift which occurs when\nlearning new tasks. Exemplar replay, however, comes at the cost of retaining\nsamples from previous tasks which for many applications may not be possible. To\naddress the problem of continual ViT training, we first propose gated\nclass-attention to minimize the drift in the final ViT transformer block. This\nmask-based gating is applied to class-attention mechanism of the last\ntransformer block and strongly regulates the weights crucial for previous\ntasks. Importantly, gated class-attention does not require the task-ID during\ninference, which distinguishes it from other parameter isolation methods.\nSecondly, we propose a new method of feature drift compensation that\naccommodates feature drift in the backbone when learning new tasks. The\ncombination of gated class-attention and cascaded feature drift compensation\nallows for plasticity towards new tasks while limiting forgetting of previous\nones. Extensive experiments performed on CIFAR-100, Tiny-ImageNet and\nImageNet100 demonstrate that our exemplar-free method obtains competitive\nresults when compared to rehearsal based ViT methods.",
    "published": "2022-11-22T14:13:15Z",
    "updated": "2023-07-27T08:29:15Z",
    "authors": [
      "Marco Cotogni",
      "Fei Yang",
      "Claudio Cusano",
      "Andrew D. Bagdanov",
      "Joost van de Weijer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.13897v2",
    "title": "AFR-Net: Attention-Driven Fingerprint Recognition Network",
    "summary": "The use of vision transformers (ViT) in computer vision is increasing due to\nlimited inductive biases (e.g., locality, weight sharing, etc.) and increased\nscalability compared to other deep learning methods. This has led to some\ninitial studies on the use of ViT for biometric recognition, including\nfingerprint recognition. In this work, we improve on these initial studies for\ntransformers in fingerprint recognition by i.) evaluating additional\nattention-based architectures, ii.) scaling to larger and more diverse training\nand evaluation datasets, and iii.) combining the complimentary representations\nof attention-based and CNN-based embeddings for improved state-of-the-art\n(SOTA) fingerprint recognition (both authentication and identification). Our\ncombined architecture, AFR-Net (Attention-Driven Fingerprint Recognition\nNetwork), outperforms several baseline transformer and CNN-based models,\nincluding a SOTA commercial fingerprint system, Verifinger v12.3, across\nintra-sensor, cross-sensor, and latent to rolled fingerprint matching datasets.\nAdditionally, we propose a realignment strategy using local embeddings\nextracted from intermediate feature maps within the networks to refine the\nglobal embeddings in low certainty situations, which boosts the overall\nrecognition accuracy significantly across each of the models. This realignment\nstrategy requires no additional training and can be applied as a wrapper to any\nexisting deep learning network (including attention-based, CNN-based, or both)\nto boost its performance.",
    "published": "2022-11-25T05:10:39Z",
    "updated": "2022-12-03T20:28:36Z",
    "authors": [
      "Steven A. Grosz",
      "Anil K. Jain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.16368v1",
    "title": "DBA: Efficient Transformer with Dynamic Bilinear Low-Rank Attention",
    "summary": "Many studies have been conducted to improve the efficiency of Transformer\nfrom quadric to linear. Among them, the low-rank-based methods aim to learn the\nprojection matrices to compress the sequence length. However, the projection\nmatrices are fixed once they have been learned, which compress sequence length\nwith dedicated coefficients for tokens in the same position. Adopting such\ninput-invariant projections ignores the fact that the most informative part of\na sequence varies from sequence to sequence, thus failing to preserve the most\nuseful information that lies in varied positions. In addition, previous\nefficient Transformers only focus on the influence of sequence length while\nneglecting the effect of hidden state dimension. To address the aforementioned\nproblems, we present an efficient yet effective attention mechanism, namely the\nDynamic Bilinear Low-Rank Attention (DBA), which compresses the sequence length\nby input-sensitive dynamic projection matrices and achieves linear time and\nspace complexity by jointly optimizing the sequence length and hidden state\ndimension while maintaining state-of-the-art performance. Specifically, we\nfirst theoretically demonstrate that the sequence length can be compressed\nnon-destructively from a novel perspective of information theory, with\ncompression matrices dynamically determined by the input sequence. Furthermore,\nwe show that the hidden state dimension can be approximated by extending the\nJohnson-Lindenstrauss lemma, optimizing the attention in bilinear form.\nTheoretical analysis shows that DBA is proficient in capturing high-order\nrelations in cross-attention problems. Experiments over tasks with diverse\nsequence length conditions show that DBA achieves state-of-the-art performance\ncompared with various strong baselines while maintaining less memory\nconsumption with higher speed.",
    "published": "2022-11-24T03:06:36Z",
    "updated": "2022-11-24T03:06:36Z",
    "authors": [
      "Bosheng Qin",
      "Juncheng Li",
      "Siliang Tang",
      "Yueting Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.13510v2",
    "title": "3D Former: Monocular Scene Reconstruction with 3D SDF Transformers",
    "summary": "Monocular scene reconstruction from posed images is challenging due to the\ncomplexity of a large environment. Recent volumetric methods learn to directly\npredict the TSDF volume and have demonstrated promising results in this task.\nHowever, most methods focus on how to extract and fuse the 2D features to a 3D\nfeature volume, but none of them improve the way how the 3D volume is\naggregated. In this work, we propose an SDF transformer network, which replaces\nthe role of 3D CNN for better 3D feature aggregation. To reduce the explosive\ncomputation complexity of the 3D multi-head attention, we propose a sparse\nwindow attention module, where the attention is only calculated between the\nnon-empty voxels within a local window. Then a top-down-bottom-up 3D attention\nnetwork is built for 3D feature aggregation, where a dilate-attention structure\nis proposed to prevent geometry degeneration, and two global modules are\nemployed to equip with global receptive fields. The experiments on multiple\ndatasets show that this 3D transformer network generates a more accurate and\ncomplete reconstruction, which outperforms previous methods by a large margin.\nRemarkably, the mesh accuracy is improved by 41.8%, and the mesh completeness\nis improved by 25.3% on the ScanNet dataset. Project page:\nhttps://weihaosky.github.io/sdfformer.",
    "published": "2023-01-31T09:54:20Z",
    "updated": "2023-03-09T10:08:19Z",
    "authors": [
      "Weihao Yuan",
      "Xiaodong Gu",
      "Heng Li",
      "Zilong Dong",
      "Siyu Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.03112v2",
    "title": "Calibrating Undisciplined Over-Smoothing in Transformer for Weakly\n  Supervised Semantic Segmentation",
    "summary": "Weakly supervised semantic segmentation (WSSS) has recently attracted\nconsiderable attention because it requires fewer annotations than fully\nsupervised approaches, making it especially promising for large-scale image\nsegmentation tasks. Although many vision transformer-based methods leverage\nself-attention affinity matrices to refine Class Activation Maps (CAMs), they\noften treat each layer's affinity equally and thus introduce considerable\nbackground noise at deeper layers, where attention tends to converge\nexcessively on certain tokens (i.e., over-smoothing). We observe that this\ndeep-level attention naturally converges on a subset of tokens, yet unregulated\nquery-key affinity can generate unpredictable activation patterns\n(undisciplined over-smoothing), adversely affecting CAM accuracy. To address\nthese limitations, we propose an Adaptive Re-Activation Mechanism (AReAM),\nwhich exploits shallow-level affinity to guide deeper-layer convergence in an\nentropy-aware manner, thereby suppressing background noise and re-activating\ncrucial semantic regions in the CAMs. Experiments on two commonly used datasets\ndemonstrate that AReAM substantially improves segmentation performance compared\nwith existing WSSS methods, reducing noise while sharpening focus on relevant\nsemantic regions. Overall, this work underscores the importance of controlling\ndeep-level attention to mitigate undisciplined over-smoothing, introduces an\nentropy-aware mechanism that harmonizes shallow and deep-level affinities, and\nprovides a refined approach to enhance transformer-based WSSS accuracy by\nre-activating CAMs.",
    "published": "2023-05-04T19:11:33Z",
    "updated": "2025-05-29T12:31:57Z",
    "authors": [
      "Lechao Cheng",
      "Zerun Liu",
      "Jingxuan He",
      "Chaowei Fang",
      "Dingwen Zhang",
      "Meng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.14581v1",
    "title": "Explainable Techniques for Analyzing Flow Cytometry Cell Transformers",
    "summary": "Explainability for Deep Learning Models is especially important for clinical\napplications, where decisions of automated systems have far-reaching\nconsequences.\n  While various post-hoc explainable methods, such as attention visualization\nand saliency maps, already exist for common data modalities, including natural\nlanguage and images, little work has been done to adapt them to the modality of\nFlow CytoMetry (FCM) data.\n  In this work, we evaluate the usage of a transformer architecture called\nReluFormer that ease attention visualization as well as we propose a gradient-\nand an attention-based visualization technique tailored for FCM. We\nqualitatively evaluate the visualization techniques for cell classification and\npolygon regression on pediatric Acute Lymphoblastic Leukemia (ALL) FCM samples.\nThe results outline the model's decision process and demonstrate how to utilize\nthe proposed techniques to inspect the trained model. The gradient-based\nvisualization not only identifies cells that are most significant for a\nparticular prediction but also indicates the directions in the FCM feature\nspace in which changes have the most impact on the prediction. The attention\nvisualization provides insights on the transformer's decision process when\nhandling FCM data. We show that different attention heads specialize by\nattending to different biologically meaningful sub-populations in the data,\neven though the model retrieved solely supervised binary classification signals\nduring training.",
    "published": "2023-07-27T02:03:52Z",
    "updated": "2023-07-27T02:03:52Z",
    "authors": [
      "Florian Kowarsch",
      "Lisa Weijler",
      "FLorian Kleber",
      "Matthias WÃ¶dlinger",
      "Michael Reiter",
      "Margarita Maurer-Granofszky",
      "Michael Dworzak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.01310v1",
    "title": "ExMobileViT: Lightweight Classifier Extension for Mobile Vision\n  Transformer",
    "summary": "The paper proposes an efficient structure for enhancing the performance of\nmobile-friendly vision transformer with small computational overhead. The\nvision transformer (ViT) is very attractive in that it reaches outperforming\nresults in image classification, compared to conventional convolutional neural\nnetworks (CNNs). Due to its need of high computational resources,\nMobileNet-based ViT models such as MobileViT-S have been developed. However,\ntheir performance cannot reach the original ViT model. The proposed structure\nrelieves the above weakness by storing the information from early attention\nstages and reusing it in the final classifier. This paper is motivated by the\nidea that the data itself from early attention stages can have important\nmeaning for the final classification. In order to reuse the early information\nin attention stages, the average pooling results of various scaled features\nfrom early attention stages are used to expand channels in the fully-connected\nlayer of the final classifier. It is expected that the inductive bias\nintroduced by the averaged features can enhance the final performance. Because\nthe proposed structure only needs the average pooling of features from the\nattention stages and channel expansions in the final classifier, its\ncomputational and storage overheads are very small, keeping the benefits of\nlow-cost MobileNet-based ViT (MobileViT). Compared with the original MobileViTs\non the ImageNet dataset, the proposed ExMobileViT has noticeable accuracy\nenhancements, having only about 5% additional parameters.",
    "published": "2023-09-04T01:51:33Z",
    "updated": "2023-09-04T01:51:33Z",
    "authors": [
      "Gyeongdong Yang",
      "Yungwook Kwon",
      "Hyunjin Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.05375v2",
    "title": "Toward a Deeper Understanding: RetNet Viewed through Convolution",
    "summary": "The success of Vision Transformer (ViT) has been widely reported on a wide\nrange of image recognition tasks. ViT can learn global dependencies superior to\nCNN, yet CNN's inherent locality can substitute for expensive training\nresources. Recently, the outstanding performance of RetNet in the field of\nlanguage modeling has garnered attention, surpassing that of the Transformer\nwith explicit local modeling, shifting researchers' focus towards Transformers\nin the CV field. This paper investigates the effectiveness of RetNet from a CNN\nperspective and presents a variant of RetNet tailored to the visual domain.\nSimilar to RetNet we improves ViT's local modeling by applying a weight mask on\nthe original self-attention matrix. A straightforward way to locally adapt the\nself-attention matrix can be realized by an element-wise learnable weight mask\n(ELM), for which our preliminary results show promising results. However, the\nelement-wise simple learnable weight mask not only induces a non-trivial\nadditional parameter overhead but also increases the optimization complexity.\nTo this end, this work proposes a novel Gaussian mixture mask (GMM) in which\none mask only has two learnable parameters and it can be conveniently used in\nany ViT variants whose attention mechanism allows the use of masks.\nExperimental results on multiple small datasets demonstrate that the\neffectiveness of our proposed Gaussian mask for boosting ViTs for free (almost\nzero additional parameter or computation cost). Our code can be publicly\navailable at https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention.",
    "published": "2023-09-11T10:54:22Z",
    "updated": "2023-10-29T07:08:04Z",
    "authors": [
      "Chenghao Li",
      "Chaoning Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.01655v3",
    "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
    "summary": "The quadratic time and memory complexity inherent to self-attention\nmechanisms, with respect to sequence length, presents a critical computational\nbottleneck in the training and deployment of large-scale Transformer-based\nlanguage models. Recent theoretical results indicate the intractability of\nsub-quadratic softmax attention approximation under reasonable complexity\nassumptions. This paper addresses this challenge by first demonstrating that\npolynomial attention with high degree can effectively replace softmax without\nsacrificing model quality. Next, we develop polynomial sketching techniques\nfrom numerical linear algebra to achieve linear-time polynomial attention with\napproximation guarantees. Crucially, our approach achieves this speedup without\nrequiring the sparsification of attention matrices. We also present a\nblock-based algorithm to apply causal masking efficiently. Combining these\ntechniques, we provide \\emph{PolySketchFormer}, a practical linear-time\nTransformer architecture for language modeling that offers provable guarantees.\n  We validate PolySketchFormer empirically by training language models capable\nof handling long contexts. These experiments utilize both synthetic and\nreal-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context\nlengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in\ntraining compared to FlashAttention, with no observed degradation in quality\nacross our experiments.",
    "published": "2023-10-02T21:39:04Z",
    "updated": "2024-03-17T23:35:24Z",
    "authors": [
      "Praneeth Kacham",
      "Vahab Mirrokni",
      "Peilin Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.13502v1",
    "title": "Bitformer: An efficient Transformer with bitwise operation-based\n  attention for Big Data Analytics at low-cost low-precision devices",
    "summary": "In the current landscape of large models, the Transformer stands as a\ncornerstone, playing a pivotal role in shaping the trajectory of modern models.\nHowever, its application encounters challenges attributed to the substantial\ncomputational intricacies intrinsic to its attention mechanism. Moreover, its\nreliance on high-precision floating-point operations presents specific hurdles,\nparticularly evident in computation-intensive scenarios such as edge computing\nenvironments. These environments, characterized by resource-constrained devices\nand a preference for lower precision, necessitate innovative solutions.\n  To tackle the exacting data processing demands posed by edge devices, we\nintroduce the Bitformer model, an inventive extension of the Transformer\nparadigm. Central to this innovation is a novel attention mechanism that\nadeptly replaces conventional floating-point matrix multiplication with bitwise\noperations. This strategic substitution yields dual advantages. Not only does\nit maintain the attention mechanism's prowess in capturing intricate long-range\ninformation dependencies, but it also orchestrates a profound reduction in the\ncomputational complexity inherent in the attention operation. The transition\nfrom an $O(n^2d)$ complexity, typical of floating-point operations, to an\n$O(n^2T)$ complexity characterizing bitwise operations, substantiates this\nadvantage. Notably, in this context, the parameter $T$ remains markedly smaller\nthan the conventional dimensionality parameter $d$.\n  The Bitformer model in essence endeavors to reconcile the indomitable\nrequirements of modern computing landscapes with the constraints posed by edge\ncomputing scenarios. By forging this innovative path, we bridge the gap between\nhigh-performing models and resource-scarce environments, thus unveiling a\npromising trajectory for further advancements in the field.",
    "published": "2023-11-22T16:20:24Z",
    "updated": "2023-11-22T16:20:24Z",
    "authors": [
      "Gaoxiang Duan",
      "Junkai Zhang",
      "Xiaoying Zheng",
      "Yongxin Zhu",
      "Victor Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.17893v2",
    "title": "Betrayed by Attention: A Simple yet Effective Approach for\n  Self-supervised Video Object Segmentation",
    "summary": "In this paper, we propose a simple yet effective approach for self-supervised\nvideo object segmentation (VOS). Our key insight is that the inherent\nstructural dependencies present in DINO-pretrained Transformers can be\nleveraged to establish robust spatio-temporal correspondences in videos.\nFurthermore, simple clustering on this correspondence cue is sufficient to\nyield competitive segmentation results. Previous self-supervised VOS techniques\nmajorly resort to auxiliary modalities or utilize iterative slot attention to\nassist in object discovery, which restricts their general applicability and\nimposes higher computational requirements. To deal with these challenges, we\ndevelop a simplified architecture that capitalizes on the emerging objectness\nfrom DINO-pretrained Transformers, bypassing the need for additional modalities\nor slot attention. Specifically, we first introduce a single spatio-temporal\nTransformer block to process the frame-wise DINO features and establish\nspatio-temporal dependencies in the form of self-attention. Subsequently,\nutilizing these attention maps, we implement hierarchical clustering to\ngenerate object segmentation masks. To train the spatio-temporal block in a\nfully self-supervised manner, we employ semantic and dynamic motion consistency\ncoupled with entropy normalization. Our method demonstrates state-of-the-art\nperformance across multiple unsupervised VOS benchmarks and particularly excels\nin complex real-world multi-object video segmentation tasks such as\nDAVIS-17-Unsupervised and YouTube-VIS-19. The code and model checkpoints will\nbe released at https://github.com/shvdiwnkozbw/SSL-UVOS.",
    "published": "2023-11-29T18:47:17Z",
    "updated": "2024-07-08T05:33:12Z",
    "authors": [
      "Shuangrui Ding",
      "Rui Qian",
      "Haohang Xu",
      "Dahua Lin",
      "Hongkai Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.00452v3",
    "title": "Multi-scale cross-attention transformer encoder for event classification",
    "summary": "We deploy an advanced Machine Learning (ML) environment, leveraging a\nmulti-scale cross-attention encoder for event classification, towards the\nidentification of the $gg\\to H\\to hh\\to b\\bar b b\\bar b$ process at the High\nLuminosity Large Hadron Collider (HL-LHC), where $h$ is the discovered Standard\nModel (SM)-like Higgs boson and $H$ a heavier version of it (with $m_H>2m_h$).\nIn the ensuing boosted Higgs regime, the final state consists of two fat jets.\nOur multi-modal network can extract information from the jet substructure and\nthe kinematics of the final state particles through self-attention transformer\nlayers. The diverse learned information is subsequently integrated to improve\nclassification performance using an additional transformer encoder with\ncross-attention heads. We ultimately prove that our approach surpasses in\nperformance current alternative methods used to establish sensitivity to this\nprocess, whether solely based on kinematic analysis or else on a combination of\nthis with mainstream ML approaches. Then, we employ various interpretive\nmethods to evaluate the network results, including attention map analysis and\nvisual representation of Gradient-weighted Class Activation Mapping (Grad-CAM).\nFinally, we note that the proposed network is generic and can be applied to\nanalyse any process carrying information at different scales. Our code is\npublicly available for generic use.",
    "published": "2023-12-31T11:03:28Z",
    "updated": "2024-02-15T02:43:00Z",
    "authors": [
      "A. Hammad",
      "S. Moretti",
      "M. Nojiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.14727v1",
    "title": "SparseCoder: Identifier-Aware Sparse Transformer for File-Level Code\n  Summarization",
    "summary": "Code summarization aims to generate natural language descriptions of source\ncode, facilitating programmers to understand and maintain it rapidly. While\nprevious code summarization efforts have predominantly focused on method-level,\nthis paper studies file-level code summarization, which can assist programmers\nin understanding and maintaining large source code projects. Unlike\nmethod-level code summarization,file-level code summarization typically\ninvolves long source code within a single file, which makes it challenging for\nTransformer-based models to understand the code semantics for the maximum input\nlength of these models is difficult to set to a large number that can handle\nlong code input well, due to the quadratic scaling of computational complexity\nwith the input sequence length. To address this challenge, we propose\nSparseCoder, an identifier-aware sparse transformer for effectively handling\nlong code sequences. Specifically, the SparseCoder employs a sliding window\nmechanism for self-attention to model short-term dependencies and leverages the\nstructure message of code to capture long-term dependencies among source code\nidentifiers by introducing two types of sparse attention patterns named global\nand identifier attention. To evaluate the performance of SparseCoder, we\nconstruct a new dataset FILE-CS for file-level code summarization in Python.\nExperimental results show that our SparseCoder model achieves state-of-the-art\nperformance compared with other pre-trained models, including full\nself-attention and sparse models. Additionally, our model has low memory\noverhead and achieves comparable performance with models using full\nself-attention mechanism.",
    "published": "2024-01-26T09:23:27Z",
    "updated": "2024-01-26T09:23:27Z",
    "authors": [
      "Yanlin Wang",
      "Yanxian Huang",
      "Daya Guo",
      "Hongyu Zhang",
      "Zibin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.10793v3",
    "title": "An end-to-end attention-based approach for learning on graphs",
    "summary": "There has been a recent surge in transformer-based architectures for learning\non graphs, mainly motivated by attention as an effective learning mechanism and\nthe desire to supersede handcrafted operators characteristic of message passing\nschemes. However, concerns over their empirical effectiveness, scalability, and\ncomplexity of the pre-processing steps have been raised, especially in relation\nto much simpler graph neural networks that typically perform on par with them\nacross a wide range of benchmarks. To tackle these shortcomings, we consider\ngraphs as sets of edges and propose a purely attention-based approach\nconsisting of an encoder and an attention pooling mechanism. The encoder\nvertically interleaves masked and vanilla self-attention modules to learn an\neffective representations of edges, while allowing for tackling possible\nmisspecifications in input graphs. Despite its simplicity, the approach\noutperforms fine-tuned message passing baselines and recently proposed\ntransformer-based methods on more than 70 node and graph-level tasks, including\nchallenging long-range benchmarks. Moreover, we demonstrate state-of-the-art\nperformance across different tasks, ranging from molecular to vision graphs,\nand heterophilous node classification. The approach also outperforms graph\nneural networks and transformers in transfer learning settings, and scales much\nbetter than alternatives with a similar performance level or expressive power.",
    "published": "2024-02-16T16:20:11Z",
    "updated": "2025-06-09T13:24:10Z",
    "authors": [
      "David Buterez",
      "Jon Paul Janet",
      "Dino Oglic",
      "Pietro Lio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.14471v2",
    "title": "S2LIC: Learned Image Compression with the SwinV2 Block, Adaptive\n  Channel-wise and Global-inter Attention Context",
    "summary": "Recently, deep learning technology has been successfully applied in the field\nof image compression, leading to superior rate-distortion performance. It is\ncrucial to design an effective and efficient entropy model to estimate the\nprobability distribution of the latent representation. However, the majority of\nentropy models primarily focus on one-dimensional correlation processing\nbetween channel and spatial information. In this paper, we propose an Adaptive\nChannel-wise and Global-inter attention Context (ACGC) entropy model, which can\nefficiently achieve dual feature aggregation in both inter-slice and intraslice\ncontexts. Specifically, we divide the latent representation into different\nslices and then apply the ACGC model in a parallel checkerboard context to\nachieve faster decoding speed and higher rate-distortion performance. In order\nto capture redundant global features across different slices, we utilize\ndeformable attention in adaptive global-inter attention to dynamically refine\nthe attention weights based on the actual spatial relationships and context.\nFurthermore, in the main transformation structure, we propose a\nhigh-performance S2LIC model. We introduce the residual SwinV2 Transformer\nmodel to capture global feature information and utilize a dense block network\nas the feature enhancement module to improve the nonlinear representation of\nthe image within the transformation structure. Experimental results demonstrate\nthat our method achieves faster encoding and decoding speeds and outperforms\nVTM-17.1 and some recent learned image compression methods in both PSNR and\nMS-SSIM metrics.",
    "published": "2024-03-21T15:18:21Z",
    "updated": "2024-07-02T10:04:44Z",
    "authors": [
      "Yongqiang Wang",
      "Haisheng Fu",
      "Qi Cao",
      "Shang Wang",
      "Zhenjiao Chen",
      "Feng Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.03952v1",
    "title": "HAFFormer: A Hierarchical Attention-Free Framework for Alzheimer's\n  Disease Detection From Spontaneous Speech",
    "summary": "Automatically detecting Alzheimer's Disease (AD) from spontaneous speech\nplays an important role in its early diagnosis. Recent approaches highly rely\non the Transformer architectures due to its efficiency in modelling long-range\ncontext dependencies. However, the quadratic increase in computational\ncomplexity associated with self-attention and the length of audio poses a\nchallenge when deploying such models on edge devices. In this context, we\nconstruct a novel framework, namely Hierarchical Attention-Free Transformer\n(HAFFormer), to better deal with long speech for AD detection. Specifically, we\nemploy an attention-free module of Multi-Scale Depthwise Convolution to replace\nthe self-attention and thus avoid the expensive computation, and a GELU-based\nGated Linear Unit to replace the feedforward layer, aiming to automatically\nfilter out the redundant information. Moreover, we design a hierarchical\nstructure to force it to learn a variety of information grains, from the frame\nlevel to the dialogue level. By conducting extensive experiments on the\nADReSS-M dataset, the introduced HAFFormer can achieve competitive results\n(82.6% accuracy) with other recent work, but with significant computational\ncomplexity and model size reduction compared to the standard Transformer. This\nshows the efficiency of HAFFormer in dealing with long audio for AD detection.",
    "published": "2024-05-07T02:19:16Z",
    "updated": "2024-05-07T02:19:16Z",
    "authors": [
      "Zhongren Dong",
      "Zixing Zhang",
      "Weixiang Xu",
      "Jing Han",
      "Jianjun Ou",
      "BjÃ¶rn W. Schuller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.19605v2",
    "title": "Look Hear: Gaze Prediction for Speech-directed Human Attention",
    "summary": "For computer systems to effectively interact with humans using spoken\nlanguage, they need to understand how the words being generated affect the\nusers' moment-by-moment attention. Our study focuses on the incremental\nprediction of attention as a person is seeing an image and hearing a referring\nexpression defining the object in the scene that should be fixated by gaze. To\npredict the gaze scanpaths in this incremental object referral task, we\ndeveloped the Attention in Referral Transformer model or ART, which predicts\nthe human fixations spurred by each word in a referring expression. ART uses a\nmultimodal transformer encoder to jointly learn gaze behavior and its\nunderlying grounding tasks, and an autoregressive transformer decoder to\npredict, for each word, a variable number of fixations based on fixation\nhistory. To train ART, we created RefCOCO-Gaze, a large-scale dataset of 19,738\nhuman gaze scanpaths, corresponding to 2,094 unique image-expression pairs,\nfrom 220 participants performing our referral task. In our quantitative and\nqualitative analyses, ART not only outperforms existing methods in scanpath\nprediction, but also appears to capture several human attention patterns, such\nas waiting, scanning, and verification.",
    "published": "2024-07-28T22:35:08Z",
    "updated": "2024-09-10T01:34:13Z",
    "authors": [
      "Sounak Mondal",
      "Seoyoung Ahn",
      "Zhibo Yang",
      "Niranjan Balasubramanian",
      "Dimitris Samaras",
      "Gregory Zelinsky",
      "Minh Hoai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.08108v1",
    "title": "Unsupervised Part Discovery via Dual Representation Alignment",
    "summary": "Object parts serve as crucial intermediate representations in various\ndownstream tasks, but part-level representation learning still has not received\nas much attention as other vision tasks. Previous research has established that\nVision Transformer can learn instance-level attention without labels,\nextracting high-quality instance-level representations for boosting downstream\ntasks. In this paper, we achieve unsupervised part-specific attention learning\nusing a novel paradigm and further employ the part representations to improve\npart discovery performance. Specifically, paired images are generated from the\nsame image with different geometric transformations, and multiple part\nrepresentations are extracted from these paired images using a novel module,\nnamed PartFormer. These part representations from the paired images are then\nexchanged to improve geometric transformation invariance. Subsequently, the\npart representations are aligned with the feature map extracted by a feature\nmap encoder, achieving high similarity with the pixel representations of the\ncorresponding part regions and low similarity in irrelevant regions. Finally,\nthe geometric and semantic constraints are applied to the part representations\nthrough the intermediate results in alignment for part-specific attention\nlearning, encouraging the PartFormer to focus locally and the part\nrepresentations to explicitly include the information of the corresponding\nparts. Moreover, the aligned part representations can further serve as a series\nof reliable detectors in the testing phase, predicting pixel masks for part\ndiscovery. Extensive experiments are carried out on four widely used datasets,\nand our results demonstrate that the proposed method achieves competitive\nperformance and robustness due to its part-specific attention.",
    "published": "2024-08-15T12:11:20Z",
    "updated": "2024-08-15T12:11:20Z",
    "authors": [
      "Jiahao Xia",
      "Wenjian Huang",
      "Min Xu",
      "Jianguo Zhang",
      "Haimin Zhang",
      "Ziyu Sheng",
      "Dong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.13634v1",
    "title": "Enhanced Astronomical Source Classification with Integration of\n  Attention Mechanisms and Vision Transformers",
    "summary": "Accurate classification of celestial objects is essential for advancing our\nunderstanding of the universe. MargNet is a recently developed deep\nlearning-based classifier applied to SDSS DR16 dataset to segregate stars,\nquasars, and compact galaxies using photometric data. MargNet utilizes a\nstacked architecture, combining a Convolutional Neural Network (CNN) for image\nmodelling and an Artificial Neural Network (ANN) for modelling photometric\nparameters. In this study, we propose enhancing MargNet's performance by\nincorporating attention mechanisms and Vision Transformer (ViT)-based models\nfor processing image data. The attention mechanism allows the model to focus on\nrelevant features and capture intricate patterns within images, effectively\ndistinguishing between different classes of celestial objects. Additionally, we\nleverage ViTs, a transformer-based deep learning architecture renowned for\nexceptional performance in image classification tasks. We enhance the model's\nunderstanding of complex astronomical images by utilizing ViT's ability to\ncapture global dependencies and contextual information. Our approach uses a\ncurated dataset comprising 240,000 compact and 150,000 faint objects. The\nmodels learn classification directly from the data, minimizing human\nintervention. Furthermore, we explore ViT as a hybrid architecture that uses\nphotometric features and images together as input to predict astronomical\nobjects. Our results demonstrate that the proposed attention mechanism\naugmented CNN in MargNet marginally outperforms the traditional MargNet and the\nproposed ViT-based MargNet models. Additionally, the ViT-based hybrid model\nemerges as the most lightweight and easy-to-train model with classification\naccuracy similar to that of the best-performing attention-enhanced MargNet.",
    "published": "2024-08-24T17:28:45Z",
    "updated": "2024-08-24T17:28:45Z",
    "authors": [
      "Srinadh Reddy Bhavanam",
      "Sumohana S. Channappayya",
      "P. K. Srijith",
      "Shantanu Desai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.04803v5",
    "title": "Cross-attention Inspired Selective State Space Models for Target Sound\n  Extraction",
    "summary": "The Transformer model, particularly its cross-attention module, is widely\nused for feature fusion in target sound extraction which extracts the signal of\ninterest based on given clues. Despite its effectiveness, this approach suffers\nfrom low computational efficiency. Recent advancements in state space models,\nnotably the latest work Mamba, have shown comparable performance to\nTransformer-based methods while significantly reducing computational complexity\nin various tasks. However, Mamba's applicability in target sound extraction is\nlimited due to its inability to capture dependencies between different\nsequences as the cross-attention does. In this paper, we propose CrossMamba for\ntarget sound extraction, which leverages the hidden attention mechanism of\nMamba to compute dependencies between the given clues and the audio mixture.\nThe calculation of Mamba can be divided to the query, key and value. We utilize\nthe clue to generate the query and the audio mixture to derive the key and\nvalue, adhering to the principle of the cross-attention mechanism in\nTransformers. Experimental results from two representative target sound\nextraction methods validate the efficacy of the proposed CrossMamba.",
    "published": "2024-09-07T12:01:08Z",
    "updated": "2025-06-25T09:10:23Z",
    "authors": [
      "Donghang Wu",
      "Yiwen Wang",
      "Xihong Wu",
      "Tianshu Qu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.12034v1",
    "title": "A Survey on Deep Tabular Learning",
    "summary": "Tabular data, widely used in industries like healthcare, finance, and\ntransportation, presents unique challenges for deep learning due to its\nheterogeneous nature and lack of spatial structure. This survey reviews the\nevolution of deep learning models for tabular data, from early fully connected\nnetworks (FCNs) to advanced architectures like TabNet, SAINT, TabTranSELU, and\nMambaNet. These models incorporate attention mechanisms, feature embeddings,\nand hybrid architectures to address tabular data complexities. TabNet uses\nsequential attention for instance-wise feature selection, improving\ninterpretability, while SAINT combines self-attention and intersample attention\nto capture complex interactions across features and data points, both advancing\nscalability and reducing computational overhead. Hybrid architectures such as\nTabTransformer and FT-Transformer integrate attention mechanisms with\nmulti-layer perceptrons (MLPs) to handle categorical and numerical data, with\nFT-Transformer adapting transformers for tabular datasets. Research continues\nto balance performance and efficiency for large datasets. Graph-based models\nlike GNN4TDL and GANDALF combine neural networks with decision trees or graph\nstructures, enhancing feature representation and mitigating overfitting in\nsmall datasets through advanced regularization techniques. Diffusion-based\nmodels like the Tabular Denoising Diffusion Probabilistic Model (TabDDPM)\ngenerate synthetic data to address data scarcity, improving model robustness.\nSimilarly, models like TabPFN and Ptab leverage pre-trained language models,\nincorporating transfer learning and self-supervised techniques into tabular\ntasks. This survey highlights key advancements and outlines future research\ndirections on scalability, generalization, and interpretability in diverse\ntabular data applications.",
    "published": "2024-10-15T20:08:08Z",
    "updated": "2024-10-15T20:08:08Z",
    "authors": [
      "Shriyank Somvanshi",
      "Subasish Das",
      "Syed Aaqib Javed",
      "Gian Antariksa",
      "Ahmed Hossain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.22649v2",
    "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
    "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
    "published": "2024-10-30T02:36:55Z",
    "updated": "2024-11-21T03:34:44Z",
    "authors": [
      "Aobo Liang",
      "Yan Sun",
      "Nadra Guizani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.06439v1",
    "title": "Local Attention Transformers for High-Detail Optical Flow Upsampling",
    "summary": "Most recent works on optical flow use convex upsampling as the last step to\nobtain high-resolution flow. In this work, we show and discuss several issues\nand limitations of this currently widely adopted convex upsampling approach. We\npropose a series of changes, in an attempt to resolve current issues. First, we\npropose to decouple the weights for the final convex upsampler, making it\neasier to find the correct convex combination. For the same reason, we also\nprovide extra contextual features to the convex upsampler. Then, we increase\nthe convex mask size by using an attention-based alternative convex upsampler;\nTransformers for Convex Upsampling. This upsampler is based on the observation\nthat convex upsampling can be reformulated as attention, and we propose to use\nlocal attention masks as a drop-in replacement for convex masks to increase the\nmask size. We provide empirical evidence that a larger mask size increases the\nlikelihood of the existence of the convex combination. Lastly, we propose an\nalternative training scheme to remove bilinear interpolation artifacts from the\nmodel output. Our proposed ideas could theoretically be applied to almost every\ncurrent state-of-the-art optical flow architecture. On the FlyingChairs +\nFlyingThings3D training setting we reduce the Sintel Clean training\nend-point-error of RAFT from 1.42 to 1.26, GMA from 1.31 to 1.18, and that of\nFlowFormer from 0.94 to 0.90, by solely adapting the convex upsampler.",
    "published": "2024-12-09T12:30:59Z",
    "updated": "2024-12-09T12:30:59Z",
    "authors": [
      "Alexander Gielisse",
      "Nergis TÃ¶men",
      "Jan van Gemert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01776v2",
    "title": "Sparse VideoGen: Accelerating Video Diffusion Transformers with\n  Spatial-Temporal Sparsity",
    "summary": "Diffusion Transformers (DiTs) dominate video generation but their high\ncomputational cost severely limits real-world applicability, usually requiring\ntens of minutes to generate a few seconds of video even on high-performance\nGPUs. This inefficiency primarily arises from the quadratic computational\ncomplexity of 3D Full Attention with respect to the context length. In this\npaper, we propose a training-free framework termed Sparse VideoGen (SVG) that\nleverages the inherent sparsity in 3D Full Attention to boost inference\nefficiency. We reveal that the attention heads can be dynamically classified\ninto two groups depending on distinct sparse patterns: (1) Spatial Head, where\nonly spatially-related tokens within each frame dominate the attention output,\nand (2) Temporal Head, where only temporally-related tokens across different\nframes dominate. Based on this insight, SVG proposes an online profiling\nstrategy to capture the dynamic sparse patterns and predicts the type of\nattention head. Combined with a novel hardware-efficient tensor layout\ntransformation and customized kernel implementations, SVG achieves up to 2.28x\nand 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively,\nwhile preserving generation quality. Our code is open-sourced and is available\nat https://github.com/svg-project/Sparse-VideoGen",
    "published": "2025-02-03T19:29:16Z",
    "updated": "2025-04-27T00:10:11Z",
    "authors": [
      "Haocheng Xi",
      "Shuo Yang",
      "Yilong Zhao",
      "Chenfeng Xu",
      "Muyang Li",
      "Xiuyu Li",
      "Yujun Lin",
      "Han Cai",
      "Jintao Zhang",
      "Dacheng Li",
      "Jianfei Chen",
      "Ion Stoica",
      "Kurt Keutzer",
      "Song Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.12344v1",
    "title": "Hardware-Software Co-Design for Accelerating Transformer Inference\n  Leveraging Compute-in-Memory",
    "summary": "Transformers have become the backbone of neural network architecture for most\nmachine learning applications. Their widespread use has resulted in multiple\nefforts on accelerating attention, the basic building block of transformers.\nThis paper tackles the challenges associated with accelerating attention\nthrough a hardware-software co-design approach while leveraging\ncompute-in-memory(CIM) architecture. In particular, our energy- and\narea-efficient CIM based accelerator, named HASTILY, aims to accelerate softmax\ncomputation, an integral operation in attention, and minimize their high\non-chip memory requirements that grows quadratically with input sequence\nlength. Our architecture consists of novel CIM units called unified compute and\nlookup modules(UCLMs) that integrate both lookup and multiply-accumulate\nfunctionality within the same SRAM array, incurring minimal area overhead over\nstandard CIM arrays. Designed in TSMC 65nm, UCLMs can be used to concurrently\nperform exponential and matrix-vector multiplication operations. Complementing\nthe proposed architecture, HASTILY features a fine-grained pipelining strategy\nfor scheduling both attention and feed-forward layers, to reduce the quadratic\ndependence on sequence length to linear dependence. Further, for fast softmax\ncomputation which involves computing the maxima and sum of exponential values,\nsuch operations are parallelized across multiple cores using reduce and gather\nstrategy. We evaluate our proposed architecture using a compiler tailored\ntowards attention computation and a standard cycle-level CIM simulator. Our\nevaluation shows end-to-end throughput(TOPS) improvement of 4.4x-9.8x and\n1.7x-5.9x over Nvidia A40 GPU and baseline CIM hardware, respectively, for BERT\nmodels with INT-8 precision. Additionally, it shows gains of 16x-36x in\nenergy-efficiency(TOPS/W) over A40 GPU and similar energy-efficiency as\nbaseline CIM hardware.",
    "published": "2025-02-17T22:16:34Z",
    "updated": "2025-02-17T22:16:34Z",
    "authors": [
      "Dong Eun Kim",
      "Tanvi Sharma",
      "Kaushik Roy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.18845v2",
    "title": "Sliding Window Attention Training for Efficient Large Language Models",
    "summary": "Recent advances in transformer-based Large Language Models (LLMs) have\ndemonstrated remarkable capabilities across various tasks. However, their\nquadratic computational complexity concerning sequence length remains a\nsignificant bottleneck for processing long documents. As a result, many efforts\nlike sparse attention and state space models have been proposed to improve the\nefficiency of LLMs over long sequences. Though effective, these approaches\ncompromise the performance or introduce structural complexity. This calls for a\nsimple yet efficient model that preserves the fundamental Transformer\narchitecture. To this end, we introduce SWAT, which enables efficient\nlong-context handling via Sliding Window Attention Training. This paper first\nattributes the inefficiency of Transformers to the attention sink phenomenon\nresulting from the high variance of softmax operation. Then, we replace softmax\nwith the sigmoid function and utilize a balanced ALiBi and Rotary Position\nEmbedding for efficient information compression and retention. Experiments\ndemonstrate that SWAT achieves SOTA performance compared with state-of-the-art\nlinear recurrent architectures on eight benchmarks. Code is available at\nhttps://github.com/Fzkuji/swat-attention.",
    "published": "2025-02-26T05:31:44Z",
    "updated": "2025-06-04T08:36:19Z",
    "authors": [
      "Zichuan Fu",
      "Wentao Song",
      "Yejing Wang",
      "Xian Wu",
      "Yefeng Zheng",
      "Yingying Zhang",
      "Derong Xu",
      "Xuetao Wei",
      "Tong Xu",
      "Xiangyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.09535v1",
    "title": "Evaluating Visual Explanations of Attention Maps for Transformer-based\n  Medical Imaging",
    "summary": "Although Vision Transformers (ViTs) have recently demonstrated superior\nperformance in medical imaging problems, they face explainability issues\nsimilar to previous architectures such as convolutional neural networks. Recent\nresearch efforts suggest that attention maps, which are part of decision-making\nprocess of ViTs can potentially address the explainability issue by identifying\nregions influencing predictions, especially in models pretrained with\nself-supervised learning. In this work, we compare the visual explanations of\nattention maps to other commonly used methods for medical imaging problems. To\ndo so, we employ four distinct medical imaging datasets that involve the\nidentification of (1) colonic polyps, (2) breast tumors, (3) esophageal\ninflammation, and (4) bone fractures and hardware implants. Through large-scale\nexperiments on the aforementioned datasets using various supervised and\nself-supervised pretrained ViTs, we find that although attention maps show\npromise under certain conditions and generally surpass GradCAM in\nexplainability, they are outperformed by transformer-specific interpretability\nmethods. Our findings indicate that the efficacy of attention maps as a method\nof interpretability is context-dependent and may be limited as they do not\nconsistently provide the comprehensive insights required for robust medical\ndecision-making.",
    "published": "2025-03-12T16:52:52Z",
    "updated": "2025-03-12T16:52:52Z",
    "authors": [
      "Minjae Chung",
      "Jong Bum Won",
      "Ganghyun Kim",
      "Yujin Kim",
      "Utku Ozbulak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.02277v2",
    "title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA)\n  Block for Improved Multi-Label Diagnosis Using Knowledge Distillation",
    "summary": "Medical imaging, particularly X-ray analysis, often involves detecting\nmultiple conditions simultaneously within a single scan, making multi-label\nclassification crucial for real-world clinical applications. We present the\nMedical X-ray Attention (MXA) block, a novel attention mechanism tailored\nspecifically to address the unique challenges of X-ray abnormality detection.\nThe MXA block enhances traditional Multi-Head Self Attention (MHSA) by\nintegrating a specialized module that efficiently captures both detailed local\ninformation and broader global context. To the best of our knowledge, this is\nthe first work to propose a task-specific attention mechanism for diagnosing\nchest X-rays, as well as to attempt multi-label classification using an\nEfficient Vision Transformer (EfficientViT). By embedding the MXA block within\nthe EfficientViT architecture and employing knowledge distillation, our\nproposed model significantly improves performance on the CheXpert dataset, a\nwidely used benchmark for multi-label chest X-ray abnormality detection. Our\napproach achieves an area under the curve (AUC) of 0.85, an absolute\nimprovement of 0.19 compared to our baseline model's AUC of 0.66, corresponding\nto a substantial approximate 233% relative improvement over random guessing\n(AUC = 0.5).",
    "published": "2025-04-03T04:55:42Z",
    "updated": "2025-05-18T05:07:13Z",
    "authors": [
      "Amit Rand",
      "Hadi Ibrahim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.06354v3",
    "title": "Multihead self-attention in cortico-thalamic circuits",
    "summary": "Both biological cortico-thalamic networks and artificial transformer networks\nuse canonical computations to perform a wide range of cognitive tasks. In this\nwork, we propose that the structure of cortico-thalamic circuits is well suited\nto realize a computation analogous to multihead self-attention, the main\nalgorithmic innovation of transformer networks. We assign distinct\ncomputational roles to superficial and deep pyramidal cells of the cortex:\nwhile superficial pyramidal cells maintain a key-value memory, deep pyramidal\ncells encode the current query, gain-modulated by the key-value memory in the\nsuperficial layer. We show that the structure of this computation matches the\nfine-grained structure of core and matrix projections from the thalamus to the\ncortex. We then suggest the parallel between one head of attention and a\ncortical area, and propose that a thalamo-cortico-thalamic pathway implements a\ncomputation akin to a multihead, unnormalized, linear self-attention block.\nCross-attention corresponds to the key-value memory of one cortical area being\nused for retrieval by the query in another cortical area. Finally, as a first\nstep towards a mechanistic theory of synaptic learning of cortical\ntransformers, we derive the formal gradients of a typical loss function with\nrespect to the parameters of such computation.",
    "published": "2025-04-08T18:04:05Z",
    "updated": "2025-08-10T20:48:42Z",
    "authors": [
      "Arno Granier",
      "Walter Senn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10352v1",
    "title": "SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with\n  Hamming Attention and $\\mathcal{O}(T)$ Complexity",
    "summary": "Spiking Neural Networks (SNNs) have shown competitive performance to\nArtificial Neural Networks (ANNs) in various vision tasks, while offering\nsuperior energy efficiency. However, existing SNN-based Transformers primarily\nfocus on single-image tasks, emphasizing spatial features while not effectively\nleveraging SNNs' efficiency in video-based vision tasks. In this paper, we\nintroduce SpikeVideoFormer, an efficient spike-driven video Transformer,\nfeaturing linear temporal complexity $\\mathcal{O}(T)$. Specifically, we design\na spike-driven Hamming attention (SDHA) which provides a theoretically guided\nadaptation from traditional real-valued attention to spike-driven attention.\nBuilding on SDHA, we further analyze various spike-driven space-time attention\ndesigns and identify an optimal scheme that delivers appealing performance for\nvideo tasks, while maintaining only linear temporal complexity. The\ngeneralization ability and efficiency of our model are demonstrated across\ndiverse downstream video tasks, including classification, human pose tracking,\nand semantic segmentation. Empirical results show our method achieves\nstate-of-the-art (SOTA) performance compared to existing SNN approaches, with\nover 15\\% improvement on the latter two tasks. Additionally, it matches the\nperformance of recent ANN-based methods while offering significant efficiency\ngains, achieving $\\times 16$, $\\times 10$ and $\\times 5$ improvements on the\nthree tasks. https://github.com/JimmyZou/SpikeVideoFormer",
    "published": "2025-05-15T14:43:35Z",
    "updated": "2025-05-15T14:43:35Z",
    "authors": [
      "Shihao Zou",
      "Qingfeng Li",
      "Wei Ji",
      "Jingjing Li",
      "Yongkui Yang",
      "Guoqi Li",
      "Chao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16463v3",
    "title": "AnchorFormer: Differentiable Anchor Attention for Efficient Vision\n  Transformer",
    "summary": "Recently, vision transformers (ViTs) have achieved excellent performance on\nvision tasks by measuring the global self-attention among the image patches.\nGiven $n$ patches, they will have quadratic complexity such as\n$\\mathcal{O}(n^2)$ and the time cost is high when splitting the input image\nwith a small granularity. Meanwhile, the pivotal information is often randomly\ngathered in a few regions of an input image, some tokens may not be helpful for\nthe downstream tasks. To handle this problem, we introduce an anchor-based\nefficient vision transformer (AnchorFormer), which employs the anchor tokens to\nlearn the pivotal information and accelerate the inference. Firstly, by\nestimating the bipartite attention between the anchors and tokens, the\ncomplexity will be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$, where\n$m$ is an anchor number and $m < n$. Notably, by representing the anchors with\nthe neurons in a neural layer, we can differentiably learn these anchors and\napproximate global self-attention through the Markov process. It avoids the\nburden caused by non-differentiable operations and further speeds up the\napproximate attention. Moreover, we extend the proposed model to three\ndownstream tasks including classification, detection, and segmentation.\nExtensive experiments show the effectiveness of our AnchorFormer, e.g.,\nachieving up to a 9.0% higher accuracy or 46.7% FLOPs reduction on ImageNet\nclassification, 81.3% higher mAP on COCO detection under comparable FLOPs, as\ncompared to the current baselines.",
    "published": "2025-05-22T09:44:44Z",
    "updated": "2025-06-18T18:55:01Z",
    "authors": [
      "Jiquan Shan",
      "Junxiao Wang",
      "Lifeng Zhao",
      "Liang Cai",
      "Hongyuan Zhang",
      "Ioannis Liritzis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07923v1",
    "title": "Efficient Seismic Data Interpolation via Sparse Attention Transformer\n  and Diffusion Model",
    "summary": "Seismic data interpolation is a critical pre-processing step for improving\nseismic imaging quality and remains a focus of academic innovation. To address\nthe computational inefficiencies caused by extensive iterative resampling in\ncurrent plug-and-play diffusion interpolation methods, we propose the\ndiffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep\nlearning framework. Our model integrates transformer architectures and\ndiffusion models via a Seismic Prior Extraction Network (SPEN), which serves as\na bridge module. Full-layer sparse multi-head attention and feed-forward\npropagation capture global information distributions, while the diffusion model\nprovides robust prior guidance. To mitigate the computational burden of\nhigh-dimensional representations, self-attention is computed along the channel\nrather than the spatial dimension. We show that using negative squared\nEuclidean distance to compute sparse affinity matrices better suits seismic\ndata modeling, enabling broader contribution from amplitude feature nodes. An\nadaptive ReLU function further discards low or irrelevant self-attention\nvalues. We conduct training within a single-stage optimization framework,\nrequiring only a few reverse diffusion sampling steps during inference.\nExtensive experiments demonstrate improved interpolation fidelity and\ncomputational efficiency for both random and continuous missing data, offering\na new paradigm for high-efficiency seismic data reconstruction under complex\ngeological conditions.",
    "published": "2025-06-09T16:36:00Z",
    "updated": "2025-06-09T16:36:00Z",
    "authors": [
      "Xiaoli Wei",
      "Chunxia Zhang",
      "Baisong Jiang",
      "Anxiang Di",
      "Deng Xiong",
      "Jiangshe Zhang",
      "Mingming Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15940v2",
    "title": "Polyline Path Masked Attention for Vision Transformer",
    "summary": "Global dependency modeling and spatial position modeling are two core issues\nof the foundational architecture design in current deep learning frameworks.\nRecently, Vision Transformers (ViTs) have achieved remarkable success in\ncomputer vision, leveraging the powerful global dependency modeling capability\nof the self-attention mechanism. Furthermore, Mamba2 has demonstrated its\nsignificant potential in natural language processing tasks by explicitly\nmodeling the spatial adjacency prior through the structured mask. In this\npaper, we propose Polyline Path Masked Attention (PPMA) that integrates the\nself-attention mechanism of ViTs with an enhanced structured mask of Mamba2,\nharnessing the complementary strengths of both architectures. Specifically, we\nfirst ameliorate the traditional structured mask of Mamba2 by introducing a 2D\npolyline path scanning strategy and derive its corresponding structured mask,\npolyline path mask, which better preserves the adjacency relationships among\nimage tokens. Notably, we conduct a thorough theoretical analysis on the\nstructural characteristics of the proposed polyline path mask and design an\nefficient algorithm for the computation of the polyline path mask. Next, we\nembed the polyline path mask into the self-attention mechanism of ViTs,\nenabling explicit modeling of spatial adjacency prior. Extensive experiments on\nstandard benchmarks, including image classification, object detection, and\nsegmentation, demonstrate that our model outperforms previous state-of-the-art\napproaches based on both state-space models and Transformers. For example, our\nproposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K\nsemantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%,\nrespectively. Code is available at https://github.com/zhongchenzhao/PPMA.",
    "published": "2025-06-19T00:52:30Z",
    "updated": "2025-10-21T07:16:28Z",
    "authors": [
      "Zhongchen Zhao",
      "Chaodong Xiao",
      "Hui Lin",
      "Qi Xie",
      "Lei Zhang",
      "Deyu Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07606v1",
    "title": "Transformer-Based Indirect Structural Health Monitoring of Rail\n  Infrastructure with Attention-Driven Detection and Localization of Transient\n  Defects",
    "summary": "Indirect structural health monitoring (iSHM) for broken rail detection using\nonboard sensors presents a cost-effective paradigm for railway track\nassessment, yet reliably detecting small, transient anomalies (2-10 cm) remains\na significant challenge due to complex vehicle dynamics, signal noise, and the\nscarcity of labeled data limiting supervised approaches. This study addresses\nthese issues through unsupervised deep learning. We introduce an incremental\nsynthetic data benchmark designed to systematically evaluate model robustness\nagainst progressively complex challenges like speed variations, multi-channel\ninputs, and realistic noise patterns encountered in iSHM. Using this benchmark,\nwe evaluate several established unsupervised models alongside our proposed\nAttention-Focused Transformer. Our model employs a self-attention mechanism,\ntrained via reconstruction but innovatively deriving anomaly scores primarily\nfrom deviations in learned attention weights, aiming for both effectiveness and\ncomputational efficiency. Benchmarking results reveal that while\ntransformer-based models generally outperform others, all tested models exhibit\nsignificant vulnerability to high-frequency localized noise, identifying this\nas a critical bottleneck for practical deployment. Notably, our proposed model\nachieves accuracy comparable to the state-of-the-art solution while\ndemonstrating better inference speed. This highlights the crucial need for\nenhanced noise robustness in future iSHM models and positions our more\nefficient attention-based approach as a promising foundation for developing\npractical onboard anomaly detection systems.",
    "published": "2025-10-08T23:01:53Z",
    "updated": "2025-10-08T23:01:53Z",
    "authors": [
      "Sizhe Ma",
      "Katherine A. Flanigan",
      "Mario BergÃ©s",
      "James D. Brooks"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.18825v1",
    "title": "Unifying and Enhancing Graph Transformers via a Hierarchical Mask\n  Framework",
    "summary": "Graph Transformers (GTs) have emerged as a powerful paradigm for graph\nrepresentation learning due to their ability to model diverse node\ninteractions. However, existing GTs often rely on intricate architectural\ndesigns tailored to specific interactions, limiting their flexibility. To\naddress this, we propose a unified hierarchical mask framework that reveals an\nunderlying equivalence between model architecture and attention mask\nconstruction. This framework enables a consistent modeling paradigm by\ncapturing diverse interactions through carefully designed attention masks.\nTheoretical analysis under this framework demonstrates that the probability of\ncorrect classification positively correlates with the receptive field size and\nlabel consistency, leading to a fundamental design principle: an effective\nattention mask should ensure both a sufficiently large receptive field and a\nhigh level of label consistency. While no single existing mask satisfies this\nprinciple across all scenarios, our analysis reveals that hierarchical masks\noffer complementary strengths, motivating their effective integration. Then, we\nintroduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer with\nMulti-Level Masking and Dual Attention Computation. M3Dphormer incorporates\nthree theoretically grounded hierarchical masks and employs a bi-level expert\nrouting mechanism to adaptively integrate multi-level interaction information.\nTo ensure scalability, we further introduce a dual attention computation scheme\nthat dynamically switches between dense and sparse modes based on local mask\nsparsity. Extensive experiments across multiple benchmarks demonstrate that\nM3Dphormer achieves state-of-the-art performance, validating the effectiveness\nof our unified framework and model design.",
    "published": "2025-10-21T17:22:32Z",
    "updated": "2025-10-21T17:22:32Z",
    "authors": [
      "Yujie Xing",
      "Xiao Wang",
      "Bin Wu",
      "Hai Huang",
      "Chuan Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.25542v1",
    "title": "Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided\n  Mutual Information",
    "summary": "Uncovering hidden graph structures underlying real-world data is a critical\nchallenge with broad applications across scientific domains. Recently,\ntransformer-based models leveraging the attention mechanism have demonstrated\nstrong empirical success in capturing complex dependencies within graphs.\nHowever, the theoretical understanding of their training dynamics has been\nlimited to tree-like graphs, where each node depends on a single parent.\nExtending provable guarantees to more general directed acyclic graphs (DAGs) --\nwhich involve multiple parents per node -- remains challenging, primarily due\nto the difficulty in designing training objectives that enable different\nattention heads to separately learn multiple different parent relationships.\n  In this work, we address this problem by introducing a novel\ninformation-theoretic metric: the kernel-guided mutual information (KG-MI),\nbased on the $f$-divergence. Our objective combines KG-MI with a multi-head\nattention framework, where each head is associated with a distinct marginal\ntransition kernel to model diverse parent-child dependencies effectively. We\nprove that, given sequences generated by a $K$-parent DAG, training a\nsingle-layer, multi-head transformer via gradient ascent converges to the\nglobal optimum in polynomial time. Furthermore, we characterize the attention\nscore patterns at convergence. In addition, when particularizing the\n$f$-divergence to the KL divergence, the learned attention scores accurately\nreflect the ground-truth adjacency matrix, thereby provably recovering the\nunderlying graph structure. Experimental results validate our theoretical\nfindings.",
    "published": "2025-10-29T14:07:12Z",
    "updated": "2025-10-29T14:07:12Z",
    "authors": [
      "Yuan Cheng",
      "Yu Huang",
      "Zhe Xiong",
      "Yingbin Liang",
      "Vincent Y. F. Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1907.01750v4",
    "title": "Attention routing between capsules",
    "summary": "In this paper, we propose a new capsule network architecture called Attention\nRouting CapsuleNet (AR CapsNet). We replace the dynamic routing and squash\nactivation function of the capsule network with dynamic routing (CapsuleNet)\nwith the attention routing and capsule activation. The attention routing is a\nrouting between capsules through an attention module. The attention routing is\na fast forward-pass while keeping spatial information. On the other hand, the\nintuitive interpretation of the dynamic routing is finding a centroid of the\nprediction capsules. Thus, the squash activation function and its variant focus\non preserving a vector orientation. However, the capsule activation focuses on\nperforming a capsule-scale activation function.\n  We evaluate our proposed model on the MNIST, affNIST, and CIFAR-10\nclassification tasks. The proposed model achieves higher accuracy with fewer\nparameters (x0.65 in the MNIST, x0.82 in the CIFAR-10) and less training time\nthan CapsuleNet (x0.19 in the MNIST, x0.35 in the CIFAR-10). These results\nvalidate that designing a capsule-scale operation is a key factor to implement\nthe capsule concept.\n  Also, our experiment shows that our proposed model is transformation\nequivariant as CapsuleNet. As we perturb each element of the output capsule,\nthe decoder attached to the output capsules shows global variations. Further\nexperiments show that the difference in the capsule features caused by applying\naffine transformations on an input image is significantly aligned in one\ndirection.",
    "published": "2019-07-03T06:01:16Z",
    "updated": "2019-11-13T08:02:00Z",
    "authors": [
      "Jaewoong Choi",
      "Hyun Seo",
      "Suii Im",
      "Myungjoo Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2001.10667v1",
    "title": "Interpretable Rumor Detection in Microblogs by Attending to User\n  Interactions",
    "summary": "We address rumor detection by learning to differentiate between the\ncommunity's response to real and fake claims in microblogs. Existing\nstate-of-the-art models are based on tree models that model conversational\ntrees. However, in social media, a user posting a reply might be replying to\nthe entire thread rather than to a specific user. We propose a post-level\nattention model (PLAN) to model long distance interactions between tweets with\nthe multi-head attention mechanism in a transformer network. We investigated\nvariants of this model: (1) a structure aware self-attention model (StA-PLAN)\nthat incorporates tree structure information in the transformer network, and\n(2) a hierarchical token and post-level attention model (StA-HiTPLAN) that\nlearns a sentence representation with token-level self-attention. To the best\nof our knowledge, we are the first to evaluate our models on two rumor\ndetection data sets: the PHEME data set as well as the Twitter15 and Twitter16\ndata sets. We show that our best models outperform current state-of-the-art\nmodels for both data sets. Moreover, the attention mechanism allows us to\nexplain rumor detection predictions at both token-level and post-level.",
    "published": "2020-01-29T02:37:11Z",
    "updated": "2020-01-29T02:37:11Z",
    "authors": [
      "Ling Min Serena Khoo",
      "Hai Leong Chieu",
      "Zhong Qian",
      "Jing Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.10697v2",
    "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive\n  Biases",
    "summary": "Convolutional architectures have proven extremely successful for vision\ntasks. Their hard inductive biases enable sample-efficient learning, but come\nat the cost of a potentially lower performance ceiling. Vision Transformers\n(ViTs) rely on more flexible self-attention layers, and have recently\noutperformed CNNs for image classification. However, they require costly\npre-training on large external datasets or distillation from pre-trained\nconvolutional networks. In this paper, we ask the following question: is it\npossible to combine the strengths of these two architectures while avoiding\ntheir respective limitations? To this end, we introduce gated positional\nself-attention (GPSA), a form of positional self-attention which can be\nequipped with a ``soft\" convolutional inductive bias. We initialise the GPSA\nlayers to mimic the locality of convolutional layers, then give each attention\nhead the freedom to escape locality by adjusting a gating parameter regulating\nthe attention paid to position versus content information. The resulting\nconvolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet,\nwhile offering a much improved sample efficiency. We further investigate the\nrole of locality in learning by first quantifying how it is encouraged in\nvanilla self-attention layers, then analysing how it is escaped in GPSA layers.\nWe conclude by presenting various ablations to better understand the success of\nthe ConViT. Our code and models are released publicly at\nhttps://github.com/facebookresearch/convit.",
    "published": "2021-03-19T09:11:20Z",
    "updated": "2021-06-10T08:44:33Z",
    "authors": [
      "StÃ©phane d'Ascoli",
      "Hugo Touvron",
      "Matthew Leavitt",
      "Ari Morcos",
      "Giulio Biroli",
      "Levent Sagun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.07331v2",
    "title": "ETMA: Efficient Transformer Based Multilevel Attention framework for\n  Multimodal Fake News Detection",
    "summary": "In this new digital era, social media has created a severe impact on the\nlives of people. In recent times, fake news content on social media has become\none of the major challenging problems for society. The dissemination of\nfabricated and false news articles includes multimodal data in the form of text\nand images. The previous methods have mainly focused on unimodal analysis.\nMoreover, for multimodal analysis, researchers fail to keep the unique\ncharacteristics corresponding to each modality. This paper aims to overcome\nthese limitations by proposing an Efficient Transformer based Multilevel\nAttention (ETMA) framework for multimodal fake news detection, which comprises\nthe following components: visual attention-based encoder, textual\nattention-based encoder, and joint attention-based learning. Each component\nutilizes the different forms of attention mechanism and uniquely deals with\nmultimodal data to detect fraudulent content. The efficacy of the proposed\nnetwork is validated by conducting several experiments on four real-world fake\nnews datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,\nand Risdal Fake News Dataset using multiple evaluation metrics. The results\nshow that the proposed method outperforms the baseline methods on all four\ndatasets. Further, the computation time of the model is also lower than the\nstate-of-the-art methods.",
    "published": "2022-06-15T07:26:27Z",
    "updated": "2023-03-13T06:52:04Z",
    "authors": [
      "Ashima Yadav",
      "Shivani Gaba",
      "Haneef Khan",
      "Ishan Budhiraja",
      "Akansha Singh",
      "Krishan Kant Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.11863v2",
    "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image\n  Clustering",
    "summary": "We propose a self-supervised Gaussian ATtention network for image Clustering\n(GATCluster). Rather than extracting intermediate features first and then\nperforming the traditional clustering algorithm, GATCluster directly outputs\nsemantic cluster labels without further post-processing. Theoretically, we give\na Label Feature Theorem to guarantee the learned features are one-hot encoded\nvectors, and the trivial solutions are avoided. To train the GATCluster in a\ncompletely unsupervised manner, we design four self-learning tasks with the\nconstraints of transformation invariance, separability maximization, entropy\nanalysis, and attention mapping. Specifically, the transformation invariance\nand separability maximization tasks learn the relationships between sample\npairs. The entropy analysis task aims to avoid trivial solutions. To capture\nthe object-oriented semantics, we design a self-supervised attention mechanism\nthat includes a parameterized attention module and a soft-attention loss. All\nthe guiding signals for clustering are self-generated during the training\nprocess. Moreover, we develop a two-step learning algorithm that is\nmemory-efficient for clustering large-size images. Extensive experiments\ndemonstrate the superiority of our proposed method in comparison with the\nstate-of-the-art image clustering benchmarks. Our code has been made publicly\navailable at https://github.com/niuchuangnn/GATCluster.",
    "published": "2020-02-27T00:57:18Z",
    "updated": "2020-06-06T20:09:39Z",
    "authors": [
      "Chuang Niu",
      "Jun Zhang",
      "Ge Wang",
      "Jimin Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.03902v3",
    "title": "NystrÃ¶mformer: A NystrÃ¶m-Based Algorithm for Approximating\n  Self-Attention",
    "summary": "Transformers have emerged as a powerful tool for a broad range of natural\nlanguage processing tasks. A key component that drives the impressive\nperformance of Transformers is the self-attention mechanism that encodes the\ninfluence or dependence of other tokens on each specific token. While\nbeneficial, the quadratic complexity of self-attention on the input sequence\nlength has limited its application to longer sequences -- a topic being\nactively studied in the community. To address this limitation, we propose\nNystr\\\"{o}mformer -- a model that exhibits favorable scalability as a function\nof sequence length. Our idea is based on adapting the Nystr\\\"{o}m method to\napproximate standard self-attention with $O(n)$ complexity. The scalability of\nNystr\\\"{o}mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE\nbenchmark and IMDB reviews with standard sequence length, and find that our\nNystr\\\"{o}mformer performs comparably, or in a few cases, even slightly better,\nthan standard self-attention. On longer sequence tasks in the Long Range Arena\n(LRA) benchmark, Nystr\\\"{o}mformer performs favorably relative to other\nefficient self-attention methods. Our code is available at\nhttps://github.com/mlpen/Nystromformer.",
    "published": "2021-02-07T20:06:59Z",
    "updated": "2021-03-31T20:40:39Z",
    "authors": [
      "Yunyang Xiong",
      "Zhanpeng Zeng",
      "Rudrasis Chakraborty",
      "Mingxing Tan",
      "Glenn Fung",
      "Yin Li",
      "Vikas Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.02309v1",
    "title": "MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for\n  Discriminative Music Modeling on Raw Waveforms",
    "summary": "In this work, we aim to improve the expressive capacity of waveform-based\ndiscriminative music networks by modeling both sequential (temporal) and\nhierarchical information in an efficient end-to-end architecture. We present\nMuSLCAT, or Multi-scale and Multi-level Convolutional Attention Transformer, a\nnovel architecture for learning robust representations of complex music tags\ndirectly from raw waveform recordings. We also introduce a lightweight variant\nof MuSLCAT called MuSLCAN, short for Multi-scale and Multi-level Convolutional\nAttention Network. Both MuSLCAT and MuSLCAN model features from multiple scales\nand levels by integrating a frontend-backend architecture. The frontend targets\ndifferent frequency ranges while modeling long-range dependencies and\nmulti-level interactions by using two convolutional attention networks with\nattention-augmented convolution (AAC) blocks. The backend dynamically\nrecalibrates multi-scale and level features extracted from the frontend by\nincorporating self-attention. The difference between MuSLCAT and MuSLCAN is\ntheir backend components. MuSLCAT's backend is a modified version of BERT.\nWhile MuSLCAN's is a simple AAC block. We validate the proposed MuSLCAT and\nMuSLCAN architectures by comparing them to state-of-the-art networks on four\nbenchmark datasets for music tagging and genre recognition. Our experiments\nshow that MuSLCAT and MuSLCAN consistently yield competitive results when\ncompared to state-of-the-art waveform-based models yet require considerably\nfewer parameters.",
    "published": "2021-04-06T06:17:22Z",
    "updated": "2021-04-06T06:17:22Z",
    "authors": [
      "Kai Middlebrook",
      "Shyam Sudhakaran",
      "David Guy Brizan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.11116v1",
    "title": "TransFER: Learning Relation-aware Facial Expression Representations with\n  Transformers",
    "summary": "Facial expression recognition (FER) has received increasing interest in\ncomputer vision. We propose the TransFER model which can learn rich\nrelation-aware local representations. It mainly consists of three components:\nMulti-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping\n(MSAD). First, local patches play an important role in distinguishing various\nexpressions, however, few existing works can locate discriminative and diverse\nlocal patches. This can cause serious problems when some patches are invisible\ndue to pose variations or viewpoint changes. To address this issue, the MAD is\nproposed to randomly drop an attention map. Consequently, models are pushed to\nexplore diverse local patches adaptively. Second, to build rich relations\nbetween different local patches, the Vision Transformers (ViT) are used in FER,\ncalled ViT-FER. Since the global scope is used to reinforce each local patch, a\nbetter representation is obtained to boost the FER performance. Thirdly, the\nmulti-head self-attention allows ViT to jointly attend to features from\ndifferent information subspaces at different positions. Given no explicit\nguidance, however, multiple self-attentions may extract similar relations. To\naddress this, the MSAD is proposed to randomly drop one self-attention module.\nAs a result, models are forced to learn rich relations among diverse local\npatches. Our proposed TransFER model outperforms the state-of-the-art methods\non several FER benchmarks, showing its effectiveness and usefulness.",
    "published": "2021-08-25T08:28:34Z",
    "updated": "2021-08-25T08:28:34Z",
    "authors": [
      "Fanglei Xue",
      "Qiangchang Wang",
      "Guodong Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.11707v5",
    "title": "Deps-SAN: Neural Machine Translation with Dependency-Scaled\n  Self-Attention Network",
    "summary": "Syntax knowledge contributes its powerful strength in Neural machine\ntranslation (NMT) tasks. Early NMT works supposed that syntax details can be\nautomatically learned from numerous texts via attention networks. However,\nsucceeding researches pointed out that limited by the uncontrolled nature of\nattention computation, the NMT model requires an external syntax to capture the\ndeep syntactic awareness. Although existing syntax-aware NMT methods have born\ngreat fruits in combining syntax, the additional workloads they introduced\nrender the model heavy and slow. Particularly, these efforts scarcely involve\nthe Transformer-based NMT and modify its core self-attention network (SAN). To\nthis end, we propose a parameter-free, Dependency-scaled Self-Attention Network\n(Deps-SAN) for syntax-aware Transformer-based NMT. A quantified matrix of\ndependency closeness between tokens is constructed to impose explicit syntactic\nconstraints into the SAN for learning syntactic details and dispelling the\ndispersion of attention distributions. Two knowledge sparsing techniques are\nfurther integrated to avoid the model overfitting the dependency noises\nintroduced by the external parser. Experiments and analyses on IWSLT14\nGerman-to-English and WMT16 German-to-English benchmark NMT tasks verify the\neffectiveness of our approach.",
    "published": "2021-11-23T08:01:21Z",
    "updated": "2022-10-04T07:29:31Z",
    "authors": [
      "Ru Peng",
      "Nankai Lin",
      "Yi Fang",
      "Shengyi Jiang",
      "Tianyong Hao",
      "Boyu Chen",
      "Junbo Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.11987v2",
    "title": "PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers",
    "summary": "Vision Transformers (ViTs) are built on the assumption of treating image\npatches as ``visual tokens\" and learn patch-to-patch attention. The patch\nembedding based tokenizer has a semantic gap with respect to its counterpart,\nthe textual tokenizer. The patch-to-patch attention suffers from the quadratic\ncomplexity issue, and also makes it non-trivial to explain learned ViTs. To\naddress these issues in ViT, this paper proposes to learn Patch-to-Cluster\nattention (PaCa) in ViT. Queries in our PaCa-ViT starts with patches, while\nkeys and values are directly based on clustering (with a predefined small\nnumber of clusters). The clusters are learned end-to-end, leading to better\ntokenizers and inducing joint clustering-for-attention and\nattention-for-clustering for better and interpretable models. The quadratic\ncomplexity is relaxed to linear complexity. The proposed PaCa module is used in\ndesigning efficient and interpretable ViT backbones and semantic segmentation\nhead networks. In experiments, the proposed methods are tested on ImageNet-1k\nimage classification, MS-COCO object detection and instance segmentation and\nMIT-ADE20k semantic segmentation. Compared with the prior art, it obtains\nbetter performance in all the three benchmarks than the SWin and the PVTs by\nsignificant margins in ImageNet-1k and MIT-ADE20k. It is also significantly\nmore efficient than PVT models in MS-COCO and MIT-ADE20k due to the linear\ncomplexity. The learned clusters are semantically meaningful. Code and model\ncheckpoints are available at https://github.com/iVMCL/PaCaViT.",
    "published": "2022-03-22T18:28:02Z",
    "updated": "2023-04-07T00:46:43Z",
    "authors": [
      "Ryan Grainger",
      "Thomas Paniagua",
      "Xi Song",
      "Naresh Cuntoor",
      "Mun Wai Lee",
      "Tianfu Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.08575v1",
    "title": "SegNeXt: Rethinking Convolutional Attention Design for Semantic\n  Segmentation",
    "summary": "We present SegNeXt, a simple convolutional network architecture for semantic\nsegmentation. Recent transformer-based models have dominated the field of\nsemantic segmentation due to the efficiency of self-attention in encoding\nspatial information. In this paper, we show that convolutional attention is a\nmore efficient and effective way to encode contextual information than the\nself-attention mechanism in transformers. By re-examining the characteristics\nowned by successful segmentation models, we discover several key components\nleading to the performance improvement of segmentation models. This motivates\nus to design a novel convolutional attention network that uses cheap\nconvolutional operations. Without bells and whistles, our SegNeXt significantly\nimproves the performance of previous state-of-the-art methods on popular\nbenchmarks, including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal\nContext, and iSAID. Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and\nachieves 90.6% mIoU on the Pascal VOC 2012 test leaderboard using only 1/10\nparameters of it. On average, SegNeXt achieves about 2.0% mIoU improvements\ncompared to the state-of-the-art methods on the ADE20K datasets with the same\nor fewer computations. Code is available at https://github.com/uyzhang/JSeg\n(Jittor) and https://github.com/Visual-Attention-Network/SegNeXt (Pytorch).",
    "published": "2022-09-18T14:33:49Z",
    "updated": "2022-09-18T14:33:49Z",
    "authors": [
      "Meng-Hao Guo",
      "Cheng-Ze Lu",
      "Qibin Hou",
      "Zhengning Liu",
      "Ming-Ming Cheng",
      "Shi-Min Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.05830v1",
    "title": "P-Transformer: Towards Better Document-to-Document Neural Machine\n  Translation",
    "summary": "Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.",
    "published": "2022-12-12T11:19:05Z",
    "updated": "2022-12-12T11:19:05Z",
    "authors": [
      "Yachao Li",
      "Junhui Li",
      "Jing Jiang",
      "Shimin Tao",
      "Hao Yang",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.01054v1",
    "title": "VoxelFormer: Bird's-Eye-View Feature Generation based on Dual-view\n  Attention for Multi-view 3D Object Detection",
    "summary": "In recent years, transformer-based detectors have demonstrated remarkable\nperformance in 2D visual perception tasks. However, their performance in\nmulti-view 3D object detection remains inferior to the state-of-the-art (SOTA)\nof convolutional neural network based detectors. In this work, we investigate\nthis issue from the perspective of bird's-eye-view (BEV) feature generation.\nSpecifically, we examine the BEV feature generation method employed by the\ntransformer-based SOTA, BEVFormer, and identify its two limitations: (i) it\nonly generates attention weights from BEV, which precludes the use of lidar\npoints for supervision, and (ii) it aggregates camera view features to the BEV\nthrough deformable sampling, which only selects a small subset of features and\nfails to exploit all information. To overcome these limitations, we propose a\nnovel BEV feature generation method, dual-view attention, which generates\nattention weights from both the BEV and camera view. This method encodes all\ncamera features into the BEV feature. By combining dual-view attention with the\nBEVFormer architecture, we build a new detector named VoxelFormer. Extensive\nexperiments are conducted on the nuScenes benchmark to verify the superiority\nof dual-view attention and VoxelForer. We observe that even only adopting 3\nencoders and 1 historical frame during training, VoxelFormer still outperforms\nBEVFormer significantly. When trained in the same setting, VoxelFormer can\nsurpass BEVFormer by 4.9% NDS point. Code is available at:\nhttps://github.com/Lizhuoling/VoxelFormer-public.git.",
    "published": "2023-04-03T15:00:36Z",
    "updated": "2023-04-03T15:00:36Z",
    "authors": [
      "Zhuoling Li",
      "Chuanrui Zhang",
      "Wei-Chiu Ma",
      "Yipin Zhou",
      "Linyan Huang",
      "Haoqian Wang",
      "SerNam Lim",
      "Hengshuang Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.06345v3",
    "title": "ASR: Attention-alike Structural Re-parameterization",
    "summary": "The structural re-parameterization (SRP) technique is a novel deep learning\ntechnique that achieves interconversion between different network architectures\nthrough equivalent parameter transformations. This technique enables the\nmitigation of the extra costs for performance improvement during training, such\nas parameter size and inference time, through these transformations during\ninference, and therefore SRP has great potential for industrial and practical\napplications. The existing SRP methods have successfully considered many\ncommonly used architectures, such as normalizations, pooling methods, and\nmulti-branch convolution. However, the widely used attention modules which\ndrastically slow inference speed cannot be directly implemented by SRP due to\nthese modules usually act on the backbone network in a multiplicative manner\nand the modules' output is input-dependent during inference, which limits the\napplication scenarios of SRP. In this paper, we conduct extensive experiments\nfrom a statistical perspective and discover an interesting phenomenon Stripe\nObservation, which reveals that channel attention values quickly approach some\nconstant vectors during training. This observation inspires us to propose a\nsimple-yet-effective attention-alike structural re-parameterization (ASR) that\nallows us to achieve SRP for a given network while enjoying the effectiveness\nof the attention mechanism. Extensive experiments conducted on several standard\nbenchmarks demonstrate the effectiveness of ASR in generally improving the\nperformance of existing backbone networks, attention modules, and SRP methods\nwithout any elaborated model crafting. We also analyze the limitations and\nprovide experimental and theoretical evidence for the strong robustness of the\nproposed ASR.",
    "published": "2023-04-13T08:52:34Z",
    "updated": "2024-08-07T02:46:46Z",
    "authors": [
      "Shanshan Zhong",
      "Zhongzhan Huang",
      "Wushao Wen",
      "Jinghui Qin",
      "Liang Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.11862v4",
    "title": "Universal Domain Adaptation via Compressive Attention Matching",
    "summary": "Universal domain adaptation (UniDA) aims to transfer knowledge from the\nsource domain to the target domain without any prior knowledge about the label\nset. The challenge lies in how to determine whether the target samples belong\nto common categories. The mainstream methods make judgments based on the sample\nfeatures, which overemphasizes global information while ignoring the most\ncrucial local objects in the image, resulting in limited accuracy. To address\nthis issue, we propose a Universal Attention Matching (UniAM) framework by\nexploiting the self-attention mechanism in vision transformer to capture the\ncrucial object information. The proposed framework introduces a novel\nCompressive Attention Matching (CAM) approach to explore the core information\nby compressively representing attentions. Furthermore, CAM incorporates a\nresidual-based measurement to determine the sample commonness. By utilizing the\nmeasurement, UniAM achieves domain-wise and category-wise Common Feature\nAlignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the first\nmethod utilizing the attention in vision transformer directly to perform\nclassification tasks. Extensive experiments show that UniAM outperforms the\ncurrent state-of-the-art methods on various benchmark datasets.",
    "published": "2023-04-24T07:16:54Z",
    "updated": "2023-08-30T02:55:09Z",
    "authors": [
      "Didi Zhu",
      "Yincuan Li",
      "Junkun Yuan",
      "Zexi Li",
      "Kun Kuang",
      "Chao Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.00108v1",
    "title": "Laplacian-Former: Overcoming the Limitations of Vision Transformers in\n  Local Texture Detection",
    "summary": "Vision Transformer (ViT) models have demonstrated a breakthrough in a wide\nrange of computer vision tasks. However, compared to the Convolutional Neural\nNetwork (CNN) models, it has been observed that the ViT models struggle to\ncapture high-frequency components of images, which can limit their ability to\ndetect local textures and edge information. As abnormalities in human tissue,\nsuch as tumors and lesions, may greatly vary in structure, texture, and shape,\nhigh-frequency information such as texture is crucial for effective semantic\nsegmentation tasks. To address this limitation in ViT models, we propose a new\ntechnique, Laplacian-Former, that enhances the self-attention map by adaptively\nre-calibrating the frequency information in a Laplacian pyramid. More\nspecifically, our proposed method utilizes a dual attention mechanism via\nefficient attention and frequency attention while the efficient attention\nmechanism reduces the complexity of self-attention to linear while producing\nthe same output, selectively intensifying the contribution of shape and texture\nfeatures. Furthermore, we introduce a novel efficient enhancement multi-scale\nbridge that effectively transfers spatial information from the encoder to the\ndecoder while preserving the fundamental features. We demonstrate the efficacy\nof Laplacian-former on multi-organ and skin lesion segmentation tasks with\n+1.87\\% and +0.76\\% dice scores compared to SOTA approaches, respectively. Our\nimplementation is publically available at\nhttps://github.com/mindflow-institue/Laplacian-Former",
    "published": "2023-08-31T19:56:14Z",
    "updated": "2023-08-31T19:56:14Z",
    "authors": [
      "Reza Azad",
      "Amirhossein Kazerouni",
      "Babak Azad",
      "Ehsan Khodapanah Aghdam",
      "Yury Velichko",
      "Ulas Bagci",
      "Dorit Merhof"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.18698v1",
    "title": "Triplet Attention Transformer for Spatiotemporal Predictive Learning",
    "summary": "Spatiotemporal predictive learning offers a self-supervised learning paradigm\nthat enables models to learn both spatial and temporal patterns by predicting\nfuture sequences based on historical sequences. Mainstream methods are\ndominated by recurrent units, yet they are limited by their lack of\nparallelization and often underperform in real-world scenarios. To improve\nprediction quality while maintaining computational efficiency, we propose an\ninnovative triplet attention transformer designed to capture both inter-frame\ndynamics and intra-frame static features. Specifically, the model incorporates\nthe Triplet Attention Module (TAM), which replaces traditional recurrent units\nby exploring self-attention mechanisms in temporal, spatial, and channel\ndimensions. In this configuration: (i) temporal tokens contain abstract\nrepresentations of inter-frame, facilitating the capture of inherent temporal\ndependencies; (ii) spatial and channel attention combine to refine the\nintra-frame representation by performing fine-grained interactions across\nspatial and channel dimensions. Alternating temporal, spatial, and\nchannel-level attention allows our approach to learn more complex short- and\nlong-range spatiotemporal dependencies. Extensive experiments demonstrate\nperformance surpassing existing recurrent-based and recurrent-free methods,\nachieving state-of-the-art under multi-scenario examination including moving\nobject trajectory prediction, traffic flow prediction, driving scene\nprediction, and human motion capture.",
    "published": "2023-10-28T12:49:33Z",
    "updated": "2023-10-28T12:49:33Z",
    "authors": [
      "Xuesong Nie",
      "Xi Chen",
      "Haoyuan Jin",
      "Zhihang Zhu",
      "Yunfeng Yan",
      "Donglian Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.02364v3",
    "title": "Class-Discriminative Attention Maps for Vision Transformers",
    "summary": "Importance estimators are explainability methods that quantify feature\nimportance for deep neural networks (DNN). In vision transformers (ViT), the\nself-attention mechanism naturally leads to attention maps, which are sometimes\ninterpreted as importance scores that indicate which input features ViT models\nare focusing on. However, attention maps do not account for signals from\ndownstream tasks. To generate explanations that are sensitive to downstream\ntasks, we have developed class-discriminative attention maps (CDAM), a\ngradient-based extension that estimates feature importance with respect to a\nknown class or a latent concept. CDAM scales attention scores by how relevant\nthe corresponding tokens are for the predictions of a classifier head. In\naddition to targeting the supervised classifier, CDAM can explain an arbitrary\nconcept shared by selected samples by measuring similarity in the latent space\nof ViT. Additionally, we introduce Smooth CDAM and Integrated CDAM, which\naverage a series of CDAMs with slightly altered tokens. Our quantitative\nbenchmarks include correctness, compactness, and class sensitivity, in\ncomparison to 7 other importance estimators. Vanilla, Smooth, and Integrated\nCDAM excel across all three benchmarks. In particular, our results suggest that\nexisting importance estimators may not provide sufficient class-sensitivity. We\ndemonstrate the utility of CDAM in medical images by training and explaining\nmalignancy and biomarker prediction models based on lung Computed Tomography\n(CT) scans. Overall, CDAM is shown to be highly class-discriminative and\nsemantically relevant, while providing compact explanations.",
    "published": "2023-12-04T21:46:21Z",
    "updated": "2024-10-25T08:27:37Z",
    "authors": [
      "Lennart Brocki",
      "Jakub Binda",
      "Neo Christopher Chung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.13512v1",
    "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of\n  Generative Transformers",
    "summary": "Modern language models rely on the transformer architecture and attention\nmechanism to perform language understanding and text generation. In this work,\nwe study learning a 1-layer self-attention model from a set of prompts and\nassociated output data sampled from the model. We first establish a precise\nmapping between the self-attention mechanism and Markov models: Inputting a\nprompt to the model samples the output token according to a context-conditioned\nMarkov chain (CCMC) which weights the transition matrix of a base Markov chain.\nAdditionally, incorporating positional encoding results in position-dependent\nscaling of the transition probabilities. Building on this formalism, we develop\nidentifiability/coverage conditions for the prompt distribution that guarantee\nconsistent estimation and establish sample complexity guarantees under IID\nsamples. Finally, we study the problem of learning from a single output\ntrajectory generated from an initial prompt. We characterize an intriguing\nwinner-takes-all phenomenon where the generative process implemented by\nself-attention collapses into sampling a limited subset of tokens due to its\nnon-mixing nature. This provides a mathematical explanation to the tendency of\nmodern LLMs to generate repetitive text. In summary, the equivalence to CCMC\nprovides a simple but powerful framework to study self-attention and its\nproperties.",
    "published": "2024-02-21T03:51:34Z",
    "updated": "2024-02-21T03:51:34Z",
    "authors": [
      "M. Emrullah Ildiz",
      "Yixiao Huang",
      "Yingcong Li",
      "Ankit Singh Rawat",
      "Samet Oymak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.10842v4",
    "title": "Twin Transformer using Gated Dynamic Learnable Attention mechanism for\n  Fault Detection and Diagnosis in the Tennessee Eastman Process",
    "summary": "Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety\nand efficiency of industrial processes. We propose a novel FDD methodology for\nthe Tennessee Eastman Process (TEP), a widely used benchmark for chemical\nprocess control. The model employs two separate Transformer branches, enabling\nindependent processing of input data and potential extraction of diverse\ninformation. A novel attention mechanism, Gated Dynamic Learnable Attention\n(GDLAttention), is introduced which integrates a gating mechanism and dynamic\nlearning capabilities. The gating mechanism modulates the attention weights,\nallowing the model to focus on the most relevant parts of the input. The\ndynamic learning approach adapts the attention strategy during training,\npotentially leading to improved performance. The attention mechanism uses a\nbilinear similarity function, providing greater flexibility in capturing\ncomplex relationships between query and key vectors. In order to assess the\neffectiveness of our approach, we tested it against 21 and 18 distinct fault\nscenarios in TEP, and compared its performance with several established FDD\ntechniques. The outcomes indicate that the method outperforms others in terms\nof accuracy, false alarm rate, and misclassification rate. This underscores the\nrobustness and efficacy of the approach for FDD in intricate industrial\nprocesses.",
    "published": "2024-03-16T07:40:23Z",
    "updated": "2024-11-25T17:22:53Z",
    "authors": [
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri",
      "Hanieh Ajami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.01337v1",
    "title": "Multi-view Action Recognition via Directed Gromov-Wasserstein\n  Discrepancy",
    "summary": "Action recognition has become one of the popular research topics in computer\nvision. There are various methods based on Convolutional Networks and\nself-attention mechanisms as Transformers to solve both spatial and temporal\ndimensions problems of action recognition tasks that achieve competitive\nperformances. However, these methods lack a guarantee of the correctness of the\naction subject that the models give attention to, i.e., how to ensure an action\nrecognition model focuses on the proper action subject to make a reasonable\naction prediction. In this paper, we propose a multi-view attention consistency\nmethod that computes the similarity between two attentions from two different\nviews of the action videos using Directed Gromov-Wasserstein Discrepancy.\nFurthermore, our approach applies the idea of Neural Radiance Field to\nimplicitly render the features from novel views when training on single-view\ndatasets. Therefore, the contributions in this work are three-fold. Firstly, we\nintroduce the multi-view attention consistency to solve the problem of\nreasonable prediction in action recognition. Secondly, we define a new metric\nfor multi-view consistent attention using Directed Gromov-Wasserstein\nDiscrepancy. Thirdly, we built an action recognition model based on Video\nTransformers and Neural Radiance Fields. Compared to the recent action\nrecognition methods, the proposed approach achieves state-of-the-art results on\nthree large-scale datasets, i.e., Jester, Something-Something V2, and\nKinetics-400.",
    "published": "2024-05-02T14:43:21Z",
    "updated": "2024-05-02T14:43:21Z",
    "authors": [
      "Hoang-Quan Nguyen",
      "Thanh-Dat Truong",
      "Khoa Luu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.01814v2",
    "title": "Efficient Heterogeneous Large Language Model Decoding with\n  Model-Attention Disaggregation",
    "summary": "Transformer-based large language models (LLMs) exhibit impressive performance\nin generative tasks but also introduce significant challenges in real-world\nserving due to inefficient use of the expensive, computation-optimized\naccelerators. Although disaggregated serving architectures have been proposed\nto split different phases of LLM inference, the efficiency of decoding phase is\nstill low. This is caused by the varying resource demands of different\noperators in the transformer-based LLMs. Specifically, the attention operator\nis memory-intensive, exhibiting a memory access pattern that clashes with the\nstrengths of modern accelerators, especially for long context requests. To\nenhance the efficiency of LLM decoding, we introduce model-attention\ndisaggregation. This approach leverages a collection of cheap, memory-optimized\ndevices for the attention operator while still utilizing high-end accelerators\nfor other parts of the model. This heterogeneous setup ensures that each\ncomponent is tailored to its specific workload, maximizing overall performance\nand cost efficiency. Our comprehensive analysis and experiments confirm the\nviability of splitting the attention computation over multiple devices. Also,\nthe communication bandwidth required between heterogeneous devices proves to be\nmanageable with prevalent networking technologies. To further validate our\ntheory, we develop and deploy Lamina, an LLM inference system that incorporates\nmodel-attention disaggregation in a distributed heterogeneous cluster.\nExperimental results indicate that Lamina can provide 16.1 ~ 90.1% higher\nestimated throughput than existing solutions with similar costs.",
    "published": "2024-05-03T02:15:15Z",
    "updated": "2025-04-10T14:56:01Z",
    "authors": [
      "Shaoyuan Chen",
      "Wencong Xiao",
      "Yutong Lin",
      "Mingxing Zhang",
      "Yingdi Shan",
      "Jinlei Jiang",
      "Kang Chen",
      "Yongwei Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.03481v1",
    "title": "AnchorGT: Efficient and Flexible Attention Architecture for Scalable\n  Graph Transformers",
    "summary": "Graph Transformers (GTs) have significantly advanced the field of graph\nrepresentation learning by overcoming the limitations of message-passing graph\nneural networks (GNNs) and demonstrating promising performance and expressive\npower. However, the quadratic complexity of self-attention mechanism in GTs has\nlimited their scalability, and previous approaches to address this issue often\nsuffer from expressiveness degradation or lack of versatility. To address this\nissue, we propose AnchorGT, a novel attention architecture for GTs with global\nreceptive field and almost linear complexity, which serves as a flexible\nbuilding block to improve the scalability of a wide range of GT models.\nInspired by anchor-based GNNs, we employ structurally important $k$-dominating\nnode set as anchors and design an attention mechanism that focuses on the\nrelationship between individual nodes and anchors, while retaining the global\nreceptive field for all nodes. With its intuitive design, AnchorGT can easily\nreplace the attention module in various GT models with different network\narchitectures and structural encodings, resulting in reduced computational\noverhead without sacrificing performance. In addition, we theoretically prove\nthat AnchorGT attention can be strictly more expressive than Weisfeiler-Lehman\ntest, showing its superiority in representing graph structures. Our experiments\non three state-of-the-art GT models demonstrate that their AnchorGT variants\ncan achieve better results while being faster and significantly more memory\nefficient.",
    "published": "2024-05-06T13:53:09Z",
    "updated": "2024-05-06T13:53:09Z",
    "authors": [
      "Wenhao Zhu",
      "Guojie Song",
      "Liang Wang",
      "Shaoguo Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.05786v3",
    "title": "CAMS: Convolution and Attention-Free Mamba-based Cardiac Image\n  Segmentation",
    "summary": "Convolutional Neural Networks (CNNs) and Transformer-based self-attention\nmodels have become the standard for medical image segmentation. This paper\ndemonstrates that convolution and self-attention, while widely used, are not\nthe only effective methods for segmentation. Breaking with convention, we\npresent a Convolution and self-Attention-free Mamba-based semantic Segmentation\nNetwork named CAMS-Net. Specifically, we design Mamba-based Channel Aggregator\nand Spatial Aggregator, which are applied independently in each encoder-decoder\nstage. The Channel Aggregator extracts information across different channels,\nand the Spatial Aggregator learns features across different spatial locations.\nWe also propose a Linearly Interconnected Factorized Mamba (LIFM) block to\nreduce the computational complexity of a Mamba block and to enhance its\ndecision function by introducing a non-linearity between two factorized Mamba\nblocks. Our model outperforms the existing state-of-the-art CNN,\nself-attention, and Mamba-based methods on CMR and M&Ms-2 Cardiac segmentation\ndatasets, showing how this innovative, convolution, and self-attention-free\nmethod can inspire further research beyond CNN and Transformer paradigms,\nachieving linear complexity and reducing the number of parameters. Source code\nand pre-trained models are available at: https://github.com/kabbas570/CAMS-Net.",
    "published": "2024-06-09T13:53:05Z",
    "updated": "2024-10-29T11:29:17Z",
    "authors": [
      "Abbas Khan",
      "Muhammad Asad",
      "Martin Benning",
      "Caroline Roney",
      "Gregory Slabaugh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.06339v1",
    "title": "Noise-Free Explanation for Driving Action Prediction",
    "summary": "Although attention mechanisms have achieved considerable progress in\nTransformer-based architectures across various Artificial Intelligence (AI)\ndomains, their inner workings remain to be explored. Existing explainable\nmethods have different emphases but are rather one-sided. They primarily\nanalyse the attention mechanisms or gradient-based attribution while neglecting\nthe magnitudes of input feature values or the skip-connection module. Moreover,\nthey inevitably bring spurious noisy pixel attributions unrelated to the\nmodel's decision, hindering humans' trust in the spotted visualization result.\nHence, we propose an easy-to-implement but effective way to remedy this flaw:\nSmooth Noise Norm Attention (SNNA). We weigh the attention by the norm of the\ntransformed value vector and guide the label-specific signal with the attention\ngradient, then randomly sample the input perturbations and average the\ncorresponding gradients to produce noise-free attribution. Instead of\nevaluating the explanation method on the binary or multi-class classification\ntasks like in previous works, we explore the more complex multi-label\nclassification scenario in this work, i.e., the driving action prediction task,\nand trained a model for it specifically. Both qualitative and quantitative\nevaluation results show the superiority of SNNA compared to other SOTA\nattention-based explainable methods in generating a clearer visual explanation\nmap and ranking the input pixel importance.",
    "published": "2024-07-08T19:21:24Z",
    "updated": "2024-07-08T19:21:24Z",
    "authors": [
      "Hongbo Zhu",
      "Theodor Wulff",
      "Rahul Singh Maharjan",
      "Jinpei Han",
      "Angelo Cangelosi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.11505v1",
    "title": "Haze-Aware Attention Network for Single-Image Dehazing",
    "summary": "Single-image dehazing is a pivotal challenge in computer vision that seeks to\nremove haze from images and restore clean background details. Recognizing the\nlimitations of traditional physical model-based methods and the inefficiencies\nof current attention-based solutions, we propose a new dehazing network\ncombining an innovative Haze-Aware Attention Module (HAAM) with a Multiscale\nFrequency Enhancement Module (MFEM). The HAAM is inspired by the atmospheric\nscattering model, thus skillfully integrating physical principles into\nhigh-dimensional features for targeted dehazing. It picks up on latent features\nduring the image restoration process, which gives a significant boost to the\nmetrics, while the MFEM efficiently enhances high-frequency details, thus\nsidestepping wavelet or Fourier transform complexities. It employs multiscale\nfields to extract and emphasize key frequency components with minimal parameter\noverhead. Integrated into a simple U-Net framework, our Haze-Aware Attention\nNetwork (HAA-Net) for single-image dehazing significantly outperforms existing\nattention-based and transformer models in efficiency and effectiveness. Tested\nacross various public datasets, the HAA-Net sets new performance benchmarks.\nOur work not only advances the field of image dehazing but also offers insights\ninto the design of attention mechanisms for broader applications in computer\nvision.",
    "published": "2024-07-16T08:42:39Z",
    "updated": "2024-07-16T08:42:39Z",
    "authors": [
      "Lihan Tong",
      "Yun Liu",
      "Weijia Li",
      "Liyuan Chen",
      "Erkang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.13335v1",
    "title": "OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction",
    "summary": "Visual search is important in our daily life. The efficient allocation of\nvisual attention is critical to effectively complete visual search tasks. Prior\nresearch has predominantly modelled the spatial allocation of visual attention\nin images at the pixel level, e.g. using a saliency map. However, emerging\nevidence shows that visual attention is guided by objects rather than pixel\nintensities. This paper introduces the Object-level Attention Transformer\n(OAT), which predicts human scanpaths as they search for a target object within\na cluttered scene of distractors. OAT uses an encoder-decoder architecture. The\nencoder captures information about the position and appearance of the objects\nwithin an image and about the target. The decoder predicts the gaze scanpath as\na sequence of object fixations, by integrating output features from both the\nencoder and decoder. We also propose a new positional encoding that better\nreflects spatial relationships between objects. We evaluated OAT on the Amazon\nbook cover dataset and a new dataset for visual search that we collected. OAT's\npredicted gaze scanpaths align more closely with human gaze patterns, compared\nto predictions by algorithms based on spatial attention on both established\nmetrics and a novel behavioural-based metric. Our results demonstrate the\ngeneralization ability of OAT, as it accurately predicts human scanpaths for\nunseen layouts and target objects.",
    "published": "2024-07-18T09:33:17Z",
    "updated": "2024-07-18T09:33:17Z",
    "authors": [
      "Yini Fang",
      "Jingling Yu",
      "Haozheng Zhang",
      "Ralf van der Lans",
      "Bertram Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.01443v2",
    "title": "Activating Self-Attention for Multi-Scene Absolute Pose Regression",
    "summary": "Multi-scene absolute pose regression addresses the demand for fast and\nmemory-efficient camera pose estimation across various real-world environments.\nNowadays, transformer-based model has been devised to regress the camera pose\ndirectly in multi-scenes. Despite its potential, transformer encoders are\nunderutilized due to the collapsed self-attention map, having low\nrepresentation capacity. This work highlights the problem and investigates it\nfrom a new perspective: distortion of query-key embedding space. Based on the\nstatistical analysis, we reveal that queries and keys are mapped in completely\ndifferent spaces while only a few keys are blended into the query region. This\nleads to the collapse of the self-attention map as all queries are considered\nsimilar to those few keys. Therefore, we propose simple but effective solutions\nto activate self-attention. Concretely, we present an auxiliary loss that\naligns queries and keys, preventing the distortion of query-key space and\nencouraging the model to find global relations by self-attention. In addition,\nthe fixed sinusoidal positional encoding is adopted instead of undertrained\nlearnable one to reflect appropriate positional clues into the inputs of\nself-attention. As a result, our approach resolves the aforementioned problem\neffectively, thus outperforming existing methods in both outdoor and indoor\nscenes.",
    "published": "2024-11-03T06:00:36Z",
    "updated": "2024-11-18T02:45:17Z",
    "authors": [
      "Miso Lee",
      "Jihwan Kim",
      "Jae-Pil Heo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.19214v2",
    "title": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation",
    "summary": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP).",
    "published": "2025-02-26T15:15:01Z",
    "updated": "2025-08-04T12:11:37Z",
    "authors": [
      "Anthony M. Smaldone",
      "Yu Shee",
      "Gregory W. Kyro",
      "Marwa H. Farag",
      "Zohim Chandani",
      "Elica Kyoseva",
      "Victor S. Batista"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.16653v2",
    "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
    "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
    "published": "2025-03-20T19:10:37Z",
    "updated": "2025-03-24T03:18:49Z",
    "authors": [
      "Hanxiao Wang",
      "Biao Zhang",
      "Weize Quan",
      "Dong-Ming Yan",
      "Peter Wonka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.13173v1",
    "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
    "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
    "published": "2025-04-17T17:59:33Z",
    "updated": "2025-04-17T17:59:33Z",
    "authors": [
      "Ali Behrouz",
      "Meisam Razaviyayn",
      "Peilin Zhong",
      "Vahab Mirrokni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22107v4",
    "title": "Curse of High Dimensionality Issue in Transformer for Long-context\n  Modeling",
    "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention.",
    "published": "2025-05-28T08:34:46Z",
    "updated": "2025-08-14T11:51:31Z",
    "authors": [
      "Shuhai Zhang",
      "Zeng You",
      "Yaofo Chen",
      "Zhiquan Wen",
      "Qianyue Wang",
      "Zhijie Qiu",
      "Yuanqing Li",
      "Mingkui Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.09316v3",
    "title": "On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear\n  Attention",
    "summary": "Large language models (LLMs) excel at capturing global token dependencies via\nself-attention but face prohibitive compute and memory costs on lengthy inputs.\nWhile sub-quadratic methods (e.g., linear attention) can reduce these costs,\nthey often degrade accuracy due to overemphasizing recent tokens. In this work,\nwe first propose dual-state linear attention (DSLA), a novel design that\nmaintains two specialized hidden states-one for preserving historical context\nand one for tracking recency-thereby mitigating the short-range bias typical of\nlinear-attention architectures. To further balance efficiency and accuracy\nunder dynamic workload conditions, we introduce DSLA-Serve, an online adaptive\ndistillation framework that progressively replaces Transformer layers with DSLA\nlayers at inference time, guided by a sensitivity-based layer ordering.\nDSLA-Serve uses a chained fine-tuning strategy to ensure that each newly\nconverted DSLA layer remains consistent with previously replaced layers,\npreserving the overall quality. Extensive evaluations on commonsense reasoning,\nlong-context QA, and text summarization demonstrate that DSLA-Serve yields 2.3x\nfaster inference than Llama2-7B and 3.0x faster than the hybrid Zamba-7B, while\nretaining comparable performance across downstream tasks. Our ablation studies\nshow that DSLA's dual states capture both global and local dependencies,\naddressing the historical-token underrepresentation seen in prior linear\nattentions. Codes are available at https://github.com/utnslab/DSLA-Serve.",
    "published": "2025-06-11T01:25:06Z",
    "updated": "2025-06-17T04:56:46Z",
    "authors": [
      "Yeonju Ro",
      "Zhenyu Zhang",
      "Souvik Kundu",
      "Zhangyang Wang",
      "Aditya Akella"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17245v1",
    "title": "DistrAttention: An Efficient and Flexible Self-Attention Mechanism on\n  Modern GPUs",
    "summary": "The Transformer architecture has revolutionized deep learning, delivering the\nstate-of-the-art performance in areas such as natural language processing,\ncomputer vision, and time series prediction. However, its core component,\nself-attention, has the quadratic time complexity relative to input sequence\nlength, which hinders the scalability of Transformers. The exsiting approaches\non optimizing self-attention either discard full-contextual information or lack\nof flexibility. In this work, we design DistrAttention, an effcient and\nflexible self-attention mechanism with the full context. DistrAttention\nachieves this by grouping data on the embedding dimensionality, usually\nreferred to as $d$. We realize DistrAttention with a lightweight sampling and\nfusion method that exploits locality-sensitive hashing to group similar data. A\nblock-wise grouping framework is further designed to limit the errors\nintroduced by locality sensitive hashing. By optimizing the selection of block\nsizes, DistrAttention could be easily integrated with FlashAttention-2, gaining\nhigh-performance on modern GPUs. We evaluate DistrAttention with extensive\nexperiments. The results show that our method is 37% faster than\nFlashAttention-2 on calculating self-attention. In ViT inference,\nDistrAttention is the fastest and the most accurate among approximate\nself-attention mechanisms. In Llama3-1B, DistrAttention still achieves the\nlowest inference time with only 1% accuray loss.",
    "published": "2025-07-23T06:29:38Z",
    "updated": "2025-07-23T06:29:38Z",
    "authors": [
      "Haolin Jin",
      "Mengbai Xiao",
      "Yuan Yuan",
      "Xiao Zhang",
      "Dongxiao Yu",
      "Guanghui Zhang",
      "Haoliang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18457v1",
    "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for\n  Robust Cross-Population Blood Glucose Forecasting",
    "summary": "This paper proposes GluMind, a transformer-based multimodal framework\ndesigned for continual and long-term blood glucose forecasting. GluMind devises\ntwo attention mechanisms, including cross-attention and multi-scale attention,\nwhich operate in parallel and deliver accurate predictive performance.\nCross-attention effectively integrates blood glucose data with other\nphysiological and behavioral signals such as activity, stress, and heart rate,\naddressing challenges associated with varying sampling rates and their adverse\nimpacts on robust prediction. Moreover, the multi-scale attention mechanism\ncaptures long-range temporal dependencies. To mitigate catastrophic forgetting,\nGluMind incorporates a knowledge retention technique into the transformer-based\nforecasting model. The knowledge retention module not only enhances the model's\nability to retain prior knowledge but also boosts its overall forecasting\nperformance. We evaluate GluMind on the recently released AIREADI dataset,\nwhich contains behavioral and physiological data collected from healthy people,\nindividuals with prediabetes, and those with type 2 diabetes. We examine the\nperformance stability and adaptability of GluMind in learning continuously as\nnew patient cohorts are introduced. Experimental results show that GluMind\nconsistently outperforms other state-of-the-art forecasting models, achieving\napproximately 15% and 9% improvements in root mean squared error (RMSE) and\nmean absolute error (MAE), respectively.",
    "published": "2025-09-22T22:27:58Z",
    "updated": "2025-09-22T22:27:58Z",
    "authors": [
      "Ebrahim Farahmand",
      "Reza Rahimi Azghan",
      "Nooshin Taheri Chatrudi",
      "Velarie Yaa Ansu-Baidoo",
      "Eric Kim",
      "Gautham Krishna Gudur",
      "Mohit Malu",
      "Owen Krueger",
      "Edison Thomaz",
      "Giulia Pedrielli",
      "Pavan Turaga",
      "Hassan Ghasemzadeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20950v1",
    "title": "Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference\n  for Physical Equations",
    "summary": "Prior-data fitted networks (PFNs) are a promising alternative to\ntime-consuming Gaussian Process (GP) inference for creating fast surrogates of\nphysical systems. PFN reduces the computational burden of GP-training by\nreplacing Bayesian inference in GP with a single forward pass of a learned\nprediction model. However, with standard Transformer attention, PFNs show\nlimited effectiveness on high-dimensional regression tasks. We introduce\nDecoupled-Value Attention (DVA)-- motivated by the GP property that the\nfunction space is fully characterized by the kernel over inputs and the\npredictive mean is a weighted sum of training targets. DVA computes\nsimilarities from inputs only and propagates labels solely through values.\nThus, the proposed DVA mirrors the Gaussian-process update while remaining\nkernel-free. We demonstrate that the crucial factor for scaling PFNs is the\nattention rule rather than the architecture itself. Specifically, our results\ndemonstrate that (a) localized attention consistently reduces out-of-sample\nvalidation loss in PFNs across different dimensional settings, with validation\nloss reduced by more than 50% in five- and ten-dimensional cases, and (b) the\nrole of attention is more decisive than the choice of backbone architecture,\nshowing that CNN-based PFNs can perform at par with their Transformer-based\ncounterparts. The proposed PFNs provide 64-dimensional power flow equation\napproximations with a mean absolute error of the order of 1E-3, while being\nover 80x faster than exact GP inference.",
    "published": "2025-09-25T09:32:42Z",
    "updated": "2025-09-25T09:32:42Z",
    "authors": [
      "Kaustubh Sharma",
      "Simardeep Singh",
      "Parikshit Pareek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23681v2",
    "title": "QuantSparse: Comprehensively Compressing Video Diffusion Transformer\n  with Model Quantization and Attention Sparsification",
    "summary": "Diffusion transformers exhibit remarkable video generation capability, yet\ntheir prohibitive computational and memory costs hinder practical deployment.\nModel quantization and attention sparsification are two promising directions\nfor compression, but each alone suffers severe performance degradation under\naggressive compression. Combining them promises compounded efficiency gains,\nbut naive integration is ineffective. The sparsity-induced information loss\nexacerbates quantization noise, leading to amplified attention shifts. To\naddress this, we propose \\textbf{QuantSparse}, a unified framework that\nintegrates model quantization with attention sparsification. Specifically, we\nintroduce \\textit{Multi-Scale Salient Attention Distillation}, which leverages\nboth global structural guidance and local salient supervision to mitigate\nquantization-induced bias. In addition, we develop \\textit{Second-Order Sparse\nAttention Reparameterization}, which exploits the temporal stability of\nsecond-order residuals to efficiently recover information lost under sparsity.\nExperiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88\nPSNR, substantially outperforming the state-of-the-art quantization baseline\nQ-VDiT (16.85 PSNR), while simultaneously delivering a \\textbf{3.68$\\times$}\nreduction in storage and \\textbf{1.88$\\times$} acceleration in end-to-end\ninference. Our code will be released in\nhttps://github.com/wlfeng0509/QuantSparse.",
    "published": "2025-09-28T06:49:44Z",
    "updated": "2025-09-30T03:14:18Z",
    "authors": [
      "Weilun Feng",
      "Chuanguang Yang",
      "Haotong Qin",
      "Mingqiang Wu",
      "Yuqi Li",
      "Xiangqi Li",
      "Zhulin An",
      "Libo Huang",
      "Yulun Zhang",
      "Michele Magno",
      "Yongjun Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06949v1",
    "title": "Grouped Differential Attention",
    "summary": "The self-attention mechanism, while foundational to modern Transformer\narchitectures, suffers from a critical inefficiency: it frequently allocates\nsubstantial attention to redundant or noisy context. Differential Attention\naddressed this by using subtractive attention maps for signal and noise, but\nits required balanced head allocation imposes rigid constraints on\nrepresentational flexibility and scalability.\n  To overcome this, we propose Grouped Differential Attention (GDA), a novel\napproach that introduces unbalanced head allocation between signal-preserving\nand noise-control groups. GDA significantly enhances signal focus by\nstrategically assigning more heads to signal extraction and fewer to\nnoise-control, stabilizing the latter through controlled repetition (akin to\nGQA). This design achieves stronger signal fidelity with minimal computational\noverhead. We further extend this principle to group-differentiated growth, a\nscalable strategy that selectively replicates only the signal-focused heads,\nthereby ensuring efficient capacity expansion.\n  Through large-scale pretraining and continual training experiments, we\ndemonstrate that moderate imbalance ratios in GDA yield substantial\nimprovements in generalization and stability compared to symmetric baselines.\nOur results collectively establish that ratio-aware head allocation and\nselective expansion offer an effective and practical path toward designing\nscalable, computation-efficient Transformer architectures.",
    "published": "2025-10-08T12:32:28Z",
    "updated": "2025-10-08T12:32:28Z",
    "authors": [
      "Junghwan Lim",
      "Sungmin Lee",
      "Dongseok Kim",
      "Wai Ting Cheung",
      "Beomgyu Kim",
      "Taehwan Kim",
      "Haesol Lee",
      "Junhyeok Lee",
      "Dongpin Oh",
      "Eunhwan Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.02647v1",
    "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks",
    "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.",
    "published": "2025-11-04T15:14:58Z",
    "updated": "2025-11-04T15:14:58Z",
    "authors": [
      "Xiumei Deng",
      "Zehui Xiong",
      "Binbin Chen",
      "Dong In Kim",
      "Merouane Debbah",
      "H. Vincent Poor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1809.08895v3",
    "title": "Neural Speech Synthesis with Transformer Network",
    "summary": "Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2)\nare proposed and achieve state-of-the-art performance, they still suffer from\ntwo problems: 1) low efficiency during training and inference; 2) hard to model\nlong dependency using current recurrent neural networks (RNNs). Inspired by the\nsuccess of Transformer network in neural machine translation (NMT), in this\npaper, we introduce and adapt the multi-head attention mechanism to replace the\nRNN structures and also the original attention mechanism in Tacotron2. With the\nhelp of multi-head self-attention, the hidden states in the encoder and decoder\nare constructed in parallel, which improves the training efficiency. Meanwhile,\nany two inputs at different times are connected directly by self-attention\nmechanism, which solves the long range dependency problem effectively. Using\nphoneme sequences as input, our Transformer TTS network generates mel\nspectrograms, followed by a WaveNet vocoder to output the final audio results.\nExperiments are conducted to test the efficiency and performance of our new\nnetwork. For the efficiency, our Transformer TTS network can speed up the\ntraining about 4.25 times faster compared with Tacotron2. For the performance,\nrigorous human tests show that our proposed model achieves state-of-the-art\nperformance (outperforms Tacotron2 with a gap of 0.048) and is very close to\nhuman quality (4.39 vs 4.44 in MOS).",
    "published": "2018-09-19T07:41:17Z",
    "updated": "2019-01-30T12:40:57Z",
    "authors": [
      "Naihan Li",
      "Shujie Liu",
      "Yanqing Liu",
      "Sheng Zhao",
      "Ming Liu",
      "Ming Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.13915v2",
    "title": "SG-Net: Syntax Guided Transformer for Language Representation",
    "summary": "Understanding human language is one of the key themes of artificial\nintelligence. For language representation, the capacity of effectively modeling\nthe linguistic knowledge from the detail-riddled and lengthy texts and getting\nrid of the noises is essential to improve its performance. Traditional\nattentive models attend to all words without explicit constraint, which results\nin inaccurate concentration on some dispensable words. In this work, we propose\nusing syntax to guide the text modeling by incorporating explicit syntactic\nconstraints into attention mechanisms for better linguistically motivated word\nrepresentations. In detail, for self-attention network (SAN) sponsored\nTransformer-based encoder, we introduce syntactic dependency of interest (SDOI)\ndesign into the SAN to form an SDOI-SAN with syntax-guided self-attention.\nSyntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the\nSAN from the original Transformer encoder through a dual contextual\narchitecture for better linguistics inspired representation. The proposed\nSG-Net is applied to typical Transformer encoders. Extensive experiments on\npopular benchmark tasks, including machine reading comprehension, natural\nlanguage inference, and neural machine translation show the effectiveness of\nthe proposed SG-Net design.",
    "published": "2020-12-27T11:09:35Z",
    "updated": "2021-01-07T05:48:45Z",
    "authors": [
      "Zhuosheng Zhang",
      "Yuwei Wu",
      "Junru Zhou",
      "Sufeng Duan",
      "Hai Zhao",
      "Rui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.05422v2",
    "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?",
    "summary": "Transformers have sprung up in the field of computer vision. In this work, we\nexplore whether the core self-attention module in Transformer is the key to\nachieving excellent performance in image recognition. To this end, we build an\nattention-free network called sMLPNet based on the existing MLP-based vision\nmodels. Specifically, we replace the MLP module in the token-mixing step with a\nnovel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along\nthe axial directions and the parameters are shared among rows or columns. By\nsparse connection and weight sharing, sMLP module significantly reduces the\nnumber of model parameters and computational complexity, avoiding the common\nover-fitting problem that plagues the performance of MLP-like models. When only\ntrained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1\naccuracy with only 24M parameters, which is much better than most CNNs and\nvision Transformers under the same model size constraint. When scaling up to\n66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the\nstate-of-the-art Swin Transformer. The success of sMLPNet suggests that the\nself-attention mechanism is not necessarily a silver bullet in computer vision.\nThe code and models are publicly available at\nhttps://github.com/microsoft/SPACH",
    "published": "2021-09-12T04:05:15Z",
    "updated": "2022-05-29T11:44:24Z",
    "authors": [
      "Chuanxin Tang",
      "Yucheng Zhao",
      "Guangting Wang",
      "Chong Luo",
      "Wenxuan Xie",
      "Wenjun Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.10716v2",
    "title": "Hierarchical Label-wise Attention Transformer Model for Explainable ICD\n  Coding",
    "summary": "International Classification of Diseases (ICD) coding plays an important role\nin systematically classifying morbidity and mortality data. In this study, we\npropose a hierarchical label-wise attention Transformer model (HiLAT) for the\nexplainable prediction of ICD codes from clinical documents. HiLAT firstly\nfine-tunes a pretrained Transformer model to represent the tokens of clinical\ndocuments. We subsequently employ a two-level hierarchical label-wise attention\nmechanism that creates label-specific document representations. These\nrepresentations are in turn used by a feed-forward neural network to predict\nwhether a specific ICD code is assigned to the input clinical document of\ninterest. We evaluate HiLAT using hospital discharge summaries and their\ncorresponding ICD-9 codes from the MIMIC-III database. To investigate the\nperformance of different types of Transformer models, we develop\nClinicalplusXLNet, which conducts continual pretraining from XLNet-Base using\nall the MIMIC-III clinical notes. The experiment results show that the F1\nscores of the HiLAT+ClinicalplusXLNet outperform the previous state-of-the-art\nmodels for the top-50 most frequent ICD-9 codes from MIMIC-III. Visualisations\nof attention weights present a potential explainability tool for checking the\nface validity of ICD code predictions.",
    "published": "2022-04-22T14:12:22Z",
    "updated": "2022-09-30T04:34:02Z",
    "authors": [
      "Leibo Liu",
      "Oscar Perez-Concha",
      "Anthony Nguyen",
      "Vicki Bennett",
      "Louisa Jorm"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.17252v1",
    "title": "Multi-Camera Calibration Free BEV Representation for 3D Object Detection",
    "summary": "In advanced paradigms of autonomous driving, learning Bird's Eye View (BEV)\nrepresentation from surrounding views is crucial for multi-task framework.\nHowever, existing methods based on depth estimation or camera-driven attention\nare not stable to obtain transformation under noisy camera parameters, mainly\nwith two challenges, accurate depth prediction and calibration. In this work,\nwe present a completely Multi-Camera Calibration Free Transformer (CFT) for\nrobust BEV representation, which focuses on exploring implicit mapping, not\nrelied on camera intrinsics and extrinsics. To guide better feature learning\nfrom image views to BEV, CFT mines potential 3D information in BEV via our\ndesigned position-aware enhancement (PA). Instead of camera-driven point-wise\nor global transformation, for interaction within more effective region and\nlower computation cost, we propose a view-aware attention which also reduces\nredundant computation and promotes converge. CFT achieves 49.7% NDS on the\nnuScenes detection task leaderboard, which is the first work removing camera\nparameters, comparable to other geometry-guided methods. Without temporal input\nand other modal information, CFT achieves second highest performance with a\nsmaller image input 1600 * 640. Thanks to view-attention variant, CFT reduces\nmemory and transformer FLOPs for vanilla attention by about 12% and 60%,\nrespectively, with improved NDS by 1.0%. Moreover, its natural robustness to\nnoisy camera parameters makes CFT more competitive.",
    "published": "2022-10-31T12:18:08Z",
    "updated": "2022-10-31T12:18:08Z",
    "authors": [
      "Hongxiang Jiang",
      "Wenming Meng",
      "Hongmei Zhu",
      "Qian Zhang",
      "Jihao Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.13777v2",
    "title": "Towards Unsupervised Domain Adaptation via Domain-Transformer",
    "summary": "As a vital problem in pattern analysis and machine intelligence, Unsupervised\nDomain Adaptation (UDA) attempts to transfer an effective feature learner from\na labeled source domain to an unlabeled target domain. Inspired by the success\nof the Transformer, several advances in UDA are achieved by adopting pure\ntransformers as network architectures, but such a simple application can only\ncapture patch-level information and lacks interpretability. To address these\nissues, we propose the Domain-Transformer (DoT) with domain-level attention\nmechanism to capture the long-range correspondence between the cross-domain\nsamples. On the theoretical side, we provide a mathematical understanding of\nDoT: 1) We connect the domain-level attention with optimal transport theory,\nwhich provides interpretability from Wasserstein geometry; 2) From the\nperspective of learning theory, Wasserstein distance-based generalization\nbounds are derived, which explains the effectiveness of DoT for knowledge\ntransfer. On the methodological side, DoT integrates the domain-level attention\nand manifold structure regularization, which characterize the sample-level\ninformation and locality consistency for cross-domain cluster structures.\nBesides, the domain-level attention mechanism can be used as a plug-and-play\nmodule, so DoT can be implemented under different neural network architectures.\nInstead of explicitly modeling the distribution discrepancy at domain-level or\nclass-level, DoT learns transferable features under the guidance of long-range\ncorrespondence, so it is free of pseudo-labels and explicit domain discrepancy\noptimization. Extensive experiment results on several benchmark datasets\nvalidate the effectiveness of DoT.",
    "published": "2022-02-24T02:30:15Z",
    "updated": "2024-08-13T01:54:46Z",
    "authors": [
      "Ren Chuan-Xian",
      "Zhai Yi-Ming",
      "Luo You-Wei",
      "Yan Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.03646v2",
    "title": "A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA\n  Through Sparse Attention and Dynamic Pipelining",
    "summary": "Transformers are considered one of the most important deep learning models\nsince 2018, in part because it establishes state-of-the-art (SOTA) records and\ncould potentially replace existing Deep Neural Networks (DNNs). Despite the\nremarkable triumphs, the prolonged turnaround time of Transformer models is a\nwidely recognized roadblock. The variety of sequence lengths imposes additional\ncomputing overhead where inputs need to be zero-padded to the maximum sentence\nlength in the batch to accommodate the parallel computing platforms. This paper\ntargets the field-programmable gate array (FPGA) and proposes a coherent\nsequence length adaptive algorithm-hardware co-design for Transformer\nacceleration. Particularly, we develop a hardware-friendly sparse attention\noperator and a length-aware hardware resource scheduling algorithm. The\nproposed sparse attention operator brings the complexity of attention-based\nmodels down to linear complexity and alleviates the off-chip memory traffic.\nThe proposed length-aware resource hardware scheduling algorithm dynamically\nallocates the hardware resources to fill up the pipeline slots and eliminates\nbubbles for NLP tasks. Experiments show that our design has very small accuracy\nloss and has 80.2 $\\times$ and 2.6 $\\times$ speedup compared to CPU and GPU\nimplementation, and 4 $\\times$ higher energy efficiency than state-of-the-art\nGPU accelerator optimized via CUBLAS GEMM.",
    "published": "2022-08-07T05:48:38Z",
    "updated": "2022-08-20T23:04:20Z",
    "authors": [
      "Hongwu Peng",
      "Shaoyi Huang",
      "Shiyang Chen",
      "Bingbing Li",
      "Tong Geng",
      "Ang Li",
      "Weiwen Jiang",
      "Wujie Wen",
      "Jinbo Bi",
      "Hang Liu",
      "Caiwen Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.11390v2",
    "title": "Efficient Attention Mechanism for Visual Dialog that can Handle All the\n  Interactions between Multiple Inputs",
    "summary": "It has been a primary concern in recent studies of vision and language tasks\nto design an effective attention mechanism dealing with interactions between\nthe two modalities. The Transformer has recently been extended and applied to\nseveral bi-modal tasks, yielding promising results. For visual dialog, it\nbecomes necessary to consider interactions between three or more inputs, i.e.,\nan image, a question, and a dialog history, or even its individual dialog\ncomponents. In this paper, we present a neural architecture named Light-weight\nTransformer for Many Inputs (LTMI) that can efficiently deal with all the\ninteractions between multiple such inputs in visual dialog. It has a block\nstructure similar to the Transformer and employs the same design of attention\ncomputation, whereas it has only a small number of parameters, yet has\nsufficient representational power for the purpose. Assuming a standard setting\nof visual dialog, a layer built upon the proposed attention block has less than\none-tenth of parameters as compared with its counterpart, a natural Transformer\nextension. The experimental results on the VisDial datasets validate the\neffectiveness of the proposed approach, showing improvements of the best NDCG\nscore on the VisDial v1.0 dataset from 57.59 to 60.92 with a single model, from\n64.47 to 66.53 with ensemble models, and even to 74.88 with additional\nfinetuning. Our implementation code is available at\nhttps://github.com/davidnvq/visdial.",
    "published": "2019-11-26T08:10:02Z",
    "updated": "2020-07-17T14:10:12Z",
    "authors": [
      "Van-Quang Nguyen",
      "Masanori Suganuma",
      "Takayuki Okatani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.04139v2",
    "title": "Learning Texture Transformer Network for Image Super-Resolution",
    "summary": "We study on image super-resolution (SR), which aims to recover realistic\ntextures from a low-resolution (LR) image. Recent progress has been made by\ntaking high-resolution images as references (Ref), so that relevant textures\ncan be transferred to LR images. However, existing SR approaches neglect to use\nattention mechanisms to transfer high-resolution (HR) textures from Ref images,\nwhich limits these approaches in challenging cases. In this paper, we propose a\nnovel Texture Transformer Network for Image Super-Resolution (TTSR), in which\nthe LR and Ref images are formulated as queries and keys in a transformer,\nrespectively. TTSR consists of four closely-related modules optimized for image\ngeneration tasks, including a learnable texture extractor by DNN, a relevance\nembedding module, a hard-attention module for texture transfer, and a\nsoft-attention module for texture synthesis. Such a design encourages joint\nfeature learning across LR and Ref images, in which deep feature\ncorrespondences can be discovered by attention, and thus accurate texture\nfeatures can be transferred. The proposed texture transformer can be further\nstacked in a cross-scale way, which enables texture recovery from different\nlevels (e.g., from 1x to 4x magnification). Extensive experiments show that\nTTSR achieves significant improvements over state-of-the-art approaches on both\nquantitative and qualitative evaluations.",
    "published": "2020-06-07T12:55:34Z",
    "updated": "2020-06-22T12:19:51Z",
    "authors": [
      "Fuzhi Yang",
      "Huan Yang",
      "Jianlong Fu",
      "Hongtao Lu",
      "Baining Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.11908v2",
    "title": "Analysis of GraphSum's Attention Weights to Improve the Explainability\n  of Multi-Document Summarization",
    "summary": "Modern multi-document summarization (MDS) methods are based on transformer\narchitectures. They generate state of the art summaries, but lack\nexplainability. We focus on graph-based transformer models for MDS as they\ngained recent popularity. We aim to improve the explainability of the\ngraph-based MDS by analyzing their attention weights. In a graph-based MDS such\nas GraphSum, vertices represent the textual units, while the edges form some\nsimilarity graph over the units. We compare GraphSum's performance utilizing\ndifferent textual units, i. e., sentences versus paragraphs, on two news\nbenchmark datasets, namely WikiSum and MultiNews. Our experiments show that\nparagraph-level representations provide the best summarization performance.\nThus, we subsequently focus oAnalysisn analyzing the paragraph-level attention\nweights of GraphSum's multi-heads and decoding layers in order to improve the\nexplainability of a transformer-based MDS model. As a reference metric, we\ncalculate the ROUGE scores between the input paragraphs and each sentence in\nthe generated summary, which indicate source origin information via text\nsimilarity. We observe a high correlation between the attention weights and\nthis reference metric, especially on the the later decoding layers of the\ntransformer architecture. Finally, we investigate if the generated summaries\nfollow a pattern of positional bias by extracting which paragraph provided the\nmost information for each generated summary. Our results show that there is a\nhigh correlation between the position in the summary and the source origin.",
    "published": "2021-05-19T08:18:59Z",
    "updated": "2022-12-06T20:52:21Z",
    "authors": [
      "M. Lautaro Hickmann",
      "Fabian Wurzberger",
      "Megi Hoxhalli",
      "Arne Lochner",
      "Jessica TÃ¶llich",
      "Ansgar Scherp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.14742v2",
    "title": "TENT: Tensorized Encoder Transformer for Temperature Forecasting",
    "summary": "Reliable weather forecasting is of great importance in science, business, and\nsociety. The best performing data-driven models for weather prediction tasks\nrely on recurrent or convolutional neural networks, where some of which\nincorporate attention mechanisms. In this work, we introduce a novel model\nbased on Transformer architecture for weather forecasting. The proposed\nTensorial Encoder Transformer (TENT) model is equipped with tensorial attention\nand thus it exploits the spatiotemporal structure of weather data by processing\nit in multidimensional tensorial format. We show that compared to the classical\nencoder transformer, 3D convolutional neural networks, LSTM, and Convolutional\nLSTM, the proposed TENT model can better learn the underlying complex pattern\nof the weather data for the studied temperature prediction task. Experiments on\ntwo real-life weather datasets are performed. The datasets consist of\nhistorical measurements from weather stations in the USA, Canada and Europe.\nThe first dataset contains hourly measurements of weather attributes for 30\ncities in the USA and Canada from October 2012 to November 2017. The second\ndataset contains daily measurements of weather attributes of 18 cities across\nEurope from May 2005 to April 2020. Two attention scores are introduced based\non the obtained tonsorial attention and are visualized in order to shed light\non the decision-making process of our model and provide insight knowledge on\nthe most important cities for the target cities.",
    "published": "2021-06-28T14:17:22Z",
    "updated": "2022-02-21T22:25:39Z",
    "authors": [
      "Onur Bilgin",
      "PaweÅ MÄka",
      "Thomas Vergutz",
      "Siamak Mehrkanoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.02341v3",
    "title": "Feature Fusion Vision Transformer for Fine-Grained Visual Categorization",
    "summary": "The core for tackling the fine-grained visual categorization (FGVC) is to\nlearn subtle yet discriminative features. Most previous works achieve this by\nexplicitly selecting the discriminative parts or integrating the attention\nmechanism via CNN-based approaches.However, these methods enhance the\ncomputational complexity and make the modeldominated by the regions containing\nthe most of the objects. Recently, vision trans-former (ViT) has achieved SOTA\nperformance on general image recognition tasks. Theself-attention mechanism\naggregates and weights the information from all patches to the classification\ntoken, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation\ntoken in the deep layer pays more attention to the global information, lacking\nthe local and low-level features that are essential for FGVC. In this work, we\nproposea novel pure transformer-based framework Feature Fusion Vision\nTransformer (FFVT)where we aggregate the important tokens from each transformer\nlayer to compensate thelocal, low-level and middle-level information. We design\na novel token selection mod-ule called mutual attention weight selection (MAWS)\nto guide the network effectively and efficiently towards selecting\ndiscriminative tokens without introducing extra param-eters. We verify the\neffectiveness of FFVT on three benchmarks where FFVT achieves the\nstate-of-the-art performance.",
    "published": "2021-07-06T01:48:43Z",
    "updated": "2022-02-28T20:31:54Z",
    "authors": [
      "Jun Wang",
      "Xiaohan Yu",
      "Yongsheng Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.01177v3",
    "title": "MutualFormer: Multi-Modality Representation Learning via Cross-Diffusion\n  Attention",
    "summary": "Aggregating multi-modality data to obtain reliable data representation\nattracts more and more attention. Recent studies demonstrate that Transformer\nmodels usually work well for multi-modality tasks. Existing Transformers\ngenerally either adopt the Cross-Attention (CA) mechanism or simple\nconcatenation to achieve the information interaction among different modalities\nwhich generally ignore the issue of modality gap. In this work, we re-think\nTransformer and extend it to MutualFormer for multi-modality data\nrepresentation. Rather than CA in Transformer, MutualFormer employs our new\ndesign of Cross-Diffusion Attention (CDA) to conduct the information\ncommunication among different modalities. Comparing with CA, the main\nadvantages of the proposed CDA are three aspects. First, the crossaffinities in\nCDA are defined based on the individual modality affinities in the metric space\nwhich thus can naturally avoid the issue of modality/domain gap in feature\nbased CA definition. Second, CDA provides a general scheme which can either be\nused for multimodality representation or serve as the post-optimization for\nexisting CA models. Third, CDA is implemented efficiently. We successfully\napply the MutualFormer on different multi-modality learning tasks (i.e.,\nRGB-Depth SOD, RGB-NIR object ReID). Extensive experiments demonstrate the\neffectiveness of the proposed MutualFormer.",
    "published": "2021-12-02T12:48:37Z",
    "updated": "2023-03-16T07:04:35Z",
    "authors": [
      "Xixi Wang",
      "Xiao Wang",
      "Bo Jiang",
      "Jin Tang",
      "Bin Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.11435v2",
    "title": "Learned Queries for Efficient Local Attention",
    "summary": "Vision Transformers (ViT) serve as powerful vision models. Unlike\nconvolutional neural networks, which dominated vision research in previous\nyears, vision transformers enjoy the ability to capture long-range dependencies\nin the data. Nonetheless, an integral part of any transformer architecture, the\nself-attention mechanism, suffers from high latency and inefficient memory\nutilization, making it less suitable for high-resolution input images. To\nalleviate these shortcomings, hierarchical vision models locally employ\nself-attention on non-interleaving windows. This relaxation reduces the\ncomplexity to be linear in the input size; however, it limits the cross-window\ninteraction, hurting the model performance. In this paper, we propose a new\nshift-invariant local attention layer, called query and attend (QnA), that\naggregates the input locally in an overlapping manner, much like convolutions.\nThe key idea behind QnA is to introduce learned queries, which allow fast and\nefficient implementation. We verify the effectiveness of our layer by\nincorporating it into a hierarchical vision transformer model. We show\nimprovements in speed and memory complexity while achieving comparable accuracy\nwith state-of-the-art models. Finally, our layer scales especially well with\nwindow size, requiring up-to x10 less memory while being up-to x5 faster than\nexisting methods. The code is publicly available at\n\\url{https://github.com/moabarar/qna}.",
    "published": "2021-12-21T18:52:33Z",
    "updated": "2022-04-19T17:49:50Z",
    "authors": [
      "Moab Arar",
      "Ariel Shamir",
      "Amit H. Bermano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.01932v2",
    "title": "Contextual Attention Network: Transformer Meets U-Net",
    "summary": "Currently, convolutional neural networks (CNN) (e.g., U-Net) have become the\nde facto standard and attained immense success in medical image segmentation.\nHowever, as a downside, CNN based methods are a double-edged sword as they fail\nto build long-range dependencies and global context connections due to the\nlimited receptive field that stems from the intrinsic characteristics of the\nconvolution operation. Hence, recent articles have exploited Transformer\nvariants for medical image segmentation tasks which open up great opportunities\ndue to their innate capability of capturing long-range correlations through the\nattention mechanism. Although being feasibly designed, most of the cohort\nstudies incur prohibitive performance in capturing local information, thereby\nresulting in less lucidness of boundary areas. In this paper, we propose a\ncontextual attention network to tackle the aforementioned limitations. The\nproposed method uses the strength of the Transformer module to model the\nlong-range contextual dependency. Simultaneously, it utilizes the CNN encoder\nto capture local semantic information. In addition, an object-level\nrepresentation is included to model the regional interaction map. The extracted\nhierarchical features are then fed to the contextual attention module to\nadaptively recalibrate the representation space using the local information.\nThen, they emphasize the informative regions while taking into account the\nlong-range contextual dependency derived by the Transformer module. We validate\nour method on several large-scale public medical image segmentation datasets\nand achieve state-of-the-art performance. We have provided the implementation\ncode in https://github.com/rezazad68/TMUnet.",
    "published": "2022-03-02T21:10:24Z",
    "updated": "2022-03-31T04:54:57Z",
    "authors": [
      "Reza Azad",
      "Moein Heidari",
      "Yuli Wu",
      "Dorit Merhof"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.09830v1",
    "title": "Laneformer: Object-aware Row-Column Transformers for Lane Detection",
    "summary": "We present Laneformer, a conceptually simple yet powerful transformer-based\narchitecture tailored for lane detection that is a long-standing research topic\nfor visual perception in autonomous driving. The dominant paradigms rely on\npurely CNN-based architectures which often fail in incorporating relations of\nlong-range lane points and global contexts induced by surrounding objects\n(e.g., pedestrians, vehicles). Inspired by recent advances of the transformer\nencoder-decoder architecture in various vision tasks, we move forwards to\ndesign a new end-to-end Laneformer architecture that revolutionizes the\nconventional transformers into better capturing the shape and semantic\ncharacteristics of lanes, with minimal overhead in latency. First, coupling\nwith deformable pixel-wise self-attention in the encoder, Laneformer presents\ntwo new row and column self-attention operations to efficiently mine point\ncontext along with the lane shapes. Second, motivated by the appearing objects\nwould affect the decision of predicting lane segments, Laneformer further\nincludes the detected object instances as extra inputs of multi-head attention\nblocks in the encoder and decoder to facilitate the lane point detection by\nsensing semantic contexts. Specifically, the bounding box locations of objects\nare added into Key module to provide interaction with each pixel and query\nwhile the ROI-aligned features are inserted into Value module. Extensive\nexperiments demonstrate our Laneformer achieves state-of-the-art performances\non CULane benchmark, in terms of 77.1% F1 score. We hope our simple and\neffective Laneformer will serve as a strong baseline for future research in\nself-attention models for lane detection.",
    "published": "2022-03-18T10:14:35Z",
    "updated": "2022-03-18T10:14:35Z",
    "authors": [
      "Jianhua Han",
      "Xiajun Deng",
      "Xinyue Cai",
      "Zhen Yang",
      "Hang Xu",
      "Chunjing Xu",
      "Xiaodan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.04437v3",
    "title": "Activating More Pixels in Image Super-Resolution Transformer",
    "summary": "Transformer-based methods have shown impressive performance in low-level\nvision tasks, such as image super-resolution. However, we find that these\nnetworks can only utilize a limited spatial range of input information through\nattribution analysis. This implies that the potential of Transformer is still\nnot fully exploited in existing networks. In order to activate more input\npixels for better reconstruction, we propose a novel Hybrid Attention\nTransformer (HAT). It combines both channel attention and window-based\nself-attention schemes, thus making use of their complementary advantages of\nbeing able to utilize global statistics and strong local fitting capability.\nMoreover, to better aggregate the cross-window information, we introduce an\noverlapping cross-attention module to enhance the interaction between\nneighboring window features. In the training stage, we additionally adopt a\nsame-task pre-training strategy to exploit the potential of the model for\nfurther improvement. Extensive experiments show the effectiveness of the\nproposed modules, and we further scale up the model to demonstrate that the\nperformance of this task can be greatly improved. Our overall method\nsignificantly outperforms the state-of-the-art methods by more than 1dB. Codes\nand models are available at https://github.com/XPixelGroup/HAT.",
    "published": "2022-05-09T17:36:58Z",
    "updated": "2023-03-19T01:25:49Z",
    "authors": [
      "Xiangyu Chen",
      "Xintao Wang",
      "Jiantao Zhou",
      "Yu Qiao",
      "Chao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.05833v2",
    "title": "Earthformer: Exploring Space-Time Transformers for Earth System\n  Forecasting",
    "summary": "Conventionally, Earth system (e.g., weather and climate) forecasting relies\non numerical simulation with complex physical models and are hence both\nexpensive in computation and demanding on domain expertise. With the explosive\ngrowth of the spatiotemporal Earth observation data in the past decade,\ndata-driven models that apply Deep Learning (DL) are demonstrating impressive\npotential for various Earth system forecasting tasks. The Transformer as an\nemerging DL architecture, despite its broad success in other domains, has\nlimited adoption in this area. In this paper, we propose Earthformer, a\nspace-time Transformer for Earth system forecasting. Earthformer is based on a\ngeneric, flexible and efficient space-time attention block, named Cuboid\nAttention. The idea is to decompose the data into cuboids and apply\ncuboid-level self-attention in parallel. These cuboids are further connected\nwith a collection of global vectors. We conduct experiments on the MovingMNIST\ndataset and a newly proposed chaotic N-body MNIST dataset to verify the\neffectiveness of cuboid attention and figure out the best design of\nEarthformer. Experiments on two real-world benchmarks about precipitation\nnowcasting and El Nino/Southern Oscillation (ENSO) forecasting show Earthformer\nachieves state-of-the-art performance. Code is available:\nhttps://github.com/amazon-science/earth-forecasting-transformer .",
    "published": "2022-07-12T20:52:26Z",
    "updated": "2023-03-01T04:40:21Z",
    "authors": [
      "Zhihan Gao",
      "Xingjian Shi",
      "Hao Wang",
      "Yi Zhu",
      "Yuyang Wang",
      "Mu Li",
      "Dit-Yan Yeung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.06124v3",
    "title": "DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation",
    "summary": "One key challenge of exemplar-guided image generation lies in establishing\nfine-grained correspondences between input and guided images. Prior approaches,\ndespite the promising results, have relied on either estimating dense attention\nto compute per-point matching, which is limited to only coarse scales due to\nthe quadratic memory cost, or fixing the number of correspondences to achieve\nlinear complexity, which lacks flexibility. In this paper, we propose a dynamic\nsparse attention based Transformer model, termed Dynamic Sparse Transformer\n(DynaST), to achieve fine-level matching with favorable efficiency. The heart\nof our approach is a novel dynamic-attention unit, dedicated to covering the\nvariation on the optimal number of tokens one position should focus on.\nSpecifically, DynaST leverages the multi-layer nature of Transformer structure,\nand performs the dynamic attention scheme in a cascaded manner to refine\nmatching results and synthesize visually-pleasing outputs. In addition, we\nintroduce a unified training objective for DynaST, making it a versatile\nreference-based image translation framework for both supervised and\nunsupervised scenarios. Extensive experiments on three applications,\npose-guided person image generation, edge-based face synthesis, and undistorted\nimage style transfer, demonstrate that DynaST achieves superior performance in\nlocal details, outperforming the state of the art while reducing the\ncomputational cost significantly. Our code is available at\nhttps://github.com/Huage001/DynaST",
    "published": "2022-07-13T11:12:03Z",
    "updated": "2023-03-27T07:55:32Z",
    "authors": [
      "Songhua Liu",
      "Jingwen Ye",
      "Sucheng Ren",
      "Xinchao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.11995v2",
    "title": "3D Siamese Transformer Network for Single Object Tracking on Point\n  Clouds",
    "summary": "Siamese network based trackers formulate 3D single object tracking as\ncross-correlation learning between point features of a template and a search\narea. Due to the large appearance variation between the template and search\narea during tracking, how to learn the robust cross correlation between them\nfor identifying the potential target in the search area is still a challenging\nproblem. In this paper, we explicitly use Transformer to form a 3D Siamese\nTransformer network for learning robust cross correlation between the template\nand the search area of point clouds. Specifically, we develop a Siamese point\nTransformer network to learn shape context information of the target. Its\nencoder uses self-attention to capture non-local information of point clouds to\ncharacterize the shape information of the object, and the decoder utilizes\ncross-attention to upsample discriminative point features. After that, we\ndevelop an iterative coarse-to-fine correlation network to learn the robust\ncross correlation between the template and the search area. It formulates the\ncross-feature augmentation to associate the template with the potential target\nin the search area via cross attention. To further enhance the potential\ntarget, it employs the ego-feature augmentation that applies self-attention to\nthe local k-NN graph of the feature space to aggregate target features.\nExperiments on the KITTI, nuScenes, and Waymo datasets show that our method\nachieves state-of-the-art performance on the 3D single object tracking task.",
    "published": "2022-07-25T09:08:30Z",
    "updated": "2022-07-26T08:43:25Z",
    "authors": [
      "Le Hui",
      "Lingpeng Wang",
      "Linghua Tang",
      "Kaihao Lan",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.12659v1",
    "title": "Graph Neural Network and Spatiotemporal Transformer Attention for 3D\n  Video Object Detection from Point Clouds",
    "summary": "Previous works for LiDAR-based 3D object detection mainly focus on the\nsingle-frame paradigm. In this paper, we propose to detect 3D objects by\nexploiting temporal information in multiple frames, i.e., the point cloud\nvideos. We empirically categorize the temporal information into short-term and\nlong-term patterns. To encode the short-term data, we present a Grid Message\nPassing Network (GMPNet), which considers each grid (i.e., the grouped points)\nas a node and constructs a k-NN graph with the neighbor grids. To update\nfeatures for a grid, GMPNet iteratively collects information from its\nneighbors, thus mining the motion cues in grids from nearby frames. To further\naggregate the long-term frames, we propose an Attentive Spatiotemporal\nTransformer GRU (AST-GRU), which contains a Spatial Transformer Attention (STA)\nmodule and a Temporal Transformer Attention (TTA) module. STA and TTA enhance\nthe vanilla GRU to focus on small objects and better align the moving objects.\nOur overall framework supports both online and offline video object detection\nin point clouds. We implement our algorithm based on prevalent anchor-based and\nanchor-free detectors. The evaluation results on the challenging nuScenes\nbenchmark show the superior performance of our method, achieving the 1st on the\nleaderboard without any bells and whistles, by the time the paper is submitted.",
    "published": "2022-07-26T05:16:28Z",
    "updated": "2022-07-26T05:16:28Z",
    "authors": [
      "Junbo Yin",
      "Jianbing Shen",
      "Xin Gao",
      "David Crandall",
      "Ruigang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.06218v3",
    "title": "A Unified View of Long-Sequence Models towards Modeling Million-Scale\n  Dependencies",
    "summary": "Ever since their conception, Transformers have taken over traditional\nsequence models in many tasks, such as NLP, image classification, and\nvideo/audio processing, for their fast training and superior performance. Much\nof the merit is attributable to positional encoding and multi-head attention.\nHowever, Transformers fall short in learning long-range dependencies mainly due\nto the quadratic complexity scaled with context length, in terms of both time\nand space. Consequently, over the past five years, a myriad of methods has been\nproposed to make Transformers more efficient. In this work, we first take a\nstep back, study and compare existing solutions to long-sequence modeling in\nterms of their pure mathematical formulation. Specifically, we summarize them\nusing a unified template, given their shared nature of token mixing. Through\nbenchmarks, we then demonstrate that long context length does yield better\nperformance, albeit application-dependent, and traditional Transformer models\nfall short in taking advantage of long-range dependencies. Next, inspired by\nemerging sparse models of huge capacity, we propose a machine learning system\nfor handling million-scale dependencies. As a proof of concept, we evaluate the\nperformance of one essential component of this system, namely, the distributed\nmulti-head attention. We show that our algorithm can scale up attention\ncomputation by almost $40\\times$ using four GeForce RTX 4090 GPUs, compared to\nvanilla multi-head attention mechanism. We believe this study is an\ninstrumental step towards modeling million-scale dependencies.",
    "published": "2023-02-13T09:47:31Z",
    "updated": "2023-02-16T08:55:43Z",
    "authors": [
      "Hongyu HÃ¨",
      "Marko Kabic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.07408v1",
    "title": "Pose-Oriented Transformer with Uncertainty-Guided Refinement for\n  2D-to-3D Human Pose Estimation",
    "summary": "There has been a recent surge of interest in introducing transformers to 3D\nhuman pose estimation (HPE) due to their powerful capabilities in modeling\nlong-term dependencies. However, existing transformer-based methods treat body\njoints as equally important inputs and ignore the prior knowledge of human\nskeleton topology in the self-attention mechanism. To tackle this issue, in\nthis paper, we propose a Pose-Oriented Transformer (POT) with uncertainty\nguided refinement for 3D HPE. Specifically, we first develop novel\npose-oriented self-attention mechanism and distance-related position embedding\nfor POT to explicitly exploit the human skeleton topology. The pose-oriented\nself-attention mechanism explicitly models the topological interactions between\nbody joints, whereas the distance-related position embedding encodes the\ndistance of joints to the root joint to distinguish groups of joints with\ndifferent difficulties in regression. Furthermore, we present an\nUncertainty-Guided Refinement Network (UGRN) to refine pose predictions from\nPOT, especially for the difficult joints, by considering the estimated\nuncertainty of each joint with uncertainty-guided sampling strategy and\nself-attention mechanism. Extensive experiments demonstrate that our method\nsignificantly outperforms the state-of-the-art methods with reduced model\nparameters on 3D HPE benchmarks such as Human3.6M and MPI-INF-3DHP",
    "published": "2023-02-15T00:22:02Z",
    "updated": "2023-02-15T00:22:02Z",
    "authors": [
      "Han Li",
      "Bowen Shi",
      "Wenrui Dai",
      "Hongwei Zheng",
      "Botao Wang",
      "Yu Sun",
      "Min Guo",
      "Chenlin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.03140v1",
    "title": "From Saliency to DINO: Saliency-guided Vision Transformer for Few-shot\n  Keypoint Detection",
    "summary": "Unlike current deep keypoint detectors that are trained to recognize limited\nnumber of body parts, few-shot keypoint detection (FSKD) attempts to localize\nany keypoints, including novel or base keypoints, depending on the reference\nsamples. FSKD requires the semantically meaningful relations for keypoint\nsimilarity learning to overcome the ubiquitous noise and ambiguous local\npatterns. One rescue comes with vision transformer (ViT) as it captures\nlong-range relations well. However, ViT may model irrelevant features outside\nof the region of interest due to the global attention matrix, thus degrading\nsimilarity learning between support and query features. In this paper, we\npresent a novel saliency-guided vision transformer, dubbed SalViT, for few-shot\nkeypoint detection. Our SalViT enjoys a uniquely designed masked self-attention\nand a morphology learner, where the former introduces saliency map as a soft\nmask to constrain the self-attention on foregrounds, while the latter leverages\nthe so-called power normalization to adjust morphology of saliency map,\nrealizing ``dynamically changing receptive field''. Moreover, as salinecy\ndetectors add computations, we show that attentive masks of DINO transformer\ncan replace saliency. On top of SalViT, we also investigate i) transductive\nFSKD that enhances keypoint representations with unlabelled data and ii) FSKD\nunder occlusions. We show that our model performs well on five public datasets\nand achieves ~10% PCK higher than the normally trained model under severe\nocclusions.",
    "published": "2023-04-06T15:22:34Z",
    "updated": "2023-04-06T15:22:34Z",
    "authors": [
      "Changsheng Lu",
      "Hao Zhu",
      "Piotr Koniusz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.16175v3",
    "title": "$\\mathbf{C}^2$Former: Calibrated and Complementary Transformer for\n  RGB-Infrared Object Detection",
    "summary": "Object detection on visible (RGB) and infrared (IR) images, as an emerging\nsolution to facilitate robust detection for around-the-clock applications, has\nreceived extensive attention in recent years. With the help of IR images,\nobject detectors have been more reliable and robust in practical applications\nby using RGB-IR combined information. However, existing methods still suffer\nfrom modality miscalibration and fusion imprecision problems. Since transformer\nhas the powerful capability to model the pairwise correlations between\ndifferent features, in this paper, we propose a novel Calibrated and\nComplementary Transformer called $\\mathrm{C}^2$Former to address these two\nproblems simultaneously. In $\\mathrm{C}^2$Former, we design an Inter-modality\nCross-Attention (ICA) module to obtain the calibrated and complementary\nfeatures by learning the cross-attention relationship between the RGB and IR\nmodality. To reduce the computational cost caused by computing the global\nattention in ICA, an Adaptive Feature Sampling (AFS) module is introduced to\ndecrease the dimension of feature maps. Because $\\mathrm{C}^2$Former performs\nin the feature domain, it can be embedded into existed RGB-IR object detectors\nvia the backbone network. Thus, one single-stage and one two-stage object\ndetector both incorporating our $\\mathrm{C}^2$Former are constructed to\nevaluate its effectiveness and versatility. With extensive experiments on the\nDroneVehicle and KAIST RGB-IR datasets, we verify that our method can fully\nutilize the RGB-IR complementary information and achieve robust detection\nresults. The code is available at\nhttps://github.com/yuanmaoxun/Calibrated-and-Complementary-Transformer-for-RGB-Infrared-Object-Detection.git.",
    "published": "2023-06-28T12:52:48Z",
    "updated": "2024-03-13T10:57:24Z",
    "authors": [
      "Maoxun Yuan",
      "Xingxing Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.17759v2",
    "title": "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width\n  Limit",
    "summary": "In deep learning theory, the covariance matrix of the representations serves\nas a proxy to examine the network's trainability. Motivated by the success of\nTransformers, we study the covariance matrix of a modified Softmax-based\nattention model with skip connections in the proportional limit of\ninfinite-depth-and-width. We show that at initialization the limiting\ndistribution can be described by a stochastic differential equation (SDE)\nindexed by the depth-to-width ratio. To achieve a well-defined stochastic\nlimit, the Transformer's attention mechanism is modified by centering the\nSoftmax output at identity, and scaling the Softmax logits by a width-dependent\ntemperature parameter. We examine the stability of the network through the\ncorresponding SDE, showing how the scale of both the drift and diffusion can be\nelegantly controlled with the aid of residual connections. The existence of a\nstable SDE implies that the covariance structure is well-behaved, even for very\nlarge depth and width, thus preventing the notorious issues of rank degeneracy\nin deep attention models. Finally, we show, through simulations, that the SDE\nprovides a surprisingly good description of the corresponding finite-size\nmodel. We coin the name shaped Transformer for these architectural\nmodifications.",
    "published": "2023-06-30T16:10:36Z",
    "updated": "2023-12-09T19:59:40Z",
    "authors": [
      "Lorenzo Noci",
      "Chuning Li",
      "Mufan Bill Li",
      "Bobby He",
      "Thomas Hofmann",
      "Chris Maddison",
      "Daniel M. Roy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.13442v2",
    "title": "Unlocking Fine-Grained Details with Wavelet-based High-Frequency\n  Enhancement in Transformers",
    "summary": "Medical image segmentation is a critical task that plays a vital role in\ndiagnosis, treatment planning, and disease monitoring. Accurate segmentation of\nanatomical structures and abnormalities from medical images can aid in the\nearly detection and treatment of various diseases. In this paper, we address\nthe local feature deficiency of the Transformer model by carefully re-designing\nthe self-attention map to produce accurate dense prediction in medical images.\nTo this end, we first apply the wavelet transformation to decompose the input\nfeature map into low-frequency (LF) and high-frequency (HF) subbands. The LF\nsegment is associated with coarse-grained features while the HF components\npreserve fine-grained features such as texture and edge information. Next, we\nreformulate the self-attention operation using the efficient Transformer to\nperform both spatial and context attention on top of the frequency\nrepresentation. Furthermore, to intensify the importance of the boundary\ninformation, we impose an additional attention map by creating a Gaussian\npyramid on top of the HF components. Moreover, we propose a multi-scale context\nenhancement block within skip connections to adaptively model inter-scale\ndependencies to overcome the semantic gap among stages of the encoder and\ndecoder modules. Throughout comprehensive experiments, we demonstrate the\neffectiveness of our strategy on multi-organ and skin lesion segmentation\nbenchmarks. The implementation code will be available upon acceptance.\n\\href{https://github.com/mindflow-institue/WaveFormer}{GitHub}.",
    "published": "2023-08-25T15:42:19Z",
    "updated": "2023-09-12T18:41:16Z",
    "authors": [
      "Reza Azad",
      "Amirhossein Kazerouni",
      "Alaa Sulaiman",
      "Afshin Bozorgpour",
      "Ehsan Khodapanah Aghdam",
      "Abin Jose",
      "Dorit Merhof"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.03817v1",
    "title": "Logical Languages Accepted by Transformer Encoders with Hard Attention",
    "summary": "We contribute to the study of formal languages that can be recognized by\ntransformer encoders. We focus on two self-attention mechanisms: (1) UHAT\n(Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention\nTransformers). UHAT encoders are known to recognize only languages inside the\ncircuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized\nand depth-bounded boolean circuits with unbounded fan-ins. On the other hand,\nAHAT encoders can recognize languages outside ${\\sf AC}^0$), but their\nexpressive power still lies within the bigger circuit complexity class ${\\sf\nTC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates. We first show a\nnegative result that there is an ${\\sf AC}^0$-language that cannot be\nrecognized by an UHAT encoder. On the positive side, we show that UHAT encoders\ncan recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages\ndefinable in first-order logic with arbitrary unary numerical predicates. This\nlogic, includes, for example, all regular languages from ${\\sf AC}^0$. We then\nshow that AHAT encoders can recognize all languages of our logic even when we\nenrich it with counting terms. We apply these results to derive new results on\nthe expressive power of UHAT and AHAT up to permutation of letters (a.k.a.\nParikh images).",
    "published": "2023-10-05T18:13:40Z",
    "updated": "2023-10-05T18:13:40Z",
    "authors": [
      "Pablo Barcelo",
      "Alexander Kozachinskiy",
      "Anthony Widjaja Lin",
      "Vladimir Podolskii"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.04134v2",
    "title": "TiC: Exploring Vision Transformer in Convolution",
    "summary": "While models derived from Vision Transformers (ViTs) have been phonemically\nsurging, pre-trained models cannot seamlessly adapt to arbitrary resolution\nimages without altering the architecture and configuration, such as sampling\nthe positional encoding, limiting their flexibility for various vision tasks.\nFor instance, the Segment Anything Model (SAM) based on ViT-Huge requires all\ninput images to be resized to 1024$\\times$1024. To overcome this limitation, we\npropose the Multi-Head Self-Attention Convolution (MSA-Conv) that incorporates\nSelf-Attention within generalized convolutions, including standard, dilated,\nand depthwise ones. Enabling transformers to handle images of varying sizes\nwithout retraining or rescaling, the use of MSA-Conv further reduces\ncomputational costs compared to global attention in ViT, which grows costly as\nimage size increases. Later, we present the Vision Transformer in Convolution\n(TiC) as a proof of concept for image classification with MSA-Conv, where two\ncapacity enhancing strategies, namely Multi-Directional Cyclic Shifted\nMechanism and Inter-Pooling Mechanism, have been proposed, through establishing\nlong-distance connections between tokens and enlarging the effective receptive\nfield. Extensive experiments have been carried out to validate the overall\neffectiveness of TiC. Additionally, ablation studies confirm the performance\nimprovement made by MSA-Conv and the two capacity enhancing strategies\nseparately. Note that our proposal aims at studying an alternative to the\nglobal attention used in ViT, while MSA-Conv meets our goal by making TiC\ncomparable to state-of-the-art on ImageNet-1K. Code will be released at\nhttps://github.com/zs670980918/MSA-Conv.",
    "published": "2023-10-06T10:16:26Z",
    "updated": "2024-05-27T14:37:59Z",
    "authors": [
      "Song Zhang",
      "Qingzhong Wang",
      "Jiang Bian",
      "Haoyi Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.17683v1",
    "title": "Sliceformer: Make Multi-head Attention as Simple as Sorting in\n  Discriminative Tasks",
    "summary": "As one of the most popular neural network modules, Transformer plays a\ncentral role in many fundamental deep learning models, e.g., the ViT in\ncomputer vision and the BERT and GPT in natural language processing. The\neffectiveness of the Transformer is often attributed to its multi-head\nattention (MHA) mechanism. In this study, we discuss the limitations of MHA,\nincluding the high computational complexity due to its ``query-key-value''\narchitecture and the numerical issue caused by its softmax operation.\nConsidering the above problems and the recent development tendency of the\nattention layer, we propose an effective and efficient surrogate of the\nTransformer, called Sliceformer. Our Sliceformer replaces the classic MHA\nmechanism with an extremely simple ``slicing-sorting'' operation, i.e.,\nprojecting inputs linearly to a latent space and sorting them along different\nfeature dimensions (or equivalently, called channels). For each feature\ndimension, the sorting operation implicitly generates an implicit attention map\nwith sparse, full-rank, and doubly-stochastic structures. We consider different\nimplementations of the slicing-sorting operation and analyze their impacts on\nthe Sliceformer. We test the Sliceformer in the Long-Range Arena benchmark,\nimage classification, text classification, and molecular property prediction,\ndemonstrating its advantage in computational complexity and universal\neffectiveness in discriminative tasks. Our Sliceformer achieves comparable or\nbetter performance with lower memory cost and faster speed than the Transformer\nand its variants. Moreover, the experimental results reveal that applying our\nSliceformer can empirically suppress the risk of mode collapse when\nrepresenting data. The code is available at\n\\url{https://github.com/SDS-Lab/sliceformer}.",
    "published": "2023-10-26T14:43:07Z",
    "updated": "2023-10-26T14:43:07Z",
    "authors": [
      "Shen Yuan",
      "Hongteng Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.11112v1",
    "title": "ConDaFormer: Disassembled Transformer with Local Structure Enhancement\n  for 3D Point Cloud Understanding",
    "summary": "Transformers have been recently explored for 3D point cloud understanding\nwith impressive progress achieved. A large number of points, over 0.1 million,\nmake the global self-attention infeasible for point cloud data. Thus, most\nmethods propose to apply the transformer in a local region, e.g., spherical or\ncubic window. However, it still contains a large number of Query-Key pairs,\nwhich requires high computational costs. In addition, previous methods usually\nlearn the query, key, and value using a linear projection without modeling the\nlocal 3D geometric structure. In this paper, we attempt to reduce the costs and\nmodel the local geometry prior by developing a new transformer block, named\nConDaFormer. Technically, ConDaFormer disassembles the cubic window into three\northogonal 2D planes, leading to fewer points when modeling the attention in a\nsimilar range. The disassembling operation is beneficial to enlarging the range\nof attention without increasing the computational complexity, but ignores some\ncontexts. To provide a remedy, we develop a local structure enhancement\nstrategy that introduces a depth-wise convolution before and after the\nattention. This scheme can also capture the local geometric information. Taking\nadvantage of these designs, ConDaFormer captures both long-range contextual\ninformation and local priors. The effectiveness is demonstrated by experimental\nresults on several 3D point cloud understanding benchmarks. Code is available\nat https://github.com/LHDuan/ConDaFormer .",
    "published": "2023-12-18T11:19:45Z",
    "updated": "2023-12-18T11:19:45Z",
    "authors": [
      "Lunhao Duan",
      "Shanshan Zhao",
      "Nan Xue",
      "Mingming Gong",
      "Gui-Song Xia",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.14502v1",
    "title": "ViStripformer: A Token-Efficient Transformer for Versatile Video\n  Restoration",
    "summary": "Video restoration is a low-level vision task that seeks to restore clean,\nsharp videos from quality-degraded frames. One would use the temporal\ninformation from adjacent frames to make video restoration successful.\nRecently, the success of the Transformer has raised awareness in the\ncomputer-vision community. However, its self-attention mechanism requires much\nmemory, which is unsuitable for high-resolution vision tasks like video\nrestoration. In this paper, we propose ViStripformer (Video Stripformer), which\nutilizes spatio-temporal strip attention to catch long-range data correlations,\nconsisting of intra-frame strip attention (Intra-SA) and inter-frame strip\nattention (Inter-SA) for extracting spatial and temporal information. It\ndecomposes video frames into strip-shaped features in horizontal and vertical\ndirections for Intra-SA and Inter-SA to address degradation patterns with\nvarious orientations and magnitudes. Besides, ViStripformer is an effective and\nefficient transformer architecture with much lower memory usage than the\nvanilla transformer. Extensive experiments show that the proposed model\nachieves superior results with fast inference time on video restoration tasks,\nincluding video deblurring, demoireing, and deraining.",
    "published": "2023-12-22T08:05:38Z",
    "updated": "2023-12-22T08:05:38Z",
    "authors": [
      "Fu-Jen Tsai",
      "Yan-Tsung Peng",
      "Chen-Yu Chang",
      "Chan-Yu Li",
      "Yen-Yu Lin",
      "Chung-Chi Tsai",
      "Chia-Wen Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.04743v2",
    "title": "Speech Emotion Recognition Via CNN-Transformer and Multidimensional\n  Attention Mechanism",
    "summary": "Speech Emotion Recognition (SER) is crucial in human-machine interactions.\nMainstream approaches utilize Convolutional Neural Networks or Recurrent Neural\nNetworks to learn local energy feature representations of speech segments from\nspeech information, but struggle with capturing global information such as the\nduration of energy in speech. Some use Transformers to capture global\ninformation, but there is room for improvement in terms of parameter count and\nperformance. Furthermore, existing attention mechanisms focus on spatial or\nchannel dimensions, hindering learning of important temporal information in\nspeech. In this paper, to model local and global information at different\nlevels of granularity in speech and capture temporal, spatial and channel\ndependencies in speech signals, we propose a Speech Emotion Recognition network\nbased on CNN-Transformer and multi-dimensional attention mechanisms.\nSpecifically, a stack of CNN blocks is dedicated to capturing local information\nin speech from a time-frequency perspective. In addition, a time-channel-space\nattention mechanism is used to enhance features across three dimensions.\nMoreover, we model local and global dependencies of feature sequences using\nlarge convolutional kernels with depthwise separable convolutions and\nlightweight Transformer modules. We evaluate the proposed method on IEMOCAP and\nEmo-DB datasets and show our approach significantly improves the performance\nover the state-of-the-art methods.",
    "published": "2024-03-07T18:49:29Z",
    "updated": "2024-06-04T14:56:01Z",
    "authors": [
      "Xiaoyu Tang",
      "Yixin Lin",
      "Ting Dang",
      "Yuanfang Zhang",
      "Jintao Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.15721v2",
    "title": "SPARO: Selective Attention for Robust and Compositional Transformer\n  Encodings for Vision",
    "summary": "Selective attention helps us focus on task-relevant aspects in the constant\nflood of our sensory input. This constraint in our perception allows us to\nrobustly generalize under distractions and to new compositions of perceivable\nconcepts. Transformers employ a similar notion of attention in their\narchitecture, but representation learning models with transformer backbones\nlike CLIP and DINO often fail to demonstrate robustness and compositionality.\nWe highlight a missing architectural prior: unlike human perception,\ntransformer encodings do not separately attend over individual concepts. In\nresponse, we propose SPARO, a read-out mechanism that partitions encodings into\nseparately-attended slots, each produced by a single attention head. Using\nSPARO with CLIP imparts an inductive bias that the vision and text modalities\nare different views of a shared compositional world with the same corresponding\nconcepts. Using SPARO, we demonstrate improvements on downstream recognition,\nrobustness, retrieval, and compositionality benchmarks with CLIP (up to +14%\nfor ImageNet, +4% for SugarCrepe), and on nearest neighbors and linear probe\nfor ImageNet with DINO (+3% each). We also showcase a powerful ability to\nintervene and select individual SPARO concepts to further improve downstream\ntask performance (up from +4% to +9% for SugarCrepe) and use this ability to\nstudy the robustness of SPARO's representation structure. Finally, we provide\ninsights through ablation experiments and visualization of learned concepts.",
    "published": "2024-04-24T08:15:36Z",
    "updated": "2024-09-14T05:05:01Z",
    "authors": [
      "Ankit Vani",
      "Bac Nguyen",
      "Samuel Lavoie",
      "Ranjay Krishna",
      "Aaron Courville"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.07395v1",
    "title": "CaFA: Global Weather Forecasting with Factorized Attention on Sphere",
    "summary": "Accurate weather forecasting is crucial in various sectors, impacting\ndecision-making processes and societal events. Data-driven approaches based on\nmachine learning models have recently emerged as a promising alternative to\nnumerical weather prediction models given their potential to capture physics of\ndifferent scales from historical data and the significantly lower computational\ncost during the prediction stage. Renowned for its state-of-the-art performance\nacross diverse domains, the Transformer model has also gained popularity in\nmachine learning weather prediction. Yet applying Transformer architectures to\nweather forecasting, particularly on a global scale is computationally\nchallenging due to the quadratic complexity of attention and the quadratic\nincrease in spatial points as resolution increases. In this work, we propose a\nfactorized-attention-based model tailored for spherical geometries to mitigate\nthis issue. More specifically, it utilizes multi-dimensional factorized kernels\nthat convolve over different axes where the computational complexity of the\nkernel is only quadratic to the axial resolution instead of overall resolution.\nThe deterministic forecasting accuracy of the proposed model on $1.5^\\circ$ and\n0-7 days' lead time is on par with state-of-the-art purely data-driven machine\nlearning weather prediction models. We also showcase the proposed model holds\ngreat potential to push forward the Pareto front of accuracy-efficiency for\nTransformer weather models, where it can achieve better accuracy with less\ncomputational cost compared to Transformer based models with standard\nattention.",
    "published": "2024-05-12T23:18:14Z",
    "updated": "2024-05-12T23:18:14Z",
    "authors": [
      "Zijie Li",
      "Anthony Zhou",
      "Saurabh Patil",
      "Amir Barati Farimani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.11941v1",
    "title": "Crossfusor: A Cross-Attention Transformer Enhanced Conditional Diffusion\n  Model for Car-Following Trajectory Prediction",
    "summary": "Vehicle trajectory prediction is crucial for advancing autonomous driving and\nadvanced driver assistance systems (ADAS), enhancing road safety and traffic\nefficiency. While traditional methods have laid foundational work, modern deep\nlearning techniques, particularly transformer-based models and generative\napproaches, have significantly improved prediction accuracy by capturing\ncomplex and non-linear patterns in vehicle motion and traffic interactions.\nHowever, these models often overlook the detailed car-following behaviors and\ninter-vehicle interactions essential for real-world driving scenarios. This\nstudy introduces a Cross-Attention Transformer Enhanced Conditional Diffusion\nModel (Crossfusor) specifically designed for car-following trajectory\nprediction. Crossfusor integrates detailed inter-vehicular interactions and\ncar-following dynamics into a robust diffusion framework, improving both the\naccuracy and realism of predicted trajectories. The model leverages a novel\ntemporal feature encoding framework combining GRU, location-based attention\nmechanisms, and Fourier embedding to capture historical vehicle dynamics. It\nemploys noise scaled by these encoded historical features in the forward\ndiffusion process, and uses a cross-attention transformer to model intricate\ninter-vehicle dependencies in the reverse denoising process. Experimental\nresults on the NGSIM dataset demonstrate that Crossfusor outperforms\nstate-of-the-art models, particularly in long-term predictions, showcasing its\npotential for enhancing the predictive capabilities of autonomous driving\nsystems.",
    "published": "2024-06-17T17:35:47Z",
    "updated": "2024-06-17T17:35:47Z",
    "authors": [
      "Junwei You",
      "Haotian Shi",
      "Keshu Wu",
      "Keke Long",
      "Sicheng Fu",
      "Sikai Chen",
      "Bin Ran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.02654v1",
    "title": "Deconstructing Recurrence, Attention, and Gating: Investigating the\n  transferability of Transformers and Gated Recurrent Neural Networks in\n  forecasting of dynamical systems",
    "summary": "Machine learning architectures, including transformers and recurrent neural\nnetworks (RNNs) have revolutionized forecasting in applications ranging from\ntext processing to extreme weather. Notably, advanced network architectures,\ntuned for applications such as natural language processing, are transferable to\nother tasks such as spatiotemporal forecasting tasks. However, there is a\nscarcity of ablation studies to illustrate the key components that enable this\nforecasting accuracy. The absence of such studies, although explainable due to\nthe associated computational cost, intensifies the belief that these models\nought to be considered as black boxes. In this work, we decompose the key\narchitectural components of the most powerful neural architectures, namely\ngating and recurrence in RNNs, and attention mechanisms in transformers. Then,\nwe synthesize and build novel hybrid architectures from the standard blocks,\nperforming ablation studies to identify which mechanisms are effective for each\ntask. The importance of considering these components as hyper-parameters that\ncan augment the standard architectures is exhibited on various forecasting\ndatasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96\nsystem, the Kuramoto-Sivashinsky equation, as well as standard real world\ntime-series benchmarks. A key finding is that neural gating and attention\nimproves the performance of all standard RNNs in most tasks, while the addition\nof a notion of recurrence in transformers is detrimental. Furthermore, our\nstudy reveals that a novel, sparsely used, architecture which integrates\nRecurrent Highway Networks with neural gating and attention mechanisms, emerges\nas the best performing architecture in high-dimensional spatiotemporal\nforecasting of dynamical systems.",
    "published": "2024-10-03T16:41:51Z",
    "updated": "2024-10-03T16:41:51Z",
    "authors": [
      "Hunter S. Heidenreich",
      "Pantelis R. Vlachas",
      "Petros Koumoutsakos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.09420v3",
    "title": "SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph\n  Attention for Vision Transformers",
    "summary": "Vision Transformers (ViTs) have redefined image classification by leveraging\nself-attention to capture complex patterns and long-range dependencies between\nimage patches. However, a key challenge for ViTs is efficiently incorporating\nmulti-scale feature representations, which is inherent in convolutional neural\nnetworks (CNNs) through their hierarchical structure. Graph transformers have\nmade strides in addressing this by leveraging graph-based modeling, but they\noften lose or insufficiently represent spatial hierarchies, especially since\nredundant or less relevant areas dilute the image's contextual representation.\nTo bridge this gap, we propose SAG-ViT, a Scale-Aware Graph Attention ViT that\nintegrates multi-scale feature capabilities of CNNs, representational power of\nViTs, graph-attended patching to enable richer contextual representation. Using\nEfficientNetV2 as a backbone, the model extracts multi-scale feature maps,\ndividing them into patches to preserve richer semantic information compared to\ndirectly patching the input images. The patches are structured into a graph\nusing spatial and feature similarities, where a Graph Attention Network (GAT)\nrefines the node embeddings. This refined graph representation is then\nprocessed by a Transformer encoder, capturing long-range dependencies and\ncomplex interactions. We evaluate SAG-ViT on benchmark datasets across various\ndomains, validating its effectiveness in advancing image classification tasks.\nOur code and weights are available at https://github.com/shravan-18/SAG-ViT.",
    "published": "2024-11-14T13:15:27Z",
    "updated": "2025-01-08T04:31:16Z",
    "authors": [
      "Shravan Venkatraman",
      "Jaskaran Singh Walia",
      "Joe Dhanith P R"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.00776v4",
    "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
    "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
    "published": "2024-12-01T11:43:46Z",
    "updated": "2025-05-25T05:26:02Z",
    "authors": [
      "Chongyang Zhao",
      "Dong Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.06106v1",
    "title": "Enhanced Computationally Efficient Long LoRA Inspired Perceiver\n  Architectures for Auto-Regressive Language Modeling",
    "summary": "The Transformer architecture has revolutionized the Natural Language\nProcessing field and is the backbone of Large Language Models (LLMs). The\nTransformer uses the attention mechanism that computes the pair-wise similarity\nbetween its input tokens to produce latent vectors that are able to understand\nthe semantic meaning of the input text. One of the challenges in the\nTransformer architecture is the quadratic complexity of the attention mechanism\nthat prohibits the efficient processing of long sequence lengths. While many\nrecent research works have attempted to provide a reduction from $O(n^2)$ time\ncomplexity of attention to semi-linear complexity, it remains an unsolved\nproblem in the sense of maintaining a high performance when such complexity is\nreduced. One of the important works in this respect is the Perceiver class of\narchitectures that have demonstrated excellent performance while reducing the\ncomputation complexity. In this paper, we use the PerceiverAR that was proposed\nfor Auto-Regressive modeling as a baseline, and provide three different\narchitectural enhancements to it with varying computation overhead tradeoffs.\nInspired by the recently proposed efficient attention computation approach of\nLong-LoRA, we then present an equally efficient Perceiver-based architecture\n(termed as Long LoRA Pereceiver - LLP) that can be used as the base\narchitecture in LLMs instead of just a fine-tuning add-on. Our results on\ndifferent benchmarks indicate impressive improvements compared to recent\nTransformer based models.",
    "published": "2024-12-08T23:41:38Z",
    "updated": "2024-12-08T23:41:38Z",
    "authors": [
      "Kaleel Mahmood",
      "Shaoyi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16128v1",
    "title": "Hybrid Knowledge Transfer through Attention and Logit Distillation for\n  On-Device Vision Systems in Agricultural IoT",
    "summary": "Integrating deep learning applications into agricultural IoT systems faces a\nserious challenge of balancing the high accuracy of Vision Transformers (ViTs)\nwith the efficiency demands of resource-constrained edge devices. Large\ntransformer models like the Swin Transformers excel in plant disease\nclassification by capturing global-local dependencies. However, their\ncomputational complexity (34.1 GFLOPs) limits applications and renders them\nimpractical for real-time on-device inference. Lightweight models such as\nMobileNetV3 and TinyML would be suitable for on-device inference but lack the\nrequired spatial reasoning for fine-grained disease detection. To bridge this\ngap, we propose a hybrid knowledge distillation framework that synergistically\ntransfers logit and attention knowledge from a Swin Transformer teacher to a\nMobileNetV3 student model. Our method includes the introduction of adaptive\nattention alignment to resolve cross-architecture mismatch (resolution,\nchannels) and a dual-loss function optimizing both class probabilities and\nspatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled\nMobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%\nreduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU\nand 86ms/image on smartphone CPUs). Key innovations include IoT-centric\nvalidation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching\nattention maps. Comparative experiments show significant improvements over\nstandalone CNNs and prior distillation methods, with a 3.5% accuracy gain over\nMobileNetV3 baselines. Significantly, this work advances real-time,\nenergy-efficient crop monitoring in precision agriculture and demonstrates how\nwe can attain ViT-level diagnostic precision on edge devices. Code and models\nwill be made available for replication after acceptance.",
    "published": "2025-04-21T06:56:41Z",
    "updated": "2025-04-21T06:56:41Z",
    "authors": [
      "Stanley Mugisha",
      "Rashid Kisitu",
      "Florence Tushabe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16455v1",
    "title": "Cross Paradigm Representation and Alignment Transformer for Image\n  Deraining",
    "summary": "Transformer-based networks have achieved strong performance in low-level\nvision tasks like image deraining by utilizing spatial or channel-wise\nself-attention. However, irregular rain patterns and complex geometric overlaps\nchallenge single-paradigm architectures, necessitating a unified framework to\nintegrate complementary global-local and spatial-channel representations. To\naddress this, we propose a novel Cross Paradigm Representation and Alignment\nTransformer (CPRAformer). Its core idea is the hierarchical representation and\nalignment, leveraging the strengths of both paradigms (spatial-channel and\nglobal-local) to aid image reconstruction. It bridges the gap within and\nbetween paradigms, aligning and coordinating them to enable deep interaction\nand fusion of features. Specifically, we use two types of self-attention in the\nTransformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial\npixel refinement self-attention (SPR-SA). SPC-SA enhances global channel\ndependencies through dynamic sparsity, while SPR-SA focuses on spatial rain\ndistribution and fine-grained texture recovery. To address the feature\nmisalignment and knowledge differences between them, we introduce the Adaptive\nAlignment Frequency Module (AAFM), which aligns and interacts with features in\na two-stage progressive manner, enabling adaptive guidance and complementarity.\nThis reduces the information gap within and between paradigms. Through this\nunified cross-paradigm dynamic interaction framework, we achieve the extraction\nof the most valuable interactive fusion information from the two paradigms.\nExtensive experiments demonstrate that our model achieves state-of-the-art\nperformance on eight benchmark datasets and further validates CPRAformer's\nrobustness in other image restoration tasks and downstream applications.",
    "published": "2025-04-23T06:44:46Z",
    "updated": "2025-04-23T06:44:46Z",
    "authors": [
      "Shun Zou",
      "Yi Zou",
      "Juncheng Li",
      "Guangwei Gao",
      "Guojun Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.19740v1",
    "title": "Graph Fourier Transformer with Structure-Frequency Information",
    "summary": "Graph Transformers (GTs) have shown advantages in numerous graph structure\ntasks but their self-attention mechanism ignores the generalization bias of\ngraphs, with existing methods mainly compensating for this bias from aspects\nlike position encoding, attention bias and relative distance yet still having\nsub-optimal performance and being insufficient by only considering the\nstructural perspective of generalization bias. To address this, this paper\nproposes Grafourierformer, which innovatively combines GT with inductive bias\ncontaining Frequency-Structure information by applying Graph Fourier Transform\nto the Attention Matrix: specifically, eigenvalues from the Graph Laplacian\nmatrix are used to construct an Eigenvalue matrix mask (reflecting node\npositions and structural relationships with neighboring nodes to enable\nconsideration of node range structural characteristics and focus on local graph\ndetails), and inverse Fourier transform is employed to extract node\nhigh-frequency and low-frequency features, calculate low-frequency and\nhigh-frequency energy, and construct a node frequency-energy matrix to filter\nthe eigenvalue matrix mask, allowing attention heads to incorporate both graph\nstructural information and node frequency information optimization, adaptively\ndistinguish global trends from local details, and effectively suppress\nredundant information interference. Extensive experiments on various benchmarks\nshow Grafourierformer consistently outperforms GNN and GT-based models in graph\nclassification and node classification tasks, with ablation experiments further\nvalidating the effectiveness and necessity of the method. Codes are available\nat https://github.com/Arichibald/Grafourierformer.git",
    "published": "2025-04-28T12:38:02Z",
    "updated": "2025-04-28T12:38:02Z",
    "authors": [
      "Yonghui Zhai",
      "Yang Zhang",
      "Minghao Shang",
      "Lihua Pang",
      "Yaxin Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.17759v1",
    "title": "LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for\n  Hyperspectral Imaging",
    "summary": "Hyperspectral image classification remains a challenging task due to the high\ndimensionality of spectral data, significant inter-band redundancy, and the\nlimited availability of annotated samples. While recent transformer-based\nmodels have improved the global modeling of spectral-spatial dependencies,\ntheir scalability and adaptability under label-scarce conditions remain\nlimited. In this work, we propose \\textbf{LoLA-SpecViT}(Low-rank adaptation\nLocal Attention Spectral Vision Transformer), a lightweight spectral vision\ntransformer that addresses these limitations through a parameter-efficient\narchitecture tailored to the unique characteristics of hyperspectral imagery.\nOur model combines a 3D convolutional spectral front-end with local\nwindow-based self-attention, enhancing both spectral feature extraction and\nspatial consistency while reducing computational complexity. To further improve\nadaptability, we integrate low-rank adaptation (LoRA) into attention and\nprojection layers, enabling fine-tuning with over 80\\% fewer trainable\nparameters. A novel cyclical learning rate scheduler modulates LoRA adaptation\nstrength during training, improving convergence and generalisation. Extensive\nexperiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and\nSalinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art\nbaselines, achieving up to 99.91\\% accuracy with substantially fewer parameters\nand enhanced robustness under low-label regimes. The proposed framework\nprovides a scalable and generalizable solution for real-world HSI applications\nin agriculture, environmental monitoring, and remote sensing analytics. Our\ncode is available in the following\n\\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.",
    "published": "2025-06-21T16:46:00Z",
    "updated": "2025-06-21T16:46:00Z",
    "authors": [
      "Fadi Abdeladhim Zidi",
      "Djamel Eddine Boukhari",
      "Abdellah Zakaria Sellam",
      "Abdelkrim Ouafi",
      "Cosimo Distante",
      "Salah Eddine Bekhouche",
      "Abdelmalik Taleb-Ahmed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02488v2",
    "title": "MedFormer: Hierarchical Medical Vision Transformer with Content-Aware\n  Dual Sparse Selection Attention",
    "summary": "Medical image recognition serves as a key way to aid in clinical diagnosis,\nenabling more accurate and timely identification of diseases and abnormalities.\nVision transformer-based approaches have proven effective in handling various\nmedical recognition tasks. However, these methods encounter two primary\nchallenges. First, they are often task-specific and architecture-tailored,\nlimiting their general applicability. Second, they usually either adopt full\nattention to model long-range dependencies, resulting in high computational\ncosts, or rely on handcrafted sparse attention, potentially leading to\nsuboptimal performance. To tackle these issues, we present MedFormer, an\nefficient medical vision transformer with two key ideas. First, it employs a\npyramid scaling structure as a versatile backbone for various medical image\nrecognition tasks, including image classification and dense prediction tasks\nsuch as semantic segmentation and lesion detection. This structure facilitates\nhierarchical feature representation while reducing the computation load of\nfeature maps, highly beneficial for boosting performance. Second, it introduces\na novel Dual Sparse Selection Attention (DSSA) with content awareness to\nimprove computational efficiency and robustness against noise while maintaining\nhigh performance. As the core building technique of MedFormer, DSSA is designed\nto explicitly attend to the most relevant content. Theoretical analysis\ndemonstrates that MedFormer outperforms existing medical vision transformers in\nterms of generality and efficiency. Extensive experiments across various\nimaging modality datasets show that MedFormer consistently enhances performance\nin all three medical image recognition tasks mentioned above. MedFormer\nprovides an efficient and versatile solution for medical image recognition,\nwith strong potential for clinical application.",
    "published": "2025-07-03T09:51:45Z",
    "updated": "2025-08-05T03:35:06Z",
    "authors": [
      "Zunhui Xia",
      "Hongxing Li",
      "Libin Lan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.03197v1",
    "title": "Quantifying Cross-Attention Interaction in Transformers for Interpreting\n  TCR-pMHC Binding",
    "summary": "CD8+ \"killer\" T cells and CD4+ \"helper\" T cells play a central role in the\nadaptive immune system by recognizing antigens presented by Major\nHistocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs).\nModeling binding between T cells and the pMHC complex is fundamental to\nunderstanding basic mechanisms of human immune response as well as in\ndeveloping therapies. While transformer-based models such as TULIP have\nachieved impressive performance in this domain, their black-box nature\nprecludes interpretability and thus limits a deeper mechanistic understanding\nof T cell response. Most existing post-hoc explainable AI (XAI) methods are\nconfined to encoder-only, co-attention, or model-specific architectures and\ncannot handle encoder-decoder transformers used in TCR-pMHC modeling. To\naddress this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a\nnew post-hoc method designed to interpret the cross-attention mechanisms in\ntransformer decoders. Quantitative evaluation is a challenge for XAI methods;\nwe have compiled TCR-XAI, a benchmark consisting of 274 experimentally\ndetermined TCR-pMHC structures to serve as ground truth for binding. Using\nthese structures we compute physical distances between relevant amino acid\nresidues in the TCR-pMHC interaction region and evaluate how well our method\nand others estimate the importance of residues in this region across the\ndataset. We show that QCAI achieves state-of-the-art performance on both\ninterpretability and prediction accuracy under the TCR-XAI benchmark.",
    "published": "2025-07-03T22:18:54Z",
    "updated": "2025-07-03T22:18:54Z",
    "authors": [
      "Jiarui Li",
      "Zixiang Yin",
      "Haley Smith",
      "Zhengming Ding",
      "Samuel J. Landry",
      "Ramgopal R. Mettu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.17550v2",
    "title": "In-Context Algorithm Emulation in Fixed-Weight Transformers",
    "summary": "We prove that a minimal Transformer with frozen weights emulates a broad\nclass of algorithms by in-context prompting. We formalize two modes of\nin-context algorithm emulation. In the task-specific mode, for any continuous\nfunction $f: \\mathbb{R} \\to \\mathbb{R}$, we show the existence of a single-head\nsoftmax attention layer whose forward pass reproduces functions of the form\n$f(w^\\top x - y)$ to arbitrary precision. This general template subsumes many\npopular machine learning algorithms (e.g., gradient descent, linear regression,\nridge regression). In the prompt-programmable mode, we prove universality: a\nsingle fixed-weight two-layer softmax attention module emulates all algorithms\nfrom the task-specific class (i.e., each implementable by a single softmax\nattention) via only prompting. Our key idea is to construct prompts that encode\nan algorithm's parameters into token representations, creating sharp\ndot-product gaps that force the softmax attention to follow the intended\ncomputation. This construction requires no feed-forward layers and no parameter\nupdates. All adaptation happens through the prompt alone. Numerical results\ncorroborate our theory. These findings forge a direct link between in-context\nlearning and algorithmic emulation, and offer a simple mechanism for large\nTransformers to serve as prompt-programmable libraries of algorithms. They\nilluminate how GPT-style foundation models may swap algorithms via prompts\nalone, and establish a form of algorithmic universality in modern Transformer\nmodels.",
    "published": "2025-08-24T23:20:31Z",
    "updated": "2025-09-26T15:04:11Z",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Hude Liu",
      "Jennifer Yuntong Zhang",
      "Han Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03339v1",
    "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models",
    "summary": "Transformer models have become the dominant backbone for sequence modeling,\nleveraging self-attention to produce contextualized token representations.\nThese are typically aggregated into fixed-size vectors via pooling operations\nfor downstream tasks. While much of the literature has focused on attention\nmechanisms, the role of pooling remains underexplored despite its critical\nimpact on model behavior. In this paper, we introduce a theoretical framework\nthat rigorously characterizes the expressivity of Transformer-based models\nequipped with widely used pooling methods by deriving closed-form bounds on\ntheir representational capacity and the ability to distinguish similar inputs.\nOur analysis extends to different variations of attention formulations,\ndemonstrating that these bounds hold across diverse architectural variants. We\nempirically evaluate pooling strategies across tasks requiring both global and\nlocal contextual understanding, spanning three major modalities: computer\nvision, natural language processing, and time-series analysis. Results reveal\nconsistent trends in how pooling choices affect accuracy, sensitivity, and\noptimization behavior. Our findings unify theoretical and empirical\nperspectives, providing practical guidance for selecting or designing pooling\nmechanisms suited to specific tasks. This work positions pooling as a key\narchitectural component in Transformer models and lays the foundation for more\nprincipled model design beyond attention alone.",
    "published": "2025-10-02T11:17:24Z",
    "updated": "2025-10-02T11:17:24Z",
    "authors": [
      "Sofiane Ennadir",
      "Levente ZÃ³lyomi",
      "Oleg Smirnov",
      "Tianze Wang",
      "John Pertoft",
      "Filip Cornell",
      "Lele Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1910.05886v2",
    "title": "A New Local Transformation Module for Few-shot Segmentation",
    "summary": "Few-shot segmentation segments object regions of new classes with a few of\nmanual annotations. Its key step is to establish the transformation module\nbetween support images (annotated images) and query images (unlabeled images),\nso that the segmentation cues of support images can guide the segmentation of\nquery images. The existing methods form transformation model based on global\ncues, which however ignores the local cues that are verified in this paper to\nbe very important for the transformation. This paper proposes a new\ntransformation module based on local cues, where the relationship of the local\nfeatures is used for transformation. To enhance the generalization performance\nof the network, the relationship matrix is calculated in a high-dimensional\nmetric embedding space based on cosine distance. In addition, to handle the\nchallenging mapping problem from the low-level local relationships to\nhigh-level semantic cues, we propose to apply generalized inverse matrix of the\nannotation matrix of support images to transform the relationship matrix\nlinearly, which is non-parametric and class-agnostic. The result by the matrix\ntransformation can be regarded as an attention map with high-level semantic\ncues, based on which a transformation module can be built simply.The proposed\ntransformation module is a general module that can be used to replace the\ntransformation module in the existing few-shot segmentation frameworks. We\nverify the effectiveness of the proposed method on Pascal VOC 2012 dataset. The\nvalue of mIoU achieves at 57.0% in 1-shot and 60.6% in 5-shot, which\noutperforms the state-of-the-art method by 1.6% and 3.5%, respectively.",
    "published": "2019-10-14T01:52:21Z",
    "updated": "2019-11-12T01:02:48Z",
    "authors": [
      "Yuwei Yang",
      "Fanman Meng",
      "Hongliang Li",
      "Qingbo Wu",
      "Xiaolong Xu",
      "Shuai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.05464v1",
    "title": "Are Transformers More Robust Than CNNs?",
    "summary": "Transformer emerges as a powerful tool for visual recognition. In addition to\ndemonstrating competitive performance on a broad range of visual benchmarks,\nrecent works also argue that Transformers are much more robust than\nConvolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these\nconclusions are drawn from unfair experimental settings, where Transformers and\nCNNs are compared at different scales and are applied with distinct training\nframeworks. In this paper, we aim to provide the first fair & in-depth\ncomparisons between Transformers and CNNs, focusing on robustness evaluations.\n  With our unified training setup, we first challenge the previous belief that\nTransformers outshine CNNs when measuring adversarial robustness. More\nsurprisingly, we find CNNs can easily be as robust as Transformers on defending\nagainst adversarial attacks, if they properly adopt Transformers' training\nrecipes. While regarding generalization on out-of-distribution samples, we show\npre-training on (external) large-scale datasets is not a fundamental request\nfor enabling Transformers to achieve better performance than CNNs. Moreover,\nour ablations suggest such stronger generalization is largely benefited by the\nTransformer's self-attention-like architectures per se, rather than by other\ntraining setups. We hope this work can help the community better understand and\nbenchmark the robustness of Transformers and CNNs. The code and models are\npublicly available at https://github.com/ytongbai/ViTs-vs-CNNs.",
    "published": "2021-11-10T00:18:59Z",
    "updated": "2021-11-10T00:18:59Z",
    "authors": [
      "Yutong Bai",
      "Jieru Mei",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1612.04949v1",
    "title": "Recurrent Image Captioner: Describing Images with Spatial-Invariant\n  Transformation and Attention Filtering",
    "summary": "Along with the prosperity of recurrent neural network in modelling sequential\ndata and the power of attention mechanism in automatically identify salient\ninformation, image captioning, a.k.a., image description, has been remarkably\nadvanced in recent years. Nonetheless, most existing paradigms may suffer from\nthe deficiency of invariance to images with different scaling, rotation, etc.;\nand effective integration of standalone attention to form a holistic end-to-end\nsystem. In this paper, we propose a novel image captioning architecture, termed\nRecurrent Image Captioner (\\textbf{RIC}), which allows visual encoder and\nlanguage decoder to coherently cooperate in a recurrent manner. Specifically,\nwe first equip CNN-based visual encoder with a differentiable layer to enable\nspatially invariant transformation of visual signals. Moreover, we deploy an\nattention filter module (differentiable) between encoder and decoder to\ndynamically determine salient visual parts. We also employ bidirectional LSTM\nto preprocess sentences for generating better textual representations. Besides,\nwe propose to exploit variational inference to optimize the whole architecture.\nExtensive experimental results on three benchmark datasets (i.e., Flickr8k,\nFlickr30k and MS COCO) demonstrate the superiority of our proposed architecture\nas compared to most of the state-of-the-art methods.",
    "published": "2016-12-15T07:19:46Z",
    "updated": "2016-12-15T07:19:46Z",
    "authors": [
      "Hao Liu",
      "Yang Yang",
      "Fumin Shen",
      "Lixin Duan",
      "Heng Tao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1809.03985v1",
    "title": "On The Alignment Problem In Multi-Head Attention-Based Neural Machine\n  Translation",
    "summary": "This work investigates the alignment problem in state-of-the-art multi-head\nattention models based on the transformer architecture. We demonstrate that\nalignment extraction in transformer models can be improved by augmenting an\nadditional alignment head to the multi-head source-to-target attention\ncomponent. This is used to compute sharper attention weights. We describe how\nto use the alignment head to achieve competitive performance. To study the\neffect of adding the alignment head, we simulate a dictionary-guided\ntranslation task, where the user wants to guide translation using pre-defined\ndictionary entries. Using the proposed approach, we achieve up to $3.8$ % BLEU\nimprovement when using the dictionary, in comparison to $2.4$ % BLEU in the\nbaseline case. We also propose alignment pruning to speed up decoding in\nalignment-based neural machine translation (ANMT), which speeds up translation\nby a factor of $1.8$ without loss in translation performance. We carry out\nexperiments on the shared WMT 2016 English$\\to$Romanian news task and the BOLT\nChinese$\\to$English discussion forum task.",
    "published": "2018-09-11T15:41:12Z",
    "updated": "2018-09-11T15:41:12Z",
    "authors": [
      "Tamer Alkhouli",
      "Gabriel Bretschner",
      "Hermann Ney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1907.02698v1",
    "title": "A Bi-directional Transformer for Musical Chord Recognition",
    "summary": "Chord recognition is an important task since chords are highly abstract and\ndescriptive features of music. For effective chord recognition, it is essential\nto utilize relevant context in audio sequence. While various machine learning\nmodels such as convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) have been employed for the task, most of them have limitations\nin capturing long-term dependency or require training of an additional model.\nIn this work, we utilize a self-attention mechanism for chord recognition to\nfocus on certain regions of chords. Training of the proposed bi-directional\nTransformer for chord recognition (BTC) consists of a single phase while\nshowing competitive performance. Through an attention map analysis, we have\nvisualized how attention was performed. It turns out that the model was able to\ndivide segments of chords by utilizing adaptive receptive field of the\nattention mechanism. Furthermore, it was observed that the model was able to\neffectively capture long-term dependencies, making use of essential information\nregardless of distance.",
    "published": "2019-07-05T07:00:38Z",
    "updated": "2019-07-05T07:00:38Z",
    "authors": [
      "Jonggwon Park",
      "Kyoyun Choi",
      "Sungwook Jeon",
      "Dokyun Kim",
      "Jonghun Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.04862v4",
    "title": "Listen Attentively, and Spell Once: Whole Sentence Generation via a\n  Non-Autoregressive Architecture for Low-Latency Speech Recognition",
    "summary": "Although attention based end-to-end models have achieved promising\nperformance in speech recognition, the multi-pass forward computation in\nbeam-search increases inference time cost, which limits their practical\napplications. To address this issue, we propose a non-autoregressive end-to-end\nspeech recognition system called LASO (listen attentively, and spell once).\nBecause of the non-autoregressive property, LASO predicts a textual token in\nthe sequence without the dependence on other tokens. Without beam-search, the\none-pass propagation much reduces inference time cost of LASO. And because the\nmodel is based on the attention based feedforward structure, the computation\ncan be implemented in parallel efficiently. We conduct experiments on publicly\navailable Chinese dataset AISHELL-1. LASO achieves a character error rate of\n6.4%, which outperforms the state-of-the-art autoregressive transformer model\n(6.7%). The average inference latency is 21 ms, which is 1/50 of the\nautoregressive transformer model.",
    "published": "2020-05-11T04:45:02Z",
    "updated": "2020-08-06T01:26:15Z",
    "authors": [
      "Ye Bai",
      "Jiangyan Yi",
      "Jianhua Tao",
      "Zhengkun Tian",
      "Zhengqi Wen",
      "Shuai Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.06772v1",
    "title": "ViTOL: Vision Transformer for Weakly Supervised Object Localization",
    "summary": "Weakly supervised object localization (WSOL) aims at predicting object\nlocations in an image using only image-level category labels. Common challenges\nthat image classification models encounter when localizing objects are, (a)\nthey tend to look at the most discriminative features in an image that confines\nthe localization map to a very small region, (b) the localization maps are\nclass agnostic, and the models highlight objects of multiple classes in the\nsame image and, (c) the localization performance is affected by background\nnoise. To alleviate the above challenges we introduce the following simple\nchanges through our proposed method ViTOL. We leverage the vision-based\ntransformer for self-attention and introduce a patch-based attention dropout\nlayer (p-ADL) to increase the coverage of the localization map and a gradient\nattention rollout mechanism to generate class-dependent attention maps. We\nconduct extensive quantitative, qualitative and ablation experiments on the\nImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2\nlocalization scores of 70.47% and 73.17% on the two datasets respectively. Code\nis available on https://github.com/Saurav-31/ViTOL",
    "published": "2022-04-14T06:16:34Z",
    "updated": "2022-04-14T06:16:34Z",
    "authors": [
      "Saurav Gupta",
      "Sourav Lakhotia",
      "Abhay Rawat",
      "Rahul Tallamraju"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.13091v1",
    "title": "Attention Consistency on Visual Corruptions for Single-Source Domain\n  Generalization",
    "summary": "Generalizing visual recognition models trained on a single distribution to\nunseen input distributions (i.e. domains) requires making them robust to\nsuperfluous correlations in the training set. In this work, we achieve this\ngoal by altering the training images to simulate new domains and imposing\nconsistent visual attention across the different views of the same sample. We\ndiscover that the first objective can be simply and effectively met through\nvisual corruptions. Specifically, we alter the content of the training images\nusing the nineteen corruptions of the ImageNet-C benchmark and three additional\ntransformations based on Fourier transform. Since these corruptions preserve\nobject locations, we propose an attention consistency loss to ensure that class\nactivation maps across original and corrupted versions of the same training\nsample are aligned. We name our model Attention Consistency on Visual\nCorruptions (ACVC). We show that ACVC consistently achieves the state of the\nart on three single-source domain generalization benchmarks, PACS, COCO, and\nthe large-scale DomainNet.",
    "published": "2022-04-27T17:39:13Z",
    "updated": "2022-04-27T17:39:13Z",
    "authors": [
      "Ilke Cugu",
      "Massimiliano Mancini",
      "Yanbei Chen",
      "Zeynep Akata"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.08431v1",
    "title": "Modeling Context With Linear Attention for Scalable Document-Level\n  Translation",
    "summary": "Document-level machine translation leverages inter-sentence dependencies to\nproduce more coherent and consistent translations. However, these models,\npredominantly based on transformers, are difficult to scale to long documents\nas their attention layers have quadratic complexity in the sequence length.\nRecent efforts on efficient attention improve scalability, but their effect on\ndocument translation remains unexplored. In this work, we investigate the\nefficacy of a recent linear attention model by Peng et al. (2021) on document\ntranslation and augment it with a sentential gate to promote a recency\ninductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018\nagainst the transformer, demonstrating substantially increased decoding speed\non long sequences with similar or better BLEU scores. We show that sentential\ngating further improves translation quality on IWSLT.",
    "published": "2022-10-16T03:41:50Z",
    "updated": "2022-10-16T03:41:50Z",
    "authors": [
      "Zhaofeng Wu",
      "Hao Peng",
      "Nikolaos Pappas",
      "Noah A. Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1901.11359v1",
    "title": "Adding Interpretable Attention to Neural Translation Models Improves\n  Word Alignment",
    "summary": "Multi-layer models with multiple attention heads per layer provide superior\ntranslation quality compared to simpler and shallower models, but determining\nwhat source context is most relevant to each target word is more challenging as\na result. Therefore, deriving high-accuracy word alignments from the\nactivations of a state-of-the-art neural machine translation model is an open\nchallenge. We propose a simple model extension to the Transformer architecture\nthat makes use of its hidden representations and is restricted to attend solely\non encoder information to predict the next word. It can be trained on bilingual\ndata without word-alignment information. We further introduce a novel alignment\ninference procedure which applies stochastic gradient descent to directly\noptimize the attention activations towards a given target word. The resulting\nalignments dramatically outperform the naive approach to interpreting\nTransformer attention activations, and are comparable to Giza++ on two publicly\navailable data sets.",
    "published": "2019-01-31T14:05:02Z",
    "updated": "2019-01-31T14:05:02Z",
    "authors": [
      "Thomas Zenkel",
      "Joern Wuebker",
      "John DeNero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1904.03100v1",
    "title": "Information Aggregation for Multi-Head Attention with\n  Routing-by-Agreement",
    "summary": "Multi-head attention is appealing for its ability to jointly extract\ndifferent types of information from multiple representation subspaces.\nConcerning the information aggregation, a common practice is to use a\nconcatenation followed by a linear transformation, which may not fully exploit\nthe expressiveness of multi-head attention. In this work, we propose to improve\nthe information aggregation for multi-head attention with a more powerful\nrouting-by-agreement algorithm. Specifically, the routing algorithm iteratively\nupdates the proportion of how much a part (i.e. the distinct information\nlearned from a specific subspace) should be assigned to a whole (i.e. the final\noutput representation), based on the agreement between parts and wholes.\nExperimental results on linguistic probing tasks and machine translation tasks\nprove the superiority of the advanced information aggregation over the standard\nlinear transformation.",
    "published": "2019-04-05T14:52:28Z",
    "updated": "2019-04-05T14:52:28Z",
    "authors": [
      "Jian Li",
      "Baosong Yang",
      "Zi-Yi Dou",
      "Xing Wang",
      "Michael R. Lyu",
      "Zhaopeng Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.01057v1",
    "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention",
    "summary": "Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.",
    "published": "2020-10-02T15:38:03Z",
    "updated": "2020-10-02T15:38:03Z",
    "authors": [
      "Ikuya Yamada",
      "Akari Asai",
      "Hiroyuki Shindo",
      "Hideaki Takeda",
      "Yuji Matsumoto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.01857v1",
    "title": "Brain Cancer Survival Prediction on Treatment-na ive MRI using Deep\n  Anchor Attention Learning with Vision Transformer",
    "summary": "Image-based brain cancer prediction models, based on radiomics, quantify the\nradiologic phenotype from magnetic resonance imaging (MRI). However, these\nfeatures are difficult to reproduce because of variability in acquisition and\npreprocessing pipelines. Despite evidence of intra-tumor phenotypic\nheterogeneity, the spatial diversity between different slices within an MRI\nscan has been relatively unexplored using such methods. In this work, we\npropose a deep anchor attention aggregation strategy with a Vision Transformer\nto predict survival risk for brain cancer patients. A Deep Anchor Attention\nLearning (DAAL) algorithm is proposed to assign different weights to\nslice-level representations with trainable distance measurements. We evaluated\nour method on N = 326 MRIs. Our results outperformed attention multiple\ninstance learning-based techniques. DAAL highlights the importance of critical\nslices and corroborates the clinical intuition that inter-slice spatial\ndiversity can reflect disease severity and is implicated in outcome.",
    "published": "2022-02-03T21:33:08Z",
    "updated": "2022-02-03T21:33:08Z",
    "authors": [
      "Xuan Xu",
      "Prateek Prasanna"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.11255v1",
    "title": "Attention-aware contrastive learning for predicting T cell\n  receptor-antigen binding specificity",
    "summary": "It has been verified that only a small fraction of the neoantigens presented\nby MHC class I molecules on the cell surface can elicit T cells. The limitation\ncan be attributed to the binding specificity of T cell receptor (TCR) to\npeptide-MHC complex (pMHC). Computational prediction of T cell binding to\nneoantigens is an challenging and unresolved task. In this paper, we propose an\nattentive-mask contrastive learning model, ATMTCR, for inferring TCR-antigen\nbinding specificity. For each input TCR sequence, we used Transformer encoder\nto transform it to latent representation, and then masked a proportion of\nresidues guided by attention weights to generate its contrastive view.\nPretraining on large-scale TCR CDR3 sequences, we verified that contrastive\nlearning significantly improved the prediction performance of TCR binding to\npeptide-MHC complex (pMHC). Beyond the detection of important amino acids and\ntheir locations in the TCR sequence, our model can also extracted high-order\nsemantic information underlying the TCR-antigen binding specificity. Comparison\nexperiments were conducted on two independent datasets, our method achieved\nbetter performance than other existing algorithms. Moreover, we effectively\nidentified important amino acids and their positional preferences through\nattention weights, which indicated the interpretability of our proposed model.",
    "published": "2022-05-17T10:53:32Z",
    "updated": "2022-05-17T10:53:32Z",
    "authors": [
      "Yiming Fang",
      "Xuejun Liu",
      "Hui Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.08201v1",
    "title": "Understanding Long Documents with Different Position-Aware Attentions",
    "summary": "Despite several successes in document understanding, the practical task for\nlong document understanding is largely under-explored due to several challenges\nin computation and how to efficiently absorb long multimodal input. Most\ncurrent transformer-based approaches only deal with short documents and employ\nsolely textual information for attention due to its prohibitive computation and\nmemory limit. To address those issues in long document understanding, we\nexplore different approaches in handling 1D and new 2D position-aware attention\nwith essentially shortened context. Experimental results show that our proposed\nmodels have advantages for this task based on various evaluation metrics.\nFurthermore, our model makes changes only to the attention and thus can be\neasily adapted to any transformer-based architecture.",
    "published": "2022-08-17T10:13:15Z",
    "updated": "2022-08-17T10:13:15Z",
    "authors": [
      "Hai Pham",
      "Guoxin Wang",
      "Yijuan Lu",
      "Dinei Florencio",
      "Cha Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.09878v1",
    "title": "DPTNet: A Dual-Path Transformer Architecture for Scene Text Detection",
    "summary": "The prosperity of deep learning contributes to the rapid progress in scene\ntext detection. Among all the methods with convolutional networks,\nsegmentation-based ones have drawn extensive attention due to their superiority\nin detecting text instances of arbitrary shapes and extreme aspect ratios.\nHowever, the bottom-up methods are limited to the performance of their\nsegmentation models. In this paper, we propose DPTNet (Dual-Path Transformer\nNetwork), a simple yet effective architecture to model the global and local\ninformation for the scene text detection task. We further propose a parallel\ndesign that integrates the convolutional network with a powerful self-attention\nmechanism to provide complementary clues between the attention path and\nconvolutional path. Moreover, a bi-directional interaction module across the\ntwo paths is developed to provide complementary clues in the channel and\nspatial dimensions. We also upgrade the concentration operation by adding an\nextra multi-head attention layer to it. Our DPTNet achieves state-of-the-art\nresults on the MSRA-TD500 dataset, and provides competitive results on other\nstandard benchmarks in terms of both detection accuracy and speed.",
    "published": "2022-08-21T12:58:45Z",
    "updated": "2022-08-21T12:58:45Z",
    "authors": [
      "Jingyu Lin",
      "Jie Jiang",
      "Yan Yan",
      "Chunchao Guo",
      "Hongfa Wang",
      "Wei Liu",
      "Hanzi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.02615v2",
    "title": "Estimating Extreme 3D Image Rotation with Transformer Cross-Attention",
    "summary": "The estimation of large and extreme image rotation plays a key role in\nmultiple computer vision domains, where the rotated images are related by a\nlimited or a non-overlapping field of view. Contemporary approaches apply\nconvolutional neural networks to compute a 4D correlation volume to estimate\nthe relative rotation between image pairs. In this work, we propose a\ncross-attention-based approach that utilizes CNN feature maps and a\nTransformer-Encoder, to compute the cross-attention between the activation maps\nof the image pairs, which is shown to be an improved equivalent of the 4D\ncorrelation volume, used in previous works. In the suggested approach, higher\nattention scores are associated with image regions that encode visual cues of\nrotation. Our approach is end-to-end trainable and optimizes a simple\nregression loss. It is experimentally shown to outperform contemporary\nstate-of-the-art schemes when applied to commonly used image rotation datasets\nand benchmarks, and establishes a new state-of-the-art accuracy on these\ndatasets. We make our code publicly available.",
    "published": "2023-03-05T09:07:26Z",
    "updated": "2024-03-08T19:29:10Z",
    "authors": [
      "Shay Dekel",
      "Yosi Keller",
      "Martin Cadik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.12585v2",
    "title": "Exploring and Distilling Cross-Modal Information for Image Captioning",
    "summary": "Recently, attention-based encoder-decoder models have been used extensively\nin image captioning. Yet there is still great difficulty for the current\nmethods to achieve deep image understanding. In this work, we argue that such\nunderstanding requires visual attention to correlated image regions and\nsemantic attention to coherent attributes of interest. Based on the\nTransformer, to perform effective attention, we explore image captioning from a\ncross-modal perspective and propose the Global-and-Local Information\nExploring-and-Distilling approach that explores and distills the source\ninformation in vision and language. It globally provides the aspect vector, a\nspatial and relational representation of images based on caption contexts,\nthrough the extraction of salient region groupings and attribute collocations,\nand locally extracts the fine-grained regions and attributes in reference to\nthe aspect vector for word selection. Our Transformer-based model achieves a\nCIDEr score of 129.3 in offline COCO evaluation on the COCO testing set with\nremarkable efficiency in terms of accuracy, speed, and parameter budget.",
    "published": "2020-02-28T07:46:48Z",
    "updated": "2020-03-15T11:53:51Z",
    "authors": [
      "Fenglin Liu",
      "Xuancheng Ren",
      "Yuanxin Liu",
      "Kai Lei",
      "Xu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.00740v3",
    "title": "Influence Patterns for Explaining Information Flow in BERT",
    "summary": "While attention is all you need may be proving true, we do not know why:\nattention-based transformer models such as BERT are superior but how\ninformation flows from input tokens to output predictions are unclear. We\nintroduce influence patterns, abstractions of sets of paths through a\ntransformer model. Patterns quantify and localize the flow of information to\npaths passing through a sequence of model nodes. Experimentally, we find that\nsignificant portion of information flow in BERT goes through skip connections\ninstead of attention heads. We further show that consistency of patterns across\ninstances is an indicator of BERT's performance. Finally, We demonstrate that\npatterns account for far more model performance than previous attention-based\nand layer-based methods.",
    "published": "2020-11-02T04:28:16Z",
    "updated": "2021-12-01T03:04:49Z",
    "authors": [
      "Kaiji Lu",
      "Zifan Wang",
      "Piotr Mardziel",
      "Anupam Datta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.03770v1",
    "title": "Know What You Don't Need: Single-Shot Meta-Pruning for Attention Heads",
    "summary": "Deep pre-trained Transformer models have achieved state-of-the-art results\nover a variety of natural language processing (NLP) tasks. By learning rich\nlanguage knowledge with millions of parameters, these models are usually\noverparameterized and significantly increase the computational overhead in\napplications. It is intuitive to address this issue by model compression. In\nthis work, we propose a method, called Single-Shot Meta-Pruning, to compress\ndeep pre-trained Transformers before fine-tuning. Specifically, we focus on\npruning unnecessary attention heads adaptively for different downstream tasks.\nTo measure the informativeness of attention heads, we train our Single-Shot\nMeta-Pruner (SMP) with a meta-learning paradigm aiming to maintain the\ndistribution of text representations after pruning. Compared with existing\ncompression methods for pre-trained models, our method can reduce the overhead\nof both fine-tuning and inference. Experimental results show that our pruner\ncan selectively prune 50% of attention heads with little impact on the\nperformance on downstream tasks and even provide better text representations.\nThe source code will be released in the future.",
    "published": "2020-11-07T12:58:37Z",
    "updated": "2020-11-07T12:58:37Z",
    "authors": [
      "Zhengyan Zhang",
      "Fanchao Qi",
      "Zhiyuan Liu",
      "Qun Liu",
      "Maosong Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.09645v1",
    "title": "Multi-Task Time Series Forecasting With Shared Attention",
    "summary": "Time series forecasting is a key component in many industrial and business\ndecision processes and recurrent neural network (RNN) based models have\nachieved impressive progress on various time series forecasting tasks. However,\nmost of the existing methods focus on single-task forecasting problems by\nlearning separately based on limited supervised objectives, which often suffer\nfrom insufficient training instances. As the Transformer architecture and other\nattention-based models have demonstrated its great capability of capturing long\nterm dependency, we propose two self-attention based sharing schemes for\nmulti-task time series forecasting which can train jointly across multiple\ntasks. We augment a sequence of paralleled Transformer encoders with an\nexternal public multi-head attention function, which is updated by all data of\nall tasks. Experiments on a number of real-world multi-task time series\nforecasting tasks show that our proposed architectures can not only outperform\nthe state-of-the-art single-task forecasting baselines but also outperform the\nRNN-based multi-task forecasting method.",
    "published": "2021-01-24T04:25:08Z",
    "updated": "2021-01-24T04:25:08Z",
    "authors": [
      "Zekai Chen",
      "Jiaze E",
      "Xiao Zhang",
      "Hao Sheng",
      "Xiuzheng Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.01778v3",
    "title": "AST: Audio Spectrogram Transformer",
    "summary": "In the past decade, convolutional neural networks (CNNs) have been widely\nadopted as the main building block for end-to-end audio classification models,\nwhich aim to learn a direct mapping from audio spectrograms to corresponding\nlabels. To better capture long-range global context, a recent trend is to add a\nself-attention mechanism on top of the CNN, forming a CNN-attention hybrid\nmodel. However, it is unclear whether the reliance on a CNN is necessary, and\nif neural networks purely based on attention are sufficient to obtain good\nperformance in audio classification. In this paper, we answer the question by\nintroducing the Audio Spectrogram Transformer (AST), the first\nconvolution-free, purely attention-based model for audio classification. We\nevaluate AST on various audio classification benchmarks, where it achieves new\nstate-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,\nand 98.1% accuracy on Speech Commands V2.",
    "published": "2021-04-05T05:26:29Z",
    "updated": "2021-07-08T20:16:28Z",
    "authors": [
      "Yuan Gong",
      "Yu-An Chung",
      "James Glass"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.08771v2",
    "title": "Cross-Attention is All You Need: Adapting Pretrained Transformers for\n  Machine Translation",
    "summary": "We study the power of cross-attention in the Transformer architecture within\nthe context of transfer learning for machine translation, and extend the\nfindings of studies into cross-attention when training from scratch. We conduct\na series of experiments through fine-tuning a translation model on data where\neither the source or target language has changed. These experiments reveal that\nfine-tuning only the cross-attention parameters is nearly as effective as\nfine-tuning all parameters (i.e., the entire translation model). We provide\ninsights into why this is the case and observe that limiting fine-tuning in\nthis manner yields cross-lingually aligned embeddings. The implications of this\nfinding for researchers and practitioners include a mitigation of catastrophic\nforgetting, the potential for zero-shot translation, and the ability to extend\nmachine translation models to several new language pairs with reduced parameter\nstorage overhead.",
    "published": "2021-04-18T08:41:01Z",
    "updated": "2021-09-14T16:07:09Z",
    "authors": [
      "Mozhdeh Gheini",
      "Xiang Ren",
      "Jonathan May"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.04803v2",
    "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
    "summary": "Transformers have attracted increasing interests in computer vision, but they\nstill fall behind state-of-the-art convolutional networks. In this work, we\nshow that while Transformers tend to have larger model capacity, their\ngeneralization can be worse than convolutional networks due to the lack of the\nright inductive bias. To effectively combine the strengths from both\narchitectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid\nmodels built from two key insights: (1) depthwise Convolution and\nself-Attention can be naturally unified via simple relative attention; (2)\nvertically stacking convolution layers and attention layers in a principled way\nis surprisingly effective in improving generalization, capacity and efficiency.\nExperiments show that our CoAtNets achieve state-of-the-art performance under\ndifferent resource constraints across various datasets: Without extra data,\nCoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M\nimages from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching\nViT-huge pre-trained with 300M images from JFT-300M while using 23x less data;\nNotably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1\naccuracy on ImageNet, establishing a new state-of-the-art result.",
    "published": "2021-06-09T04:35:31Z",
    "updated": "2021-09-15T06:05:13Z",
    "authors": [
      "Zihang Dai",
      "Hanxiao Liu",
      "Quoc V. Le",
      "Mingxing Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.11906v1",
    "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for\n  Sequences",
    "summary": "We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.",
    "published": "2021-07-25T23:07:03Z",
    "updated": "2021-07-25T23:07:03Z",
    "authors": [
      "Zhenhai Zhu",
      "Radu Soricut"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.05039v1",
    "title": "Symmetry-Enhanced Attention Network for Acute Ischemic Infarct\n  Segmentation with Non-Contrast CT Images",
    "summary": "Quantitative estimation of the acute ischemic infarct is crucial to improve\nneurological outcomes of the patients with stroke symptoms. Since the density\nof lesions is subtle and can be confounded by normal physiologic changes,\nanatomical asymmetry provides useful information to differentiate the ischemic\nand healthy brain tissue. In this paper, we propose a symmetry enhanced\nattention network (SEAN) for acute ischemic infarct segmentation. Our proposed\nnetwork automatically transforms an input CT image into the standard space\nwhere the brain tissue is bilaterally symmetric. The transformed image is\nfurther processed by a Ushape network integrated with the proposed symmetry\nenhanced attention for pixel-wise labelling. The symmetry enhanced attention\ncan efficiently capture context information from the opposite side of the image\nby estimating long-range dependencies. Experimental results show that the\nproposed SEAN outperforms some symmetry-based state-of-the-art methods in terms\nof both dice coefficient and infarct localization.",
    "published": "2021-10-11T07:13:26Z",
    "updated": "2021-10-11T07:13:26Z",
    "authors": [
      "Kongming Liang",
      "Kai Han",
      "Xiuli Li",
      "Xiaoqing Cheng",
      "Yiming Li",
      "Yizhou Wang",
      "Yizhou Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.03940v1",
    "title": "Convolutional Gated MLP: Combining Convolutions & gMLP",
    "summary": "To the best of our knowledge, this is the first paper to introduce\nConvolutions to Gated MultiLayer Perceptron and contributes an implementation\nof this novel Deep Learning architecture. Google Brain introduced the gMLP in\nMay 2021. Microsoft introduced Convolutions in Vision Transformer in Mar 2021.\nInspired by both gMLP and CvT, we introduce convolutional layers in gMLP. CvT\ncombined the power of Convolutions and Attention. Our implementation combines\nthe best of Convolutional learning along with spatial gated MLP. Further, the\npaper visualizes how CgMLP learns. Visualizations show how CgMLP learns from\nfeatures such as outline of a car. While Attention was the basis of much of\nrecent progress in Deep Learning, gMLP proposed an approach that doesn't use\nAttention computation. In Transformer based approaches, a whole lot of\nAttention matrixes need to be learnt using vast amount of training data. In\ngMLP, the fine tunning for new tasks can be challenging by transfer learning\nwith smaller datasets. We implement CgMLP and compares it with gMLP on CIFAR\ndataset. Experimental results explore the power of generaliza-tion of CgMLP,\nwhile gMLP tend to drastically overfit the training data.\n  To summarize, the paper contributes a novel Deep Learning architecture and\ndemonstrates the learning mechanism of CgMLP through visualizations, for the\nfirst time in literature.",
    "published": "2021-11-06T19:11:24Z",
    "updated": "2021-11-06T19:11:24Z",
    "authors": [
      "A. Rajagopal",
      "V. Nirmala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.01527v3",
    "title": "Masked-attention Mask Transformer for Universal Image Segmentation",
    "summary": "Image segmentation is about grouping pixels with different semantics, e.g.,\ncategory or instance membership, where each choice of semantics defines a task.\nWhile only the semantics of each task differ, current research focuses on\ndesigning specialized architectures for each task. We present Masked-attention\nMask Transformer (Mask2Former), a new architecture capable of addressing any\nimage segmentation task (panoptic, instance or semantic). Its key components\ninclude masked attention, which extracts localized features by constraining\ncross-attention within predicted mask regions. In addition to reducing the\nresearch effort by at least three times, it outperforms the best specialized\narchitectures by a significant margin on four popular datasets. Most notably,\nMask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on\nCOCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7\nmIoU on ADE20K).",
    "published": "2021-12-02T18:59:58Z",
    "updated": "2022-06-15T20:58:09Z",
    "authors": [
      "Bowen Cheng",
      "Ishan Misra",
      "Alexander G. Schwing",
      "Alexander Kirillov",
      "Rohit Girdhar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.07254v1",
    "title": "Improving Hybrid CTC/Attention End-to-end Speech Recognition with\n  Pretrained Acoustic and Language Model",
    "summary": "Recently, self-supervised pretraining has achieved impressive results in\nend-to-end (E2E) automatic speech recognition (ASR). However, the dominant\nsequence-to-sequence (S2S) E2E model is still hard to fully utilize the\nself-supervised pre-training methods because its decoder is conditioned on\nacoustic representation thus cannot be pretrained separately. In this paper, we\npropose a pretrained Transformer (Preformer) S2S ASR architecture based on\nhybrid CTC/attention E2E models to fully utilize the pretrained acoustic models\n(AMs) and language models (LMs). In our framework, the encoder is initialized\nwith a pretrained AM (wav2vec2.0). The Preformer leverages CTC as an auxiliary\ntask during training and inference. Furthermore, we design a one-cross decoder\n(OCD), which relaxes the dependence on acoustic representations so that it can\nbe initialized with pretrained LM (DistilGPT2). Experiments are conducted on\nthe AISHELL-1 corpus and achieve a $4.6\\%$ character error rate (CER) on the\ntest set. Compared with our vanilla hybrid CTC/attention Transformer baseline,\nour proposed CTC/attention-based Preformer yields $27\\%$ relative CER\nreduction. To the best of our knowledge, this is the first work to utilize both\npretrained AM and LM in a S2S ASR system.",
    "published": "2021-12-14T09:38:31Z",
    "updated": "2021-12-14T09:38:31Z",
    "authors": [
      "Keqi Deng",
      "Songjun Cao",
      "Yike Zhang",
      "Long Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.04212v3",
    "title": "Measuring the Mixing of Contextual Information in the Transformer",
    "summary": "The Transformer architecture aggregates input information through the\nself-attention mechanism, but there is no clear understanding of how this\ninformation is mixed across the entire model. Additionally, recent works have\ndemonstrated that attention weights alone are not enough to describe the flow\nof information. In this paper, we consider the whole attention block --\nmulti-head attention, residual connection, and layer normalization -- and\ndefine a metric to measure token-to-token interactions within each layer. Then,\nwe aggregate layer-wise interpretations to provide input attribution scores for\nmodel predictions. Experimentally, we show that our method, ALTI (Aggregation\nof Layer-wise Token-to-token Interactions), provides more faithful explanations\nand increased robustness than gradient-based methods.",
    "published": "2022-03-08T17:21:27Z",
    "updated": "2022-10-22T02:00:36Z",
    "authors": [
      "Javier Ferrando",
      "Gerard I. GÃ¡llego",
      "Marta R. Costa-jussÃ "
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.13671v3",
    "title": "Transformer for Partial Differential Equations' Operator Learning",
    "summary": "Data-driven learning of partial differential equations' solution operators\nhas recently emerged as a promising paradigm for approximating the underlying\nsolutions. The solution operators are usually parameterized by deep learning\nmodels that are built upon problem-specific inductive biases. An example is a\nconvolutional or a graph neural network that exploits the local grid structure\nwhere functions' values are sampled. The attention mechanism, on the other\nhand, provides a flexible way to implicitly exploit the patterns within inputs,\nand furthermore, relationship between arbitrary query locations and inputs. In\nthis work, we present an attention-based framework for data-driven operator\nlearning, which we term Operator Transformer (OFormer). Our framework is built\nupon self-attention, cross-attention, and a set of point-wise multilayer\nperceptrons (MLPs), and thus it makes few assumptions on the sampling pattern\nof the input function or query locations. We show that the proposed framework\nis competitive on standard benchmark problems and can flexibly be adapted to\nrandomly sampled input.",
    "published": "2022-05-26T23:17:53Z",
    "updated": "2023-04-27T21:01:23Z",
    "authors": [
      "Zijie Li",
      "Kazem Meidani",
      "Amir Barati Farimani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.10655v3",
    "title": "Mega: Moving Average Equipped Gated Attention",
    "summary": "The design choices in the Transformer attention mechanism, including weak\ninductive bias and quadratic computational complexity, have limited its\napplication for modeling long sequences. In this paper, we introduce Mega, a\nsimple, theoretically grounded, single-head gated attention mechanism equipped\nwith (exponential) moving average to incorporate inductive bias of\nposition-aware local dependencies into the position-agnostic attention\nmechanism. We further propose a variant of Mega that offers linear time and\nspace complexity yet yields only minimal quality loss, by efficiently splitting\nthe whole sequence into multiple chunks with fixed length. Extensive\nexperiments on a wide range of sequence modeling benchmarks, including the Long\nRange Arena, neural machine translation, auto-regressive language modeling, and\nimage and speech classification, show that Mega achieves significant\nimprovements over other sequence models, including variants of Transformers and\nrecent state space models.",
    "published": "2022-09-21T20:52:17Z",
    "updated": "2023-01-28T06:33:20Z",
    "authors": [
      "Xuezhe Ma",
      "Chunting Zhou",
      "Xiang Kong",
      "Junxian He",
      "Liangke Gui",
      "Graham Neubig",
      "Jonathan May",
      "Luke Zettlemoyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.05770v3",
    "title": "Efficient Image Generation with Variadic Attention Heads",
    "summary": "While the integration of transformers in vision models have yielded\nsignificant improvements on vision tasks they still require significant amounts\nof computation for both training and inference. Restricted attention mechanisms\nsignificantly reduce these computational burdens but come at the cost of losing\neither global or local coherence. We propose a simple, yet powerful method to\nreduce these trade-offs: allow the attention heads of a single transformer to\nattend to multiple receptive fields.\n  We demonstrate our method utilizing Neighborhood Attention (NA) and integrate\nit into a StyleGAN based architecture for image generation. With this work,\ndubbed StyleNAT, we are able to achieve a FID of 2.05 on FFHQ, a 6% improvement\nover StyleGAN-XL, while utilizing 28% fewer parameters and with 4$\\times$ the\nthroughput capacity. StyleNAT achieves the Pareto Frontier on FFHQ-256 and\ndemonstrates powerful and efficient image generation on other datasets. Our\ncode and model checkpoints are publicly available at:\nhttps://github.com/SHI-Labs/StyleNAT",
    "published": "2022-11-10T18:55:48Z",
    "updated": "2025-06-26T05:07:48Z",
    "authors": [
      "Steven Walton",
      "Ali Hassani",
      "Xingqian Xu",
      "Zhangyang Wang",
      "Humphrey Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.07722v1",
    "title": "The Birds Need Attention Too: Analysing usage of Self Attention in\n  identifying bird calls in soundscapes",
    "summary": "Birds are vital parts of ecosystems across the world and are an excellent\nmeasure of the quality of life on earth. Many bird species are endangered while\nothers are already extinct. Ecological efforts in understanding and monitoring\nbird populations are important to conserve their habitat and species, but this\nmostly relies on manual methods in rough terrains. Recent advances in Machine\nLearning and Deep Learning have made automatic bird recognition in diverse\nenvironments possible. Birdcall recognition till now has been performed using\nconvolutional neural networks. In this work, we try and understand how\nself-attention can aid in this endeavor. With that we build an pre-trained\nAttention-based Spectrogram Transformer baseline for BirdCLEF 2022 and compare\nthe results against the pre-trained Convolution-based baseline. Our results\nshow that the transformer models outperformed the convolutional model and we\nfurther validate our results by building baselines and analyzing the results\nfor the previous year BirdCLEF 2021 challenge. Source code available at\nhttps://github.com/ck090/BirdCLEF-22",
    "published": "2022-11-14T19:48:45Z",
    "updated": "2022-11-14T19:48:45Z",
    "authors": [
      "Chandra Kanth Nagesh",
      "Abhishek Purushothama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.04869v1",
    "title": "RCDT: Relational Remote Sensing Change Detection with Transformer",
    "summary": "Deep learning based change detection methods have received wide attentoion,\nthanks to their strong capability in obtaining rich features from images.\nHowever, existing AI-based CD methods largely rely on three\nfunctionality-enhancing modules, i.e., semantic enhancement, attention\nmechanisms, and correspondence enhancement. The stacking of these modules leads\nto great model complexity. To unify these three modules into a simple pipeline,\nwe introduce Relational Change Detection Transformer (RCDT), a novel and simple\nframework for remote sensing change detection tasks. The proposed RCDT consists\nof three major components, a weight-sharing Siamese Backbone to obtain\nbi-temporal features, a Relational Cross Attention Module (RCAM) that\nimplements offset cross attention to obtain bi-temporal relation-aware\nfeatures, and a Features Constrain Module (FCM) to achieve the final refined\npredictions with high-resolution constraints. Extensive experiments on four\ndifferent publically available datasets suggest that our proposed RCDT exhibits\nsuperior change detection performance compared with other competing methods.\nThe therotical, methodogical, and experimental knowledge of this study is\nexpected to benefit future change detection efforts that involve the cross\nattention mechanism.",
    "published": "2022-12-09T14:21:42Z",
    "updated": "2022-12-09T14:21:42Z",
    "authors": [
      "Kaixuan Lu",
      "Xiao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.02068v1",
    "title": "Towards Long-Term Time-Series Forecasting: Feature, Pattern, and\n  Distribution",
    "summary": "Long-term time-series forecasting (LTTF) has become a pressing demand in many\napplications, such as wind power supply planning. Transformer models have been\nadopted to deliver high prediction capacity because of the high computational\nself-attention mechanism. Though one could lower the complexity of Transformers\nby inducing the sparsity in point-wise self-attentions for LTTF, the limited\ninformation utilization prohibits the model from exploring the complex\ndependencies comprehensively. To this end, we propose an efficient\nTransformerbased model, named Conformer, which differentiates itself from\nexisting methods for LTTF in three aspects: (i) an encoder-decoder architecture\nincorporating a linear complexity without sacrificing information utilization\nis proposed on top of sliding-window attention and Stationary and Instant\nRecurrent Network (SIRN); (ii) a module derived from the normalizing flow is\ndevised to further improve the information utilization by inferring the outputs\nwith the latent variables in SIRN directly; (iii) the inter-series correlation\nand temporal dynamics in time-series data are modeled explicitly to fuel the\ndownstream self-attention mechanism. Extensive experiments on seven real-world\ndatasets demonstrate that Conformer outperforms the state-of-the-art methods on\nLTTF and generates reliable prediction results with uncertainty quantification.",
    "published": "2023-01-05T13:59:29Z",
    "updated": "2023-01-05T13:59:29Z",
    "authors": [
      "Yan Li",
      "Xinjiang Lu",
      "Haoyi Xiong",
      "Jian Tang",
      "Jiantao Su",
      "Bo Jin",
      "Dejing Dou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.07239v2",
    "title": "T-former: An Efficient Transformer for Image Inpainting",
    "summary": "Benefiting from powerful convolutional neural networks (CNNs), learning-based\nimage inpainting methods have made significant breakthroughs over the years.\nHowever, some nature of CNNs (e.g. local prior, spatially shared parameters)\nlimit the performance in the face of broken images with diverse and complex\nforms. Recently, a class of attention-based network architectures, called\ntransformer, has shown significant performance on natural language processing\nfields and high-level vision tasks. Compared with CNNs, attention operators are\nbetter at long-range modeling and have dynamic weights, but their computational\ncomplexity is quadratic in spatial resolution, and thus less suitable for\napplications involving higher resolution images, such as image inpainting. In\nthis paper, we design a novel attention linearly related to the resolution\naccording to Taylor expansion. And based on this attention, a network called\n$T$-former is designed for image inpainting. Experiments on several benchmark\ndatasets demonstrate that our proposed method achieves state-of-the-art\naccuracy while maintaining a relatively low number of parameters and\ncomputational complexity. The code can be found at\n\\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\\_image\\_inpainting}",
    "published": "2023-05-12T04:10:42Z",
    "updated": "2023-05-19T02:11:54Z",
    "authors": [
      "Ye Deng",
      "Siqi Hui",
      "Sanping Zhou",
      "Deyu Meng",
      "Jinjun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.15302v1",
    "title": "Multi-Modal Mutual Attention and Iterative Interaction for Referring\n  Image Segmentation",
    "summary": "We address the problem of referring image segmentation that aims to generate\na mask for the object specified by a natural language expression. Many recent\nworks utilize Transformer to extract features for the target object by\naggregating the attended visual regions. However, the generic attention\nmechanism in Transformer only uses the language input for attention weight\ncalculation, which does not explicitly fuse language features in its output.\nThus, its output feature is dominated by vision information, which limits the\nmodel to comprehensively understand the multi-modal information, and brings\nuncertainty for the subsequent mask decoder to extract the output mask. To\naddress this issue, we propose Multi-Modal Mutual Attention ($\\mathrm{M^3Att}$)\nand Multi-Modal Mutual Decoder ($\\mathrm{M^3Dec}$) that better fuse information\nfrom the two input modalities. Based on {$\\mathrm{M^3Dec}$}, we further propose\nIterative Multi-modal Interaction ($\\mathrm{IMI}$) to allow continuous and\nin-depth interactions between language and vision features. Furthermore, we\nintroduce Language Feature Reconstruction ($\\mathrm{LFR}$) to prevent the\nlanguage information from being lost or distorted in the extracted feature.\nExtensive experiments show that our proposed approach significantly improves\nthe baseline and outperforms state-of-the-art referring image segmentation\nmethods on RefCOCO series datasets consistently.",
    "published": "2023-05-24T16:26:05Z",
    "updated": "2023-05-24T16:26:05Z",
    "authors": [
      "Chang Liu",
      "Henghui Ding",
      "Yulun Zhang",
      "Xudong Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.18234v1",
    "title": "Temporal Aware Mixed Attention-based Convolution and Transformer Network\n  (MACTN) for EEG Emotion Recognition",
    "summary": "Emotion recognition plays a crucial role in human-computer interaction, and\nelectroencephalography (EEG) is advantageous for reflecting human emotional\nstates. In this study, we propose MACTN, a hierarchical hybrid model for\njointly modeling local and global temporal information. The model is inspired\nby neuroscience research on the temporal dynamics of emotions. MACTN extracts\nlocal emotional features through a convolutional neural network (CNN) and\nintegrates sparse global emotional features through a transformer. Moreover, we\nemploy channel attention mechanisms to identify the most task-relevant\nchannels. Through extensive experimentation on two publicly available datasets,\nnamely THU-EP and DEAP, our proposed method, MACTN, consistently achieves\nsuperior classification accuracy and F1 scores compared to other existing\nmethods in most experimental settings. Furthermore, ablation studies have shown\nthat the integration of both self-attention mechanisms and channel attention\nmechanisms leads to improved classification performance. Finally, an earlier\nversion of this method, which shares the same ideas, won the Emotional BCI\nCompetition's final championship in the 2022 World Robot Contest.",
    "published": "2023-05-18T11:04:50Z",
    "updated": "2023-05-18T11:04:50Z",
    "authors": [
      "Xiaopeng Si",
      "Dong Huang",
      "Yulin Sun",
      "Dong Ming"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.01594v2",
    "title": "A Novel Vision Transformer with Residual in Self-attention for\n  Biomedical Image Classification",
    "summary": "Biomedical image classification requires capturing of bio-informatics based\non specific feature distribution. In most of such applications, there are\nmainly challenges due to limited availability of samples for diseased cases and\nimbalanced nature of dataset. This article presents the novel framework of\nmulti-head self-attention for vision transformer (ViT) which makes capable of\ncapturing the specific image features for classification and analysis. The\nproposed method uses the concept of residual connection for accumulating the\nbest attention output in each block of multi-head attention. The proposed\nframework has been evaluated on two small datasets: (i) blood cell\nclassification dataset and (ii) brain tumor detection using brain MRI images.\nThe results show the significant improvement over traditional ViT and other\nconvolution based state-of-the-art classification models.",
    "published": "2023-06-02T15:06:14Z",
    "updated": "2023-06-05T04:45:36Z",
    "authors": [
      "Arun K. Sharma",
      "Nishchal K. Verma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.05807v2",
    "title": "A Gated Attention Transformer for Multi-Person Pose Tracking",
    "summary": "Multi-person pose tracking is an important element for many applications and\nrequires to estimate the human poses of all persons in a video and to track\nthem over time. The association of poses across frames remains an open research\nproblem, in particular for online tracking methods, due to motion blur, crowded\nscenes and occlusions. To tackle the association challenge, we propose a Gated\nAttention Transformer. The core aspect of our model is the gating mechanism\nthat automatically adapts the impact of appearance embeddings and embeddings\nbased on temporal pose similarity in the attention layers. In order to\nre-identify persons that have been occluded, we incorporate a pose-conditioned\nre-identification network that provides initial embeddings and allows to match\npersons even if the number of visible joints differ between frames. We further\npropose a matching layer based on gated attention for pose-to-track association\nand duplicate removal. We evaluate our approach on PoseTrack 2018 and\nPoseTrack21.",
    "published": "2023-06-09T10:44:44Z",
    "updated": "2023-08-21T17:45:29Z",
    "authors": [
      "Andreas Doering",
      "Juergen Gall"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.02263v1",
    "title": "Efficient Monaural Speech Enhancement using Spectrum Attention Fusion",
    "summary": "Speech enhancement is a demanding task in automated speech processing\npipelines, focusing on separating clean speech from noisy channels. Transformer\nbased models have recently bested RNN and CNN models in speech enhancement,\nhowever at the same time they are much more computationally expensive and\nrequire much more high quality training data, which is always hard to come by.\nIn this paper, we present an improvement for speech enhancement models that\nmaintains the expressiveness of self-attention while significantly reducing\nmodel complexity, which we have termed Spectrum Attention Fusion. We carefully\nconstruct a convolutional module to replace several self-attention layers in a\nspeech Transformer, allowing the model to more efficiently fuse spectral\nfeatures. Our proposed model is able to achieve comparable or better results\nagainst SOTA models but with significantly smaller parameters (0.58M) on the\nVoice Bank + DEMAND dataset.",
    "published": "2023-08-04T11:39:29Z",
    "updated": "2023-08-04T11:39:29Z",
    "authors": [
      "Jinyu Long",
      "Jetic GÅ«",
      "Binhao Bai",
      "Zhibo Yang",
      "Ping Wei",
      "Junli Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.05110v1",
    "title": "Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A\n  Case Study on Hemorrhagic Stroke",
    "summary": "Stroke is a significant cause of mortality and morbidity, necessitating early\npredictive strategies to minimize risks. Traditional methods for evaluating\npatients, such as Acute Physiology and Chronic Health Evaluation (APACHE II,\nIV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy\nand interpretability. This paper proposes a novel approach: an interpretable,\nattention-based transformer model for early stroke mortality prediction. This\nmodel seeks to address the limitations of previous predictive models, providing\nboth interpretability (providing clear, understandable explanations of the\nmodel) and fidelity (giving a truthful explanation of the model's dynamics from\ninput to output). Furthermore, the study explores and compares fidelity and\ninterpretability scores using Shapley values and attention-based scores to\nimprove model explainability. The research objectives include designing an\ninterpretable attention-based transformer model, evaluating its performance\ncompared to existing models, and providing feature importance derived from the\nmodel.",
    "published": "2023-08-04T04:28:07Z",
    "updated": "2023-08-04T04:28:07Z",
    "authors": [
      "Qizhang Feng",
      "Jiayi Yuan",
      "Forhan Bin Emdad",
      "Karim Hanna",
      "Xia Hu",
      "Zhe He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.11295v3",
    "title": "Uncertainty Estimation of Transformers' Predictions via Topological\n  Analysis of the Attention Matrices",
    "summary": "Transformer-based language models have set new benchmarks across a wide range\nof NLP tasks, yet reliably estimating the uncertainty of their predictions\nremains a significant challenge. Existing uncertainty estimation (UE)\ntechniques often fall short in classification tasks, either offering minimal\nimprovements over basic heuristics or relying on costly ensemble models.\nMoreover, attempts to leverage common embeddings for UE in linear probing\nscenarios have yielded only modest gains, indicating that alternative model\ncomponents should be explored.\n  We tackle these limitations by harnessing the geometry of attention maps\nacross multiple heads and layers to assess model confidence. Our approach\nextracts topological features from attention matrices, providing a\nlow-dimensional, interpretable representation of the model's internal dynamics.\nAdditionally, we introduce topological features to compare attention patterns\nacross heads and layers. Our method significantly outperforms existing UE\ntechniques on benchmarks for acceptability judgments and artificial text\ndetection, offering a more efficient and interpretable solution for uncertainty\nestimation in large-scale language models.",
    "published": "2023-08-22T09:17:45Z",
    "updated": "2024-09-17T09:44:27Z",
    "authors": [
      "Elizaveta Kostenok",
      "Daniil Cherniavskii",
      "Alexey Zaytsev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.14064v1",
    "title": "Multi-model fusion for Aerial Vision and Dialog Navigation based on\n  human attention aids",
    "summary": "Drones have been widely used in many areas of our daily lives. It relieves\npeople of the burden of holding a controller all the time and makes drone\ncontrol easier to use for people with disabilities or occupied hands. However,\nthe control of aerial robots is more complicated compared to normal robots due\nto factors such as uncontrollable height. Therefore, it is crucial to develop\nan intelligent UAV that has the ability to talk to humans and follow natural\nlanguage commands. In this report, we present an aerial navigation task for the\n2023 ICCV Conversation History. Based on the AVDN dataset containing more than\n3k recorded navigation trajectories and asynchronous human-robot conversations,\nwe propose an effective method of fusion training of Human Attention Aided\nTransformer model (HAA-Transformer) and Human Attention Aided LSTM (HAA-LSTM)\nmodel, which achieves the prediction of the navigation routing points and human\nattention. The method not only achieves high SR and SPL metrics, but also shows\na 7% improvement in GP metrics compared to the baseline model.",
    "published": "2023-08-27T10:32:52Z",
    "updated": "2023-08-27T10:32:52Z",
    "authors": [
      "Xinyi Wang",
      "Xuan Cui",
      "Danxu Li",
      "Fang Liu",
      "Licheng Jiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.07109v2",
    "title": "SparseCoder: Advancing Source Code Analysis with Sparse Attention and\n  Learned Token Pruning",
    "summary": "As software projects rapidly evolve, software artifacts become more complex\nand defects behind get harder to identify. The emerging Transformer-based\napproaches, though achieving remarkable performance, struggle with long code\nsequences due to their self-attention mechanism, which scales quadratically\nwith the sequence length. This paper introduces SparseCoder, an innovative\napproach incorporating sparse attention and learned token pruning (LTP) method\n(adapted from natural language processing) to address this limitation. Compared\nto previous state-of-the-art models CodeBERT, RoBERTa, and CodeT5, our\nexperiments demonstrate that SparseCoder can handle significantly longer input\nsequences--at least twice as long, within the limits of our hardware resources\nand data statistics. Additionally, SparseCoder is four times faster than other\nmethods measured in runtime, achieving a 50% reduction in floating point\noperations per second (FLOPs) with a negligible performance drop of less than\n1% compared to Transformers using sparse attention (Sparse Atten). Plotting\nFLOPs of model inference against token lengths reveals that SparseCoder scales\nlinearly, whereas other methods, including the current state-of-the-art model\nCodeT5, scale quadratically. Moreover, SparseCoder enhances interpretability by\nvisualizing non-trivial tokens layer-wise.",
    "published": "2023-10-11T01:11:30Z",
    "updated": "2024-09-11T23:15:44Z",
    "authors": [
      "Xueqi Yang",
      "Mariusz Jakubowski",
      "Li Kang",
      "Haojie Yu",
      "Tim Menzies"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.08618v1",
    "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global\n  Attention",
    "summary": "This paper introduces a novel approach to enhance the capabilities of Large\nLanguage Models (LLMs) in processing and understanding extensive text\nsequences, a critical aspect in applications requiring deep comprehension and\nsynthesis of large volumes of information. Recognizing the inherent challenges\nin extending the context window for LLMs, primarily built on Transformer\narchitecture, we propose a new model architecture, referred to as Zebra. This\narchitecture efficiently manages the quadratic time and memory complexity\nissues associated with full attention in the Transformer by employing grouped\nlocal-global attention layers. Our model, akin to a zebra's alternating\nstripes, balances local and global attention layers, significantly reducing\ncomputational requirements and memory consumption. Comprehensive experiments,\nincluding pretraining from scratch, continuation of long context adaptation\ntraining, and long instruction tuning, are conducted to evaluate the Zebra's\nperformance. The results show that Zebra achieves comparable or superior\nperformance on both short and long sequence benchmarks, while also enhancing\ntraining and inference efficiency.",
    "published": "2023-12-14T02:45:31Z",
    "updated": "2023-12-14T02:45:31Z",
    "authors": [
      "Kaiqiang Song",
      "Xiaoyang Wang",
      "Sangwoo Cho",
      "Xiaoman Pan",
      "Dong Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.09541v1",
    "title": "Picking the Underused Heads: A Network Pruning Perspective of Attention\n  Head Selection for Fusing Dialogue Coreference Information",
    "summary": "The Transformer-based models with the multi-head self-attention mechanism are\nwidely used in natural language processing, and provide state-of-the-art\nresults. While the pre-trained language backbones are shown to implicitly\ncapture certain linguistic knowledge, explicitly incorporating structure-aware\nfeatures can bring about further improvement on the downstream tasks. However,\nsuch enhancement often requires additional neural components and increases\ntraining parameter size. In this work, we investigate the attention head\nselection and manipulation strategy for feature injection from a network\npruning perspective, and conduct a case study on dialogue summarization. We\nfirst rank attention heads in a Transformer-based summarizer with layer-wise\nimportance. We then select the underused heads through extensive analysis, and\ninject structure-aware features by manipulating the selected heads.\nExperimental results show that the importance-based head selection is effective\nfor feature injection, and dialogue summarization can be improved by\nincorporating coreference information via head manipulation.",
    "published": "2023-12-15T05:27:24Z",
    "updated": "2023-12-15T05:27:24Z",
    "authors": [
      "Zhengyuan Liu",
      "Nancy F. Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.11639v2",
    "title": "In-Context Learning with Transformers: Softmax Attention Adapts to\n  Function Lipschitzness",
    "summary": "A striking property of transformers is their ability to perform in-context\nlearning (ICL), a machine learning framework in which the learner is presented\nwith a novel context during inference implicitly through some data, and tasked\nwith making a prediction in that context. As such, that learner must adapt to\nthe context without additional training. We explore the role of softmax\nattention in an ICL setting where each context encodes a regression task. We\nshow that an attention unit learns a window that it uses to implement a\nnearest-neighbors predictor adapted to the landscape of the pretraining tasks.\nSpecifically, we show that this window widens with decreasing Lipschitzness and\nincreasing label noise in the pretraining tasks. We also show that on low-rank,\nlinear problems, the attention unit learns to project onto the appropriate\nsubspace before inference. Further, we show that this adaptivity relies\ncrucially on the softmax activation and thus cannot be replicated by the linear\nactivation often studied in prior theoretical analyses.",
    "published": "2024-02-18T16:37:32Z",
    "updated": "2024-05-28T05:15:53Z",
    "authors": [
      "Liam Collins",
      "Advait Parulekar",
      "Aryan Mokhtari",
      "Sujay Sanghavi",
      "Sanjay Shakkottai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.01479v3",
    "title": "Align-to-Distill: Trainable Attention Alignment for Knowledge\n  Distillation in Neural Machine Translation",
    "summary": "The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.",
    "published": "2024-03-03T11:13:44Z",
    "updated": "2024-03-25T08:46:15Z",
    "authors": [
      "Heegon Jin",
      "Seonil Son",
      "Jemin Park",
      "Youngseok Kim",
      "Hyungjong Noh",
      "Yeonsoo Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.01643v3",
    "title": "Cost-Effective Attention Mechanisms for Low Resource Settings: Necessity\n  & Sufficiency of Linear Transformations",
    "summary": "From natural language processing to vision, Scaled Dot Product Attention\n(SDPA) is the backbone of most modern deep learning applications.\nUnfortunately, its memory and computational requirements can be prohibitive in\nlow-resource settings. In this paper, we improve its efficiency without\nsacrificing its versatility. We propose three attention variants where we\nremove consecutive linear transformations or add a novel one, and evaluate them\non a range of standard NLP and vision tasks. Our proposed models are\nsubstantially lighter than standard SDPA (and have 25-50% fewer parameters). We\nshow that the performance cost of these changes is negligible relative to size\nreduction and that in one case (Super Attention) we succeed in outperforming\nSDPA by up to 10% while improving its speed and reducing its parameters by 25%.",
    "published": "2024-03-03T23:40:35Z",
    "updated": "2025-02-16T14:14:16Z",
    "authors": [
      "Peyman Hosseini",
      "Mehran Hosseini",
      "Ignacio Castro",
      "Matthew Purver"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.02920v2",
    "title": "TaylorShift: Shifting the Complexity of Self-Attention from Squared to\n  Linear (and Back) using Taylor-Softmax",
    "summary": "The quadratic complexity of the attention mechanism represents one of the\nbiggest hurdles for processing long sequences using Transformers. Current\nmethods, relying on sparse representations or stateful recurrence, sacrifice\ntoken-to-token interactions, which ultimately leads to compromises in\nperformance. This paper introduces TaylorShift, a novel reformulation of the\nTaylor softmax that enables computing full token-to-token interactions in\nlinear time and space. We analytically determine the crossover points where\nemploying TaylorShift becomes more efficient than traditional attention,\naligning closely with empirical measurements. Specifically, our findings\ndemonstrate that TaylorShift enhances memory efficiency for sequences as short\nas 800 tokens and accelerates inference for inputs of approximately 1700 tokens\nand beyond. For shorter sequences, TaylorShift scales comparably with the\nvanilla attention. Furthermore, a classification benchmark across five tasks\ninvolving long sequences reveals no degradation in accuracy when employing\nTransformers equipped with TaylorShift. For reproducibility, we provide access\nto our code under https://github.com/tobna/TaylorShift.",
    "published": "2024-03-05T12:38:14Z",
    "updated": "2024-07-17T14:32:01Z",
    "authors": [
      "Tobias Christian Nauen",
      "Sebastian Palacio",
      "Andreas Dengel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.12552v1",
    "title": "M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for\n  Autonomous Driving",
    "summary": "End-to-end autonomous driving has witnessed remarkable progress. However, the\nextensive deployment of autonomous vehicles has yet to be realized, primarily\ndue to 1) inefficient multi-modal environment perception: how to integrate data\nfrom multi-modal sensors more efficiently; 2) non-human-like scene\nunderstanding: how to effectively locate and predict critical risky agents in\ntraffic scenarios like an experienced driver. To overcome these challenges, in\nthis paper, we propose a Multi-Modal fusion transformer incorporating Driver\nAttention (M2DA) for autonomous driving. To better fuse multi-modal data and\nachieve higher alignment between different modalities, a novel\nLidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By\nincorporating driver attention, we empower the human-like scene understanding\nability to autonomous vehicles to identify crucial areas within complex\nscenarios precisely and ensure safety. We conduct experiments on the CARLA\nsimulator and achieve state-of-the-art performance with less data in\nclosed-loop benchmarks. Source codes are available at\nhttps://anonymous.4open.science/r/M2DA-4772.",
    "published": "2024-03-19T08:54:52Z",
    "updated": "2024-03-19T08:54:52Z",
    "authors": [
      "Dongyang Xu",
      "Haokun Li",
      "Qingfan Wang",
      "Ziying Song",
      "Lei Chen",
      "Hanming Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.03924v1",
    "title": "Learning Correlation Structures for Vision Transformers",
    "summary": "We introduce a new attention mechanism, dubbed structural self-attention\n(StructSA), that leverages rich correlation patterns naturally emerging in\nkey-query interactions of attention. StructSA generates attention maps by\nrecognizing space-time structures of key-query correlations via convolution and\nuses them to dynamically aggregate local contexts of value features. This\neffectively leverages rich structural patterns in images and videos such as\nscene layouts, object motion, and inter-object relations. Using StructSA as a\nmain building block, we develop the structural vision transformer (StructViT)\nand evaluate its effectiveness on both image and video classification tasks,\nachieving state-of-the-art results on ImageNet-1K, Kinetics-400,\nSomething-Something V1 & V2, Diving-48, and FineGym.",
    "published": "2024-04-05T07:13:28Z",
    "updated": "2024-04-05T07:13:28Z",
    "authors": [
      "Manjin Kim",
      "Paul Hongsuck Seo",
      "Cordelia Schmid",
      "Minsu Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.05001v1",
    "title": "Dual-Scale Transformer for Large-Scale Single-Pixel Imaging",
    "summary": "Single-pixel imaging (SPI) is a potential computational imaging technique\nwhich produces image by solving an illposed reconstruction problem from few\nmeasurements captured by a single-pixel detector. Deep learning has achieved\nimpressive success on SPI reconstruction. However, previous poor reconstruction\nperformance and impractical imaging model limit its real-world applications. In\nthis paper, we propose a deep unfolding network with hybrid-attention\nTransformer on Kronecker SPI model, dubbed HATNet, to improve the imaging\nquality of real SPI cameras. Specifically, we unfold the computation graph of\nthe iterative shrinkagethresholding algorithm (ISTA) into two alternative\nmodules: efficient tensor gradient descent and hybrid-attention multiscale\ndenoising. By virtue of Kronecker SPI, the gradient descent module can avoid\nhigh computational overheads rooted in previous gradient descent modules based\non vectorized SPI. The denoising module is an encoder-decoder architecture\npowered by dual-scale spatial attention for high- and low-frequency aggregation\nand channel attention for global information recalibration. Moreover, we build\na SPI prototype to verify the effectiveness of the proposed method. Extensive\nexperiments on synthetic and real data demonstrate that our method achieves the\nstate-of-the-art performance. The source code and pre-trained models are\navailable at https://github.com/Gang-Qu/HATNet-SPI.",
    "published": "2024-04-07T15:53:21Z",
    "updated": "2024-04-07T15:53:21Z",
    "authors": [
      "Gang Qu",
      "Ping Wang",
      "Xin Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.07969v1",
    "title": "An End-to-End Structure with Novel Position Mechanism and Improved EMD\n  for Stock Forecasting",
    "summary": "As a branch of time series forecasting, stock movement forecasting is one of\nthe challenging problems for investors and researchers. Since Transformer was\nintroduced to analyze financial data, many researchers have dedicated\nthemselves to forecasting stock movement using Transformer or attention\nmechanisms. However, existing research mostly focuses on individual stock\ninformation but ignores stock market information and high noise in stock data.\nIn this paper, we propose a novel method using the attention mechanism in which\nboth stock market information and individual stock information are considered.\nMeanwhile, we propose a novel EMD-based algorithm for reducing short-term noise\nin stock data. Two randomly selected exchange-traded funds (ETFs) spanning over\nten years from US stock markets are used to demonstrate the superior\nperformance of the proposed attention-based method. The experimental analysis\ndemonstrates that the proposed attention-based method significantly outperforms\nother state-of-the-art baselines. Code is available at\nhttps://github.com/DurandalLee/ACEFormer.",
    "published": "2024-03-25T15:23:22Z",
    "updated": "2024-03-25T15:23:22Z",
    "authors": [
      "Chufeng Li",
      "Jianyong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.03055v1",
    "title": "Multi-hop graph transformer network for 3D human pose estimation",
    "summary": "Accurate 3D human pose estimation is a challenging task due to occlusion and\ndepth ambiguity. In this paper, we introduce a multi-hop graph transformer\nnetwork designed for 2D-to-3D human pose estimation in videos by leveraging the\nstrengths of multi-head self-attention and multi-hop graph convolutional\nnetworks with disentangled neighborhoods to capture spatio-temporal\ndependencies and handle long-range interactions. The proposed network\narchitecture consists of a graph attention block composed of stacked layers of\nmulti-head self-attention and graph convolution with learnable adjacency\nmatrix, and a multi-hop graph convolutional block comprised of multi-hop\nconvolutional and dilated convolutional layers. The combination of multi-head\nself-attention and multi-hop graph convolutional layers enables the model to\ncapture both local and global dependencies, while the integration of dilated\nconvolutional layers enhances the model's ability to handle spatial details\nrequired for accurate localization of the human body joints. Extensive\nexperiments demonstrate the effectiveness and generalization ability of our\nmodel, achieving competitive performance on benchmark datasets.",
    "published": "2024-05-05T21:29:20Z",
    "updated": "2024-05-05T21:29:20Z",
    "authors": [
      "Zaedul Islam",
      "A. Ben Hamza"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.07435v1",
    "title": "An Efficient Multimodal Learning Framework to Comprehend Consumer\n  Preferences Using BERT and Cross-Attention",
    "summary": "Today, the acquisition of various behavioral log data has enabled deeper\nunderstanding of customer preferences and future behaviors in the marketing\nfield. In particular, multimodal deep learning has achieved highly accurate\npredictions by combining multiple types of data. Many of these studies utilize\nwith feature fusion to construct multimodal models, which combines extracted\nrepresentations from each modality. However, since feature fusion treats\ninformation from each modality equally, it is difficult to perform flexible\nanalysis such as the attention mechanism that has been used extensively in\nrecent years. Therefore, this study proposes a context-aware multimodal deep\nlearning model that combines Bidirectional Encoder Representations from\nTransformers (BERT) and cross-attention Transformer, which dynamically changes\nthe attention of deep-contextualized word representations based on background\ninformation such as consumer demographic and lifestyle variables. We conduct a\ncomprehensive analysis and demonstrate the effectiveness of our model by\ncomparing it with six reference models in three categories using behavioral\nlogs stored on an online platform. In addition, we present an efficient\nmultimodal learning method by comparing the learning efficiency depending on\nthe optimizers and the prediction accuracy depending on the number of tokens in\nthe text data.",
    "published": "2024-05-13T02:27:45Z",
    "updated": "2024-05-13T02:27:45Z",
    "authors": [
      "Junichiro Niimi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.08204v1",
    "title": "A Semantic and Motion-Aware Spatiotemporal Transformer Network for\n  Action Detection",
    "summary": "This paper presents a novel spatiotemporal transformer network that\nintroduces several original components to detect actions in untrimmed videos.\nFirst, the multi-feature selective semantic attention model calculates the\ncorrelations between spatial and motion features to model spatiotemporal\ninteractions between different action semantics properly. Second, the\nmotion-aware network encodes the locations of action semantics in video frames\nutilizing the motion-aware 2D positional encoding algorithm. Such a\nmotion-aware mechanism memorizes the dynamic spatiotemporal variations in\naction frames that current methods cannot exploit. Third, the sequence-based\ntemporal attention model captures the heterogeneous temporal dependencies in\naction frames. In contrast to standard temporal attention used in natural\nlanguage processing, primarily aimed at finding similarities between linguistic\nwords, the proposed sequence-based temporal attention is designed to determine\nboth the differences and similarities between video frames that jointly define\nthe meaning of actions. The proposed approach outperforms the state-of-the-art\nsolutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24,\nand EPIC-Kitchens.",
    "published": "2024-05-13T21:47:35Z",
    "updated": "2024-05-13T21:47:35Z",
    "authors": [
      "Matthew Korban",
      "Peter Youngs",
      "Scott T. Acton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.02830v1",
    "title": "Too Big to Fail: Larger Language Models are Disproportionately Resilient\n  to Induction of Dementia-Related Linguistic Anomalies",
    "summary": "As artificial neural networks grow in complexity, understanding their inner\nworkings becomes increasingly challenging, which is particularly important in\nhealthcare applications. The intrinsic evaluation metrics of autoregressive\nneural language models (NLMs), perplexity (PPL), can reflect how \"surprised\" an\nNLM model is at novel input. PPL has been widely used to understand the\nbehavior of NLMs. Previous findings show that changes in PPL when masking\nattention layers in pre-trained transformer-based NLMs reflect linguistic\nanomalies associated with Alzheimer's disease dementia. Building upon this, we\nexplore a novel bidirectional attention head ablation method that exhibits\nproperties attributed to the concepts of cognitive and brain reserve in human\nbrain studies, which postulate that people with more neurons in the brain and\nmore efficient processing are more resilient to neurodegeneration. Our results\nshow that larger GPT-2 models require a disproportionately larger share of\nattention heads to be masked/ablated to display degradation of similar\nmagnitude to masking in smaller models. These results suggest that the\nattention mechanism in transformer models may present an analogue to the\nnotions of cognitive and brain reserve and could potentially be used to model\ncertain aspects of the progression of neurodegenerative disorders and aging.",
    "published": "2024-06-05T00:31:50Z",
    "updated": "2024-06-05T00:31:50Z",
    "authors": [
      "Changye Li",
      "Zhecheng Sheng",
      "Trevor Cohen",
      "Serguei Pakhomov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.03344v1",
    "title": "Audio Mamba: Bidirectional State Space Model for Audio Representation\n  Learning",
    "summary": "Transformers have rapidly become the preferred choice for audio\nclassification, surpassing methods based on CNNs. However, Audio Spectrogram\nTransformers (ASTs) exhibit quadratic scaling due to self-attention. The\nremoval of this quadratic self-attention cost presents an appealing direction.\nRecently, state space models (SSMs), such as Mamba, have demonstrated potential\nin language and vision tasks in this regard. In this study, we explore whether\nreliance on self-attention is necessary for audio classification tasks. By\nintroducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based\nmodel for audio classification, we aim to address this question. We evaluate\nAuM on various audio datasets - comprising six different benchmarks - where it\nachieves comparable or better performance compared to well-established AST\nmodel.",
    "published": "2024-06-05T15:00:59Z",
    "updated": "2024-06-05T15:00:59Z",
    "authors": [
      "Mehmet Hamza Erol",
      "Arda Senocak",
      "Jiu Feng",
      "Joon Son Chung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.06060v1",
    "title": "Learning Physical Simulation with Message Passing Transformer",
    "summary": "Machine learning methods for physical simulation have achieved significant\nsuccess in recent years. We propose a new universal architecture based on Graph\nNeural Network, the Message Passing Transformer, which incorporates a Message\nPassing framework, employs an Encoder-Processor-Decoder structure, and applies\nGraph Fourier Loss as loss function for model optimization. To take advantage\nof the past message passing state information, we propose Hadamard-Product\nAttention to update the node attribute in the Processor, Hadamard-Product\nAttention is a variant of Dot-Product Attention that focuses on more\nfine-grained semantics and emphasizes on assigning attention weights over each\nfeature dimension rather than each position in the sequence relative to others.\nWe further introduce Graph Fourier Loss (GFL) to balance high-energy and\nlow-energy components. To improve time performance, we precompute the graph's\nLaplacian eigenvectors before the training process. Our architecture achieves\nsignificant accuracy improvements in long-term rollouts for both Lagrangian and\nEulerian dynamical systems over current methods.",
    "published": "2024-06-10T07:14:56Z",
    "updated": "2024-06-10T07:14:56Z",
    "authors": [
      "Zeyi Xu",
      "Yifei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.16495v3",
    "title": "OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to\n  construct Observer-Thinker-Conceiver-Expresser",
    "summary": "Recent research has shown that combining Mamba with Transformer architecture,\nwhich has selective state space and quadratic self-attention mechanism,\noutperforms using Mamba or Transformer architecture alone in language modeling\ntasks. The quadratic self-attention mechanism effectively alleviates the\nshortcomings of selective state space in handling long-term dependencies of any\nelement in the sequence. We propose a position information injection method\nthat connects the selective state space model with the quadratic attention, and\nintegrates these two architectures with hybrid experts with cross-sharing\ndomains, so that we can enjoy the advantages of both. We design a new\narchitecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser\n(OTCE), which can compete with well-known medium-scale open-source language\nmodels on a small scale in language modeling tasks.",
    "published": "2024-06-24T10:05:23Z",
    "updated": "2024-07-20T03:35:45Z",
    "authors": [
      "Jingze Shi",
      "Ting Xie",
      "Bingheng Wu",
      "Chunjun Zheng",
      "Kai Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.02984v1",
    "title": "Differentiation and Specialization of Attention Heads via the Refined\n  Local Learning Coefficient",
    "summary": "We introduce refined variants of the Local Learning Coefficient (LLC), a\nmeasure of model complexity grounded in singular learning theory, to study the\ndevelopment of internal structure in transformer language models during\ntraining. By applying these \\textit{refined LLCs} (rLLCs) to individual\ncomponents of a two-layer attention-only transformer, we gain novel insights\ninto the progressive differentiation and specialization of attention heads. Our\nmethodology reveals how attention heads differentiate into distinct functional\nroles over the course of training, analyzes the types of data these heads\nspecialize to process, and discovers a previously unidentified multigram\ncircuit. These findings demonstrate that rLLCs provide a principled,\nquantitative toolkit for \\textit{developmental interpretability}, which aims to\nunderstand models through their evolution across the learning process. More\nbroadly, this work takes a step towards establishing the correspondence between\ndata distributional structure, geometric properties of the loss landscape,\nlearning dynamics, and emergent computational structures in neural networks.",
    "published": "2024-10-03T20:51:02Z",
    "updated": "2024-10-03T20:51:02Z",
    "authors": [
      "George Wang",
      "Jesse Hoogland",
      "Stan van Wingerden",
      "Zach Furman",
      "Daniel Murfet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.10433v1",
    "title": "LKASeg:Remote-Sensing Image Semantic Segmentation with Large Kernel\n  Attention and Full-Scale Skip Connections",
    "summary": "Semantic segmentation of remote sensing images is a fundamental task in\ngeospatial research. However, widely used Convolutional Neural Networks (CNNs)\nand Transformers have notable drawbacks: CNNs may be limited by insufficient\nremote sensing modeling capability, while Transformers face challenges due to\ncomputational complexity. In this paper, we propose a remote-sensing image\nsemantic segmentation network named LKASeg, which combines Large Kernel\nAttention(LSKA) and Full-Scale Skip Connections(FSC). Specifically, we propose\na decoder based on Large Kernel Attention (LKA), which extract global features\nwhile avoiding the computational overhead of self-attention and providing\nchannel adaptability. To achieve full-scale feature learning and fusion, we\napply Full-Scale Skip Connections (FSC) between the encoder and decoder. We\nconducted experiments by combining the LKA-based decoder with FSC. On the ISPRS\nVaihingen dataset, the mF1 and mIoU scores achieved 90.33% and 82.77%.",
    "published": "2024-10-14T12:25:48Z",
    "updated": "2024-10-14T12:25:48Z",
    "authors": [
      "Xuezhi Xiang",
      "Yibo Ning",
      "Lei Zhang",
      "Denis Ombati",
      "Himaloy Himu",
      "Xiantong Zhen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.11396v1",
    "title": "Implementing Derivations of Definite Logic Programs with Self-Attention\n  Networks",
    "summary": "In this paper we propose that a restricted version of logical inference can\nbe implemented with self-attention networks. We are aiming at showing that LLMs\n(Large Language Models) constructed with transformer networks can make logical\ninferences. We would reveal the potential of LLMs by analyzing self-attention\nnetworks, which are main components of transformer networks. Our approach is\nnot based on semantics of natural languages but operations of logical\ninference. %point of view. We show that hierarchical constructions of\nself-attention networks with feed forward networks (FFNs) can implement\ntop-down derivations for a class of logical formulae. We also show bottom-up\nderivations are also implemented for the same class. We believe that our\nresults show that LLMs implicitly have the power of logical inference.",
    "published": "2024-10-15T08:39:28Z",
    "updated": "2024-10-15T08:39:28Z",
    "authors": [
      "Phan Thi Thanh Thuy",
      "Akihiro Yamamoto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.13973v4",
    "title": "MarineFormer: A Spatio-Temporal Attention Model for USV Navigation in\n  Dynamic Marine Environments",
    "summary": "Autonomous navigation in marine environments can be extremely challenging,\nespecially in the presence of spatially varying flow disturbances and dynamic\nand static obstacles. In this work, we demonstrate that incorporating local\nflow field measurements fundamentally alters the nature of the problem,\ntransforming otherwise unsolvable navigation scenarios into tractable ones.\nHowever, the mere availability of flow data is not sufficient; it must be\neffectively fused with conventional sensory inputs such as ego-state and\nobstacle states. To this end, we propose \\textbf{MarineFormer}, a\nTransformer-based policy architecture that integrates two complementary\nattention mechanisms: spatial attention for sensor fusion, and temporal\nattention for capturing environmental dynamics. MarineFormer is trained\nend-to-end via reinforcement learning in a 2D simulated environment with\nrealistic flow features and obstacles. Extensive evaluations against classical\nand state-of-the-art baselines show that our approach improves episode\ncompletion success rate by nearly 23\\% while reducing path length. Ablation\nstudies further highlight the critical role of flow measurements and the\neffectiveness of our proposed architecture in leveraging them.",
    "published": "2024-10-17T18:57:15Z",
    "updated": "2025-07-09T23:40:31Z",
    "authors": [
      "Ehsan Kazemi",
      "Dechen Gao",
      "Iman Soltani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.14506v1",
    "title": "SignAttention: On the Interpretability of Transformer Models for Sign\n  Language Translation",
    "summary": "This paper presents the first comprehensive interpretability analysis of a\nTransformer-based Sign Language Translation (SLT) model, focusing on the\ntranslation from video-based Greek Sign Language to glosses and text.\nLeveraging the Greek Sign Language Dataset, we examine the attention mechanisms\nwithin the model to understand how it processes and aligns visual input with\nsequential glosses. Our analysis reveals that the model pays attention to\nclusters of frames rather than individual ones, with a diagonal alignment\npattern emerging between poses and glosses, which becomes less distinct as the\nnumber of glosses increases. We also explore the relative contributions of\ncross-attention and self-attention at each decoding step, finding that the\nmodel initially relies on video frames but shifts its focus to previously\npredicted tokens as the translation progresses. This work contributes to a\ndeeper understanding of SLT models, paving the way for the development of more\ntransparent and reliable translation systems essential for real-world\napplications.",
    "published": "2024-10-18T14:38:37Z",
    "updated": "2024-10-18T14:38:37Z",
    "authors": [
      "Pedro Alejandro Dal Bianco",
      "Oscar AgustÃ­n Stanchi",
      "Facundo Manuel Quiroga",
      "Franco Ronchetti",
      "Enzo Ferrante"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.22678v1",
    "title": "Backdoor Attack Against Vision Transformers via Attention Gradient-Based\n  Image Erosion",
    "summary": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural\nNetworks (CNN) across various computer vision tasks. However, akin to CNN, ViTs\nare vulnerable to backdoor attacks, where the adversary embeds the backdoor\ninto the victim model, causing it to make wrong predictions about testing\nsamples containing a specific trigger. Existing backdoor attacks against ViTs\nhave the limitation of failing to strike an optimal balance between attack\nstealthiness and attack effectiveness.\n  In this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB)\ntargeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively\nerodes pixels in areas of maximal attention gradient, embedding a covert\nbackdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves\nan optimal balance between attack stealthiness and attack effectiveness,\nensuring the trigger remains invisible to human detection while preserving the\nmodel's accuracy on clean samples. Extensive experimental evaluations across\nvarious ViT architectures and datasets confirm the effectiveness of AGEB,\nachieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data\nAccuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated,\ndemonstrating minimal visual discrepancies between the clean and the triggered\nimages.",
    "published": "2024-10-30T04:06:12Z",
    "updated": "2024-10-30T04:06:12Z",
    "authors": [
      "Ji Guo",
      "Hongwei Li",
      "Wenbo Jiang",
      "Guoming Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.14744v1",
    "title": "Point Cloud Understanding via Attention-Driven Contrastive Learning",
    "summary": "Recently Transformer-based models have advanced point cloud understanding by\nleveraging self-attention mechanisms, however, these methods often overlook\nlatent information in less prominent regions, leading to increased sensitivity\nto perturbations and limited global comprehension. To solve this issue, we\nintroduce PointACL, an attention-driven contrastive learning framework designed\nto address these limitations. Our method employs an attention-driven dynamic\nmasking strategy that guides the model to focus on under-attended regions,\nenhancing the understanding of global structures within the point cloud. Then\nwe combine the original pre-training loss with a contrastive learning loss,\nimproving feature discrimination and generalization. Extensive experiments\nvalidate the effectiveness of PointACL, as it achieves state-of-the-art\nperformance across a variety of 3D understanding tasks, including object\nclassification, part segmentation, and few-shot learning. Specifically, when\nintegrated with different Transformer backbones like Point-MAE and PointGPT,\nPointACL demonstrates improved performance on datasets such as ScanObjectNN,\nModelNet40, and ShapeNetPart. This highlights its superior capability in\ncapturing both global and local features, as well as its enhanced robustness\nagainst perturbations and incomplete data.",
    "published": "2024-11-22T05:41:00Z",
    "updated": "2024-11-22T05:41:00Z",
    "authors": [
      "Yi Wang",
      "Jiaze Wang",
      "Ziyu Guo",
      "Renrui Zhang",
      "Donghao Zhou",
      "Guangyong Chen",
      "Anfeng Liu",
      "Pheng-Ann Heng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.16278v1",
    "title": "Even Sparser Graph Transformers",
    "summary": "Graph Transformers excel in long-range dependency modeling, but generally\nrequire quadratic memory complexity in the number of nodes in an input graph,\nand hence have trouble scaling to large graphs. Sparse attention variants such\nas Exphormer can help, but may require high-degree augmentations to the input\ngraph for good performance, and do not attempt to sparsify an already-dense\ninput graph. As the learned attention mechanisms tend to use few of these\nedges, such high-degree connections may be unnecessary. We show (empirically\nand with theoretical backing) that attention scores on graphs are usually quite\nconsistent across network widths, and use this observation to propose a\ntwo-stage procedure, which we call Spexphormer: first, train a narrow network\non the full augmented graph. Next, use only the active connections to train a\nwider network on a much sparser graph. We establish theoretical conditions when\na narrow network's attention scores can match those of a wide network, and show\nthat Spexphormer achieves good performance with drastically reduced memory\nrequirements on various graph datasets.",
    "published": "2024-11-25T10:59:25Z",
    "updated": "2024-11-25T10:59:25Z",
    "authors": [
      "Hamed Shirzad",
      "Honghao Lin",
      "Balaji Venkatachalam",
      "Ameya Velingker",
      "David Woodruff",
      "Danica Sutherland"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.00731v1",
    "title": "Refine3DNet: Scaling Precision in 3D Object Reconstruction from\n  Multi-View RGB Images using Attention",
    "summary": "Generating 3D models from multi-view 2D RGB images has gained significant\nattention, extending the capabilities of technologies like Virtual Reality,\nRobotic Vision, and human-machine interaction. In this paper, we introduce a\nhybrid strategy combining CNNs and transformers, featuring a visual\nauto-encoder with self-attention mechanisms and a 3D refiner network, trained\nusing a novel Joint Train Separate Optimization (JTSO) algorithm. Encoded\nfeatures from unordered inputs are transformed into an enhanced feature map by\nthe self-attention layer, decoded into an initial 3D volume, and further\nrefined. Our network generates 3D voxels from single or multiple 2D images from\narbitrary viewpoints. Performance evaluations using the ShapeNet datasets show\nthat our approach, combined with JTSO, outperforms state-of-the-art techniques\nin single and multi-view 3D reconstruction, achieving the highest mean\nintersection over union (IOU) scores, surpassing other models by 4.2% in\nsingle-view reconstruction.",
    "published": "2024-12-01T08:53:39Z",
    "updated": "2024-12-01T08:53:39Z",
    "authors": [
      "Ajith Balakrishnan",
      "Sreeja S",
      "Linu Shine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.17271v1",
    "title": "Multi-view Fuzzy Graph Attention Networks for Enhanced Graph Learning",
    "summary": "Fuzzy Graph Attention Network (FGAT), which combines Fuzzy Rough Sets and\nGraph Attention Networks, has shown promise in tasks requiring robust\ngraph-based learning. However, existing models struggle to effectively capture\ndependencies from multiple perspectives, limiting their ability to model\ncomplex data. To address this gap, we propose the Multi-view Fuzzy Graph\nAttention Network (MFGAT), a novel framework that constructs and aggregates\nmulti-view information using a specially designed Transformation Block. This\nblock dynamically transforms data from multiple aspects and aggregates the\nresulting representations via a weighted sum mechanism, enabling comprehensive\nmulti-view modeling. The aggregated information is fed into FGAT to enhance\nfuzzy graph convolutions. Additionally, we introduce a simple yet effective\nlearnable global pooling mechanism for improved graph-level understanding.\nExtensive experiments on graph classification tasks demonstrate that MFGAT\noutperforms state-of-the-art baselines, underscoring its effectiveness and\nversatility.",
    "published": "2024-12-23T04:39:08Z",
    "updated": "2024-12-23T04:39:08Z",
    "authors": [
      "Jinming Xing",
      "Dongwen Luo",
      "Qisen Cheng",
      "Chang Xue",
      "Ruilin Xing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.17316v2",
    "title": "Fast Gradient Computation for RoPE Attention in Almost Linear Time",
    "summary": "The Rotary Position Embedding (RoPE) mechanism has become a powerful\nenhancement to the Transformer architecture, which enables models to capture\ntoken relationships when encoding positional information. However, the RoPE\nmechanisms make the computations of attention mechanisms more complicated,\nwhich makes efficient algorithms challenging. Earlier research introduced\nalmost linear time, i.e., $n^{1+o(1)}$ where $n$ is the number of input tokens,\nalgorithms for the forward computation under specific parameter settings.\nHowever, achieving a subquadratic time algorithm for other parameter regimes\nremains impossible unless the widely accepted Strong Exponential Time\nHypothesis (SETH) is disproven. In this work, we develop the first almost\nlinear time algorithm for backward computations in the RoPE-based attention\nunder bounded entries. Our approach builds on recent advancements in fast RoPE\nattention computations, utilizing a novel combination of the polynomial method\nand the Fast Fourier Transform. Furthermore, we show that with lower bounds\nderived from the SETH, the bounded entry condition is necessary for\nsubquadratic performance.",
    "published": "2024-12-23T06:20:22Z",
    "updated": "2024-12-31T06:53:40Z",
    "authors": [
      "Yifang Chen",
      "Jiayan Huo",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.04320v2",
    "title": "ConceptAttention: Diffusion Transformers Learn Highly Interpretable\n  Features",
    "summary": "Do the rich representations of multi-modal diffusion transformers (DiTs)\nexhibit unique properties that enhance their interpretability? We introduce\nConceptAttention, a novel method that leverages the expressive power of DiT\nattention layers to generate high-quality saliency maps that precisely locate\ntextual concepts within images. Without requiring additional training,\nConceptAttention repurposes the parameters of DiT attention layers to produce\nhighly contextualized concept embeddings, contributing the major discovery that\nperforming linear projections in the output space of DiT attention layers\nyields significantly sharper saliency maps compared to commonly used\ncross-attention maps. ConceptAttention even achieves state-of-the-art\nperformance on zero-shot image segmentation benchmarks, outperforming 15 other\nzero-shot interpretability methods on the ImageNet-Segmentation dataset.\nConceptAttention works for popular image models and even seamlessly generalizes\nto video generation. Our work contributes the first evidence that the\nrepresentations of multi-modal DiTs are highly transferable to vision tasks\nlike segmentation.",
    "published": "2025-02-06T18:59:00Z",
    "updated": "2025-07-01T20:00:27Z",
    "authors": [
      "Alec Helbling",
      "Tuna Han Salih Meral",
      "Ben Hoover",
      "Pinar Yanardag",
      "Duen Horng Chau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05164v2",
    "title": "In-context denoising with one-layer transformers: connections between\n  attention and associative memory retrieval",
    "summary": "We introduce in-context denoising, a task that refines the connection between\nattention-based architectures and dense associative memory (DAM) networks, also\nknown as modern Hopfield networks. Using a Bayesian framework, we show\ntheoretically and empirically that certain restricted denoising problems can be\nsolved optimally even by a single-layer transformer. We demonstrate that a\ntrained attention layer processes each denoising prompt by performing a single\ngradient descent update on a context-aware DAM energy landscape, where context\ntokens serve as associative memories and the query token acts as an initial\nstate. This one-step update yields better solutions than exact retrieval of\neither a context token or a spurious local minimum, providing a concrete\nexample of DAM networks extending beyond the standard retrieval paradigm.\nOverall, this work solidifies the link between associative memory and attention\nmechanisms first identified by Ramsauer et al., and demonstrates the relevance\nof associative memory models in the study of in-context learning.",
    "published": "2025-02-07T18:48:25Z",
    "updated": "2025-06-06T05:00:45Z",
    "authors": [
      "Matthew Smart",
      "Alberto Bietti",
      "Anirvan M. Sengupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08285v2",
    "title": "Fully-Geometric Cross-Attention for Point Cloud Registration",
    "summary": "Point cloud registration approaches often fail when the overlap between point\nclouds is low due to noisy point correspondences. This work introduces a novel\ncross-attention mechanism tailored for Transformer-based architectures that\ntackles this problem, by fusing information from coordinates and features at\nthe super-point level between point clouds. This formulation has remained\nunexplored primarily because it must guarantee rotation and translation\ninvariance since point clouds reside in different and independent reference\nframes. We integrate the Gromov-Wasserstein distance into the cross-attention\nformulation to jointly compute distances between points across different point\nclouds and account for their geometric structure. By doing so, points from two\ndistinct point clouds can attend to each other under arbitrary rigid\ntransformations. At the point level, we also devise a self-attention mechanism\nthat aggregates the local geometric structure information into point features\nfor fine matching. Our formulation boosts the number of inlier correspondences,\nthereby yielding more precise registration results compared to state-of-the-art\napproaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch,\nKITTI, and 3DCSR datasets.",
    "published": "2025-02-12T10:44:36Z",
    "updated": "2025-06-08T16:11:07Z",
    "authors": [
      "Weijie Wang",
      "Guofeng Mei",
      "Jian Zhang",
      "Nicu Sebe",
      "Bruno Lepri",
      "Fabio Poiesi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.10957v1",
    "title": "Predicting Stock Movement with BERTweet and Transformers",
    "summary": "Applying deep learning and computational intelligence to finance has been a\npopular area of applied research, both within academia and industry, and\ncontinues to attract active attention. The inherently high volatility and\nnon-stationary of the data pose substantial challenges to machine learning\nmodels, especially so for today's expressive and highly-parameterized deep\nlearning models. Recent work has combined natural language processing on data\nfrom social media to augment models based purely on historic price data to\nimprove performance has received particular attention. Previous work has\nachieved state-of-the-art performance on this task by combining techniques such\nas bidirectional GRUs, variational autoencoders, word and document embeddings,\nself-attention, graph attention, and adversarial training. In this paper, we\ndemonstrated the efficacy of BERTweet, a variant of BERT pre-trained\nspecifically on a Twitter corpus, and the transformer architecture by achieving\ncompetitive performance with the existing literature and setting a new baseline\nfor Matthews Correlation Coefficient on the Stocknet dataset without auxiliary\ndata sources.",
    "published": "2025-03-13T23:46:24Z",
    "updated": "2025-03-13T23:46:24Z",
    "authors": [
      "Michael Charles Albada",
      "Mojolaoluwa Joshua Sonola"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.13479v2",
    "title": "EAGLE: Contextual Point Cloud Generation via Adaptive Continuous\n  Normalizing Flow with Self-Attention",
    "summary": "As 3D point clouds become the prevailing shape representation in computer\nvision, how to generate high-resolution point clouds has become a pressing\nissue. Flow-based generative models can effectively perform point cloud\ngeneration tasks. However, traditional CNN-based flow architectures rely only\non local information to extract features, making it difficult to capture global\ncontextual information. Inspired by the wide adoption of Transformers, we\nexplored the complementary roles of self-attention mechanisms in Transformers,\nCNN, and continuous normalizing flows. To this end, we propose a probabilistic\nmodel via adaptive normalizing flows and self-attention. Our idea leverages\nself-attention mechanisms to capture global contextual information. We also\npropose adaptive continuous normalizing flows by introducing adaptive bias\ncorrection mechanism. Combined with normalization, the mechanism dynamically\nhandles different input contexts and mitigates potential bias-shift issues from\nstandard initialization. Experimental results demonstrate that EAGLE achieves\ncompetitive performance in point cloud generation.",
    "published": "2025-03-05T03:02:14Z",
    "updated": "2025-10-17T07:10:18Z",
    "authors": [
      "Linhao Wang",
      "Qichang Zhang",
      "Yifan Yang",
      "Hao Wang",
      "Ye Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.14037v1",
    "title": "Intra and Inter Parser-Prompted Transformers for Effective Image\n  Restoration",
    "summary": "We propose Intra and Inter Parser-Prompted Transformers (PPTformer) that\nexplore useful features from visual foundation models for image restoration.\nSpecifically, PPTformer contains two parts: an Image Restoration Network\n(IRNet) for restoring images from degraded observations and a Parser-Prompted\nFeature Generation Network (PPFGNet) for providing IRNet with reliable parser\ninformation to boost restoration. To enhance the integration of the parser\nwithin IRNet, we propose Intra Parser-Prompted Attention (IntraPPA) and Inter\nParser-Prompted Attention (InterPPA) to implicitly and explicitly learn useful\nparser features to facilitate restoration. The IntraPPA re-considers cross\nattention between parser and restoration features, enabling implicit perception\nof the parser from a long-range and intra-layer perspective. Conversely, the\nInterPPA initially fuses restoration features with those of the parser,\nfollowed by formulating these fused features within an attention mechanism to\nexplicitly perceive parser information. Further, we propose a parser-prompted\nfeed-forward network to guide restoration within pixel-wise gating modulation.\nExperimental results show that PPTformer achieves state-of-the-art performance\non image deraining, defocus deblurring, desnowing, and low-light enhancement.",
    "published": "2025-03-18T08:56:02Z",
    "updated": "2025-03-18T08:56:02Z",
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Liyan Wang",
      "Wei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.20500v3",
    "title": "Novel Deep Neural OFDM Receiver Architectures for LLR Estimation",
    "summary": "Neural receivers have recently become a popular topic, where the received\nsignals can be directly decoded by data driven mechanisms such as machine\nlearning and deep learning. In this paper, we propose two novel neural network\nbased orthogonal frequency division multiplexing (OFDM) receivers performing\nchannel estimation and equalization tasks and directly predicting log\nlikelihood ratios (LLRs) from the received in phase and quadrature phase (IQ)\nsignals. The first network, the Dual Attention Transformer (DAT), employs a\nstate of the art (SOTA) transformer architecture with an attention mechanism.\nThe second network, the Residual Dual Non Local Attention Network (RDNLA),\nutilizes a parallel residual architecture with a non local attention block. The\nbit error rate (BER) and block error rate (BLER) performance of various SOTA\nneural receiver architectures is compared with our proposed methods across\ndifferent signal to noise ratio (SNR) levels. The simulation results show that\nDAT and RDNLA outperform both traditional communication systems and existing\nneural receiver models.",
    "published": "2025-03-26T12:39:56Z",
    "updated": "2025-05-08T16:41:56Z",
    "authors": [
      "Erhan Karakoca",
      "HÃ¼seyin Ãevik",
      "Ä°brahim HÃ¶kelek",
      "Ali GÃ¶rÃ§in"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.23174v4",
    "title": "TRA: Better Length Generalisation with Threshold Relative Attention",
    "summary": "Transformers struggle with length generalisation, displaying poor performance\neven on basic tasks. We test whether these limitations can be explained through\ntwo key failures of the self-attention mechanism. The first is the inability to\nfully remove irrelevant information. The second is tied to position, even if\nthe dot product between a key and query is highly negative (i.e. an irrelevant\nkey) learned positional biases may unintentionally up-weight such information -\ndangerous when distances become out of distribution. Put together, these two\nfailure cases lead to compounding generalisation difficulties. We test whether\nthey can be mitigated through the combination of a) selective sparsity -\ncompletely removing irrelevant keys from the attention softmax and b)\ncontextualised relative distance - distance is only considered as between the\nquery and the keys that matter. We show how refactoring the attention mechanism\nwith these two mitigations in place can substantially improve the\ngeneralisation capabilities of decoder only transformers.",
    "published": "2025-03-29T18:06:28Z",
    "updated": "2025-10-06T12:50:07Z",
    "authors": [
      "Mattia Opper",
      "Roland Fernandez",
      "Paul Smolensky",
      "Jianfeng Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.02834v1",
    "title": "Explainable Dual-Attention Tabular Transformer for Soil Electrical\n  Resistivity Prediction: A Decision Support Framework for High-Voltage\n  Substation Construction",
    "summary": "This research introduces a novel dual-attention transformer architecture for\npredicting soil electrical resistivity, a critical parameter for high-voltage\nsubstation construction. Our model employs attention mechanisms operating\nacross both features and data batches, enhanced by feature embedding layers\nthat project inputs into higher-dimensional spaces. We implements Particle\nSwarm Optimization for hyperparameter tuning, systematically optimizing\nembedding dimensions, attention heads, and neural network architecture. The\nproposed architecture achieves superior predictive performance (Mean Absolute\nPercentage Error: 0.63%) compared to recent state of the art models for tabular\ndata. Crucially, our model maintains explainability through SHapley Additive\nexPlanations value analysis, revealing that fine particle content and dry\ndensity are the most influential parameters affecting soil resistivity. We\ndevelopes a web-based application implementing this model to provide engineers\nwith an accessible decision support framework that bridges geotechnical and\nelectrical engineering requirements for the Electricity Generating Authority of\nThailand. This integrated approach satisfies both structural stability and\nelectrical safety standards, improving construction efficiency and safety\ncompliance in high-voltage infrastructure implementation.",
    "published": "2025-03-17T04:30:32Z",
    "updated": "2025-03-17T04:30:32Z",
    "authors": [
      "Warat Kongkitkul",
      "Sompote Youwai",
      "Warut Sakulpojworachai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13112v3",
    "title": "Attention-based clustering",
    "summary": "Transformers have emerged as a powerful neural network architecture capable\nof tackling a wide range of learning tasks. In this work, we provide a\ntheoretical analysis of their ability to automatically extract structure from\ndata in an unsupervised setting. In particular, we demonstrate their\nsuitability for clustering when the input data is generated from a Gaussian\nmixture model. To this end, we study a simplified two-head attention layer and\ndefine a population risk whose minimization with unlabeled data drives the head\nparameters to align with the true mixture centroids. This phenomenon highlights\nthe ability of attention-based layers to capture underlying distributional\nstructure. We further examine an attention layer with key, query, and value\nmatrices fixed to the identity, and show that, even without any trainable\nparameters, it can perform in-context quantization, revealing the surprising\ncapacity of transformer-based methods to adapt dynamically to input-specific\ndistributions.",
    "published": "2025-05-19T13:39:56Z",
    "updated": "2025-10-27T15:23:43Z",
    "authors": [
      "Rodrigo Maulen-Soto",
      "Pierre Marion",
      "Claire Boyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17357v1",
    "title": "Graph Attention Neural Network for Botnet Detection: Evaluating\n  Autoencoder, VAE and PCA-Based Dimension Reduction",
    "summary": "With the rise of IoT-based botnet attacks, researchers have explored various\nlearning models for detection, including traditional machine learning, deep\nlearning, and hybrid approaches. A key advancement involves deploying attention\nmechanisms to capture long-term dependencies among features, significantly\nimproving detection accuracy. However, most models treat attack instances\nindependently, overlooking inter-instance relationships. Graph Neural Networks\n(GNNs) address this limitation by learning an embedding space via iterative\nmessage passing where similar instances are placed closer based on node\nfeatures and relationships, enhancing classification performance. To further\nimprove detection, attention mechanisms have been embedded within GNNs,\nleveraging both long-range dependencies and inter-instance connections.\nHowever, transforming the high dimensional IoT attack datasets into a graph\nstructured dataset poses challenges, such as large graph structures leading\ncomputational overhead. To mitigate this, this paper proposes a framework that\nfirst reduces dimensionality of the NetFlow-based IoT attack dataset before\ntransforming it into a graph dataset. We evaluate three dimension reduction\ntechniques--Variational Autoencoder (VAE-encoder), classical autoencoder\n(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects\non a Graph Attention neural network (GAT) model for botnet attack detection",
    "published": "2025-05-23T00:22:14Z",
    "updated": "2025-05-23T00:22:14Z",
    "authors": [
      "Hassan Wasswa",
      "Hussein Abbass",
      "Timothy Lynar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17665v1",
    "title": "EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote\n  Sensing Images with Attention Proxy",
    "summary": "High-resolution remote sensing (HRRS) image segmentation is challenging due\nto complex spatial layouts and diverse object appearances. While CNNs excel at\ncapturing local features, they struggle with long-range dependencies, whereas\nTransformers can model global context but often neglect local details and are\ncomputationally expensive.We propose a novel approach, Region-Aware Proxy\nNetwork (RAPNet), which consists of two components: Contextual Region Attention\n(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely\non grid-based layouts, RAPNet operates at the region level for more flexible\nsegmentation. The CRA module uses a Transformer to capture region-level\ncontextual dependencies, generating a Semantic Region Mask (SRM). The GCR\nmodule learns a global class attention map to refine multi-class information,\ncombining the SRM and attention map for accurate segmentation.Experiments on\nthree public datasets show that RAPNet outperforms state-of-the-art methods,\nachieving superior multi-class segmentation accuracy.",
    "published": "2025-05-23T09:30:45Z",
    "updated": "2025-05-23T09:30:45Z",
    "authors": [
      "Yichun Yu",
      "Yuqing Lan",
      "Zhihuan Xing",
      "Xiaoyi Yang",
      "Tingyue Tang",
      "Dan Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17863v1",
    "title": "The emergence of sparse attention: impact of data distribution and\n  benefits of repetition",
    "summary": "Emergence is a fascinating property of large language models and neural\nnetworks more broadly: as models scale and train for longer, they sometimes\ndevelop new abilities in sudden ways. Despite initial studies, we still lack a\ncomprehensive understanding of how and when these abilities emerge. To address\nthis gap, we study the emergence over training of sparse attention, a critical\nand frequently observed attention pattern in Transformers. By combining\ntheoretical analysis of a toy model with empirical observations on small\nTransformers trained on a linear regression variant, we uncover the mechanics\ndriving sparse attention emergence and reveal that emergence timing follows\npower laws based on task structure, architecture, and optimizer choice. We\nadditionally find that repetition can greatly speed up emergence. Finally, we\nconfirm these results on a well-studied in-context associative recall task. Our\nfindings provide a simple, theoretically grounded framework for understanding\nhow data distributions and model design influence the learning dynamics behind\none form of emergence.",
    "published": "2025-05-23T13:14:02Z",
    "updated": "2025-05-23T13:14:02Z",
    "authors": [
      "Nicolas Zucchet",
      "Francesco d'Angelo",
      "Andrew K. Lampinen",
      "Stephanie C. Y. Chan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19602v1",
    "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression",
    "summary": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
    "published": "2025-05-26T07:11:42Z",
    "updated": "2025-05-26T07:11:42Z",
    "authors": [
      "Kunjun Li",
      "Zigeng Chen",
      "Cheng-Yen Yang",
      "Jenq-Neng Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22425v1",
    "title": "Scaling Reasoning without Attention",
    "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning.",
    "published": "2025-05-28T14:52:15Z",
    "updated": "2025-05-28T14:52:15Z",
    "authors": [
      "Xueliang Zhao",
      "Wei Wu",
      "Lingpeng Kong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.11043v1",
    "title": "A Framework for Non-Linear Attention via Modern Hopfield Networks",
    "summary": "In this work we propose an energy functional along the lines of Modern\nHopfield Networks (MNH), the stationary points of which correspond to the\nattention due to Vaswani et al. [12], thus unifying both frameworks. The minima\nof this landscape form \"context wells\" - stable configurations that encapsulate\nthe contextual relationships among tokens. A compelling picture emerges: across\n$n$ token embeddings an energy landscape is defined whose gradient corresponds\nto the attention computation. Non-linear attention mechanisms offer a means to\nenhance the capabilities of transformer models for various sequence modeling\ntasks by improving the model's understanding of complex relationships, learning\nof representations, and overall efficiency and performance. A rough analogy can\nbe seen via cubic splines which offer a richer representation of non-linear\ndata where a simpler linear model may be inadequate. This approach can be used\nfor the introduction of non-linear heads in transformer based models such as\nBERT, [6], etc.",
    "published": "2025-05-21T10:33:01Z",
    "updated": "2025-05-21T10:33:01Z",
    "authors": [
      "Ahmed Farooq"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.03421v2",
    "title": "Hybrid-View Attention Network for Clinically Significant Prostate Cancer\n  Classification in Transrectal Ultrasound",
    "summary": "Prostate cancer (PCa) is a leading cause of cancer-related mortality in men,\nand accurate identification of clinically significant PCa (csPCa) is critical\nfor timely intervention. Transrectal ultrasound (TRUS) is widely used for\nprostate biopsy; however, its low contrast and anisotropic spatial resolution\npose diagnostic challenges. To address these limitations, we propose a novel\nhybrid-view attention (HVA) network for csPCa classification in 3D TRUS that\nleverages complementary information from transverse and sagittal views. Our\napproach integrates a CNN-transformer hybrid architecture, where convolutional\nlayers extract fine-grained local features and transformer-based HVA models\nglobal dependencies. Specifically, the HVA comprises intra-view attention to\nrefine features within a single view and cross-view attention to incorporate\ncomplementary information across views. Furthermore, a hybrid-view adaptive\nfusion module dynamically aggregates features along both channel and spatial\ndimensions, enhancing the overall representation. Experiments are conducted on\nan in-house dataset containing 590 subjects who underwent prostate biopsy.\nComparative and ablation results prove the efficacy of our method. The code is\navailable at https://github.com/mock1ngbrd/HVAN.",
    "published": "2025-07-04T09:27:48Z",
    "updated": "2025-07-10T03:48:57Z",
    "authors": [
      "Zetian Feng",
      "Juan Fu",
      "Xuebin Zou",
      "Hongsheng Ye",
      "Hong Wu",
      "Jianhua Zhou",
      "Yi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.15875v1",
    "title": "Differential Multimodal Transformers",
    "summary": "Small language models have gained significant popularity due to their\nefficiency and growing capabilities. However, incorporating additional\nmodalities, such as vision, can exacerbate the challenge of limited context\nwindows by introducing noise. Recent studies have highlighted that Transformer\nattention mechanisms often disproportionately focus on irrelevant contexts. In\nthis work, we extend the Differential Attention mechanism, originally designed\nfor text-only models, to the text-vision model PaliGemma. Our aim is to\nevaluate its ability to mitigate noisy information retrieval and reduce\nhallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,\nincorporating Differential Attention, and experimented with various parameter\nsettings and configurations. We demonstrate that Differential Attention can be\nadapted and integrated into the fine-tuning of existing models to enhance noisy\ninformation retrieval and question-answering capabilities.",
    "published": "2025-07-17T09:05:34Z",
    "updated": "2025-07-17T09:05:34Z",
    "authors": [
      "Jerry Li",
      "Timothy Oh",
      "Joseph Hoang",
      "Vardhit Veeramachaneni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.19891v2",
    "title": "Interpretable Open-Vocabulary Referring Object Detection with Reverse\n  Contrast Attention",
    "summary": "We propose Reverse Contrast Attention (RCA), a plug-in method that enhances\nobject localization in vision-language transformers without retraining. RCA\nreweights final-layer attention by suppressing extremes and amplifying\nmid-level activations to let semantically relevant but subdued tokens guide\npredictions. We evaluate it on Open Vocabulary Referring Object Detection\n(OV-RefOD), introducing FitAP, a confidence-free average precision metric based\non IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with\ngains up to $+26.6\\%$. Effectiveness aligns with attention sharpness and fusion\ntiming; while late-fusion models benefit consistently, models like\n$\\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement\nas key factors. RCA offers both interpretability and performance gains for\nmultimodal transformers. Codes and dataset are available from\nhttps://github.com/earl-juanico/rca",
    "published": "2025-07-26T09:43:09Z",
    "updated": "2025-07-30T04:47:07Z",
    "authors": [
      "Drandreb Earl O. Juanico",
      "Rowel O. Atienza",
      "Jeffrey Kenneth Go"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.04422v1",
    "title": "Efficient Inter-Task Attention for Multitask Transformer Models",
    "summary": "In both Computer Vision and the wider Deep Learning field, the Transformer\narchitecture is well-established as state-of-the-art for many applications. For\nMultitask Learning, however, where there may be many more queries necessary\ncompared to single-task models, its Multi-Head-Attention often approaches the\nlimits of what is computationally feasible considering practical hardware\nlimitations. This is due to the fact that the size of the attention matrix\nscales quadratically with the number of tasks (assuming roughly equal numbers\nof queries for all tasks). As a solution, we propose our novel Deformable\nInter-Task Self-Attention for Multitask models that enables the much more\nefficient aggregation of information across the feature maps from different\ntasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we\ndemonstrate an order-of-magnitude reduction in both FLOPs count and inference\nlatency. At the same time, we also achieve substantial improvements by up to\n7.4% in the individual tasks' prediction quality metrics.",
    "published": "2025-08-06T13:09:02Z",
    "updated": "2025-08-06T13:09:02Z",
    "authors": [
      "Christian Bohn",
      "Thomas Kurbiel",
      "Klaus Friedrichs",
      "Hasan Tercan",
      "Tobias Meisen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08925v1",
    "title": "LPGNet: A Lightweight Network with Parallel Attention and Gated Fusion\n  for Multimodal Emotion Recognition",
    "summary": "Emotion recognition in conversations (ERC) aims to predict the emotional\nstate of each utterance by using multiple input types, such as text and audio.\nWhile Transformer-based models have shown strong performance in this task, they\noften face two major issues: high computational cost and heavy dependence on\nspeaker information. These problems reduce their ability to generalize in\nreal-world conversations. To solve these challenges, we propose LPGNet, a\nLightweight network with Parallel attention and Gated fusion for multimodal\nERC. The main part of LPGNet is the Lightweight Parallel Interaction Attention\n(LPIA) module. This module replaces traditional stacked Transformer layers with\nparallel dot-product attention, which can model both within-modality and\nbetween-modality relationships more efficiently. To improve emotional feature\nlearning, LPGNet also uses a dual-gated fusion method. This method filters and\ncombines features from different input types in a flexible and dynamic way. In\naddition, LPGNet removes speaker embeddings completely, which allows the model\nto work independently of speaker identity. Experiments on the IEMOCAP dataset\nshow that LPGNet reaches over 87% accuracy and F1-score in 4-class emotion\nclassification. It outperforms strong baseline models while using fewer\nparameters and showing better generalization across speakers.",
    "published": "2025-08-12T13:22:16Z",
    "updated": "2025-08-12T13:22:16Z",
    "authors": [
      "Zhining He",
      "Yang Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10758v1",
    "title": "Natively Trainable Sparse Attention for Hierarchical Point Cloud\n  Datasets",
    "summary": "Unlocking the potential of transformers on datasets of large physical systems\ndepends on overcoming the quadratic scaling of the attention mechanism. This\nwork explores combining the Erwin architecture with the Native Sparse Attention\n(NSA) mechanism to improve the efficiency and receptive field of transformer\nmodels for large-scale physical systems, addressing the challenge of quadratic\nattention complexity. We adapt the NSA mechanism for non-sequential data,\nimplement the Erwin NSA model, and evaluate it on three datasets from the\nphysical sciences -- cosmology simulations, molecular dynamics, and air\npressure modeling -- achieving performance that matches or exceeds that of the\noriginal Erwin model. Additionally, we reproduce the experimental results from\nthe Erwin paper to validate their implementation.",
    "published": "2025-08-14T15:39:34Z",
    "updated": "2025-08-14T15:39:34Z",
    "authors": [
      "Nicolas Lapautre",
      "Maria Marchenko",
      "Carlos Miguel PatiÃ±o",
      "Xin Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.13328v1",
    "title": "A Dual-Attention Graph Network for fMRI Data Classification",
    "summary": "Understanding the complex neural activity dynamics is crucial for the\ndevelopment of the field of neuroscience. Although current functional MRI\nclassification approaches tend to be based on static functional connectivity or\ncannot capture spatio-temporal relationships comprehensively, we present a new\nframework that leverages dynamic graph creation and spatiotemporal attention\nmechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in\nthis research dynamically infers functional brain connectivity in each time\ninterval using transformer-based attention mechanisms, enabling the model to\nselectively focus on crucial brain regions and time segments. By constructing\ntime-varying graphs that are then processed with Graph Convolutional Networks\n(GCNs) and transformers, our method successfully captures both localized\ninteractions and global temporal dependencies. Evaluated on the subset of ABIDE\ndataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static\ngraph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint\nmodeling of dynamic connectivity and spatio-temporal context for fMRI\nclassification. The core novelty arises from (1) attention-driven dynamic graph\ncreation that learns temporal brain region interactions and (2) hierarchical\nspatio-temporal feature fusion through GCNtransformer fusion.",
    "published": "2025-08-18T19:23:18Z",
    "updated": "2025-08-18T19:23:18Z",
    "authors": [
      "Amirali Arbab",
      "Zeinab Davarani",
      "Mehran Safayani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16884v2",
    "title": "A Lightweight Convolution and Vision Transformer integrated model with\n  Multi-scale Self-attention Mechanism",
    "summary": "Vision Transformer (ViT) has prevailed in computer vision tasks due to its\nstrong long-range dependency modelling ability. \\textcolor{blue}{However, its\nlarge model size and weak local feature modeling ability hinder its application\nin real scenarios. To balance computation efficiency and performance in\ndownstream vision tasks, we propose an efficient ViT model with sparse\nattention (dubbed SAEViT) and convolution blocks. Specifically, a Sparsely\nAggregated Attention (SAA) module has been proposed to perform adaptive sparse\nsampling and recover the feature map via deconvolution operation,} which\nsignificantly reduces the computational complexity of attention operations. In\naddition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed\nto enhance inter-channel information exchange through feature decomposition and\nredistribution, which mitigates the redundancy in traditional feed-forward\nnetworks (FFN). Finally, a hierarchical pyramid structure with embedded\ndepth-wise separable convolutional blocks (DWSConv) is devised to further\nstrengthen convolutional features. Extensive experiments on mainstream datasets\nshow that SAEViT achieves Top-1 accuracies of 76.3\\% and 79.6\\% on the\nImageNet-1K classification task with only 0.8 GFLOPs and 1.3 GFLOPs,\nrespectively, demonstrating a lightweight solution for fundamental vision\ntasks.",
    "published": "2025-08-23T03:05:34Z",
    "updated": "2025-09-11T14:34:08Z",
    "authors": [
      "Yi Zhang",
      "Lingxiao Wei",
      "Bowei Zhang",
      "Ziwei Liu",
      "Kai Yi",
      "Shu Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16554v1",
    "title": "ViTCAE: ViT-based Class-conditioned Autoencoder",
    "summary": "Vision Transformer (ViT) based autoencoders often underutilize the global\nClass token and employ static attention mechanisms, limiting both generative\ncontrol and optimization efficiency. This paper introduces ViTCAE, a framework\nthat addresses these issues by re-purposing the Class token into a generative\nlinchpin. In our architecture, the encoder maps the Class token to a global\nlatent variable that dictates the prior distribution for local, patch-level\nlatent variables, establishing a robust dependency where global semantics\ndirectly inform the synthesis of local details. Drawing inspiration from\nopinion dynamics, we treat each attention head as a dynamical system of\ninteracting tokens seeking consensus. This perspective motivates a\nconvergence-aware temperature scheduler that adaptively anneals each head's\ninfluence function based on its distributional stability. This process enables\na principled head-freezing mechanism, guided by theoretically-grounded\ndiagnostics like an attention evolution distance and a consensus/cluster\nfunctional. This technique prunes converged heads during training to\nsignificantly improve computational efficiency without sacrificing fidelity. By\nunifying a generative Class token with an adaptive attention mechanism rooted\nin multi-agent consensus theory, ViTCAE offers a more efficient and\ncontrollable approach to transformer-based generation.",
    "published": "2025-09-20T06:48:45Z",
    "updated": "2025-09-20T06:48:45Z",
    "authors": [
      "Vahid Jebraeeli",
      "Hamid Krim",
      "Derya Cansever"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03926v1",
    "title": "Sliding Window Attention for Learned Video Compression",
    "summary": "To manage the complexity of transformers in video compression, local\nattention mechanisms are a practical necessity. The common approach of\npartitioning frames into patches, however, creates architectural flaws like\nirregular receptive fields. When adapted for temporal autoregressive models,\nthis paradigm, exemplified by the Video Compression Transformer (VCT), also\nnecessitates computationally redundant overlapping windows. This work\nintroduces 3D Sliding Window Attention (SWA), a patchless form of local\nattention. By enabling a decoder-only architecture that unifies spatial and\ntemporal context processing, and by providing a uniform receptive field, our\nmethod significantly improves rate-distortion performance, achieving\nBj{\\o}rntegaard Delta-rate savings of up to 18.6 % against the VCT baseline.\nSimultaneously, by eliminating the need for overlapping windows, our method\nreduces overall decoder complexity by a factor of 2.8, while its entropy model\nis nearly 3.5 times more efficient. We further analyze our model's behavior and\nshow that while it benefits from long-range temporal context, excessive context\ncan degrade performance.",
    "published": "2025-10-04T20:11:43Z",
    "updated": "2025-10-04T20:11:43Z",
    "authors": [
      "Alexander Kopte",
      "AndrÃ© Kaup"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04212v2",
    "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash\n  Attention",
    "summary": "The pursuit of computational efficiency has driven the adoption of\nlow-precision formats for training transformer models. However, this progress\nis often hindered by notorious training instabilities. This paper provides the\nfirst mechanistic explanation for a long-standing and unresolved failure case\nwhere training with flash attention in low-precision settings leads to\ncatastrophic loss explosion. Our in-depth analysis reveals that the failure is\nnot a random artifact but caused by two intertwined phenomena: the emergence of\nsimilar low-rank representations within the attention mechanism and the\ncompounding effect of biased rounding errors inherent in low-precision\narithmetic. We demonstrate how these factors create a vicious cycle of error\naccumulation that corrupts weight updates, ultimately derailing the training\ndynamics. To validate our findings, we introduce a minimal modification to the\nflash attention that mitigates the bias in rounding errors. This simple change\nstabilizes the training process, confirming our analysis and offering a\npractical solution to this persistent problem. Code is available at\nhttps://github.com/ucker/why-low-precision-training-fails.",
    "published": "2025-10-05T14:01:24Z",
    "updated": "2025-10-10T11:14:31Z",
    "authors": [
      "Haiquan Qiu",
      "Quanming Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07401v1",
    "title": "Attention to Order: Transformers Discover Phase Transitions via\n  Learnability",
    "summary": "Phase transitions mark qualitative reorganizations of collective behavior,\nyet identifying their boundaries remains challenging whenever analytic\nsolutions are absent and conventional simulations fail. Here we introduce\nlearnability as a universal criterion, defined as the ability of a transformer\nmodel containing attention mechanism to extract structure from microscopic\nstates. Using self-supervised learning and Monte Carlo generated configurations\nof the two-dimensional Ising model, we show that ordered phases correspond to\nenhanced learnability, manifested in both reduced training loss and structured\nattention patterns, while disordered phases remain resistant to learning. Two\nunsupervised diagnostics, the sharp jump in training loss and the rise in\nattention entropy, recover the critical temperature in excellent agreement with\nthe exact value. Our results establish learnability as a data-driven marker of\nphase transitions and highlight deep parallels between long-range order in\ncondensed matter and the emergence of structure in modern language models.",
    "published": "2025-10-08T18:00:59Z",
    "updated": "2025-10-08T18:00:59Z",
    "authors": [
      "Åener ÃzÃ¶nder"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.21518v1",
    "title": "Head Pursuit: Probing Attention Specialization in Multimodal\n  Transformers",
    "summary": "Language and vision-language models have shown impressive performance across\na wide range of tasks, but their internal mechanisms remain only partly\nunderstood. In this work, we study how individual attention heads in\ntext-generative models specialize in specific semantic or visual attributes.\nBuilding on an established interpretability method, we reinterpret the practice\nof probing intermediate activations with the final decoding layer through the\nlens of signal processing. This lets us analyze multiple samples in a\nprincipled way and rank attention heads based on their relevance to target\nconcepts. Our results show consistent patterns of specialization at the head\nlevel across both unimodal and multimodal transformers. Remarkably, we find\nthat editing as few as 1% of the heads, selected using our method, can reliably\nsuppress or enhance targeted concepts in the model output. We validate our\napproach on language tasks such as question answering and toxicity mitigation,\nas well as vision-language tasks including image classification and captioning.\nOur findings highlight an interpretable and controllable structure within\nattention layers, offering simple tools for understanding and editing\nlarge-scale generative models.",
    "published": "2025-10-24T14:41:47Z",
    "updated": "2025-10-24T14:41:47Z",
    "authors": [
      "Lorenzo Basile",
      "Valentino Maiorca",
      "Diego Doimo",
      "Francesco Locatello",
      "Alberto Cazzaniga"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06644v2",
    "title": "Spark Transformer: Reactivating Sparsity in FFN and Attention",
    "summary": "The discovery of the lazy neuron phenomenon in trained Transformers, where\nthe vast majority of neurons in their feed-forward networks (FFN) are inactive\nfor each token, has spurred tremendous interests in activation sparsity for\nenhancing large model efficiency. While notable progress has been made in\ntranslating such sparsity to wall-time benefits, modern Transformers have moved\naway from the ReLU activation function crucial to this phenomenon. Existing\nefforts on re-introducing activation sparsity often degrade model quality,\nincrease parameter count, complicate or slow down training. Sparse attention,\nthe application of sparse activation to the attention mechanism, often faces\nsimilar challenges.\n  This paper introduces the Spark Transformer, a novel architecture that\nachieves a high level of activation sparsity in both FFN and the attention\nmechanism while maintaining model quality, parameter count, and standard\ntraining procedures. Our method realizes sparsity via top-k masking for\nexplicit control over sparsity level. Crucially, we introduce statistical\ntop-k, a hardware-accelerator-friendly, linear-time approximate algorithm that\navoids costly sorting and mitigates significant training slowdown from standard\ntop-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN\nparameters and attention key embeddings to form a low-cost predictor for\nidentifying activated entries. This design not only mitigates quality loss from\nenforced sparsity, but also enhances wall-time benefit. Pretrained with the\nGemma-2 recipe, Spark Transformer demonstrates competitive performance on\nstandard benchmarks while exhibiting significant sparsity: only 8% of FFN\nneurons are activated, and each token attends to a maximum of 256 tokens. This\nsparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time\nspeedups of up to 1.79x on CPU and 1.40x on GPU.",
    "published": "2025-06-07T03:51:13Z",
    "updated": "2025-10-23T03:02:01Z",
    "authors": [
      "Chong You",
      "Kan Wu",
      "Zhipeng Jia",
      "Lin Chen",
      "Srinadh Bhojanapalli",
      "Jiaxian Guo",
      "Utku Evci",
      "Jan Wassenberg",
      "Praneeth Netrapalli",
      "Jeremiah J. Willcock",
      "Suvinay Subramanian",
      "Felix Chern",
      "Alek Andreev",
      "Shreya Pathak",
      "Felix Yu",
      "Prateek Jain",
      "David E. Culler",
      "Henry M. Levy",
      "Sanjiv Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.13213v5",
    "title": "Fast Vision Transformers with HiLo Attention",
    "summary": "Vision Transformers (ViTs) have triggered the most recent and significant\nbreakthroughs in computer vision. Their efficient designs are mostly guided by\nthe indirect metric of computational complexity, i.e., FLOPs, which however has\na clear gap with the direct metric such as throughput. Thus, we propose to use\nthe direct speed evaluation on the target platform as the design principle for\nefficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT\nwhich performs favourably against the existing state-of-the-art methods across\na spectrum of different model sizes with faster speed. At the core of LITv2 is\na novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the\ninsight that high frequencies in an image capture local fine details and low\nfrequencies focus on global structures, whereas a multi-head self-attention\nlayer neglects the characteristic of different frequencies. Therefore, we\npropose to disentangle the high/low frequency patterns in an attention layer by\nseparating the heads into two groups, where one group encodes high frequencies\nvia self-attention within each local window, and another group encodes low\nfrequencies by performing global attention between the average-pooled\nlow-frequency keys and values from each window and each query position in the\ninput feature map. Benefiting from the efficient design for both groups, we\nshow that HiLo is superior to the existing attention mechanisms by\ncomprehensively benchmarking FLOPs, speed and memory consumption on GPUs and\nCPUs. For example, HiLo is 1.4x faster than spatial reduction attention and\n1.6x faster than local window attention on CPUs. Powered by HiLo, LITv2 serves\nas a strong backbone for mainstream vision tasks including image\nclassification, dense detection and segmentation. Code is available at\nhttps://github.com/ziplab/LITv2.",
    "published": "2022-05-26T08:16:14Z",
    "updated": "2023-04-19T12:04:13Z",
    "authors": [
      "Zizheng Pan",
      "Jianfei Cai",
      "Bohan Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.14837v2",
    "title": "Accurate Word Alignment Induction from Neural Machine Translation",
    "summary": "Despite its original goal to jointly learn to align and translate, prior\nresearches suggest that Transformer captures poor word alignments through its\nattention mechanism. In this paper, we show that attention weights DO capture\naccurate word alignments and propose two novel word alignment induction methods\nShift-Att and Shift-AET. The main idea is to induce alignments at the step when\nthe to-be-aligned target token is the decoder input rather than the decoder\noutput as in previous work. Shift-Att is an interpretation method that induces\nalignments from the attention weights of Transformer and does not require\nparameter update or architecture change. Shift-AET extracts alignments from an\nadditional alignment module which is tightly integrated into Transformer and\ntrained in isolation with supervision from symmetrized Shift-Att alignments.\nExperiments on three publicly available datasets demonstrate that both methods\nperform better than their corresponding neural baselines and Shift-AET\nsignificantly outperforms GIZA++ by 1.4-4.8 AER points.",
    "published": "2020-04-30T14:47:05Z",
    "updated": "2020-12-03T01:57:01Z",
    "authors": [
      "Yun Chen",
      "Yang Liu",
      "Guanhua Chen",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.04242v1",
    "title": "Texture Transform Attention for Realistic Image Inpainting",
    "summary": "Over the last few years, the performance of inpainting to fill missing\nregions has shown significant improvements by using deep neural networks. Most\nof inpainting work create a visually plausible structure and texture, however,\ndue to them often generating a blurry result, final outcomes appear unrealistic\nand make feel heterogeneity. In order to solve this problem, the existing\nmethods have used a patch based solution with deep neural network, however,\nthese methods also cannot transfer the texture properly. Motivated by these\nobservation, we propose a patch based method. Texture Transform Attention\nnetwork(TTA-Net) that better produces the missing region inpainting with fine\ndetails. The task is a single refinement network and takes the form of U-Net\narchitecture that transfers fine texture features of encoder to coarse semantic\nfeatures of decoder through skip-connection. Texture Transform Attention is\nused to create a new reassembled texture map using fine textures and coarse\nsemantics that can efficiently transfer texture information as a result. To\nstabilize training process, we use a VGG feature layer of ground truth and\npatch discriminator. We evaluate our model end-to-end with the publicly\navailable datasets CelebA-HQ and Places2 and demonstrate that images of higher\nquality can be obtained to the existing state-of-the-art approaches.",
    "published": "2020-12-08T06:28:51Z",
    "updated": "2020-12-08T06:28:51Z",
    "authors": [
      "Yejin Kim",
      "Manri Cheon",
      "Junwoo Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.06399v1",
    "title": "Spatial Temporal Transformer Network for Skeleton-based Action\n  Recognition",
    "summary": "Skeleton-based human action recognition has achieved a great interest in\nrecent years, as skeleton data has been demonstrated to be robust to\nillumination changes, body scales, dynamic camera views, and complex\nbackground. Nevertheless, an effective encoding of the latent information\nunderlying the 3D skeleton is still an open problem. In this work, we propose a\nnovel Spatial-Temporal Transformer network (ST-TR) which models dependencies\nbetween joints using the Transformer self-attention operator. In our ST-TR\nmodel, a Spatial Self-Attention module (SSA) is used to understand intra-frame\ninteractions between different body parts, and a Temporal Self-Attention module\n(TSA) to model inter-frame correlations. The two are combined in a two-stream\nnetwork which outperforms state-of-the-art models using the same input data on\nboth NTU-RGB+D 60 and NTU-RGB+D 120.",
    "published": "2020-12-11T14:58:21Z",
    "updated": "2020-12-11T14:58:21Z",
    "authors": [
      "Chiara Plizzari",
      "Marco Cannici",
      "Matteo Matteucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.05349v1",
    "title": "HYDRA -- Hyper Dependency Representation Attentions",
    "summary": "Attention is all we need as long as we have enough data. Even so, it is\nsometimes not easy to determine how much data is enough while the models are\nbecoming larger and larger. In this paper, we propose HYDRA heads, lightweight\npretrained linguistic self-attention heads to inject knowledge into transformer\nmodels without pretraining them again. Our approach is a balanced paradigm\nbetween leaving the models to learn unsupervised and forcing them to conform to\nlinguistic knowledge rigidly as suggested in previous studies. Our experiment\nproves that the approach is not only the boost performance of the model but\nalso lightweight and architecture friendly. We empirically verify our framework\non benchmark datasets to show the contribution of linguistic knowledge to a\ntransformer model. This is a promising result for a new approach to\ntransferring knowledge from linguistic resources into transformer-based models.",
    "published": "2021-09-11T19:17:34Z",
    "updated": "2021-09-11T19:17:34Z",
    "authors": [
      "Ha-Thanh Nguyen",
      "Vu Tran",
      "Tran-Binh Dang",
      "Minh-Quan Bui",
      "Minh-Phuong Nguyen",
      "Le-Minh Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.06939v1",
    "title": "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with\n  Transformer Encoders",
    "summary": "Multi-task learning with transformer encoders (MTL) has emerged as a powerful\ntechnique to improve performance on closely-related tasks for both accuracy and\nefficiency while a question still remains whether or not it would perform as\nwell on tasks that are distinct in nature. We first present MTL results on five\nNLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over\nsingle-task learning. We then conduct an extensive pruning analysis to show\nthat a certain set of attention heads get claimed by most tasks during MTL, who\ninterfere with one another to fine-tune those heads for their own objectives.\nBased on this finding, we propose the Stem Cell Hypothesis to reveal the\nexistence of attention heads naturally talented for many tasks that cannot be\njointly trained to create adequate embeddings for all of those tasks. Finally,\nwe design novel parameter-free probes to justify our hypothesis and demonstrate\nhow attention heads are transformed across the five tasks during MTL through\nlabel analysis.",
    "published": "2021-09-14T19:32:11Z",
    "updated": "2021-09-14T19:32:11Z",
    "authors": [
      "Han He",
      "Jinho D. Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.03899v1",
    "title": "Multi-Scale Wavelet Transformer for Face Forgery Detection",
    "summary": "Currently, many face forgery detection methods aggregate spatial and\nfrequency features to enhance the generalization ability and gain promising\nperformance under the cross-dataset scenario. However, these methods only\nleverage one level frequency information which limits their expressive ability.\nTo overcome these limitations, we propose a multi-scale wavelet transformer\nframework for face forgery detection. Specifically, to take full advantage of\nthe multi-scale and multi-frequency wavelet representation, we gradually\naggregate the multi-scale wavelet representation at different stages of the\nbackbone network. To better fuse the frequency feature with the spatial\nfeatures, frequency-based spatial attention is designed to guide the spatial\nfeature extractor to concentrate more on forgery traces. Meanwhile,\ncross-modality attention is proposed to fuse the frequency features with the\nspatial features. These two attention modules are calculated through a unified\ntransformer block for efficiency. A wide variety of experiments demonstrate\nthat the proposed method is efficient and effective for both within and cross\ndatasets.",
    "published": "2022-10-08T03:39:36Z",
    "updated": "2022-10-08T03:39:36Z",
    "authors": [
      "Jie Liu",
      "Jingjing Wang",
      "Peng Zhang",
      "Chunmao Wang",
      "Di Xie",
      "Shiliang Pu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.11759v1",
    "title": "Syntax-guided Localized Self-attention by Constituency Syntactic\n  Distance",
    "summary": "Recent works have revealed that Transformers are implicitly learning the\nsyntactic information in its lower layers from data, albeit is highly dependent\non the quality and scale of the training data. However, learning syntactic\ninformation from data is not necessary if we can leverage an external syntactic\nparser, which provides better parsing quality with well-defined syntactic\nstructures. This could potentially improve Transformer's performance and sample\nefficiency. In this work, we propose a syntax-guided localized self-attention\nfor Transformer that allows directly incorporating grammar structures from an\nexternal constituency parser. It prohibits the attention mechanism to\noverweight the grammatically distant tokens over close ones. Experimental\nresults show that our model could consistently improve translation performance\non a variety of machine translation datasets, ranging from small to large\ndataset sizes, and with different source languages.",
    "published": "2022-10-21T06:37:25Z",
    "updated": "2022-10-21T06:37:25Z",
    "authors": [
      "Shengyuan Hou",
      "Jushi Kai",
      "Haotian Xue",
      "Bingyu Zhu",
      "Bo Yuan",
      "Longtao Huang",
      "Xinbing Wang",
      "Zhouhan Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.15365v1",
    "title": "Li3DeTr: A LiDAR based 3D Detection Transformer",
    "summary": "Inspired by recent advances in vision transformers for object detection, we\npropose Li3DeTr, an end-to-end LiDAR based 3D Detection Transformer for\nautonomous driving, that inputs LiDAR point clouds and regresses 3D bounding\nboxes. The LiDAR local and global features are encoded using sparse convolution\nand multi-scale deformable attention respectively. In the decoder head,\nfirstly, in the novel Li3DeTr cross-attention block, we link the LiDAR global\nfeatures to 3D predictions leveraging the sparse set of object queries learnt\nfrom the data. Secondly, the object query interactions are formulated using\nmulti-head self-attention. Finally, the decoder layer is repeated $L_{dec}$\nnumber of times to refine the object queries. Inspired by DETR, we employ\nset-to-set loss to train the Li3DeTr network. Without bells and whistles, the\nLi3DeTr network achieves 61.3% mAP and 67.6% NDS surpassing the\nstate-of-the-art methods with non-maximum suppression (NMS) on the nuScenes\ndataset and it also achieves competitive performance on the KITTI dataset. We\nalso employ knowledge distillation (KD) using a teacher and student model that\nslightly improves the performance of our network.",
    "published": "2022-10-27T12:23:54Z",
    "updated": "2022-10-27T12:23:54Z",
    "authors": [
      "Gopi Krishna Erabati",
      "Helder Araujo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1904.03092v1",
    "title": "Modeling Recurrence for Transformer",
    "summary": "Recently, the Transformer model that is based solely on attention mechanisms,\nhas advanced the state-of-the-art on various machine translation tasks.\nHowever, recent studies reveal that the lack of recurrence hinders its further\nimprovement of translation capacity. In response to this problem, we propose to\ndirectly model recurrence for Transformer with an additional recurrence\nencoder. In addition to the standard recurrent neural network, we introduce a\nnovel attentive recurrent network to leverage the strengths of both attention\nand recurrent networks. Experimental results on the widely-used WMT14\nEnglish-German and WMT17 Chinese-English translation tasks demonstrate the\neffectiveness of the proposed approach. Our studies also reveal that the\nproposed model benefits from a short-cut that bridges the source and target\nsequences with a single recurrent layer, which outperforms its deep\ncounterpart.",
    "published": "2019-04-05T14:40:22Z",
    "updated": "2019-04-05T14:40:22Z",
    "authors": [
      "Jie Hao",
      "Xing Wang",
      "Baosong Yang",
      "Longyue Wang",
      "Jinfeng Zhang",
      "Zhaopeng Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.12133v2",
    "title": "O-ViT: Orthogonal Vision Transformer",
    "summary": "Inspired by the tremendous success of the self-attention mechanism in natural\nlanguage processing, the Vision Transformer (ViT) creatively applies it to\nimage patch sequences and achieves incredible performance. However, the scaled\ndot-product self-attention of ViT brings about scale ambiguity to the structure\nof the original feature space. To address this problem, we propose a novel\nmethod named Orthogonal Vision Transformer (O-ViT), to optimize ViT from the\ngeometric perspective. O-ViT limits parameters of self-attention blocks to be\non the norm-keeping orthogonal manifold, which can keep the geometry of the\nfeature space. Moreover, O-ViT achieves both orthogonal constraints and cheap\noptimization overhead by adopting a surjective mapping between the orthogonal\ngroup and its Lie algebra.We have conducted comparative experiments on image\nrecognition tasks to demonstrate O-ViT's validity and experiments show that\nO-ViT can boost the performance of ViT by up to 3.6%.",
    "published": "2022-01-28T14:18:52Z",
    "updated": "2022-02-16T13:49:43Z",
    "authors": [
      "Yanhong Fei",
      "Yingjie Liu",
      "Xian Wei",
      "Mingsong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.05282v3",
    "title": "Learning to Estimate Shapley Values with Vision Transformers",
    "summary": "Transformers have become a default architecture in computer vision, but\nunderstanding what drives their predictions remains a challenging problem.\nCurrent explanation approaches rely on attention values or input gradients, but\nthese provide a limited view of a model's dependencies. Shapley values offer a\ntheoretically sound alternative, but their computational cost makes them\nimpractical for large, high-dimensional models. In this work, we aim to make\nShapley values practical for vision transformers (ViTs). To do so, we first\nleverage an attention masking approach to evaluate ViTs with partial\ninformation, and we then develop a procedure to generate Shapley value\nexplanations via a separate, learned explainer model. Our experiments compare\nShapley values to many baseline methods (e.g., attention rollout, GradCAM,\nLRP), and we find that our approach provides more accurate explanations than\nexisting methods for ViTs.",
    "published": "2022-06-10T07:09:28Z",
    "updated": "2023-03-01T20:24:58Z",
    "authors": [
      "Ian Covert",
      "Chanwoo Kim",
      "Su-In Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.01159v4",
    "title": "BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring\n  Space for Video Object Segmentation",
    "summary": "Video Object Segmentation (VOS) is fundamental to video understanding.\nTransformer-based methods show significant performance improvement on\nsemi-supervised VOS. However, existing work faces challenges segmenting\nvisually similar objects in close proximity of each other. In this paper, we\npropose a novel Bilateral Attention Transformer in Motion-Appearance\nNeighboring space (BATMAN) for semi-supervised VOS. It captures object motion\nin the video via a novel optical flow calibration module that fuses the\nsegmentation mask with optical flow estimation to improve within-object optical\nflow smoothness and reduce noise at object boundaries. This calibrated optical\nflow is then employed in our novel bilateral attention, which computes the\ncorrespondence between the query and reference frames in the neighboring\nbilateral space considering both motion and appearance. Extensive experiments\nvalidate the effectiveness of BATMAN architecture by outperforming all existing\nstate-of-the-art on all four popular VOS benchmarks: Youtube-VOS 2019 (85.0%),\nYoutube-VOS 2018 (85.3%), DAVIS 2017Val/Testdev (86.2%/82.2%), and DAVIS 2016\n(92.5%).",
    "published": "2022-08-01T22:21:34Z",
    "updated": "2022-08-08T03:45:54Z",
    "authors": [
      "Ye Yu",
      "Jialin Yuan",
      "Gaurav Mittal",
      "Li Fuxin",
      "Mei Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.01838v2",
    "title": "Re-Attention Transformer for Weakly Supervised Object Localization",
    "summary": "Weakly supervised object localization is a challenging task which aims to\nlocalize objects with coarse annotations such as image categories. Existing\ndeep network approaches are mainly based on class activation map, which focuses\non highlighting discriminative local region while ignoring the full object. In\naddition, the emerging transformer-based techniques constantly put a lot of\nemphasis on the backdrop that impedes the ability to identify complete objects.\nTo address these issues, we present a re-attention mechanism termed token\nrefinement transformer (TRT) that captures the object-level semantics to guide\nthe localization well. Specifically, TRT introduces a novel module named token\npriority scoring module (TPSM) to suppress the effects of background noise\nwhile focusing on the target object. Then, we incorporate the class activation\nmap as the semantically aware input to restrain the attention map to the target\nobject. Extensive experiments on two benchmarks showcase the superiority of our\nproposed method against existing methods with image category annotations.\nSource code is available in\n\\url{https://github.com/su-hui-zz/ReAttentionTransformer}.",
    "published": "2022-08-03T04:34:28Z",
    "updated": "2023-02-26T03:54:20Z",
    "authors": [
      "Hui Su",
      "Yue Ye",
      "Zhiwei Chen",
      "Mingli Song",
      "Lechao Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.09266v1",
    "title": "Diverse Video Captioning by Adaptive Spatio-temporal Attention",
    "summary": "To generate proper captions for videos, the inference needs to identify\nrelevant concepts and pay attention to the spatial relationships between them\nas well as to the temporal development in the clip. Our end-to-end\nencoder-decoder video captioning framework incorporates two transformer-based\narchitectures, an adapted transformer for a single joint spatio-temporal video\nanalysis as well as a self-attention-based decoder for advanced text\ngeneration. Furthermore, we introduce an adaptive frame selection scheme to\nreduce the number of required incoming frames while maintaining the relevant\ncontent when training both transformers. Additionally, we estimate semantic\nconcepts relevant for video captioning by aggregating all ground truth captions\nof each sample. Our approach achieves state-of-the-art results on the MSVD, as\nwell as on the large-scale MSR-VTT and the VATEX benchmark datasets considering\nmultiple Natural Language Generation (NLG) metrics. Additional evaluations on\ndiversity scores highlight the expressiveness and diversity in the structure of\nour generated captions.",
    "published": "2022-08-19T11:21:59Z",
    "updated": "2022-08-19T11:21:59Z",
    "authors": [
      "Zohreh Ghaderi",
      "Leonard Salewski",
      "Hendrik P. A. Lensch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.04245v2",
    "title": "How Do Transformers Learn Topic Structure: Towards a Mechanistic\n  Understanding",
    "summary": "While the successes of transformers across many domains are indisputable,\naccurate understanding of the learning mechanics is still largely lacking.\nTheir capabilities have been probed on benchmarks which include a variety of\nstructured and reasoning tasks -- but mathematical understanding is lagging\nsubstantially behind. Recent lines of work have begun studying representational\naspects of this question: that is, the size/depth/complexity of attention-based\nnetworks to perform certain tasks. However, there is no guarantee the learning\ndynamics will converge to the constructions proposed. In our paper, we provide\nfine-grained mechanistic understanding of how transformers learn \"semantic\nstructure\", understood as capturing co-occurrence structure of words.\nPrecisely, we show, through a combination of mathematical analysis and\nexperiments on Wikipedia data and synthetic data modeled by Latent Dirichlet\nAllocation (LDA), that the embedding layer and the self-attention layer encode\nthe topical structure. In the former case, this manifests as higher average\ninner product of embeddings between same-topic words. In the latter, it\nmanifests as higher average pairwise attention between same-topic words. The\nmathematical results involve several assumptions to make the analysis\ntractable, which we verify on data, and might be of independent interest as\nwell.",
    "published": "2023-03-07T21:42:17Z",
    "updated": "2023-07-24T17:29:04Z",
    "authors": [
      "Yuchen Li",
      "Yuanzhi Li",
      "Andrej Risteski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.04599v1",
    "title": "Point Cloud Classification Using Content-based Transformer via\n  Clustering in Feature Space",
    "summary": "Recently, there have been some attempts of Transformer in 3D point cloud\nclassification. In order to reduce computations, most existing methods focus on\nlocal spatial attention, but ignore their content and fail to establish\nrelationships between distant but relevant points. To overcome the limitation\nof local spatial attention, we propose a point content-based Transformer\narchitecture, called PointConT for short. It exploits the locality of points in\nthe feature space (content-based), which clusters the sampled points with\nsimilar features into the same class and computes the self-attention within\neach class, thus enabling an effective trade-off between capturing long-range\ndependencies and computational complexity. We further introduce an Inception\nfeature aggregator for point cloud classification, which uses parallel\nstructures to aggregate high-frequency and low-frequency information in each\nbranch separately. Extensive experiments show that our PointConT model achieves\na remarkable performance on point cloud shape classification. Especially, our\nmethod exhibits 90.3% Top-1 accuracy on the hardest setting of ScanObjectNN.\nSource code of this paper is available at\nhttps://github.com/yahuiliu99/PointConT.",
    "published": "2023-03-08T14:11:05Z",
    "updated": "2023-03-08T14:11:05Z",
    "authors": [
      "Yahui Liu",
      "Bin Tian",
      "Yisheng Lv",
      "Lingxi Li",
      "Feiyue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.06179v1",
    "title": "Deformable Cross-Attention Transformer for Medical Image Registration",
    "summary": "Transformers have recently shown promise for medical image applications,\nleading to an increasing interest in developing such models for medical image\nregistration. Recent advancements in designing registration Transformers have\nfocused on using cross-attention (CA) to enable a more precise understanding of\nspatial correspondences between moving and fixed images. Here, we propose a\nnovel CA mechanism that computes windowed attention using deformable windows.\nIn contrast to existing CA mechanisms that require intensive computational\ncomplexity by either computing CA globally or locally with a fixed and expanded\nsearch window, the proposed deformable CA can selectively sample a diverse set\nof features over a large search window while maintaining low computational\ncomplexity. The proposed model was extensively evaluated on multi-modal,\nmono-modal, and atlas-to-patient registration tasks, demonstrating promising\nperformance against state-of-the-art methods and indicating its effectiveness\nfor medical image registration. The source code for this work will be available\nafter publication.",
    "published": "2023-03-10T19:22:01Z",
    "updated": "2023-03-10T19:22:01Z",
    "authors": [
      "Junyu Chen",
      "Yihao Liu",
      "Yufan He",
      "Yong Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.10321v1",
    "title": "ABC: Attention with Bilinear Correlation for Infrared Small Target\n  Detection",
    "summary": "Infrared small target detection (ISTD) has a wide range of applications in\nearly warning, rescue, and guidance. However, CNN based deep learning methods\nare not effective at segmenting infrared small target (IRST) that it lack of\nclear contour and texture features, and transformer based methods also struggle\nto achieve significant results due to the absence of convolution induction\nbias. To address these issues, we propose a new model called attention with\nbilinear correlation (ABC), which is based on the transformer architecture and\nincludes a convolution linear fusion transformer (CLFT) module with a novel\nattention mechanism for feature extraction and fusion, which effectively\nenhances target features and suppresses noise. Additionally, our model includes\na u-shaped convolution-dilated convolution (UCDC) module located deeper layers\nof the network, which takes advantage of the smaller resolution of deeper\nfeatures to obtain finer semantic information. Experimental results on public\ndatasets demonstrate that our approach achieves state-of-the-art performance.\nCode is available at https://github.com/PANPEIWEN/ABC",
    "published": "2023-03-18T03:47:06Z",
    "updated": "2023-03-18T03:47:06Z",
    "authors": [
      "Peiwen Pan",
      "Huan Wang",
      "Chenyi Wang",
      "Chang Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.03407v1",
    "title": "Question Generation from Paragraphs: A Tale of Two Hierarchical Models",
    "summary": "Automatic question generation from paragraphs is an important and challenging\nproblem, particularly due to the long context from paragraphs. In this paper,\nwe propose and study two hierarchical models for the task of question\ngeneration from paragraphs. Specifically, we propose (a) a novel hierarchical\nBiLSTM model with selective attention and (b) a novel hierarchical Transformer\narchitecture, both of which learn hierarchical representations of paragraphs.\nWe model a paragraph in terms of its constituent sentences, and a sentence in\nterms of its constituent words. While the introduction of the attention\nmechanism benefits the hierarchical BiLSTM model, the hierarchical Transformer,\nwith its inherent attention and positional encoding mechanisms also performs\nbetter than flat transformer model. We conducted empirical evaluation on the\nwidely used SQuAD and MS MARCO datasets using standard metrics. The results\ndemonstrate the overall effectiveness of the hierarchical models over their\nflat counterparts. Qualitatively, our hierarchical models are able to generate\nfluent and relevant questions",
    "published": "2019-11-08T17:49:08Z",
    "updated": "2019-11-08T17:49:08Z",
    "authors": [
      "Vishwajeet Kumar",
      "Raktim Chaki",
      "Sai Teja Talluri",
      "Ganesh Ramakrishnan",
      "Yuan-Fang Li",
      "Gholamreza Haffari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2007.08620v2",
    "title": "The Monte Carlo Transformer: a stochastic self-attention model for\n  sequence prediction",
    "summary": "This paper introduces the Sequential Monte Carlo Transformer, an original\napproach that naturally captures the observations distribution in a transformer\narchitecture. The keys, queries, values and attention vectors of the network\nare considered as the unobserved stochastic states of its hidden structure.\nThis generative model is such that at each time step the received observation\nis a random function of its past states in a given attention window. In this\ngeneral state-space setting, we use Sequential Monte Carlo methods to\napproximate the posterior distributions of the states given the observations,\nand to estimate the gradient of the log-likelihood. We hence propose a\ngenerative model giving a predictive distribution, instead of a single-point\nestimate.",
    "published": "2020-07-15T10:01:48Z",
    "updated": "2020-12-15T14:27:22Z",
    "authors": [
      "Alice Martin",
      "Charles Ollion",
      "Florian Strub",
      "Sylvain Le Corff",
      "Olivier Pietquin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.02702v3",
    "title": "TrackFormer: Multi-Object Tracking with Transformers",
    "summary": "The challenging task of multi-object tracking (MOT) requires simultaneous\nreasoning about track initialization, identity, and spatio-temporal\ntrajectories. We formulate this task as a frame-to-frame set prediction problem\nand introduce TrackFormer, an end-to-end trainable MOT approach based on an\nencoder-decoder Transformer architecture. Our model achieves data association\nbetween frames via attention by evolving a set of track predictions through a\nvideo sequence. The Transformer decoder initializes new tracks from static\nobject queries and autoregressively follows existing tracks in space and time\nwith the conceptually new and identity preserving track queries. Both query\ntypes benefit from self- and encoder-decoder attention on global frame-level\nfeatures, thereby omitting any additional graph optimization or modeling of\nmotion and/or appearance. TrackFormer introduces a new tracking-by-attention\nparadigm and while simple in its design is able to achieve state-of-the-art\nperformance on the task of multi-object tracking (MOT17 and MOT20) and\nsegmentation (MOTS20). The code is available at\nhttps://github.com/timmeinhardt/trackformer .",
    "published": "2021-01-07T18:59:29Z",
    "updated": "2022-04-29T08:39:24Z",
    "authors": [
      "Tim Meinhardt",
      "Alexander Kirillov",
      "Laura Leal-Taixe",
      "Christoph Feichtenhofer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.03138v1",
    "title": "Portfolio Optimization with 2D Relative-Attentional Gated Transformer",
    "summary": "Portfolio optimization is one of the most attentive fields that have been\nresearched with machine learning approaches. Many researchers attempted to\nsolve this problem using deep reinforcement learning due to its efficient\ninherence that can handle the property of financial markets. However, most of\nthem can hardly be applicable to real-world trading since they ignore or\nextremely simplify the realistic constraints of transaction costs. These\nconstraints have a significantly negative impact on portfolio profitability. In\nour research, a conservative level of transaction fees and slippage are\nconsidered for the realistic experiment. To enhance the performance under those\nconstraints, we propose a novel Deterministic Policy Gradient with 2D\nRelative-attentional Gated Transformer (DPGRGT) model. Applying learnable\nrelative positional embeddings for the time and assets axes, the model better\nunderstands the peculiar structure of the financial data in the portfolio\noptimization domain. Also, gating layers and layer reordering are employed for\nstable convergence of Transformers in reinforcement learning. In our experiment\nusing U.S. stock market data of 20 years, our model outperformed baseline\nmodels and demonstrated its effectiveness.",
    "published": "2020-12-27T14:08:26Z",
    "updated": "2020-12-27T14:08:26Z",
    "authors": [
      "Tae Wan Kim",
      "Matloob Khushi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.08833v2",
    "title": "SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation",
    "summary": "In this paper we introduce a Transformer-based approach to video object\nsegmentation (VOS). To address compounding error and scalability issues of\nprior work, we propose a scalable, end-to-end method for VOS called Sparse\nSpatiotemporal Transformers (SST). SST extracts per-pixel representations for\neach object in a video using sparse attention over spatiotemporal features. Our\nattention-based formulation for VOS allows a model to learn to attend over a\nhistory of multiple frames and provides suitable inductive bias for performing\ncorrespondence-like computations necessary for solving motion segmentation. We\ndemonstrate the effectiveness of attention-based over recurrent networks in the\nspatiotemporal domain. Our method achieves competitive results on YouTube-VOS\nand DAVIS 2017 with improved scalability and robustness to occlusions compared\nwith the state of the art. Code is available at\nhttps://github.com/dukebw/SSTVOS.",
    "published": "2021-01-21T20:06:12Z",
    "updated": "2021-03-29T00:59:47Z",
    "authors": [
      "Brendan Duke",
      "Abdalla Ahmed",
      "Christian Wolf",
      "Parham Aarabi",
      "Graham W. Taylor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14103v2",
    "title": "An Attention Free Transformer",
    "summary": "We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.",
    "published": "2021-05-28T20:45:30Z",
    "updated": "2021-09-21T18:04:55Z",
    "authors": [
      "Shuangfei Zhai",
      "Walter Talbott",
      "Nitish Srivastava",
      "Chen Huang",
      "Hanlin Goh",
      "Ruixiang Zhang",
      "Josh Susskind"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.05795v1",
    "title": "Transformed CNNs: recasting pre-trained convolutional layers with\n  self-attention",
    "summary": "Vision Transformers (ViT) have recently emerged as a powerful alternative to\nconvolutional networks (CNNs). Although hybrid models attempt to bridge the gap\nbetween these two architectures, the self-attention layers they rely on induce\na strong computational bottleneck, especially at large spatial resolutions. In\nthis work, we explore the idea of reducing the time spent training these layers\nby initializing them as convolutional layers. This enables us to transition\nsmoothly from any pre-trained CNN to its functionally identical hybrid model,\ncalled Transformed CNN (T-CNN). With only 50 epochs of fine-tuning, the\nresulting T-CNNs demonstrate significant performance gains over the CNN (+2.2%\ntop-1 on ImageNet-1k for a ResNet50-RS) as well as substantially improved\nrobustness (+11% top-1 on ImageNet-C). We analyze the representations learnt by\nthe T-CNN, providing deeper insights into the fruitful interplay between\nconvolutions and self-attention. Finally, we experiment initializing the T-CNN\nfrom a partially trained CNN, and find that it reaches better performance than\nthe corresponding hybrid model trained from scratch, while reducing training\ntime.",
    "published": "2021-06-10T14:56:10Z",
    "updated": "2021-06-10T14:56:10Z",
    "authors": [
      "StÃ©phane d'Ascoli",
      "Levent Sagun",
      "Giulio Biroli",
      "Ari Morcos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.00385v3",
    "title": "Transformer-based deep imitation learning for dual-arm robot\n  manipulation",
    "summary": "Deep imitation learning is promising for solving dexterous manipulation tasks\nbecause it does not require an environment model and pre-programmed robot\nbehavior. However, its application to dual-arm manipulation tasks remains\nchallenging. In a dual-arm manipulation setup, the increased number of state\ndimensions caused by the additional robot manipulators causes distractions and\nresults in poor performance of the neural networks. We address this issue using\na self-attention mechanism that computes dependencies between elements in a\nsequential input and focuses on important elements. A Transformer, a variant of\nself-attention architecture, is applied to deep imitation learning to solve\ndual-arm manipulation tasks in the real world. The proposed method has been\ntested on dual-arm manipulation tasks using a real robot. The experimental\nresults demonstrated that the Transformer-based deep imitation learning\narchitecture can attend to the important features among the sensory inputs,\ntherefore reducing distractions and improving manipulation performance when\ncompared with the baseline architecture without the self-attention mechanisms.\nData from this and related works are available at:\nhttps://sites.google.com/view/multi-task-fine.",
    "published": "2021-08-01T07:42:39Z",
    "updated": "2025-05-21T23:09:12Z",
    "authors": [
      "Heecheol Kim",
      "Yoshiyuki Ohmura",
      "Yasuo Kuniyoshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.04657v3",
    "title": "Differentiable Subset Pruning of Transformer Heads",
    "summary": "Multi-head attention, a collection of several attention mechanisms that\nindependently attend to different parts of the input, is the key ingredient in\nthe Transformer. Recent work has shown, however, that a large proportion of the\nheads in a Transformer's multi-head attention mechanism can be safely pruned\naway without significantly harming the performance of the model; such pruning\nleads to models that are noticeably smaller and faster in practice. Our work\nintroduces a new head pruning technique that we term differentiable subset\npruning. Intuitively, our method learns per-head importance variables and then\nenforces a user-specified hard constraint on the number of unpruned heads. The\nimportance variables are learned via stochastic gradient descent. We conduct\nexperiments on natural language inference and machine translation; we show that\ndifferentiable subset pruning performs comparably or better than previous works\nwhile offering precise control of the sparsity level.",
    "published": "2021-08-10T13:08:34Z",
    "updated": "2023-07-27T07:14:18Z",
    "authors": [
      "Jiaoda Li",
      "Ryan Cotterell",
      "Mrinmaya Sachan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.09635v1",
    "title": "StarVQA: Space-Time Attention for Video Quality Assessment",
    "summary": "The attention mechanism is blooming in computer vision nowadays. However, its\napplication to video quality assessment (VQA) has not been reported. Evaluating\nthe quality of in-the-wild videos is challenging due to the unknown of pristine\nreference and shooting distortion. This paper presents a novel\n\\underline{s}pace-\\underline{t}ime \\underline{a}ttention network\nfo\\underline{r} the \\underline{VQA} problem, named StarVQA. StarVQA builds a\nTransformer by alternately concatenating the divided space-time attention. To\nadapt the Transformer architecture for training, StarVQA designs a vectorized\nregression loss by encoding the mean opinion score (MOS) to the probability\nvector and embedding a special vectorized label token as the learnable\nvariable. To capture the long-range spatiotemporal dependencies of a video\nsequence, StarVQA encodes the space-time position information of each patch to\nthe input of the Transformer. Various experiments are conducted on the de-facto\nin-the-wild video datasets, including LIVE-VQC, KoNViD-1k, LSVQ, and\nLSVQ-1080p. Experimental results demonstrate the superiority of the proposed\nStarVQA over the state-of-the-art. Code and model will be available at:\nhttps://github.com/DVL/StarVQA.",
    "published": "2021-08-22T04:53:02Z",
    "updated": "2021-08-22T04:53:02Z",
    "authors": [
      "Fengchuang Xing",
      "Yuan-Gen Wang",
      "Hanpin Wang",
      "Leida Li",
      "Guopu Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.10568v1",
    "title": "Auto-Parsing Network for Image Captioning and Visual Question Answering",
    "summary": "We propose an Auto-Parsing Network (APN) to discover and exploit the input\ndata's hidden tree structures for improving the effectiveness of the\nTransformer-based vision-language systems. Specifically, we impose a\nProbabilistic Graphical Model (PGM) parameterized by the attention operations\non each self-attention layer to incorporate sparse assumption. We use this PGM\nto softly segment an input sequence into a few clusters where each cluster can\nbe treated as the parent of the inside entities. By stacking these PGM\nconstrained self-attention layers, the clusters in a lower layer compose into a\nnew sequence, and the PGM in a higher layer will further segment this sequence.\nIteratively, a sparse tree can be implicitly parsed, and this tree's\nhierarchical knowledge is incorporated into the transformed embeddings, which\ncan be used for solving the target vision-language tasks. Specifically, we\nshowcase that our APN can strengthen Transformer based networks in two major\nvision-language tasks: Captioning and Visual Question Answering. Also, a PGM\nprobability-based parsing algorithm is developed by which we can discover what\nthe hidden structure of input is during the inference.",
    "published": "2021-08-24T08:14:35Z",
    "updated": "2021-08-24T08:14:35Z",
    "authors": [
      "Xu Yang",
      "Chongyang Gao",
      "Hanwang Zhang",
      "Jianfei Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.13817v3",
    "title": "Video Frame Interpolation Transformer",
    "summary": "Existing methods for video interpolation heavily rely on deep convolution\nneural networks, and thus suffer from their intrinsic limitations, such as\ncontent-agnostic kernel weights and restricted receptive field. To address\nthese issues, we propose a Transformer-based video interpolation framework that\nallows content-aware aggregation weights and considers long-range dependencies\nwith the self-attention operations. To avoid the high computational cost of\nglobal self-attention, we introduce the concept of local attention into video\ninterpolation and extend it to the spatial-temporal domain. Furthermore, we\npropose a space-time separation strategy to save memory usage, which also\nimproves performance. In addition, we develop a multi-scale frame synthesis\nscheme to fully realize the potential of Transformers. Extensive experiments\ndemonstrate the proposed model performs favorably against the state-of-the-art\nmethods both quantitatively and qualitatively on a variety of benchmark\ndatasets.",
    "published": "2021-11-27T05:35:10Z",
    "updated": "2022-03-28T17:46:04Z",
    "authors": [
      "Zhihao Shi",
      "Xiangyu Xu",
      "Xiaohong Liu",
      "Jun Chen",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.04863v2",
    "title": "3D Medical Point Transformer: Introducing Convolution to Attention\n  Networks for Medical Point Cloud Analysis",
    "summary": "General point clouds have been increasingly investigated for different tasks,\nand recently Transformer-based networks are proposed for point cloud analysis.\nHowever, there are barely related works for medical point clouds, which are\nimportant for disease detection and treatment. In this work, we propose an\nattention-based model specifically for medical point clouds, namely 3D medical\npoint Transformer (3DMedPT), to examine the complex biological structures. By\naugmenting contextual information and summarizing local responses at query, our\nattention module can capture both local context and global content feature\ninteractions. However, the insufficient training samples of medical data may\nlead to poor feature learning, so we apply position embeddings to learn\naccurate local geometry and Multi-Graph Reasoning (MGR) to examine global\nknowledge propagation over channel graphs to enrich feature representations.\nExperiments conducted on IntrA dataset proves the superiority of 3DMedPT, where\nwe achieve the best classification and segmentation results. Furthermore, the\npromising generalization ability of our method is validated on general 3D point\ncloud benchmarks: ModelNet40 and ShapeNetPart. Code is released.",
    "published": "2021-12-09T12:31:28Z",
    "updated": "2021-12-17T00:52:57Z",
    "authors": [
      "Jianhui Yu",
      "Chaoyi Zhang",
      "Heng Wang",
      "Dingxin Zhang",
      "Yang Song",
      "Tiange Xiang",
      "Dongnan Liu",
      "Weidong Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.07916v2",
    "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
    "summary": "Recent work has shown that either (1) increasing the input length or (2)\nincreasing model size can improve the performance of Transformer-based neural\nmodels. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same\ntime. Specifically, we integrated attention ideas from long-input transformers\n(ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention\nmechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's\nlocal/global attention mechanism, but without requiring additional side-inputs.\nWe are able to achieve state-of-the-art results on several summarization tasks\nand outperform the original T5 models on question answering tasks.",
    "published": "2021-12-15T06:35:29Z",
    "updated": "2022-05-03T14:19:03Z",
    "authors": [
      "Mandy Guo",
      "Joshua Ainslie",
      "David Uthus",
      "Santiago Ontanon",
      "Jianmo Ni",
      "Yun-Hsuan Sung",
      "Yinfei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.02452v2",
    "title": "Contextformer: A Transformer with Spatio-Channel Attention for Context\n  Modeling in Learned Image Compression",
    "summary": "Entropy modeling is a key component for high-performance image compression\nalgorithms. Recent developments in autoregressive context modeling helped\nlearning-based methods to surpass their classical counterparts. However, the\nperformance of those models can be further improved due to the underexploited\nspatio-channel dependencies in latent space, and the suboptimal implementation\nof context adaptivity. Inspired by the adaptive characteristics of the\ntransformers, we propose a transformer-based context model, named\nContextformer, which generalizes the de facto standard attention mechanism to\nspatio-channel attention. We replace the context model of a modern compression\nframework with the Contextformer and test it on the widely used Kodak,\nCLIC2020, and Tecnick image datasets. Our experimental results show that the\nproposed model provides up to 11% rate savings compared to the standard\nVersatile Video Coding (VVC) Test Model (VTM) 16.2, and outperforms various\nlearning-based models in terms of PSNR and MS-SSIM.",
    "published": "2022-03-04T17:29:32Z",
    "updated": "2022-07-20T11:48:23Z",
    "authors": [
      "A. Burakhan Koyuncu",
      "Han Gao",
      "Atanas Boev",
      "Georgii Gaikov",
      "Elena Alshina",
      "Eckehard Steinbach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.04049v2",
    "title": "Graph Attention Transformer Network for Multi-Label Image Classification",
    "summary": "Multi-label classification aims to recognize multiple objects or attributes\nfrom images. However, it is challenging to learn from proper label graphs to\neffectively characterize such inter-label correlations or dependencies. Current\nmethods often use the co-occurrence probability of labels based on the training\nset as the adjacency matrix to model this correlation, which is greatly limited\nby the dataset and affects the model's generalization ability. In this paper,\nwe propose a Graph Attention Transformer Network (GATN), a general framework\nfor multi-label image classification that can effectively mine complex\ninter-label relationships. First, we use the cosine similarity based on the\nlabel word embedding as the initial correlation matrix, which can represent\nrich semantic information. Subsequently, we design the graph attention\ntransformer layer to transfer this adjacency matrix to adapt to the current\ndomain. Our extensive experiments have demonstrated that our proposed methods\ncan achieve state-of-the-art performance on three datasets.",
    "published": "2022-03-08T12:39:05Z",
    "updated": "2024-01-15T10:44:57Z",
    "authors": [
      "Jin Yuan",
      "Shikai Chen",
      "Yao Zhang",
      "Zhongchao Shi",
      "Xin Geng",
      "Jianping Fan",
      "Yong Rui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.07373v1",
    "title": "SATr: Slice Attention with Transformer for Universal Lesion Detection",
    "summary": "Universal Lesion Detection (ULD) in computed tomography plays an essential\nrole in computer-aided diagnosis. Promising ULD results have been reported by\nmulti-slice-input detection approaches which model 3D context from multiple\nadjacent CT slices, but such methods still experience difficulty in obtaining a\nglobal representation among different slices and within each individual slice\nsince they only use convolution-based fusion operations. In this paper, we\npropose a novel Slice Attention Transformer (SATr) block which can be easily\nplugged into convolution-based ULD backbones to form hybrid network structures.\nSuch newly formed hybrid backbones can better model long-distance feature\ndependency via the cascaded self-attention modules in the Transformer block\nwhile still holding a strong power of modeling local features with the\nconvolutional operations in the original backbone. Experiments with five\nstate-of-the-art methods show that the proposed SATr block can provide an\nalmost free boost to lesion detection accuracy without extra hyperparameters or\nspecial network designs.",
    "published": "2022-03-13T03:37:27Z",
    "updated": "2022-03-13T03:37:27Z",
    "authors": [
      "Han Li",
      "Long Chen",
      "Hu Han",
      "S. Kevin Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.09642v1",
    "title": "Cascade Transformers for End-to-End Person Search",
    "summary": "The goal of person search is to localize a target person from a gallery set\nof scene images, which is extremely challenging due to large scale variations,\npose/viewpoint changes, and occlusions. In this paper, we propose the Cascade\nOccluded Attention Transformer (COAT) for end-to-end person search. Our\nthree-stage cascade design focuses on detecting people in the first stage,\nwhile later stages simultaneously and progressively refine the representation\nfor person detection and re-identification. At each stage the occluded\nattention transformer applies tighter intersection over union thresholds,\nforcing the network to learn coarse-to-fine pose/scale invariant features.\nMeanwhile, we calculate each detection's occluded attention to differentiate a\nperson's tokens from other people or the background. In this way, we simulate\nthe effect of other objects occluding a person of interest at the token-level.\nThrough comprehensive experiments, we demonstrate the benefits of our method by\nachieving state-of-the-art performance on two benchmark datasets.",
    "published": "2022-03-17T22:42:12Z",
    "updated": "2022-03-17T22:42:12Z",
    "authors": [
      "Rui Yu",
      "Dawei Du",
      "Rodney LaLonde",
      "Daniel Davila",
      "Christopher Funk",
      "Anthony Hoogs",
      "Brian Clipp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.10638v3",
    "title": "V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision\n  Transformer",
    "summary": "In this paper, we investigate the application of Vehicle-to-Everything (V2X)\ncommunication to improve the perception performance of autonomous vehicles. We\npresent a robust cooperative perception framework with V2X communication using\na novel vision Transformer. Specifically, we build a holistic attention model,\nnamely V2X-ViT, to effectively fuse information across on-road agents (i.e.,\nvehicles and infrastructure). V2X-ViT consists of alternating layers of\nheterogeneous multi-agent self-attention and multi-scale window self-attention,\nwhich captures inter-agent interaction and per-agent spatial relationships.\nThese key modules are designed in a unified Transformer architecture to handle\ncommon V2X challenges, including asynchronous information sharing, pose errors,\nand heterogeneity of V2X components. To validate our approach, we create a\nlarge-scale V2X perception dataset using CARLA and OpenCDA. Extensive\nexperimental results demonstrate that V2X-ViT sets new state-of-the-art\nperformance for 3D object detection and achieves robust performance even under\nharsh, noisy environments. The code is available at\nhttps://github.com/DerrickXuNu/v2x-vit.",
    "published": "2022-03-20T20:18:25Z",
    "updated": "2022-08-08T14:52:03Z",
    "authors": [
      "Runsheng Xu",
      "Hao Xiang",
      "Zhengzhong Tu",
      "Xin Xia",
      "Ming-Hsuan Yang",
      "Jiaqi Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.12719v2",
    "title": "What to Hide from Your Students: Attention-Guided Masked Image Modeling",
    "summary": "Transformers and masked language modeling are quickly being adopted and\nexplored in computer vision as vision transformers and masked image modeling\n(MIM). In this work, we argue that image token masking differs from token\nmasking in text, due to the amount and correlation of tokens in an image. In\nparticular, to generate a challenging pretext task for MIM, we advocate a shift\nfrom random masking to informed masking. We develop and exhibit this idea in\nthe context of distillation-based MIM, where a teacher transformer encoder\ngenerates an attention map, which we use to guide masking for the student. We\nthus introduce a novel masking strategy, called attention-guided masking\n(AttMask), and we demonstrate its effectiveness over random masking for dense\ndistillation-based MIM as well as plain distillation-based self-supervised\nlearning on classification tokens. We confirm that AttMask accelerates the\nlearning process and improves the performance on a variety of downstream tasks.\nWe provide the implementation code at https://github.com/gkakogeorgiou/attmask.",
    "published": "2022-03-23T20:52:50Z",
    "updated": "2022-07-22T10:36:25Z",
    "authors": [
      "Ioannis Kakogeorgiou",
      "Spyros Gidaris",
      "Bill Psomas",
      "Yannis Avrithis",
      "Andrei Bursuc",
      "Konstantinos Karantzalos",
      "Nikos Komodakis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.13537v2",
    "title": "Efficient Visual Tracking via Hierarchical Cross-Attention Transformer",
    "summary": "In recent years, target tracking has made great progress in accuracy. This\ndevelopment is mainly attributed to powerful networks (such as transformers)\nand additional modules (such as online update and refinement modules). However,\nless attention has been paid to tracking speed. Most state-of-the-art trackers\nare satisfied with the real-time speed on powerful GPUs. However, practical\napplications necessitate higher requirements for tracking speed, especially\nwhen edge platforms with limited resources are used. In this work, we present\nan efficient tracking method via a hierarchical cross-attention transformer\nnamed HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on\nthe edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our\nHCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100,\nUAV123, and VOT2020. Code and models are available at\nhttps://github.com/chenxin-dlut/HCAT.",
    "published": "2022-03-25T09:45:27Z",
    "updated": "2022-10-30T02:24:40Z",
    "authors": [
      "Xin Chen",
      "Ben Kang",
      "Dong Wang",
      "Dongdong Li",
      "Huchuan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.03776v1",
    "title": "SparseTT: Visual Tracking with Sparse Transformers",
    "summary": "Transformers have been successfully applied to the visual tracking task and\nsignificantly promote tracking performance. The self-attention mechanism\ndesigned to model long-range dependencies is the key to the success of\nTransformers. However, self-attention lacks focusing on the most relevant\ninformation in the search regions, making it easy to be distracted by\nbackground. In this paper, we relieve this issue with a sparse attention\nmechanism by focusing the most relevant information in the search regions,\nwhich enables a much accurate tracking. Furthermore, we introduce a double-head\npredictor to boost the accuracy of foreground-background classification and\nregression of target bounding boxes, which further improve the tracking\nperformance. Extensive experiments show that, without bells and whistles, our\nmethod significantly outperforms the state-of-the-art approaches on LaSOT,\nGOT-10k, TrackingNet, and UAV123, while running at 40 FPS. Notably, the\ntraining time of our method is reduced by 75% compared to that of TransT. The\nsource code and models are available at https://github.com/fzh0917/SparseTT.",
    "published": "2022-05-08T04:00:28Z",
    "updated": "2022-05-08T04:00:28Z",
    "authors": [
      "Zhihong Fu",
      "Zehua Fu",
      "Qingjie Liu",
      "Wenrui Cai",
      "Yunhong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.05943v2",
    "title": "Exploiting Inductive Bias in Transformers for Unsupervised\n  Disentanglement of Syntax and Semantics with VAEs",
    "summary": "We propose a generative model for text generation, which exhibits\ndisentangled latent representations of syntax and semantics. Contrary to\nprevious work, this model does not need syntactic information such as\nconstituency parses, or semantic information such as paraphrase pairs. Our\nmodel relies solely on the inductive bias found in attention-based\narchitectures such as Transformers.\n  In the attention of Transformers, keys handle information selection while\nvalues specify what information is conveyed. Our model, dubbed QKVAE, uses\nAttention in its decoder to read latent variables where one latent variable\ninfers keys while another infers values. We run experiments on latent\nrepresentations and experiments on syntax/semantics transfer which show that\nQKVAE displays clear signs of disentangled syntax and semantics. We also show\nthat our model displays competitive syntax transfer capabilities when compared\nto supervised models and that comparable supervised models need a fairly large\namount of data (more than 50K samples) to outperform it on both syntactic and\nsemantic transfer. The code for our experiments is publicly available.",
    "published": "2022-05-12T08:21:38Z",
    "updated": "2022-05-19T14:09:15Z",
    "authors": [
      "Ghazi Felhi",
      "Joseph Le Roux",
      "DjamÃ© Seddah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.07177v1",
    "title": "Hero-Gang Neural Model For Named Entity Recognition",
    "summary": "Named entity recognition (NER) is a fundamental and important task in NLP,\naiming at identifying named entities (NEs) from free text. Recently, since the\nmulti-head attention mechanism applied in the Transformer model can effectively\ncapture longer contextual information, Transformer-based models have become the\nmainstream methods and have achieved significant performance in this task.\nUnfortunately, although these models can capture effective global context\ninformation, they are still limited in the local feature and position\ninformation extraction, which is critical in NER. In this paper, to address\nthis limitation, we propose a novel Hero-Gang Neural structure (HGN), including\nthe Hero and Gang module, to leverage both global and local information to\npromote NER. Specifically, the Hero module is composed of a Transformer-based\nencoder to maintain the advantage of the self-attention mechanism, and the Gang\nmodule utilizes a multi-window recurrent module to extract local features and\nposition information under the guidance of the Hero module. Afterward, the\nproposed multi-window attention effectively combines global information and\nmultiple local features for predicting entity labels. Experimental results on\nseveral benchmark datasets demonstrate the effectiveness of our proposed model.",
    "published": "2022-05-15T04:33:31Z",
    "updated": "2022-05-15T04:33:31Z",
    "authors": [
      "Jinpeng Hu",
      "Yaling Shen",
      "Yang Liu",
      "Xiang Wan",
      "Tsung-Hui Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.14575v2",
    "title": "3D-C2FT: Coarse-to-fine Transformer for Multi-view 3D Reconstruction",
    "summary": "Recently, the transformer model has been successfully employed for the\nmulti-view 3D reconstruction problem. However, challenges remain on designing\nan attention mechanism to explore the multiview features and exploit their\nrelations for reinforcing the encoding-decoding modules. This paper proposes a\nnew model, namely 3D coarse-to-fine transformer (3D-C2FT), by introducing a\nnovel coarse-to-fine(C2F) attention mechanism for encoding multi-view features\nand rectifying defective 3D objects. C2F attention mechanism enables the model\nto learn multi-view information flow and synthesize 3D surface correction in a\ncoarse to fine-grained manner. The proposed model is evaluated by ShapeNet and\nMulti-view Real-life datasets. Experimental results show that 3D-C2FT achieves\nnotable results and outperforms several competing models on these datasets.",
    "published": "2022-05-29T06:01:42Z",
    "updated": "2023-01-17T08:08:15Z",
    "authors": [
      "Leslie Ching Ow Tiong",
      "Dick Sigmund",
      "Andrew Beng Jin Teoh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.04132v2",
    "title": "Cross-Attention Transformer for Video Interpolation",
    "summary": "We propose TAIN (Transformers and Attention for video INterpolation), a\nresidual neural network for video interpolation, which aims to interpolate an\nintermediate frame given two consecutive image frames around it. We first\npresent a novel vision transformer module, named Cross Similarity (CS), to\nglobally aggregate input image features with similar appearance as those of the\npredicted interpolated frame. These CS features are then used to refine the\ninterpolated prediction. To account for occlusions in the CS features, we\npropose an Image Attention (IA) module to allow the network to focus on CS\nfeatures from one frame over those of the other. TAIN outperforms existing\nmethods that do not require flow estimation and performs comparably to\nflow-based methods while being computationally efficient in terms of inference\ntime on Vimeo90k, UCF101, and SNU-FILM benchmarks.",
    "published": "2022-07-08T21:38:54Z",
    "updated": "2022-12-02T02:48:37Z",
    "authors": [
      "Hannah Halin Kim",
      "Shuzhi Yu",
      "Shuai Yuan",
      "Carlo Tomasi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.05312v1",
    "title": "Outpainting by Queries",
    "summary": "Image outpainting, which is well studied with Convolution Neural Network\n(CNN) based framework, has recently drawn more attention in computer vision.\nHowever, CNNs rely on inherent inductive biases to achieve effective sample\nlearning, which may degrade the performance ceiling. In this paper, motivated\nby the flexible self-attention mechanism with minimal inductive biases in\ntransformer architecture, we reframe the generalised image outpainting problem\nas a patch-wise sequence-to-sequence autoregression problem, enabling\nquery-based image outpainting. Specifically, we propose a novel hybrid\nvision-transformer-based encoder-decoder framework, named \\textbf{Query}\n\\textbf{O}utpainting \\textbf{TR}ansformer (\\textbf{QueryOTR}), for\nextrapolating visual context all-side around a given image. Patch-wise mode's\nglobal modeling capacity allows us to extrapolate images from the attention\nmechanism's query standpoint. A novel Query Expansion Module (QEM) is designed\nto integrate information from the predicted queries based on the encoder's\noutput, hence accelerating the convergence of the pure transformer even with a\nrelatively small dataset. To further enhance connectivity between each patch,\nthe proposed Patch Smoothing Module (PSM) re-allocates and averages the\noverlapped regions, thus providing seamless predicted images. We experimentally\nshow that QueryOTR could generate visually appealing results smoothly and\nrealistically against the state-of-the-art image outpainting approaches.",
    "published": "2022-07-12T04:48:41Z",
    "updated": "2022-07-12T04:48:41Z",
    "authors": [
      "Kai Yao",
      "Penglei Gao",
      "Xi Yang",
      "Kaizhu Huang",
      "Jie Sun",
      "Rui Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.09312v1",
    "title": "Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for\n  COVID-19 Screening With Chest Radiography",
    "summary": "Building AI models with trustworthiness is important especially in regulated\nareas such as healthcare. In tackling COVID-19, previous work uses\nconvolutional neural networks as the backbone architecture, which has shown to\nbe prone to over-caution and overconfidence in making decisions, rendering them\nless trustworthy -- a crucial flaw in the context of medical imaging. In this\nstudy, we propose a feature learning approach using Vision Transformers, which\nuse an attention-based mechanism, and examine the representation learning\ncapability of Transformers as a new backbone architecture for medical imaging.\nThrough the task of classifying COVID-19 chest radiographs, we investigate into\nwhether generalization capabilities benefit solely from Vision Transformers'\narchitectural advances. Quantitative and qualitative evaluations are conducted\non the trustworthiness of the models, through the use of \"trust score\"\ncomputation and a visual explainability technique. We conclude that the\nattention-based feature learning approach is promising in building trustworthy\ndeep learning models for healthcare.",
    "published": "2022-07-19T14:55:42Z",
    "updated": "2022-07-19T14:55:42Z",
    "authors": [
      "Kai Ma",
      "Pengcheng Xi",
      "Karim Habashy",
      "Ashkan Ebadi",
      "StÃ©phane Tremblay",
      "Alexander Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.09725v2",
    "title": "OTPose: Occlusion-Aware Transformer for Pose Estimation in\n  Sparsely-Labeled Videos",
    "summary": "Although many approaches for multi-human pose estimation in videos have shown\nprofound results, they require densely annotated data which entails excessive\nman labor. Furthermore, there exists occlusion and motion blur that inevitably\nlead to poor estimation performance. To address these problems, we propose a\nmethod that leverages an attention mask for occluded joints and encodes\ntemporal dependency between frames using transformers. First, our framework\ncomposes different combinations of sparsely annotated frames that denote the\ntrack of the overall joint movement. We propose an occlusion attention mask\nfrom these combinations that enable encoding occlusion-aware heatmaps as a\nsemi-supervised task. Second, the proposed temporal encoder employs transformer\narchitecture to effectively aggregate the temporal relationship and\nkeypoint-wise attention from each time step and accurately refines the target\nframe's final pose estimation. We achieve state-of-the-art pose estimation\nresults for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the\nrobustness of our approach to occlusion and motion blur in sparsely annotated\nvideo data.",
    "published": "2022-07-20T08:06:06Z",
    "updated": "2022-07-28T01:43:19Z",
    "authors": [
      "Kyung-Min Jin",
      "Gun-Hee Lee",
      "Seong-Whan Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.04188v2",
    "title": "DepthFormer: Multimodal Positional Encodings and Cross-Input Attention\n  for Transformer-Based Segmentation Networks",
    "summary": "Most approaches for semantic segmentation use only information from color\ncameras to parse the scenes, yet recent advancements show that using depth data\nallows to further improve performances. In this work, we focus on\ntransformer-based deep learning architectures, that have achieved\nstate-of-the-art performances on the segmentation task, and we propose to\nemploy depth information by embedding it in the positional encoding.\nEffectively, we extend the network to multimodal data without adding any\nparameters and in a natural way that makes use of the strength of transformers'\nself-attention modules. We also investigate the idea of performing\ncross-modality operations inside the attention module, swapping the key inputs\nbetween the depth and color branches. Our approach consistently improves\nperformances on the Cityscapes benchmark.",
    "published": "2022-11-08T12:01:31Z",
    "updated": "2023-03-27T12:54:49Z",
    "authors": [
      "Francesco Barbato",
      "Giulia Rizzoli",
      "Pietro Zanuttigh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.04120v1",
    "title": "Denoising Self-attentive Sequential Recommendation",
    "summary": "Transformer-based sequential recommenders are very powerful for capturing\nboth short-term and long-term sequential item dependencies. This is mainly\nattributed to their unique self-attention networks to exploit pairwise\nitem-item interactions within the sequence. However, real-world item sequences\nare often noisy, which is particularly true for implicit feedback. For example,\na large portion of clicks do not align well with user preferences, and many\nproducts end up with negative reviews or being returned. As such, the current\nuser action only depends on a subset of items, not on the entire sequences.\nMany existing Transformer-based models use full attention distributions, which\ninevitably assign certain credits to irrelevant items. This may lead to\nsub-optimal performance if Transformers are not regularized properly.",
    "published": "2022-12-08T07:35:52Z",
    "updated": "2022-12-08T07:35:52Z",
    "authors": [
      "Huiyuan Chen",
      "Yusan Lin",
      "Menghai Pan",
      "Lan Wang",
      "Chin-Chia Michael Yeh",
      "Xiaoting Li",
      "Yan Zheng",
      "Fei Wang",
      "Hao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.01184v2",
    "title": "WeakTr: Exploring Plain Vision Transformer for Weakly-supervised\n  Semantic Segmentation",
    "summary": "This paper explores the properties of the plain Vision Transformer (ViT) for\nWeakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM)\nis of critical importance for understanding a classification network and\nlaunching WSSS. We observe that different attention heads of ViT focus on\ndifferent image areas. Thus a novel weight-based method is proposed to\nend-to-end estimate the importance of attention heads, while the self-attention\nmaps are adaptively fused for high-quality CAM results that tend to have more\ncomplete objects. Besides, we propose a ViT-based gradient clipping decoder for\nonline retraining with the CAM results to complete the WSSS task. We name this\nplain Transformer-based Weakly-supervised learning framework WeakTr. It\nachieves the state-of-the-art WSSS performance on standard benchmarks, i.e.,\n78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of\nCOCO 2014. Code is available at https://github.com/hustvl/WeakTr.",
    "published": "2023-04-03T17:54:10Z",
    "updated": "2023-04-27T03:03:50Z",
    "authors": [
      "Lianghui Zhu",
      "Yingyue Li",
      "Jiemin Fang",
      "Yan Liu",
      "Hao Xin",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.01625v3",
    "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
    "summary": "Since the proposal of transformers, these models have been limited to bounded\ninput lengths, because of their need to attend to every token in the input. In\nthis work, we propose Unlimiformer: a general approach that wraps any existing\npretrained encoder-decoder transformer, and offloads the cross-attention\ncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNN\ndistances are the attention dot-product scores. This kNN index can be kept on\neither the GPU or CPU memory and queried in sub-linear time; this way, we can\nindex practically unlimited input sequences, while every attention head in\nevery decoder layer retrieves its top-k keys, instead of attending to every\nkey. We evaluate Unlimiformer on several long-document and book-summarization\nbenchmarks, showing that it can process even 500k token-long inputs from the\nBookSum dataset, without any input truncation at test time. We demonstrate that\nUnlimiformer improves pretrained models such as BART and Longformer by\nextending them to unlimited inputs without additional learned weights and\nwithout modifying their code. We make our code and models publicly available at\nhttps://github.com/abertsch72/unlimiformer .",
    "published": "2023-05-02T17:35:08Z",
    "updated": "2023-10-30T19:44:47Z",
    "authors": [
      "Amanda Bertsch",
      "Uri Alon",
      "Graham Neubig",
      "Matthew R. Gormley"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.16592v1",
    "title": "A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music\n  Generation",
    "summary": "Recently, multi-instrument music generation has become a hot topic. Different\nfrom single-instrument generation, multi-instrument generation needs to\nconsider inter-track harmony besides intra-track coherence. This is usually\nachieved by composing note segments from different instruments into a signal\nsequence. This composition could be on different scales, such as note, bar, or\ntrack. Most existing work focuses on a particular scale, leading to a shortage\nin modeling music with diverse temporal and track dependencies.\n  This paper proposes a multi-scale attentive Transformer model to improve the\nquality of multi-instrument generation. We first employ multiple Transformer\ndecoders to learn multi-instrument representations of different scales and then\ndesign an attentive mechanism to fuse the multi-scale information. Experiments\nconducted on SOD and LMD datasets show that our model improves both\nquantitative and qualitative performance compared to models based on\nsingle-scale information. The source code and some generated samples can be\nfound at https://github.com/HaRry-qaq/MSAT.",
    "published": "2023-05-26T02:41:56Z",
    "updated": "2023-05-26T02:41:56Z",
    "authors": [
      "Xipin Wei",
      "Junhui Chen",
      "Zirui Zheng",
      "Li Guo",
      "Lantian Li",
      "Dong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.05012v1",
    "title": "Sequence-to-Sequence Model with Transformer-based Attention Mechanism\n  and Temporal Pooling for Non-Intrusive Load Monitoring",
    "summary": "This paper presents a novel Sequence-to-Sequence (Seq2Seq) model based on a\ntransformer-based attention mechanism and temporal pooling for Non-Intrusive\nLoad Monitoring (NILM) of smart buildings. The paper aims to improve the\naccuracy of NILM by using a deep learning-based method. The proposed method\nuses a Seq2Seq model with a transformer-based attention mechanism to capture\nthe long-term dependencies of NILM data. Additionally, temporal pooling is used\nto improve the model's accuracy by capturing both the steady-state and\ntransient behavior of appliances. The paper evaluates the proposed method on a\npublicly available dataset and compares the results with other state-of-the-art\nNILM techniques. The results demonstrate that the proposed method outperforms\nthe existing methods in terms of both accuracy and computational efficiency.",
    "published": "2023-06-08T08:04:56Z",
    "updated": "2023-06-08T08:04:56Z",
    "authors": [
      "Mohammad Irani Azad",
      "Roozbeh Rajabi",
      "Abouzar Estebsari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.05176v4",
    "title": "RRWKV: Capturing Long-range Dependencies in RWKV",
    "summary": "Owing to the impressive dot-product attention, the Transformers have been the\ndominant architectures in various natural language processing (NLP) tasks.\nRecently, the Receptance Weighted Key Value (RWKV) architecture follows a\nnon-transformer architecture to eliminate the drawbacks of dot-product\nattention, where memory and computational complexity exhibits quadratic scaling\nwith sequence length. Although RWKV has exploited a linearly tensor-product\nattention mechanism and achieved parallelized computations by deploying the\ntime-sequential mode, it fails to capture long-range dependencies because of\nits limitation on looking back at previous information, compared with full\ninformation obtained by direct interactions in the standard transformer.\nTherefore, the paper devises the Retrospected Receptance Weighted Key Value\n(RRWKV) architecture via incorporating the retrospecting ability into the RWKV\nto effectively absorb information, which maintains memory and computational\nefficiency as well.",
    "published": "2023-06-08T13:17:06Z",
    "updated": "2024-09-13T08:58:47Z",
    "authors": [
      "Leilei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.01496v1",
    "title": "Target-point Attention Transformer: A novel trajectory predict network\n  for end-to-end autonomous driving",
    "summary": "In the field of autonomous driving, there have been many excellent perception\nmodels for object detection, semantic segmentation, and other tasks, but how\ncan we effectively use the perception models for vehicle planning? Traditional\nautonomous vehicle trajectory prediction methods not only need to obey traffic\nrules to avoid collisions, but also need to follow the prescribed route to\nreach the destination. In this paper, we propose a Transformer-based trajectory\nprediction network for end-to-end autonomous driving without rules called\nTarget-point Attention Transformer network (TAT). We use the attention\nmechanism to realize the interaction between the predicted trajectory and the\nperception features as well as target-points. We demonstrate that our proposed\nmethod outperforms existing conditional imitation learning and GRU-based\nmethods, significantly reducing the occurrence of accidents and improving route\ncompletion. We evaluate our approach in complex closed loop driving scenarios\nin cities using the CARLA simulator and achieve state-of-the-art performance.",
    "published": "2023-08-03T01:44:40Z",
    "updated": "2023-08-03T01:44:40Z",
    "authors": [
      "Jingyu Du",
      "Yang Zhao",
      "Hong Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.12634v2",
    "title": "Towards Hierarchical Regional Transformer-based Multiple Instance\n  Learning",
    "summary": "The classification of gigapixel histopathology images with deep multiple\ninstance learning models has become a critical task in digital pathology and\nprecision medicine. In this work, we propose a Transformer-based multiple\ninstance learning approach that replaces the traditional learned attention\nmechanism with a regional, Vision Transformer inspired self-attention\nmechanism. We present a method that fuses regional patch information to derive\nslide-level predictions and show how this regional aggregation can be stacked\nto hierarchically process features on different distance levels. To increase\npredictive accuracy, especially for datasets with small, local morphological\nfeatures, we introduce a method to focus the image processing on high attention\nregions during inference. Our approach is able to significantly improve\nperformance over the baseline on two histopathology datasets and points towards\npromising directions for further research.",
    "published": "2023-08-24T08:19:15Z",
    "updated": "2023-11-20T10:06:03Z",
    "authors": [
      "Josef Cersovsky",
      "Sadegh Mohammadi",
      "Dagmar Kainmueller",
      "Johannes Hoehne"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.09306v1",
    "title": "Effective Image Tampering Localization via Enhanced Transformer and\n  Co-attention Fusion",
    "summary": "Powerful manipulation techniques have made digital image forgeries be easily\ncreated and widespread without leaving visual anomalies. The blind localization\nof tampered regions becomes quite significant for image forensics. In this\npaper, we propose an effective image tampering localization network (EITLNet)\nbased on a two-branch enhanced transformer encoder with attention-based feature\nfusion. Specifically, a feature enhancement module is designed to enhance the\nfeature representation ability of the transformer encoder. The features\nextracted from RGB and noise streams are fused effectively by the coordinate\nattention-based fusion module at multiple scales. Extensive experimental\nresults verify that the proposed scheme achieves the state-of-the-art\ngeneralization ability and robustness in various benchmark datasets. Code will\nbe public at https://github.com/multimediaFor/EITLNet.",
    "published": "2023-09-17T15:43:06Z",
    "updated": "2023-09-17T15:43:06Z",
    "authors": [
      "Kun Guo",
      "Haochen Zhu",
      "Gang Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.10765v1",
    "title": "MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily\n  Behavior Recognition in Group Settings",
    "summary": "Bodily behavioral language is an important social cue, and its automated\nanalysis helps in enhancing the understanding of artificial intelligence\nsystems. Furthermore, behavioral language cues are essential for active\nengagement in social agent-based user interactions. Despite the progress made\nin computer vision for tasks like head and body pose estimation, there is still\na need to explore the detection of finer behaviors such as gesturing, grooming,\nor fumbling. This paper proposes a multiview attention fusion method named\nMAGIC-TBR that combines features extracted from videos and their corresponding\nDiscrete Cosine Transform coefficients via a transformer-based approach. The\nexperiments are conducted on the BBSI dataset and the results demonstrate the\neffectiveness of the proposed feature fusion with multiview attention. The code\nis available at: https://github.com/surbhimadan92/MAGIC-TBR",
    "published": "2023-09-19T17:04:36Z",
    "updated": "2023-09-19T17:04:36Z",
    "authors": [
      "Surbhi Madan",
      "Rishabh Jain",
      "Gulshan Sharma",
      "Ramanathan Subramanian",
      "Abhinav Dhall"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.14758v1",
    "title": "Exploring RWKV for Memory Efficient and Low Latency Streaming ASR",
    "summary": "Recently, self-attention-based transformers and conformers have been\nintroduced as alternatives to RNNs for ASR acoustic modeling. Nevertheless, the\nfull-sequence attention mechanism is non-streamable and computationally\nexpensive, thus requiring modifications, such as chunking and caching, for\nefficient streaming ASR. In this paper, we propose to apply RWKV, a variant of\nlinear attention transformer, to streaming ASR. RWKV combines the superior\nperformance of transformers and the inference efficiency of RNNs, which is\nwell-suited for streaming ASR scenarios where the budget for latency and memory\nis restricted. Experiments on varying scales (100h - 10000h) demonstrate that\nRWKV-Transducer and RWKV-Boundary-Aware-Transducer achieve comparable to or\neven better accuracy compared with chunk conformer transducer, with minimal\nlatency and inference memory cost.",
    "published": "2023-09-26T08:41:24Z",
    "updated": "2023-09-26T08:41:24Z",
    "authors": [
      "Keyu An",
      "Shiliang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.04779v1",
    "title": "TransCC: Transformer Network for Coronary Artery CCTA Segmentation",
    "summary": "The accurate segmentation of Coronary Computed Tomography Angiography (CCTA)\nimages holds substantial clinical value for the early detection and treatment\nof Coronary Heart Disease (CHD). The Transformer, utilizing a self-attention\nmechanism, has demonstrated commendable performance in the realm of medical\nimage processing. However, challenges persist in coronary segmentation tasks\ndue to (1) the damage to target local structures caused by fixed-size image\npatch embedding, and (2) the critical role of both global and local features in\nmedical image segmentation tasks.To address these challenges, we propose a deep\nlearning framework, TransCC, that effectively amalgamates the Transformer and\nconvolutional neural networks for CCTA segmentation. Firstly, we introduce a\nFeature Interaction Extraction (FIE) module designed to capture the\ncharacteristics of image patches, thereby circumventing the loss of semantic\ninformation inherent in the original method. Secondly, we devise a Multilayer\nEnhanced Perceptron (MEP) to augment attention to local information within\nspatial dimensions, serving as a complement to the self-attention mechanism.\nExperimental results indicate that TransCC outperforms existing methods in\nsegmentation performance, boasting an average Dice coefficient of 0.730 and an\naverage Intersection over Union (IoU) of 0.582. These results underscore the\neffectiveness of TransCC in CCTA image segmentation.",
    "published": "2023-10-07T11:37:00Z",
    "updated": "2023-10-07T11:37:00Z",
    "authors": [
      "Chenchu Xu",
      "Meng Li",
      "Xue Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.03459v1",
    "title": "F3-Pruning: A Training-Free and Generalized Pruning Strategy towards\n  Faster and Finer Text-to-Video Synthesis",
    "summary": "Recently Text-to-Video (T2V) synthesis has undergone a breakthrough by\ntraining transformers or diffusion models on large-scale datasets.\nNevertheless, inferring such large models incurs huge costs.Previous inference\nacceleration works either require costly retraining or are model-specific.To\naddress this issue, instead of retraining we explore the inference process of\ntwo mainstream T2V models using transformers and diffusion models.The\nexploration reveals the redundancy in temporal attention modules of both\nmodels, which are commonly utilized to establish temporal relations among\nframes.Consequently, we propose a training-free and generalized pruning\nstrategy called F3-Pruning to prune redundant temporal attention\nweights.Specifically, when aggregate temporal attention values are ranked below\na certain ratio, corresponding weights will be pruned.Extensive experiments on\nthree datasets using a classic transformer-based model CogVideo and a typical\ndiffusion-based model Tune-A-Video verify the effectiveness of F3-Pruning in\ninference acceleration, quality assurance and broad applicability.",
    "published": "2023-12-06T12:34:47Z",
    "updated": "2023-12-06T12:34:47Z",
    "authors": [
      "Sitong Su",
      "Jianzhi Liu",
      "Lianli Gao",
      "Jingkuan Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.16237v1",
    "title": "Proximal Gradient Descent Unfolding Dense-spatial Spectral-attention\n  Transformer for Compressive Spectral Imaging",
    "summary": "The Coded Aperture Snapshot Spectral Compressive Imaging (CASSI) system\nmodulates three-dimensional hyperspectral images into two-dimensional\ncompressed images in a single exposure. Subsequently, three-dimensional\nhyperspectral images (HSI) can be reconstructed from the two-dimensional\ncompressed measurements using reconstruction algorithms. Among these methods,\ndeep unfolding techniques have demonstrated excellent performance, with\nRDLUF-MixS^2 achieving the best reconstruction results. However, RDLUF-MixS^2\nrequires extensive training time, taking approximately 14 days to train\nRDLUF-MixS^2-9stg on a single RTX 3090 GPU, making it computationally\nexpensive. Furthermore, RDLUF-MixS^2 performs poorly on real data, resulting in\nsignificant artifacts in the reconstructed images. In this study, we introduce\nthe Dense-spatial Spectral-attention Transformer (DST) into the Proximal\nGradient Descent Unfolding Framework (PGDUF), creating a novel approach called\nProximal Gradient Descent Unfolding Dense-spatial Spectral-attention\nTransformer (PGDUDST). Compared to RDLUF-MixS^2, PGDUDST not only surpasses the\nnetwork reconstruction performance limit of RDLUF-MixS^2 but also achieves\nfaster convergence. PGDUDST requires only 58% of the training time of\nRDLUF-MixS^2-9stg to achieve comparable reconstruction results. Additionally,\nPGDUDST significantly alleviates the artifact issues caused by RDLUF-MixS^2 in\nreal experimental data, demonstrating superior performance and producing\nclearer reconstructed images.",
    "published": "2023-12-25T05:51:10Z",
    "updated": "2023-12-25T05:51:10Z",
    "authors": [
      "Ziyan Chen",
      "Jing Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.01258v2",
    "title": "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field\n  Dynamics on the Attention Landscape",
    "summary": "Large language models based on the Transformer architecture have demonstrated\nimpressive capabilities to learn in context. However, existing theoretical\nstudies on how this phenomenon arises are limited to the dynamics of a single\nlayer of attention trained on linear regression tasks. In this paper, we study\nthe optimization of a Transformer consisting of a fully connected layer\nfollowed by a linear attention layer. The MLP acts as a common nonlinear\nrepresentation or feature map, greatly enhancing the power of in-context\nlearning. We prove in the mean-field and two-timescale limit that the\ninfinite-dimensional loss landscape for the distribution of parameters, while\nhighly nonconvex, becomes quite benign. We also analyze the second-order\nstability of mean-field dynamics and show that Wasserstein gradient flow almost\nalways avoids saddle points. Furthermore, we establish novel methods for\nobtaining concrete improvement rates both away from and near critical points.\nThis represents the first saddle point analysis of mean-field dynamics in\ngeneral and the techniques are of independent interest.",
    "published": "2024-02-02T09:29:40Z",
    "updated": "2024-06-02T06:31:43Z",
    "authors": [
      "Juno Kim",
      "Taiji Suzuki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.03923v6",
    "title": "Return-Aligned Decision Transformer",
    "summary": "Traditional approaches in offline reinforcement learning aim to learn the\noptimal policy that maximizes the cumulative reward, also known as return. It\nis increasingly important to adjust the performance of AI agents to meet human\nrequirements, for example, in applications like video games and education\ntools. Decision Transformer (DT) optimizes a policy that generates actions\nconditioned on the target return through supervised learning and includes a\nmechanism to control the agent's performance using the target return. However,\nthe action generation is hardly influenced by the target return because DT's\nself-attention allocates scarce attention scores to the return tokens. In this\npaper, we propose Return-Aligned Decision Transformer (RADT), designed to more\neffectively align the actual return with the target return. RADT leverages\nfeatures extracted by paying attention solely to the return, enabling action\ngeneration to consistently depend on the target return. Extensive experiments\nshow that RADT significantly reduces the discrepancies between the actual\nreturn and the target return compared to DT-based methods. Our code is\navailable at https://github.com/CyberAgentAILab/radt",
    "published": "2024-02-06T11:46:47Z",
    "updated": "2025-06-19T09:33:21Z",
    "authors": [
      "Tsunehiko Tanaka",
      "Kenshi Abe",
      "Kaito Ariu",
      "Tetsuro Morimura",
      "Edgar Simo-Serra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.11301v2",
    "title": "ReViT: Enhancing Vision Transformers Feature Diversity with Attention\n  Residual Connections",
    "summary": "Vision Transformer (ViT) self-attention mechanism is characterized by feature\ncollapse in deeper layers, resulting in the vanishing of low-level visual\nfeatures. However, such features can be helpful to accurately represent and\nidentify elements within an image and increase the accuracy and robustness of\nvision-based recognition systems. Following this rationale, we propose a novel\nresidual attention learning method for improving ViT-based architectures,\nincreasing their visual feature diversity and model robustness. In this way,\nthe proposed network can capture and preserve significant low-level features,\nproviding more details about the elements within the scene being analyzed. The\neffectiveness and robustness of the presented method are evaluated on five\nimage classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100,\nOxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances.\nAdditionally, experiments on the COCO2017 dataset show that the devised\napproach discovers and incorporates semantic and spatial relationships for\nobject detection and instance segmentation when implemented into spatial-aware\ntransformer models.",
    "published": "2024-02-17T14:44:10Z",
    "updated": "2024-08-02T20:28:10Z",
    "authors": [
      "Anxhelo Diko",
      "Danilo Avola",
      "Marco Cascio",
      "Luigi Cinque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.15398v1",
    "title": "TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow\n  Attention for Commuting Flow Prediction",
    "summary": "Understanding the link between urban planning and commuting flows is crucial\nfor guiding urban development and policymaking. This research, bridging\ncomputer science and urban studies, addresses the challenge of integrating\nthese fields with their distinct focuses. Traditional urban studies methods,\nlike the gravity and radiation models, often underperform in complex scenarios\ndue to their limited handling of multiple variables and reliance on overly\nsimplistic and unrealistic assumptions, such as spatial isotropy. While deep\nlearning models offer improved accuracy, their black-box nature poses a\ntrade-off between performance and explainability -- both vital for analyzing\ncomplex societal phenomena like commuting flows. To address this, we introduce\nTransFlower, an explainable, transformer-based model employing flow-to-flow\nattention to predict urban commuting patterns. It features a geospatial encoder\nwith an anisotropy-aware relative location encoder for nuanced flow\nrepresentation. Following this, the transformer-based flow predictor enhances\nthis by leveraging attention mechanisms to efficiently capture flow\ninteractions. Our model outperforms existing methods by up to 30.8% Common Part\nof Commuters, offering insights into mobility dynamics crucial for urban\nplanning and policy decisions.",
    "published": "2024-02-23T16:00:04Z",
    "updated": "2024-02-23T16:00:04Z",
    "authors": [
      "Yan Luo",
      "Zhuoyue Wan",
      "Yuzhong Chen",
      "Gengchen Mai",
      "Fu-lai Chung",
      "Kent Larson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.08801v2",
    "title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited\n  Context Length",
    "summary": "The quadratic complexity and weak length extrapolation of Transformers limits\ntheir ability to scale to long sequences, and while sub-quadratic solutions\nlike linear attention and state space models exist, they empirically\nunderperform Transformers in pretraining efficiency and downstream task\naccuracy. We introduce Megalodon, a neural architecture for efficient sequence\nmodeling with unlimited context length. Megalodon inherits the architecture of\nMega (exponential moving average with gated attention), and further introduces\nmultiple technical components to improve its capability and stability,\nincluding complex exponential moving average (CEMA), timestep normalization\nlayer, normalized attention mechanism and pre-norm with two-hop residual\nconfiguration. In a controlled head-to-head comparison with Llama2, Megalodon\nachieves better efficiency than Transformer in the scale of 7 billion\nparameters and 2 trillion training tokens. Megalodon reaches a training loss of\n1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code:\nhttps://github.com/XuezheMax/megalodon",
    "published": "2024-04-12T20:28:14Z",
    "updated": "2024-04-16T07:27:58Z",
    "authors": [
      "Xuezhe Ma",
      "Xiaomeng Yang",
      "Wenhan Xiong",
      "Beidi Chen",
      "Lili Yu",
      "Hao Zhang",
      "Jonathan May",
      "Luke Zettlemoyer",
      "Omer Levy",
      "Chunting Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.13434v1",
    "title": "Nested-TNT: Hierarchical Vision Transformers with Multi-Scale Feature\n  Processing",
    "summary": "Transformer has been applied in the field of computer vision due to its\nexcellent performance in natural language processing, surpassing traditional\nconvolutional neural networks and achieving new state-of-the-art. ViT divides\nan image into several local patches, known as \"visual sentences\". However, the\ninformation contained in the image is vast and complex, and focusing only on\nthe features at the \"visual sentence\" level is not enough. The features between\nlocal patches should also be taken into consideration. In order to achieve\nfurther improvement, the TNT model is proposed, whose algorithm further divides\nthe image into smaller patches, namely \"visual words,\" achieving more accurate\nresults. The core of Transformer is the Multi-Head Attention mechanism, and\ntraditional attention mechanisms ignore interactions across different attention\nheads. In order to reduce redundancy and improve utilization, we introduce the\nnested algorithm and apply the Nested-TNT to image classification tasks. The\nexperiment confirms that the proposed model has achieved better classification\nperformance over ViT and TNT, exceeding 2.25%, 1.1% on dataset CIFAR10 and\n2.78%, 0.25% on dataset FLOWERS102 respectively.",
    "published": "2024-04-20T17:56:14Z",
    "updated": "2024-04-20T17:56:14Z",
    "authors": [
      "Yuang Liu",
      "Zhiheng Qiu",
      "Xiaokai Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.14677v3",
    "title": "Streamlined jet tagging network assisted by jet prong structure",
    "summary": "Attention-based transformer models have become increasingly prevalent in\ncollider analysis, offering enhanced performance for tasks such as jet tagging.\nHowever, they are computationally intensive and require substantial data for\ntraining. In this paper, we introduce a new jet classification network using an\nMLP mixer, where two subsequent MLP operations serve to transform particle and\nfeature tokens over the jet constituents. The transformed particles are\ncombined with subjet information using multi-head cross-attention so that the\nnetwork is invariant under the permutation of the jet constituents.\n  We utilize two clustering algorithms to identify subjets: the standard\nsequential recombination algorithms with fixed radius parameters and a new\nIRC-safe, density-based algorithm of dynamic radii based on HDBSCAN. The\nproposed network demonstrates comparable classification performance to\nstate-of-the-art models while boosting computational efficiency drastically.\nFinally, we evaluate the network performance using various interpretable\nmethods, including centred kernel alignment and attention maps, to highlight\nnetwork efficacy in collider analysis tasks.",
    "published": "2024-04-23T02:05:18Z",
    "updated": "2024-06-03T07:28:05Z",
    "authors": [
      "A. Hammad",
      "Mihoko M. Nojiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.00187v1",
    "title": "Towards End-to-End Semi-Supervised Table Detection with Semantic Aligned\n  Matching Transformer",
    "summary": "Table detection within document images is a crucial task in document\nprocessing, involving the identification and localization of tables. Recent\nstrides in deep learning have substantially improved the accuracy of this task,\nbut it still heavily relies on large labeled datasets for effective training.\nSeveral semi-supervised approaches have emerged to overcome this challenge,\noften employing CNN-based detectors with anchor proposals and post-processing\ntechniques like non-maximal suppression (NMS). However, recent advancements in\nthe field have shifted the focus towards transformer-based techniques,\neliminating the need for NMS and emphasizing object queries and attention\nmechanisms. Previous research has focused on two key areas to improve\ntransformer-based detectors: refining the quality of object queries and\noptimizing attention mechanisms. However, increasing object queries can\nintroduce redundancy, while adjustments to the attention mechanism can increase\ncomplexity. To address these challenges, we introduce a semi-supervised\napproach employing SAM-DETR, a novel approach for precise alignment between\nobject queries and target features. Our approach demonstrates remarkable\nreductions in false positives and substantial enhancements in table detection\nperformance, particularly in complex documents characterized by diverse table\nstructures. This work provides more efficient and accurate table detection in\nsemi-supervised settings.",
    "published": "2024-04-30T20:25:57Z",
    "updated": "2024-04-30T20:25:57Z",
    "authors": [
      "Tahira Shehzadi",
      "Shalini Sarode",
      "Didier Stricker",
      "Muhammad Zeshan Afzal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.04807v2",
    "title": "Transformer Architecture for NetsDB",
    "summary": "Transformers models have become the backbone of the current state-of-the-art\nmodels in language, vision, and multimodal domains. These models, at their\ncore, utilize multi-head self-attention to selectively aggregate context,\ngenerating dynamic contextual embeddings and modeling long-range dependencies\nfor a clear contextual understanding. Lixi et al. \\cite{zhou2022serving}\nproposed a method to use relational databases for deploying large-scale deep\nlearning models and created an open-source implementation called NetsDB for the\nsame. We build upon the previous work of these authors by creating an\nend-to-end implementation of the Encoder part of the transformer for model\nserving in NetsDB. Specifically, we construct a two-block encoder that includes\nMulti-Head Attention and its accompanying self-attention mechanism, Layer-Norm,\nDropout, FeedForward Layers, and the necessary residual connections. We load\nout weights from our model for distributed processing, deployment, and\nefficient inferencing. To prove the efficacy of our implementation, we conduct\na comprehensive performance analysis by comparing it with existing\nimplementations in PyTorch, Tensorflow, Flax, and MxNet across key metrics such\nas inference time and model size.",
    "published": "2024-05-08T04:38:36Z",
    "updated": "2024-05-09T12:02:22Z",
    "authors": [
      "Subodh Kamble",
      "Kunal Sunil Kasodekar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.19006v4",
    "title": "Snakes and Ladders: Two Steps Up for VideoMamba",
    "summary": "Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to vision tasks, including those in video\nanalysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. Differently sized VideoMambaPro models surpass VideoMamba by 1.6-2.8%\nand 1.1-1.9% top-1 on Kinetics-400 and Something-Something V2, respectively.\nEven without extensive pre-training, our models present an increasingly\nattractive and efficient alternative to current transformer models. Moreover,\nour two solutions are orthogonal to recent advances in Vision Mamba models, and\nare likely to provide further improvements in future models.",
    "published": "2024-06-27T08:45:31Z",
    "updated": "2024-11-13T10:56:14Z",
    "authors": [
      "Hui Lu",
      "Albert Ali Salah",
      "Ronald Poppe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.08330v1",
    "title": "HDT: Hierarchical Document Transformer",
    "summary": "In this paper, we propose the Hierarchical Document Transformer (HDT), a\nnovel sparse Transformer architecture tailored for structured hierarchical\ndocuments. Such documents are extremely important in numerous domains,\nincluding science, law or medicine. However, most existing solutions are\ninefficient and fail to make use of the structure inherent to documents. HDT\nexploits document structure by introducing auxiliary anchor tokens and\nredesigning the attention mechanism into a sparse multi-level hierarchy. This\napproach facilitates information exchange between tokens at different levels\nwhile maintaining sparsity, thereby enhancing computational and memory\nefficiency while exploiting the document structure as an inductive bias. We\naddress the technical challenge of implementing HDT's sample-dependent\nhierarchical attention pattern by developing a novel sparse attention kernel\nthat considers the hierarchical structure of documents. As demonstrated by our\nexperiments, utilizing structural information present in documents leads to\nfaster convergence, higher sample efficiency and better performance on\ndownstream tasks.",
    "published": "2024-07-11T09:28:04Z",
    "updated": "2024-07-11T09:28:04Z",
    "authors": [
      "Haoyu He",
      "Markus Flicke",
      "Jan Buchmann",
      "Iryna Gurevych",
      "Andreas Geiger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.13920v1",
    "title": "DuoFormer: Leveraging Hierarchical Visual Representations by Local and\n  Global Attention",
    "summary": "We here propose a novel hierarchical transformer model that adeptly\nintegrates the feature extraction capabilities of Convolutional Neural Networks\n(CNNs) with the advanced representational potential of Vision Transformers\n(ViTs). Addressing the lack of inductive biases and dependence on extensive\ntraining datasets in ViTs, our model employs a CNN backbone to generate\nhierarchical visual representations. These representations are then adapted for\ntransformer input through an innovative patch tokenization. We also introduce a\n'scale attention' mechanism that captures cross-scale dependencies,\ncomplementing patch attention to enhance spatial understanding and preserve\nglobal perception. Our approach significantly outperforms baseline models on\nsmall and medium-sized medical datasets, demonstrating its efficiency and\ngeneralizability. The components are designed as plug-and-play for different\nCNN architectures and can be adapted for multiple applications. The code is\navailable at https://github.com/xiaoyatang/DuoFormer.git.",
    "published": "2024-07-18T22:15:35Z",
    "updated": "2024-07-18T22:15:35Z",
    "authors": [
      "Xiaoya Tang",
      "Bodong Zhang",
      "Beatrice S. Knudsen",
      "Tolga Tasdizen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.01147v1",
    "title": "Actra: Optimized Transformer Architecture for Vision-Language-Action\n  Models in Robot Learning",
    "summary": "Vision-language-action models have gained significant attention for their\nability to model trajectories in robot learning. However, most existing models\nrely on Transformer models with vanilla causal attention, which we find\nsuboptimal for processing segmented multi-modal sequences. Additionally, the\nautoregressive generation approach falls short in generating multi-dimensional\nactions. In this paper, we introduce Actra, an optimized Transformer\narchitecture featuring trajectory attention and learnable action queries,\ndesigned for effective encoding and decoding of segmented\nvision-language-action trajectories in robot imitation learning. Furthermore,\nwe devise a multi-modal contrastive learning objective to explicitly align\ndifferent modalities, complementing the primary behavior cloning objective.\nThrough extensive experiments conducted across various environments, Actra\nexhibits substantial performance improvement when compared to state-of-the-art\nmodels in terms of generalizability, dexterity, and precision.",
    "published": "2024-08-02T09:55:56Z",
    "updated": "2024-08-02T09:55:56Z",
    "authors": [
      "Yueen Ma",
      "Dafeng Chi",
      "Shiguang Wu",
      "Yuecheng Liu",
      "Yuzheng Zhuang",
      "Jianye Hao",
      "Irwin King"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.09940v1",
    "title": "ML-CrAIST: Multi-scale Low-high Frequency Information-based Cross black\n  Attention with Image Super-resolving Transformer",
    "summary": "Recently, transformers have captured significant interest in the area of\nsingle-image super-resolution tasks, demonstrating substantial gains in\nperformance. Current models heavily depend on the network's extensive ability\nto extract high-level semantic details from images while overlooking the\neffective utilization of multi-scale image details and intermediate information\nwithin the network. Furthermore, it has been observed that high-frequency areas\nin images present significant complexity for super-resolution compared to\nlow-frequency areas. This work proposes a transformer-based super-resolution\narchitecture called ML-CrAIST that addresses this gap by utilizing low-high\nfrequency information in multiple scales. Unlike most of the previous work\n(either spatial or channel), we operate spatial and channel self-attention,\nwhich concurrently model pixel interaction from both spatial and channel\ndimensions, exploiting the inherent correlations across spatial and channel\naxis. Further, we devise a cross-attention block for super-resolution, which\nexplores the correlations between low and high-frequency information.\nQuantitative and qualitative assessments indicate that our proposed ML-CrAIST\nsurpasses state-of-the-art super-resolution methods (e.g., 0.15 dB gain\n@Manga109 $\\times$4). Code is available on:\nhttps://github.com/Alik033/ML-CrAIST.",
    "published": "2024-08-19T12:23:15Z",
    "updated": "2024-08-19T12:23:15Z",
    "authors": [
      "Alik Pramanick",
      "Utsav Bheda",
      "Arijit Sur"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.14906v1",
    "title": "Kriformer: A Novel Spatiotemporal Kriging Approach Based on Graph\n  Transformers",
    "summary": "Accurately estimating data in sensor-less areas is crucial for understanding\nsystem dynamics, such as traffic state estimation and environmental monitoring.\nThis study addresses challenges posed by sparse sensor deployment and\nunreliable data by framing the problem as a spatiotemporal kriging task and\nproposing a novel graph transformer model, Kriformer. This model estimates data\nat locations without sensors by mining spatial and temporal correlations, even\nwith limited resources. Kriformer utilizes transformer architecture to enhance\nthe model's perceptual range and solve edge information aggregation challenges,\ncapturing spatiotemporal information effectively. A carefully constructed\npositional encoding module embeds the spatiotemporal features of nodes, while a\nsophisticated spatiotemporal attention mechanism enhances estimation accuracy.\nThe multi-head spatial interaction attention module captures subtle spatial\nrelationships between observed and unobserved locations. During training, a\nrandom masking strategy prompts the model to learn with partial information\nloss, allowing the spatiotemporal embedding and multi-head attention mechanisms\nto synergistically capture correlations among locations. Experimental results\nshow that Kriformer excels in representation learning for unobserved locations,\nvalidated on two real-world traffic speed datasets, demonstrating its\neffectiveness in spatiotemporal kriging tasks.",
    "published": "2024-09-23T11:01:18Z",
    "updated": "2024-09-23T11:01:18Z",
    "authors": [
      "Renbin Pan",
      "Feng Xiao",
      "Hegui Zhang",
      "Minyu Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.19599v4",
    "title": "DATransNet: Dynamic Attention Transformer Network for Infrared Small\n  Target Detection",
    "summary": "Infrared small target detection (ISTD) is widely used in civilian and\nmilitary applications. However, ISTD encounters several challenges, including\nthe tendency for small and dim targets to be obscured by complex backgrounds.\nTo address this issue, we propose the Dynamic Attention Transformer Network\n(DATransNet), which aims to extract and preserve detailed information vital for\nsmall targets. DATransNet employs the Dynamic Attention Transformer (DATrans),\nsimulating central difference convolutions (CDC) to extract gradient features.\nFurthermore, we propose a global feature extraction module (GFEM) that offers a\ncomprehensive perspective to prevent the network from focusing solely on\ndetails while neglecting the global information. We compare the network with\nstate-of-the-art (SOTA) approaches and demonstrate that our method performs\neffectively. Our source code is available at\nhttps://github.com/greekinRoma/DATransNet.",
    "published": "2024-09-29T07:32:14Z",
    "updated": "2025-03-01T17:31:31Z",
    "authors": [
      "Chen Hu",
      "Yian Huang",
      "Kexuan Li",
      "Luping Zhang",
      "Chang Long",
      "Yiming Zhu",
      "Tian Pu",
      "Zhenming Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.01531v2",
    "title": "TiVaT: A Transformer with a Single Unified Mechanism for Capturing\n  Asynchronous Dependencies in Multivariate Time Series Forecasting",
    "summary": "Multivariate time series (MTS) forecasting is vital across various domains\nbut remains challenging due to the need to simultaneously model temporal and\ninter-variate dependencies. Existing channel-dependent models, where\nTransformer-based models dominate, process these dependencies separately,\nlimiting their capacity to capture complex interactions such as lead-lag\ndynamics. To address this issue, we propose TiVaT (Time-variate Transformer), a\nnovel architecture incorporating a single unified module, a Joint-Axis (JA)\nattention module, that concurrently processes temporal and variate modeling.\nThe JA attention module dynamically selects relevant features to particularly\ncapture asynchronous interactions. In addition, we introduce distance-aware\ntime-variate sampling in the JA attention, a novel mechanism that extracts\nsignificant patterns through a learned 2D embedding space while reducing noise.\nExtensive experiments demonstrate TiVaT's overall performance across diverse\ndatasets, particularly excelling in scenarios with intricate asynchronous\ndependencies.",
    "published": "2024-10-02T13:24:24Z",
    "updated": "2025-01-31T02:32:39Z",
    "authors": [
      "Junwoo Ha",
      "Hyukjae Kwon",
      "Sungsoo Kim",
      "Kisu Lee",
      "Seungjae Park",
      "Ha Young Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.11189v1",
    "title": "Rethinking Graph Transformer Architecture Design for Node Classification",
    "summary": "Graph Transformer (GT), as a special type of Graph Neural Networks (GNNs),\nutilizes multi-head attention to facilitate high-order message passing.\nHowever, this also imposes several limitations in node classification\napplications: 1) nodes are susceptible to global noise; 2) self-attention\ncomputation cannot scale well to large graphs. In this work, we conduct\nextensive observational experiments to explore the adaptability of the GT\narchitecture in node classification tasks and draw several conclusions: the\ncurrent multi-head self-attention module in GT can be completely replaceable,\nwhile the feed-forward neural network module proves to be valuable. Based on\nthis, we decouple the propagation (P) and transformation (T) of GNNs and\nexplore a powerful GT architecture, named GNNFormer, which is based on the P/T\ncombination message passing and adapted for node classification in both\nhomophilous and heterophilous scenarios. Extensive experiments on 12 benchmark\ndatasets demonstrate that our proposed GT architecture can effectively adapt to\nnode classification tasks without being affected by global noise and\ncomputational efficiency limitations.",
    "published": "2024-10-15T02:08:16Z",
    "updated": "2024-10-15T02:08:16Z",
    "authors": [
      "Jiajun Zhou",
      "Xuanze Chen",
      "Chenxuan Xie",
      "Yu Shanqing",
      "Qi Xuan",
      "Xiaoniu Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.13314v1",
    "title": "Precipitation Nowcasting Using Diffusion Transformer with Causal\n  Attention",
    "summary": "Short-term precipitation forecasting remains challenging due to the\ndifficulty in capturing long-term spatiotemporal dependencies. Current deep\nlearning methods fall short in establishing effective dependencies between\nconditions and forecast results, while also lacking interpretability. To\naddress this issue, we propose a Precipitation Nowcasting Using Diffusion\nTransformer with Causal Attention model. Our model leverages Transformer and\ncombines causal attention mechanisms to establish spatiotemporal queries\nbetween conditional information (causes) and forecast results (results). This\ndesign enables the model to effectively capture long-term dependencies,\nallowing forecast results to maintain strong causal relationships with input\nconditions over a wide range of time and space. We explore four variants of\nspatiotemporal information interactions for DTCA, demonstrating that global\nspatiotemporal labeling interactions yield the best performance. In addition,\nwe introduce a Channel-To-Batch shift operation to further enhance the model's\nability to represent complex rainfall dynamics. We conducted experiments on two\ndatasets. Compared to state-of-the-art U-Net-based methods, our approach\nimproved the CSI (Critical Success Index) for predicting heavy precipitation by\napproximately 15% and 8% respectively, achieving state-of-the-art performance.",
    "published": "2024-10-17T08:10:41Z",
    "updated": "2024-10-17T08:10:41Z",
    "authors": [
      "ChaoRong Li",
      "XuDong Ling",
      "YiLan Xue",
      "Wenjie Luo",
      "LiHong Zhu",
      "FengQing Qin",
      "Yaodong Zhou",
      "Yuanyuan Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.01419v2",
    "title": "PSformer: Parameter-efficient Transformer with Segment Attention for\n  Time Series Forecasting",
    "summary": "Time series forecasting remains a critical challenge across various domains,\noften complicated by high-dimensional data and long-term dependencies. This\npaper presents a novel transformer architecture for time series forecasting,\nincorporating two key innovations: parameter sharing (PS) and Spatial-Temporal\nSegment Attention (SegAtt). We also define the time series segment as the\nconcatenation of sequence patches from the same positions across different\nvariables. The proposed model, PSformer, reduces the number of training\nparameters through the parameter sharing mechanism, thereby improving model\nefficiency and scalability. The introduction of SegAtt could enhance the\ncapability of capturing local spatio-temporal dependencies by computing\nattention over the segments, and improve global representation by integrating\ninformation across segments. The combination of parameter sharing and SegAtt\nsignificantly improves the forecasting performance. Extensive experiments on\nbenchmark datasets demonstrate that PSformer outperforms popular baselines and\nother transformer-based approaches in terms of accuracy and scalability,\nestablishing itself as an accurate and scalable tool for time series\nforecasting.",
    "published": "2024-11-03T03:04:00Z",
    "updated": "2025-02-11T09:50:04Z",
    "authors": [
      "Yanlong Wang",
      "Jian Xu",
      "Fei Ma",
      "Shao-Lun Huang",
      "Danny Dongning Sun",
      "Xiao-Ping Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.10231v2",
    "title": "A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image\n  Super-Resolution with Transformers and TaylorShift",
    "summary": "Transformer-based architectures have recently advanced the image\nreconstruction quality of super-resolution (SR) models. Yet, their scalability\nremains limited by quadratic attention costs and coarse patch embeddings that\nweaken pixel-level fidelity. We propose TaylorIR, a plug-and-play framework\nthat enforces 1x1 patch embeddings for true pixel-wise reasoning and replaces\nconventional self-attention with TaylorShift, a Taylor-series-based attention\nmechanism enabling full token interactions with near-linear complexity. Across\nmultiple SR benchmarks, TaylorIR delivers state-of-the-art performance while\nreducing memory consumption by up to 60%, effectively bridging the gap between\nfine-grained detail restoration and efficient transformer scaling.",
    "published": "2024-11-15T14:43:58Z",
    "updated": "2025-11-01T12:57:49Z",
    "authors": [
      "Sanath Budakegowdanadoddi Nagaraju",
      "Brian Bernhard Moser",
      "Tobias Christian Nauen",
      "Stanislav Frolov",
      "Federico Raue",
      "Andreas Dengel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.05540v1",
    "title": "Towards 3D Acceleration for low-power Mixture-of-Experts and Multi-Head\n  Attention Spiking Transformers",
    "summary": "Spiking Neural Networks(SNNs) provide a brain-inspired and event-driven\nmechanism that is believed to be critical to unlock energy-efficient deep\nlearning. The mixture-of-experts approach mirrors the parallel distributed\nprocessing of nervous systems, introducing conditional computation policies and\nexpanding model capacity without scaling up the number of computational\noperations. Additionally, spiking mixture-of-experts self-attention mechanisms\nenhance representation capacity, effectively capturing diverse patterns of\nentities and dependencies between visual or linguistic tokens. However, there\nis currently a lack of hardware support for highly parallel distributed\nprocessing needed by spiking transformers, which embody a brain-inspired\ncomputation. This paper introduces the first 3D hardware architecture and\ndesign methodology for Mixture-of-Experts and Multi-Head Attention spiking\ntransformers. By leveraging 3D integration with memory-on-logic and\nlogic-on-logic stacking, we explore such brain-inspired accelerators with\nspatially stackable circuitry, demonstrating significant optimization of energy\nefficiency and latency compared to conventional 2D CMOS integration.",
    "published": "2024-12-07T05:15:05Z",
    "updated": "2024-12-07T05:15:05Z",
    "authors": [
      "Boxun Xu",
      "Junyoung Hwang",
      "Pruek Vanna-iampikul",
      "Yuxuan Yin",
      "Sung Kyu Lim",
      "Peng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.10540v1",
    "title": "Higher Order Transformers: Enhancing Stock Movement Prediction On\n  Multimodal Time-Series Data",
    "summary": "In this paper, we tackle the challenge of predicting stock movements in\nfinancial markets by introducing Higher Order Transformers, a novel\narchitecture designed for processing multivariate time-series data. We extend\nthe self-attention mechanism and the transformer architecture to a higher\norder, effectively capturing complex market dynamics across time and variables.\nTo manage computational complexity, we propose a low-rank approximation of the\npotentially large attention tensor using tensor decomposition and employ kernel\nattention, reducing complexity to linear with respect to the data size.\nAdditionally, we present an encoder-decoder model that integrates technical and\nfundamental analysis, utilizing multimodal signals from historical prices and\nrelated tweets. Our experiments on the Stocknet dataset demonstrate the\neffectiveness of our method, highlighting its potential for enhancing stock\nmovement prediction in financial markets.",
    "published": "2024-12-13T20:26:35Z",
    "updated": "2024-12-13T20:26:35Z",
    "authors": [
      "Soroush Omranpour",
      "Guillaume Rabusseau",
      "Reihaneh Rabbany"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.19829v1",
    "title": "GFormer: Accelerating Large Language Models with Optimized Transformers\n  on Gaudi Processors",
    "summary": "Heterogeneous hardware like Gaudi processor has been developed to enhance\ncomputations, especially matrix operations for Transformer-based large language\nmodels (LLMs) for generative AI tasks. However, our analysis indicates that\nTransformers are not fully optimized on such emerging hardware, primarily due\nto inadequate optimizations in non-matrix computational kernels like Softmax\nand in heterogeneous resource utilization, particularly when processing long\nsequences. To address these issues, we propose an integrated approach (called\nGFormer) that merges sparse and linear attention mechanisms. GFormer aims to\nmaximize the computational capabilities of the Gaudi processor's Matrix\nMultiplication Engine (MME) and Tensor Processing Cores (TPC) without\ncompromising model quality. GFormer includes a windowed self-attention kernel\nand an efficient outer product kernel for causal linear attention, aiming to\noptimize LLM inference on Gaudi processors. Evaluation shows that GFormer\nsignificantly improves efficiency and model performance across various tasks on\nthe Gaudi processor and outperforms state-of-the-art GPUs.",
    "published": "2024-12-19T14:50:11Z",
    "updated": "2024-12-19T14:50:11Z",
    "authors": [
      "Chengming Zhang",
      "Xinheng Ding",
      "Baixi Sun",
      "Xiaodong Yu",
      "Weijian Zheng",
      "Zhen Xie",
      "Dingwen Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.05209v1",
    "title": "MHAFF: Multi-Head Attention Feature Fusion of CNN and Transformer for\n  Cattle Identification",
    "summary": "Convolutional Neural Networks (CNNs) have drawn researchers' attention to\nidentifying cattle using muzzle images. However, CNNs often fail to capture\nlong-range dependencies within the complex patterns of the muzzle. The\ntransformers handle these challenges. This inspired us to fuse the strengths of\nCNNs and transformers in muzzle-based cattle identification. Addition and\nconcatenation have been the most commonly used techniques for feature fusion.\nHowever, addition fails to preserve discriminative information, while\nconcatenation results in an increase in dimensionality. Both methods are simple\noperations and cannot discover the relationships or interactions between fusing\nfeatures. This research aims to overcome the issues faced by addition and\nconcatenation. This research introduces a novel approach called Multi-Head\nAttention Feature Fusion (MHAFF) for the first time in cattle identification.\nMHAFF captures relations between the different types of fusing features while\npreserving their originality. The experiments show that MHAFF outperformed\naddition and concatenation techniques and the existing cattle identification\nmethods in accuracy on two publicly available cattle datasets. MHAFF\ndemonstrates excellent performance and quickly converges to achieve optimum\naccuracy of 99.88% and 99.52% in two cattle datasets simultaneously.",
    "published": "2025-01-09T13:00:01Z",
    "updated": "2025-01-09T13:00:01Z",
    "authors": [
      "Rabin Dulal",
      "Lihong Zheng",
      "Muhammad Ashad Kabir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.09166v1",
    "title": "Attention is All You Need Until You Need Retention",
    "summary": "This work introduces a novel Retention Layer mechanism for Transformer based\narchitectures, addressing their inherent lack of intrinsic retention\ncapabilities. Unlike human cognition, which can encode and dynamically recall\nsymbolic templates, Generative Pretrained Transformers rely solely on fixed\npretrained weights and ephemeral context windows, limiting their adaptability.\nThe proposed Retention Layer incorporates a persistent memory module capable of\nreal time data population, dynamic recall, and guided output generation. This\nenhancement allows models to store, update, and reuse observed patterns across\nsessions, enabling incremental learning and bridging the gap between static\npretraining and dynamic, context sensitive adaptation. The Retention Layer\ndesign parallels social learning processes, encompassing attention, retention,\nreproduction, and motivation stages. Technically, it integrates a memory\nattention mechanism and episodic buffers to manage memory scalability, mitigate\noverfitting, and ensure efficient recall. Applications span adaptive personal\nassistants, real time fraud detection, autonomous robotics, content moderation,\nand healthcare diagnostics. In each domain, the retention mechanism enables\nsystems to learn incrementally, personalize outputs, and respond to evolving\nreal world challenges effectively. By emulating key aspects of human learning,\nthis retention enhanced architecture fosters a more fluid and responsive AI\nparadigm, paving the way for dynamic, session aware models that extend the\ncapabilities of traditional Transformers into domains requiring continual\nadaptation.",
    "published": "2025-01-15T21:33:53Z",
    "updated": "2025-01-15T21:33:53Z",
    "authors": [
      "M. Murat Yaslioglu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.15570v1",
    "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
    "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
    "published": "2025-01-26T15:56:56Z",
    "updated": "2025-01-26T15:56:56Z",
    "authors": [
      "Lin Yueyu",
      "Li Zhiyuan",
      "Peter Yue",
      "Liu Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05800v1",
    "title": "MicroViT: A Vision Transformer with Low Complexity Self Attention for\n  Edge Device",
    "summary": "The Vision Transformer (ViT) has demonstrated state-of-the-art performance in\nvarious computer vision tasks, but its high computational demands make it\nimpractical for edge devices with limited resources. This paper presents\nMicroViT, a lightweight Vision Transformer architecture optimized for edge\ndevices by significantly reducing computational complexity while maintaining\nhigh accuracy. The core of MicroViT is the Efficient Single Head Attention\n(ESHA) mechanism, which utilizes group convolution to reduce feature redundancy\nand processes only a fraction of the channels, thus lowering the burden of the\nself-attention mechanism. MicroViT is designed using a multi-stage MetaFormer\narchitecture, stacking multiple MicroViT encoders to enhance efficiency and\nperformance. Comprehensive experiments on the ImageNet-1K and COCO datasets\ndemonstrate that MicroViT achieves competitive accuracy while significantly\nimproving 3.6 faster inference speed and reducing energy consumption with 40%\nhigher efficiency than the MobileViT series, making it suitable for deployment\nin resource-constrained environments such as mobile and edge devices.",
    "published": "2025-02-09T08:04:39Z",
    "updated": "2025-02-09T08:04:39Z",
    "authors": [
      "Novendra Setyawan",
      "Chi-Chia Sun",
      "Mao-Hsiu Hsu",
      "Wen-Kai Kuo",
      "Jun-Wei Hsieh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18297v3",
    "title": "Image-to-Text for Medical Reports Using Adaptive Co-Attention and\n  Triple-LSTM Module",
    "summary": "Medical report generation requires specialized expertise that general large\nmodels often fail to accurately capture. Moreover, the inherent repetition and\nsimilarity in medical data make it difficult for models to extract meaningful\nfeatures, resulting in a tendency to overfit. So in this paper, we propose a\nmultimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learning\nmodel that combines transformer architectures with a Multi-LSTM network. Its\nCo-Attention module synergistically links a vision transformer with a text\ntransformer to better differentiate medical images with similarities, augmented\nby an adaptive weight operator to catch and amplify image labels with minor\nsimilarities. Furthermore, its Triple-LSTM module refines generated sentences\nusing targeted image objects. Extensive evaluations over three public datasets\nhave demonstrated that CA-TriNet outperforms state-of-the-art models in terms\nof comprehensive ability, even pre-trained large language models on some\nmetrics.",
    "published": "2025-03-24T03:02:11Z",
    "updated": "2025-08-15T14:30:05Z",
    "authors": [
      "Yishen Liu",
      "Shengda Luo",
      "Zishao Zhong",
      "Hudan Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.14366v2",
    "title": "Empirical Evaluation of Knowledge Distillation from Transformers to\n  Subquadratic Language Models",
    "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs), in which a smaller student model is trained to mimic a\nlarger teacher model. Typically, both the teacher and student models are\nTransformer-based architectures, leveraging softmax attention for sequence\nmodeling. However, the quadratic complexity of self-attention during inference\nremains a significant bottleneck, motivating the exploration of subquadratic\nalternatives such as structured state-space models (SSMs), linear attention,\nand recurrent architectures. In this work, we systematically evaluate the\ntransferability of knowledge distillation from a Transformer teacher model to\neight subquadratic student architectures. Our study investigates which\nsubquadratic model can most effectively approximate the teacher model's learned\nrepresentations through knowledge distillation, and how different architectural\ndesign choices influence the training dynamics. We further investigate the\nimpact of initialization strategies, such as matrix mixing and query-key-value\n(QKV) copying, on the adaptation process. Our empirical results on multiple NLP\nbenchmarks provide insights into the trade-offs between efficiency and\nperformance, highlighting key factors for successful knowledge transfer to\nsubquadratic architectures.",
    "published": "2025-04-19T17:49:52Z",
    "updated": "2025-05-24T10:10:16Z",
    "authors": [
      "Patrick Haller",
      "Jonas Golde",
      "Alan Akbik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.15634v1",
    "title": "Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar\n  Protein Folding Model with Attention-based layers",
    "summary": "Transformer-based architectures have recently propelled advances in sequence\nmodeling across domains, but their application to the hydrophobic-hydrophilic\n(H-P) model for protein folding remains relatively unexplored. In this work, we\nadapt a Deep Q-Network (DQN) integrated with attention mechanisms\n(Transformers) to address the 3D H-P protein folding problem. Our system\nformulates folding decisions as a self-avoiding walk in a reinforced\nenvironment, and employs a specialized reward function based on favorable\nhydrophobic interactions. To improve performance, the method incorporates\nvalidity check including symmetry-breaking constraints, dueling and double\nQ-learning, and prioritized replay to focus learning on critical transitions.\nExperimental evaluations on standard benchmark sequences demonstrate that our\napproach achieves several known best solutions for shorter sequences, and\nobtains near-optimal results for longer chains. This study underscores the\npromise of attention-based reinforcement learning for protein folding, and\ncreated a prototype of Transformer-based Q-network structure for 3-dimensional\nlattice models.",
    "published": "2025-04-22T06:53:36Z",
    "updated": "2025-04-22T06:53:36Z",
    "authors": [
      "Peizheng Liu",
      "Hitoshi Iba"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.18818v1",
    "title": "Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution",
    "summary": "Methods based on implicit neural representation have demonstrated remarkable\ncapabilities in arbitrary-scale super-resolution (ASSR) tasks, but they neglect\nthe potential value of the frequency domain, leading to sub-optimal\nperformance. We proposes a novel network called Frequency-Integrated\nTransformer (FIT) to incorporate and utilize frequency information to enhance\nASSR performance. FIT employs Frequency Incorporation Module (FIM) to introduce\nfrequency information in a lossless manner and Frequency Utilization\nSelf-Attention module (FUSAM) to efficiently leverage frequency information by\nexploiting spatial-frequency interrelationship and global nature of frequency.\nFIM enriches detail characterization by incorporating frequency information\nthrough a combination of Fast Fourier Transform (FFT) with real-imaginary\nmapping. In FUSAM, Interaction Implicit Self-Attention (IISA) achieves\ncross-domain information synergy by interacting spatial and frequency\ninformation in subspace, while Frequency Correlation Self-attention (FCSA)\ncaptures the global context by computing correlation in frequency. Experimental\nresults demonstrate FIT yields superior performance compared to existing\nmethods across multiple benchmark datasets. Visual feature map proves the\nsuperiority of FIM in enriching detail characterization. Frequency error map\nvalidates IISA productively improve the frequency fidelity. Local attribution\nmap validates FCSA effectively captures global context.",
    "published": "2025-04-26T06:12:49Z",
    "updated": "2025-04-26T06:12:49Z",
    "authors": [
      "Xufei Wang",
      "Fei Ge",
      "Jinchen Zhu",
      "Mingjian Zhang",
      "Qi Wu",
      "Jifeng Ren Shizhuang Weng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.00033v1",
    "title": "From Attention to Atoms: Spectral Dictionary Learning for Fast,\n  Interpretable Language Models",
    "summary": "We propose a novel spectral generative modeling framework for natural\nlanguage processing that jointly learns a global time varying Fourier\ndictionary and per token mixing coefficients, replacing the ubiquitous self\nattention mechanism in transformer architectures. By enforcing reconstruction\nlosses in both the time domain (embedding reconstruction) and the frequency\ndomain (via Short Time Fourier Transform magnitude matching) alongside a\nstandard language modeling objective, and fitting a Gaussian Mixture Model\n(GMM) prior over the learned mixing vectors, our approach achieves competitive\nperplexity and generation quality on standard benchmarks such as WikiText2 and\nPenn Treebank. In contrast to the quadratic computation complexity of self\nattention, our method operates with linear complexity, delivering substantial\nefficiency gains. We demonstrate that spectral dictionary models can achieve\ncompetitive performance compared to transformer baselines while significantly\nreducing inference latency and memory footprint, offering a compelling\nalternative for scalable language modeling.",
    "published": "2025-04-29T13:24:42Z",
    "updated": "2025-04-29T13:24:42Z",
    "authors": [
      "Andrew Kiruluta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22170v1",
    "title": "Attention-Enhanced Prompt Decision Transformers for UAV-Assisted\n  Communications with AoI",
    "summary": "Decision Transformer (DT) has recently demonstrated strong generalizability\nin dynamic resource allocation within unmanned aerial vehicle (UAV) networks,\ncompared to conventional deep reinforcement learning (DRL). However, its\nperformance is hindered due to zero-padding for varying state dimensions,\ninability to manage long-term energy constraint, and challenges in acquiring\nexpert samples for few-shot fine-tuning in new scenarios. To overcome these\nlimitations, we propose an attention-enhanced prompt Decision Transformer\n(APDT) framework to optimize trajectory planning and user scheduling, aiming to\nminimize the average age of information (AoI) under long-term energy constraint\nin UAV-assisted Internet of Things (IoT) networks. Specifically, we enhance the\nconvenional DT framework by incorporating an attention mechanism to accommodate\nvarying numbers of terrestrial users, introducing a prompt mechanism based on\nshort trajectory demonstrations for rapid adaptation to new scenarios, and\ndesigning a token-assisted method to address the UAV's long-term energy\nconstraint. The APDT framework is first pre-trained on offline datasets and\nthen efficiently generalized to new scenarios. Simulations demonstrate that\nAPDT achieves twice faster in terms of convergence rate and reduces average AoI\nby $8\\%$ compared to conventional DT.",
    "published": "2025-05-28T09:41:10Z",
    "updated": "2025-05-28T09:41:10Z",
    "authors": [
      "Chi Lu",
      "Yiyang Ni",
      "Zhe Wang",
      "Xiaoli Shi",
      "Jun Li",
      "Shi Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24244v1",
    "title": "Mamba Knockout for Unraveling Factual Information Flow",
    "summary": "This paper investigates the flow of factual information in Mamba State-Space\nModel (SSM)-based language models. We rely on theoretical and empirical\nconnections to Transformer-based architectures and their attention mechanisms.\nExploiting this relationship, we adapt attentional interpretability techniques\noriginally developed for Transformers--specifically, the Attention Knockout\nmethodology--to both Mamba-1 and Mamba-2. Using them we trace how information\nis transmitted and localized across tokens and layers, revealing patterns of\nsubject-token information emergence and layer-wise dynamics. Notably, some\nphenomena vary between mamba models and Transformer based models, while others\nappear universally across all models inspected--hinting that these may be\ninherent to LLMs in general. By further leveraging Mamba's structured\nfactorization, we disentangle how distinct \"features\" either enable\ntoken-to-token information exchange or enrich individual tokens, thus offering\na unified lens to understand Mamba internal operations.",
    "published": "2025-05-30T06:08:36Z",
    "updated": "2025-05-30T06:08:36Z",
    "authors": [
      "Nir Endy",
      "Idan Daniel Grosbard",
      "Yuval Ran-Milo",
      "Yonatan Slutzky",
      "Itay Tshuva",
      "Raja Giryes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07405v1",
    "title": "RiemannFormer: A Framework for Attention in Curved Spaces",
    "summary": "This research endeavors to offer insights into unlocking the further\npotential of transformer-based architectures. One of the primary motivations is\nto offer a geometric interpretation for the attention mechanism in\ntransformers. In our framework, the attention mainly involves metric tensors,\ntangent spaces, inner product, and how they relate to each other. These\nquantities and structures at discrete positions are intricately interconnected\nvia the parallel transport of tangent vectors. To make the learning process\nmore efficient, we reduce the number of parameters through ingenious predefined\nconfigurations. Moreover, we introduce an explicit mechanism to highlight a\nneighborhood by attenuating the remote values, given that transformers\ninherently neglect local inductive bias. Experimental results demonstrate that\nour modules deliver significant performance improvements relative to the\nbaseline. More evaluation experiments on visual and large language models will\nbe launched successively.",
    "published": "2025-06-09T03:56:18Z",
    "updated": "2025-06-09T03:56:18Z",
    "authors": [
      "Zhongping Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.11305v1",
    "title": "Don't Pay Attention",
    "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies.",
    "published": "2025-06-12T21:11:06Z",
    "updated": "2025-06-12T21:11:06Z",
    "authors": [
      "Mohammad Hammoud",
      "Devang Acharya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.13688v2",
    "title": "What Happens During the Loss Plateau? Understanding Abrupt Learning in\n  Transformers",
    "summary": "Training Transformers on algorithmic tasks frequently demonstrates an\nintriguing abrupt learning phenomenon: an extended performance plateau followed\nby a sudden, sharp improvement. This work investigates the underlying\nmechanisms for such dynamics, primarily in shallow Transformers. We reveal that\nduring the plateau, the model often develops an interpretable partial solution\nwhile simultaneously exhibiting a strong repetition bias in their outputs. This\noutput degeneracy is accompanied by internal representation collapse, where\nhidden states across different tokens become nearly parallel. We further\nidentify the slow learning of optimal attention maps as a key bottleneck.\nHidden progress in attention configuration during the plateau precedes the\neventual rapid convergence, and directly intervening on attention significantly\nalters plateau duration and the severity of repetition bias and\nrepresentational collapse. We validate that these identified\nphenomena-repetition bias and representation collapse-are not artifacts of toy\nsetups but also manifest in the early pre-training stage of large language\nmodels like Pythia and OLMo.",
    "published": "2025-06-16T16:51:18Z",
    "updated": "2025-10-22T18:00:02Z",
    "authors": [
      "Pulkit Gopalani",
      "Wei Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.19834v1",
    "title": "A standard transformer and attention with linear biases for molecular\n  conformer generation",
    "summary": "Sampling low-energy molecular conformations, spatial arrangements of atoms in\na molecule, is a critical task for many different calculations performed in the\ndrug discovery and optimization process. Numerous specialized equivariant\nnetworks have been designed to generate molecular conformations from 2D\nmolecular graphs. Recently, non-equivariant transformer models have emerged as\na viable alternative due to their capability to scale to improve\ngeneralization. However, the concern has been that non-equivariant models\nrequire a large model size to compensate the lack of equivariant bias. In this\npaper, we demonstrate that a well-chosen positional encoding effectively\naddresses these size limitations. A standard transformer model incorporating\nrelative positional encoding for molecular graphs when scaled to 25 million\nparameters surpasses the current state-of-the-art non-equivariant base model\nwith 64 million parameters on the GEOM-DRUGS benchmark. We implemented relative\npositional encoding as a negative attention bias that linearly increases with\nthe shortest path distances between graph nodes at varying slopes for different\nattention heads, similar to ALiBi, a widely adopted relative positional\nencoding technique in the NLP domain. This architecture has the potential to\nserve as a foundation for a novel class of generative models for molecular\nconformations.",
    "published": "2025-06-24T17:50:49Z",
    "updated": "2025-06-24T17:50:49Z",
    "authors": [
      "Viatcheslav Gurev",
      "Timothy Rumbell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01973v2",
    "title": "Integration of Wavelet Transform Convolution and Channel Attention with\n  LSTM for Stock Price Prediction based Portfolio Allocation",
    "summary": "Portfolio allocation via stock price prediction is inherently difficult due\nto the notoriously low signal-to-noise ratio of stock time series. This paper\nproposes a method by integrating wavelet transform convolution and channel\nattention with LSTM to implement stock price prediction based portfolio\nallocation. Stock time series data first are processed by wavelet transform\nconvolution to reduce the noise. Processed features are then reconstructed by\nchannel attention. LSTM is utilized to predict the stock price using the final\nprocessed features. We construct a portfolio consists of four stocks with\ntrading signals predicted by model. Experiments are conducted by evaluating the\nreturn, Sharpe ratio and max drawdown performance. The results indicate that\nour method achieves robust performance even during period of post-pandemic\ndownward market.",
    "published": "2025-06-23T08:50:43Z",
    "updated": "2025-07-04T05:33:54Z",
    "authors": [
      "Junjie Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.15189v1",
    "title": "CHADET: Cross-Hierarchical-Attention for Depth-Completion Using\n  Unsupervised Lightweight Transformer",
    "summary": "Depth information which specifies the distance between objects and current\nposition of the robot is essential for many robot tasks such as navigation.\nRecently, researchers have proposed depth completion frameworks to provide\ndense depth maps that offer comprehensive information about the surrounding\nenvironment. However, existing methods show significant trade-offs between\ncomputational efficiency and accuracy during inference. The substantial memory\nand computational requirements make them unsuitable for real-time applications,\nhighlighting the need to improve the completeness and accuracy of depth\ninformation while improving processing speed to enhance robot performance in\nvarious tasks. To address these challenges, in this paper, we propose\nCHADET(cross-hierarchical-attention depth-completion transformer), a\nlightweight depth-completion network that can generate accurate dense depth\nmaps from RGB images and sparse depth points. For each pair, its feature is\nextracted from the depthwise blocks and passed to the equally lightweight\ntransformer-based decoder. In the decoder, we utilize the novel\ncross-hierarchical-attention module that refines the image features from the\ndepth information. Our approach improves the quality and reduces memory usage\nof the depth map prediction, as validated in both KITTI, NYUv2, and VOID\ndatasets.",
    "published": "2025-07-21T02:22:39Z",
    "updated": "2025-07-21T02:22:39Z",
    "authors": [
      "Kevin Christiansen Marsim",
      "Jinwoo Jeon",
      "Yeeun Kim",
      "Myeongwoo Jeong",
      "Hyun Myung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18464v2",
    "title": "Vectorized Attention with Learnable Encoding for Quantum Transformer",
    "summary": "Vectorized quantum block encoding provides a way to embed classical data into\nHilbert space, offering a pathway for quantum models, such as Quantum\nTransformers (QT), that replace classical self-attention with quantum circuit\nsimulations to operate more efficiently. Current QTs rely on deep parameterized\nquantum circuits (PQCs), rendering them vulnerable to QPU noise, and thus\nhindering their practical performance. In this paper, we propose the Vectorized\nQuantum Transformer (VQT), a model that supports ideal masked attention matrix\ncomputation through quantum approximation simulation and efficient training via\nvectorized nonlinear quantum encoder, yielding shot-efficient and gradient-free\nquantum circuit simulation (QCS) and reduced classical sampling overhead. In\naddition, we demonstrate an accuracy comparison for IBM and IonQ in quantum\ncircuit simulation and competitive results in benchmarking natural language\nprocessing tasks on IBM state-of-the-art and high-fidelity Kingston QPU. Our\nnoise intermediate-scale quantum friendly VQT approach unlocks a novel\narchitecture for end-to-end machine learning in quantum computing.",
    "published": "2025-08-25T20:33:14Z",
    "updated": "2025-09-03T22:25:32Z",
    "authors": [
      "Ziqing Guo",
      "Ziwen Pan",
      "Alex Khan",
      "Jan Balewski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.07603v1",
    "title": "Transformer-Based Approach to Optimal Sensor Placement for Structural\n  Health Monitoring of Probe Cards",
    "summary": "This paper presents an innovative Transformer-based deep learning strategy\nfor optimizing the placement of sensors aiming at structural health monitoring\nof semiconductor probe cards. Failures in probe cards, including substrate\ncracks and loosened screws, would critically affect semiconductor manufacturing\nyield and reliability. Some failure modes could be detected by equipping a\nprobe card with adequate sensors. Frequency response functions from simulated\nfailure scenarios are adopted within a finite element model of a probe card. A\ncomprehensive dataset, enriched by physics-informed scenario expansion and\nphysics-aware statistical data augmentation, is exploited to train a hybrid\nConvolutional Neural Network and Transformer model. The model achieves high\naccuracy (99.83%) in classifying the probe card health states (baseline, loose\nscrew, crack) and an excellent crack detection recall (99.73%). Model\nrobustness is confirmed through a rigorous framework of 3 repetitions of\n10-fold stratified cross-validation. The attention mechanism also pinpoints\ncritical sensor locations: an analysis of the attention weights offers\nactionable insights for designing efficient, cost-effective monitoring systems\nby optimizing sensor configurations. This research highlights the capability of\nattention-based deep learning to advance proactive maintenance, enhancing\noperational reliability and yield in semiconductor manufacturing.",
    "published": "2025-09-09T11:21:49Z",
    "updated": "2025-09-09T11:21:49Z",
    "authors": [
      "Mehdi Bejani",
      "Marco Mauri",
      "Daniele Acconcia",
      "Simone Todaro",
      "Stefano Mariani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19648v2",
    "title": "S$^2$Transformer: Scalable Structured Transformers for Global Station\n  Weather Forecasting",
    "summary": "Global Station Weather Forecasting (GSWF) is a key meteorological research\narea, critical to energy, aviation, and agriculture. Existing time series\nforecasting methods often ignore or unidirectionally model spatial correlation\nwhen conducting large-scale global station forecasting. This contradicts the\nintrinsic nature underlying observations of the global weather system, limiting\nforecast performance. To address this, we propose a novel Spatial Structured\nAttention Block in this paper. It partitions the spatial graph into a set of\nsubgraphs and instantiates Intra-subgraph Attention to learn local spatial\ncorrelation within each subgraph, and aggregates nodes into subgraph\nrepresentations for message passing among the subgraphs via Inter-subgraph\nAttention -- considering both spatial proximity and global correlation.\nBuilding on this block, we develop a multiscale spatiotemporal forecasting\nmodel S$^2$Transformer by progressively expanding subgraph scales. The\nresulting model is both scalable and able to produce structured spatial\ncorrelation, and meanwhile, it is easy to implement. The experimental results\nshow that it can achieve performance improvements up to 16.8% over time series\nforecasting baselines at low running costs.",
    "published": "2025-09-10T05:33:28Z",
    "updated": "2025-09-25T03:12:08Z",
    "authors": [
      "Hongyi Chen",
      "Xiucheng Li",
      "Xinyang Chen",
      "Yun Cheng",
      "Jing Li",
      "Kehai Chen",
      "Liqiang Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23145v1",
    "title": "TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of\n  Experts",
    "summary": "Transformer-based architectures dominate time series modeling by enabling\nglobal attention over all timestamps, yet their rigid 'one-size-fits-all'\ncontext aggregation fails to address two critical challenges in real-world\ndata: (1) inherent lag effects, where the relevance of historical timestamps to\na query varies dynamically; (2) anomalous segments, which introduce noisy\nsignals that degrade forecasting accuracy. To resolve these problems, we\npropose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism\nthat reimagines key-value (K-V) pairs as local experts (each specialized in a\ndistinct temporal context) and performs adaptive expert selection for each\nquery via localized filtering of irrelevant timestamps. Complementing this\nlocal adaptation, a shared global expert preserves the Transformer's strength\nin capturing long-range dependencies. We then replace the vanilla attention\nmechanism in popular time-series Transformer frameworks (i.e., PatchTST and\nTimer) with TMOE, without extra structural modifications, yielding our specific\nversion TimeExpert and general version TimeExpert-G. Extensive experiments on\nseven real-world long-term forecasting benchmarks demonstrate that TimeExpert\nand TimeExpert-G outperform state-of-the-art methods. Code is available at\nhttps://github.com/xwmaxwma/TimeExpert.",
    "published": "2025-09-27T06:22:09Z",
    "updated": "2025-09-27T06:22:09Z",
    "authors": [
      "Xiaowen Ma",
      "Shuning Ge",
      "Fan Yang",
      "Xiangyu Li",
      "Yun Chen",
      "Mengting Ma",
      "Wei Zhang",
      "Zhipeng Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00258v1",
    "title": "Delayed Attention Training Improves Length Generalization in\n  Transformer--RNN Hybrids",
    "summary": "We study length generalization in sequence models on a composite problem\ninvolving both state tracking and associative recall. Prior work finds that\nrecurrent networks handle state tracking well but struggle with recall, whereas\nTransformers excel at recall yet fail to extend state-tracking capabilities to\nlonger sequences. Motivated by the complementary strengths of these\narchitectures, we construct hybrid models integrating recurrent and\nattention-based components, and train them on the combined task to evaluate\nwhether both capabilities can be preserved. Our results reveal that, in such\nhybrids, the Transformer component tends to exploit shortcut solutions, leading\nto poor length generalization. We identify this shortcut reliance as a key\nobstacle and propose a simple yet effective training strategy -- delaying the\ntraining of the attention layers -- that mitigates this effect and\nsignificantly improves length generalization performance. Our experiments show\nthat this approach enables hybrid models to achieve near-perfect accuracy\n($>90\\%$) on hybrid sequences three times longer than those used during\ntraining.",
    "published": "2025-09-30T20:31:14Z",
    "updated": "2025-09-30T20:31:14Z",
    "authors": [
      "Buu Phan",
      "Reza Ebrahimi",
      "Sanjay Haresh",
      "Roland Memisevic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01585v1",
    "title": "ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and\n  Long-Context Reasoning",
    "summary": "While Transformer architectures have demonstrated impressive scalability\nacross domains, they continue to face challenges in long-context reasoning,\ncomputational efficiency, and structural generalization - largely due to rigid\nlayer stacking, dense attention, and reliance on positional encodings. We\npresent ReSSFormer, a Recursive Sparse Structured Transformer that integrates\nthree complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for\niterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)\nfor efficient and focused context selection, and Self-Organizing Encoder\nStructure (SOES) for position-free structure induction. ReSSFormer replaces\nconventional depth stacking with recurrent inference, substitutes full\nattention with token- and expert-level sparsity, and models latent token\ntopology directly from content. Across language modeling, multi-hop QA, and\nstructure-sensitive tasks, ReSSFormer consistently outperforms strong baselines\nunder comparable FLOPs and parameter budgets, highlighting its scalability,\nefficiency, and structural flexibility.",
    "published": "2025-10-02T02:05:30Z",
    "updated": "2025-10-02T02:05:30Z",
    "authors": [
      "Haochen You",
      "Baojing Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03272v2",
    "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence\n  Modeling",
    "summary": "We propose PDE-Transformer, a novel sequence modeling paradigm that casts the\nforward pass of a Transformer as the numerical discretization of a continuous\nreaction-diffusion system derived from a variational energy functional. In our\nframework, token embeddings evolve under a partial differential equation whose\nnonlocal integral term models self-attention, local reaction term models\nfeed-forward layers, diffusion term encodes positional smoothing, and a\nstability control term corresponds to layer normalization. From this unifying\nperspective, we design an Adaptive PDE Diffusion Layer-an efficient, learnable\nfinite-difference stencil that enforces local smoothness in feature space with\nlinear time complexity and complements self-attention's global routing. Through\na systematic theoretical analysis based on four pillars:stability, diffusion\ngeometry, multi-scale dynamics, and component coupling, we derive principled\nguidelines for integrating the PDE layer at seven candidate points in the\nTransformer. Empirically, on the Long Range Arena benchmark, placing the layer\nimmediately after embedding yields a 4.1 pp average accuracy gain over a strong\nbaseline, and an adaptive multi-scale variant delivers further improvements.\nOur work thus offers a principled, lightweight mechanism to bolster long-range\ndependency modeling by harmonizing continuous PDE smoothing with discrete\nself-attention.",
    "published": "2025-09-27T08:58:47Z",
    "updated": "2025-10-12T14:32:47Z",
    "authors": [
      "Yukun Zhang",
      "Xueqing Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.19131v1",
    "title": "Training-Free Spectral Fingerprints of Voice Processing in Transformers",
    "summary": "Different transformer architectures implement identical linguistic\ncomputations via distinct connectivity patterns, yielding model imprinted\n``computational fingerprints'' detectable through spectral analysis. Using\ngraph signal processing on attention induced token graphs, we track changes in\nalgebraic connectivity (Fiedler value, $\\Delta\\lambda_2$) under voice\nalternation across 20 languages and three model families, with a prespecified\nearly window (layers 2--5). Our analysis uncovers clear architectural\nsignatures: Phi-3-Mini shows a dramatic English specific early layer disruption\n($\\overline{\\Delta\\lambda_2}_{[2,5]}\\!\\approx\\!-0.446$) while effects in 19\nother languages are minimal, consistent with public documentation that\npositions the model primarily for English use. Qwen2.5-7B displays small,\ndistributed shifts that are largest for morphologically rich languages, and\nLLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures\ncorrelate strongly with behavioral differences (Phi-3: $r=-0.976$) and are\nmodulated by targeted attention head ablations, linking the effect to early\nattention structure and confirming functional relevance. Taken together, the\nfindings are consistent with the view that training emphasis can leave\ndetectable computational imprints: specialized processing strategies that\nmanifest as measurable connectivity patterns during syntactic transformations.\nBeyond voice alternation, the framework differentiates reasoning modes,\nindicating utility as a simple, training free diagnostic for revealing\narchitectural biases and supporting model reliability analysis.",
    "published": "2025-10-21T23:33:43Z",
    "updated": "2025-10-21T23:33:43Z",
    "authors": [
      "Valentin NoÃ«l"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1205.5314v1",
    "title": "A general spline representation for nonparametric and semiparametric\n  density estimates using diffeomorphisms",
    "summary": "A theorem of McCann shows that for any two absolutely continuous probability\nmeasures on R^d there exists a monotone transformation sending one probability\nmeasure to the other. A consequence of this theorem, relevant to statistics, is\nthat density estimation can be recast in terms of transformations. In\nparticular, one can fix any absolutely continuous probability measure, call it\nP, and then reparameterize the whole class of absolutely continuous probability\nmeasures as monotone transformations from P. In this paper we utilize this\nreparameterization of densities, as monotone transformations from some P, to\nconstruct semiparametric and nonparametric density estimates. We focus our\nattention on classes of transformations, developed in the image processing and\ncomputational anatomy literature, which are smooth, invertible and which have\nattractive computational properties. The techniques developed for this class of\ntransformations allow us to show that a penalized maximum likelihood estimate\n(PMLE) of a smooth transformation from P exists and has a finite dimensional\ncharacterization, similar to those results found in the spline literature.\nThese results are derived utilizing an Euler-Lagrange characterization of the\nPMLE which also establishes a surprising connection to a generalization of\nStein's lemma for characterizing the normal distribution.",
    "published": "2012-05-24T00:52:33Z",
    "updated": "2012-05-24T00:52:33Z",
    "authors": [
      "Ethan Anderes",
      "Marc Coram"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1704.06135v1",
    "title": "Modelling social-ecological transformations: an adaptive network\n  proposal",
    "summary": "Transformations to create more sustainable social-ecological systems are\nurgently needed. Structural change is a feature of transformations of\nsocial-ecological systems that is of critical importance but is little\nunderstood. Here, we propose a framework for conceptualising and modelling\nsustainability transformations based on adaptive networks. Adaptive networks\nfocus attention on the interplay between the structure of a social-ecological\nsystem and the dynamics of individual entities. Adaptive networks could\nprogress transformations research by: 1) focusing research on changes in\nstructure; 2) providing a conceptual framework that clarifies the temporal\ndynamics of social-ecological transformations compared to the most commonly\nused heuristic in resilience studies, the ball-and-cup diagram; 3) providing\nquantitative modelling tools in an area of study dominated by qualitative\nmethods. We illustrate the potential application of adaptive networks to\nsocial-ecological transformations using a case study of illegal fishing in the\nSouthern Ocean and a theoretical model of socially networked resource users.",
    "published": "2017-04-18T14:27:35Z",
    "updated": "2017-04-18T14:27:35Z",
    "authors": [
      "Steven J. Lade",
      "Ãrjan Bodin",
      "Jonathan F. Donges",
      "Elin Enfors Kautsky",
      "Diego Galafassi",
      "Per Olsson",
      "Maja SchlÃ¼ter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.16469v1",
    "title": "Spatiotemporal Transformer for Video-based Person Re-identification",
    "summary": "Recently, the Transformer module has been transplanted from natural language\nprocessing to computer vision. This paper applies the Transformer to\nvideo-based person re-identification, where the key issue is to extract the\ndiscriminative information from a tracklet. We show that, despite the strong\nlearning ability, the vanilla Transformer suffers from an increased risk of\nover-fitting, arguably due to a large number of attention parameters and\ninsufficient training data. To solve this problem, we propose a novel pipeline\nwhere the model is pre-trained on a set of synthesized video data and then\ntransferred to the downstream domains with the perception-constrained\nSpatiotemporal Transformer (STT) module and Global Transformer (GT) module. The\nderived algorithm achieves significant accuracy gain on three popular\nvideo-based person re-identification benchmarks, MARS, DukeMTMC-VideoReID, and\nLS-VID, especially when the training and testing data are from different\ndomains. More importantly, our research sheds light on the application of the\nTransformer on highly-structured visual data.",
    "published": "2021-03-30T16:19:27Z",
    "updated": "2021-03-30T16:19:27Z",
    "authors": [
      "Tianyu Zhang",
      "Longhui Wei",
      "Lingxi Xie",
      "Zijie Zhuang",
      "Yongfei Zhang",
      "Bo Li",
      "Qi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.08066v2",
    "title": "Optimizing Vision Transformers for Medical Image Segmentation",
    "summary": "For medical image semantic segmentation (MISS), Vision Transformers have\nemerged as strong alternatives to convolutional neural networks thanks to their\ninherent ability to capture long-range correlations. However, existing research\nuses off-the-shelf vision Transformer blocks based on linear projections and\nfeature processing which lack spatial and local context to refine organ\nboundaries. Furthermore, Transformers do not generalize well on small medical\nimaging datasets and rely on large-scale pre-training due to limited inductive\nbiases. To address these problems, we demonstrate the design of a compact and\naccurate Transformer network for MISS, CS-Unet, which introduces convolutions\nin a multi-stage design for hierarchically enhancing spatial and local modeling\nability of Transformers. This is mainly achieved by our well-designed\nConvolutional Swin Transformer (CST) block which merges convolutions with\nMulti-Head Self-Attention and Feed-Forward Networks for providing inherent\nlocalized spatial context and inductive biases. Experiments demonstrate CS-Unet\nwithout pre-training outperforms other counterparts by large margins on\nmulti-organ and cardiac datasets with fewer parameters and achieves\nstate-of-the-art performance. Our code is available at Github.",
    "published": "2022-10-14T19:18:52Z",
    "updated": "2022-10-26T18:11:58Z",
    "authors": [
      "Qianying Liu",
      "Chaitanya Kaul",
      "Jun Wang",
      "Christos Anagnostopoulos",
      "Roderick Murray-Smith",
      "Fani Deligianni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.11395v3",
    "title": "Developing Real-time Streaming Transformer Transducer for Speech\n  Recognition on Large-scale Dataset",
    "summary": "Recently, Transformer based end-to-end models have achieved great success in\nmany areas including speech recognition. However, compared to LSTM models, the\nheavy computational cost of the Transformer during inference is a key issue to\nprevent their applications. In this work, we explored the potential of\nTransformer Transducer (T-T) models for the fist pass decoding with low latency\nand fast speed on a large-scale dataset. We combine the idea of Transformer-XL\nand chunk-wise streaming processing to design a streamable Transformer\nTransducer model. We demonstrate that T-T outperforms the hybrid model, RNN\nTransducer (RNN-T), and streamable Transformer attention-based encoder-decoder\nmodel in the streaming scenario. Furthermore, the runtime cost and latency can\nbe optimized with a relatively small look-ahead.",
    "published": "2020-10-22T03:01:21Z",
    "updated": "2021-02-28T08:02:29Z",
    "authors": [
      "Xie Chen",
      "Yu Wu",
      "Zhenghao Wang",
      "Shujie Liu",
      "Jinyu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.05706v2",
    "title": "Perspective Transformation Layer",
    "summary": "Incorporating geometric transformations that reflect the relative position\nchanges between an observer and an object into computer vision and deep\nlearning models has attracted much attention in recent years. However, the\nexisting proposals mainly focus on the affine transformation that is\ninsufficient to reflect such geometric position changes. Furthermore, current\nsolutions often apply a neural network module to learn a single transformation\nmatrix, which not only ignores the importance of multi-view analysis but also\nincludes extra training parameters from the module apart from the\ntransformation matrix parameters that increase the model complexity. In this\npaper, a perspective transformation layer is proposed in the context of deep\nlearning. The proposed layer can learn homography, therefore reflecting the\ngeometric positions between observers and objects. In addition, by directly\ntraining its transformation matrices, a single proposed layer can learn an\nadjustable number of multiple viewpoints without considering module parameters.\nThe experiments and evaluations confirm the superiority of the proposed layer.",
    "published": "2022-01-14T23:09:26Z",
    "updated": "2022-10-30T17:36:38Z",
    "authors": [
      "Nishan Khatri",
      "Agnibh Dasgupta",
      "Yucong Shen",
      "Xin Zhong",
      "Frank Y. Shih"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.15269v4",
    "title": "Deep Reinforcement Learning with Swin Transformers",
    "summary": "Transformers are neural network models that utilize multiple layers of\nself-attention heads and have exhibited enormous potential in natural language\nprocessing tasks. Meanwhile, there have been efforts to adapt transformers to\nvisual tasks of machine learning, including Vision Transformers and Swin\nTransformers. Although some researchers use Vision Transformers for\nreinforcement learning tasks, their experiments remain at a small scale due to\nthe high computational cost. This article presents the first online\nreinforcement learning scheme that is based on Swin Transformers: Swin DQN. In\ncontrast to existing research, our novel approach demonstrate the superior\nperformance with experiments on 49 games in the Arcade Learning Environment.\nThe results show that our approach achieves significantly higher maximal\nevaluation scores than the baseline method in 45 of all the 49 games (92%), and\nhigher mean evaluation scores than the baseline method in 40 of all the 49\ngames (82%).",
    "published": "2022-06-30T13:20:48Z",
    "updated": "2024-06-24T14:54:37Z",
    "authors": [
      "Li Meng",
      "Morten Goodwin",
      "Anis Yazidi",
      "Paal Engelstad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.06147v2",
    "title": "Exphormer: Sparse Transformers for Graphs",
    "summary": "Graph transformers have emerged as a promising architecture for a variety of\ngraph learning and representation tasks. Despite their successes, though, it\nremains challenging to scale graph transformers to large graphs while\nmaintaining accuracy competitive with message-passing networks. In this paper,\nwe introduce Exphormer, a framework for building powerful and scalable graph\ntransformers. Exphormer consists of a sparse attention mechanism based on two\nmechanisms: virtual global nodes and expander graphs, whose mathematical\ncharacteristics, such as spectral expansion, pseduorandomness, and sparsity,\nyield graph transformers with complexity only linear in the size of the graph,\nwhile allowing us to prove desirable theoretical properties of the resulting\ntransformer models. We show that incorporating Exphormer into the\nrecently-proposed GraphGPS framework produces models with competitive empirical\nresults on a wide variety of graph datasets, including state-of-the-art results\non three datasets. We also show that Exphormer can scale to datasets on larger\ngraphs than shown in previous graph transformer architectures. Code can be\nfound at \\url{https://github.com/hamed1375/Exphormer}.",
    "published": "2023-03-10T18:59:57Z",
    "updated": "2023-07-24T17:58:45Z",
    "authors": [
      "Hamed Shirzad",
      "Ameya Velingker",
      "Balaji Venkatachalam",
      "Danica J. Sutherland",
      "Ali Kemal Sinop"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.12738v1",
    "title": "Variational Transformers for Diverse Response Generation",
    "summary": "Despite the great promise of Transformers in many sequence modeling tasks\n(e.g., machine translation), their deterministic nature hinders them from\ngeneralizing to high entropy tasks such as dialogue response generation.\nPrevious work proposes to capture the variability of dialogue responses with a\nrecurrent neural network (RNN)-based conditional variational autoencoder\n(CVAE). However, the autoregressive computation of the RNN limits the training\nefficiency. Therefore, we propose the Variational Transformer (VT), a\nvariational self-attentive feed-forward sequence model. The VT combines the\nparallelizability and global receptive field of the Transformer with the\nvariational nature of the CVAE by incorporating stochastic latent variables\ninto Transformers. We explore two types of the VT: 1) modeling the\ndiscourse-level diversity with a global latent variable; and 2) augmenting the\nTransformer decoder with a sequence of fine-grained latent variables. Then, the\nproposed models are evaluated on three conversational datasets with both\nautomatic metric and human evaluation. The experimental results show that our\nmodels improve standard Transformers and other baselines in terms of diversity,\nsemantic relevance, and human judgment.",
    "published": "2020-03-28T07:48:02Z",
    "updated": "2020-03-28T07:48:02Z",
    "authors": [
      "Zhaojiang Lin",
      "Genta Indra Winata",
      "Peng Xu",
      "Zihan Liu",
      "Pascale Fung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.12099v2",
    "title": "Visual Saliency Transformer",
    "summary": "Existing state-of-the-art saliency detection methods heavily rely on\nCNN-based architectures. Alternatively, we rethink this task from a\nconvolution-free sequence-to-sequence perspective and predict saliency by\nmodeling long-range dependencies, which can not be achieved by convolution.\nSpecifically, we develop a novel unified model based on a pure transformer,\nnamely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient\nobject detection (SOD). It takes image patches as inputs and leverages the\ntransformer to propagate global contexts among image patches. Unlike\nconventional architectures used in Vision Transformer (ViT), we leverage\nmulti-level token fusion and propose a new token upsampling method under the\ntransformer framework to get high-resolution detection results. We also develop\na token-based multi-task decoder to simultaneously perform saliency and\nboundary detection by introducing task-related tokens and a novel\npatch-task-attention mechanism. Experimental results show that our model\noutperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most\nimportantly, our whole framework not only provides a new perspective for the\nSOD field but also shows a new paradigm for transformer-based dense prediction\nmodels. Code is available at https://github.com/nnizhang/VST.",
    "published": "2021-04-25T08:24:06Z",
    "updated": "2021-08-23T11:26:29Z",
    "authors": [
      "Nian Liu",
      "Ni Zhang",
      "Kaiyuan Wan",
      "Ling Shao",
      "Junwei Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.00479v1",
    "title": "DoT: An efficient Double Transformer for NLP tasks with tables",
    "summary": "Transformer-based approaches have been successfully used to obtain\nstate-of-the-art accuracy on natural language processing (NLP) tasks with\nsemi-structured tables. These model architectures are typically deep, resulting\nin slow training and inference, especially for long inputs. To improve\nefficiency while maintaining a high accuracy, we propose a new architecture,\nDoT, a double transformer model, that decomposes the problem into two\nsub-tasks: A shallow pruning transformer that selects the top-K tokens,\nfollowed by a deep task-specific transformer that takes as input those K\ntokens. Additionally, we modify the task-specific attention to incorporate the\npruning scores. The two transformers are jointly trained by optimizing the\ntask-specific loss. We run experiments on three benchmarks, including\nentailment and question-answering. We show that for a small drop of accuracy,\nDoT improves training and inference time by at least 50%. We also show that the\npruning transformer effectively selects relevant tokens enabling the end-to-end\nmodel to maintain similar accuracy as slower baseline models. Finally, we\nanalyse the pruning and give some insight into its impact on the task model.",
    "published": "2021-06-01T13:33:53Z",
    "updated": "2021-06-01T13:33:53Z",
    "authors": [
      "Syrine Krichene",
      "Thomas MÃ¼ller",
      "Julian Martin Eisenschlos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.13230v1",
    "title": "Video Swin Transformer",
    "summary": "The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.",
    "published": "2021-06-24T17:59:46Z",
    "updated": "2021-06-24T17:59:46Z",
    "authors": [
      "Ze Liu",
      "Jia Ning",
      "Yue Cao",
      "Yixuan Wei",
      "Zheng Zhang",
      "Stephen Lin",
      "Han Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.11084v3",
    "title": "Transformer for Single Image Super-Resolution",
    "summary": "Single image super-resolution (SISR) has witnessed great strides with the\ndevelopment of deep learning. However, most existing studies focus on building\nmore complex networks with a massive number of layers. Recently, more and more\nresearchers start to explore the application of Transformer in computer vision\ntasks. However, the heavy computational cost and high GPU memory occupation of\nthe vision Transformer cannot be ignored. In this paper, we propose a novel\nEfficient Super-Resolution Transformer (ESRT) for SISR. ESRT is a hybrid model,\nwhich consists of a Lightweight CNN Backbone (LCB) and a Lightweight\nTransformer Backbone (LTB). Among them, LCB can dynamically adjust the size of\nthe feature map to extract deep features with a low computational cost. LTB is\ncomposed of a series of Efficient Transformers (ET), which occupies a small GPU\nmemory occupation, thanks to the specially designed Efficient Multi-Head\nAttention (EMHA). Extensive experiments show that ESRT achieves competitive\nresults with low computational costs. Compared with the original Transformer\nwhich occupies 16,057M GPU memory, ESRT only occupies 4,191M GPU memory. All\ncodes are available at https://github.com/luissen/ESRT.",
    "published": "2021-08-25T07:05:30Z",
    "updated": "2022-04-22T05:56:50Z",
    "authors": [
      "Zhisheng Lu",
      "Juncheng Li",
      "Hong Liu",
      "Chaoyan Huang",
      "Linlin Zhang",
      "Tieyong Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.13107v3",
    "title": "The Nuts and Bolts of Adopting Transformer in GANs",
    "summary": "Transformer becomes prevalent in computer vision, especially for high-level\nvision tasks. However, adopting Transformer in the generative adversarial\nnetwork (GAN) framework is still an open yet challenging problem. In this\npaper, we conduct a comprehensive empirical study to investigate the properties\nof Transformer in GAN for high-fidelity image synthesis. Our analysis\nhighlights and reaffirms the importance of feature locality in image\ngeneration, although the merits of the locality are well known in the\nclassification task. Perhaps more interestingly, we find the residual\nconnections in self-attention layers harmful for learning Transformer-based\ndiscriminators and conditional generators. We carefully examine the influence\nand propose effective ways to mitigate the negative impacts. Our study leads to\na new alternative design of Transformers in GAN, a convolutional neural network\n(CNN)-free generator termed as STrans-G, which achieves competitive results in\nboth unconditional and conditional image generations. The Transformer-based\ndiscriminator, STrans-D, also significantly reduces its gap against the\nCNN-based discriminators.",
    "published": "2021-10-25T17:01:29Z",
    "updated": "2023-06-13T15:07:15Z",
    "authors": [
      "Rui Xu",
      "Xiangyu Xu",
      "Kai Chen",
      "Bolei Zhou",
      "Chen Change Loy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.12763v1",
    "title": "Sparse is Enough in Scaling Transformers",
    "summary": "Large Transformer models yield impressive results on many tasks, but are\nexpensive to train, or even fine-tune, and so slow at decoding that their use\nand study becomes out of reach. We address this problem by leveraging sparsity.\nWe study sparse variants for all layers in the Transformer and propose Scaling\nTransformers, a family of next generation Transformer models that use sparse\nlayers to scale efficiently and perform unbatched decoding much faster than the\nstandard Transformer as we scale up the model size. Surprisingly, the sparse\nlayers are enough to obtain the same perplexity as the standard Transformer\nwith the same number of parameters. We also integrate with prior sparsity\napproaches to attention and enable fast inference on long sequences even with\nlimited memory. This results in performance competitive to the state-of-the-art\non long text summarization.",
    "published": "2021-11-24T19:53:46Z",
    "updated": "2021-11-24T19:53:46Z",
    "authors": [
      "Sebastian Jaszczur",
      "Aakanksha Chowdhery",
      "Afroz Mohiuddin",
      "Åukasz Kaiser",
      "Wojciech Gajewski",
      "Henryk Michalewski",
      "Jonni Kanerva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.02279v2",
    "title": "U2-Former: A Nested U-shaped Transformer for Image Restoration",
    "summary": "While Transformer has achieved remarkable performance in various high-level\nvision tasks, it is still challenging to exploit the full potential of\nTransformer in image restoration. The crux lies in the limited depth of\napplying Transformer in the typical encoder-decoder framework for image\nrestoration, resulting from heavy self-attention computation load and\ninefficient communications across different depth (scales) of layers. In this\npaper, we present a deep and effective Transformer-based network for image\nrestoration, termed as U2-Former, which is able to employ Transformer as the\ncore operation to perform image restoration in a deep encoding and decoding\nspace. Specifically, it leverages the nested U-shaped structure to facilitate\nthe interactions across different layers with different scales of feature maps.\nFurthermore, we optimize the computational efficiency for the basic Transformer\nblock by introducing a feature-filtering mechanism to compress the token\nrepresentation. Apart from the typical supervision ways for image restoration,\nour U2-Former also performs contrastive learning in multiple aspects to further\ndecouple the noise component from the background image. Extensive experiments\non various image restoration tasks, including reflection removal, rain streak\nremoval and dehazing respectively, demonstrate the effectiveness of the\nproposed U2-Former.",
    "published": "2021-12-04T08:37:04Z",
    "updated": "2021-12-08T12:09:20Z",
    "authors": [
      "Haobo Ji",
      "Xin Feng",
      "Wenjie Pei",
      "Jinxing Li",
      "Guangming Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.01138v2",
    "title": "Transformers in Time-series Analysis: A Tutorial",
    "summary": "Transformer architecture has widespread applications, particularly in Natural\nLanguage Processing and computer vision. Recently Transformers have been\nemployed in various aspects of time-series analysis. This tutorial provides an\noverview of the Transformer architecture, its applications, and a collection of\nexamples from recent research papers in time-series analysis. We delve into an\nexplanation of the core components of the Transformer, including the\nself-attention mechanism, positional encoding, multi-head, and encoder/decoder.\nSeveral enhancements to the initial, Transformer architecture are highlighted\nto tackle time-series tasks. The tutorial also provides best practices and\ntechniques to overcome the challenge of effectively training Transformers for\ntime-series analysis.",
    "published": "2022-04-28T05:17:45Z",
    "updated": "2023-07-01T19:46:10Z",
    "authors": [
      "Sabeen Ahmed",
      "Ian E. Nielsen",
      "Aakash Tripathi",
      "Shamoon Siddiqui",
      "Ghulam Rasool",
      "Ravi P. Ramachandran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.12152v1",
    "title": "Deep Laparoscopic Stereo Matching with Transformers",
    "summary": "The self-attention mechanism, successfully employed with the transformer\nstructure is shown promise in many computer vision tasks including image\nrecognition, and object detection. Despite the surge, the use of the\ntransformer for the problem of stereo matching remains relatively unexplored.\nIn this paper, we comprehensively investigate the use of the transformer for\nthe problem of stereo matching, especially for laparoscopic videos, and propose\na new hybrid deep stereo matching framework (HybridStereoNet) that combines the\nbest of the CNN and the transformer in a unified design. To be specific, we\ninvestigate several ways to introduce transformers to volumetric stereo\nmatching pipelines by analyzing the loss landscape of the designs and\nin-domain/cross-domain accuracy. Our analysis suggests that employing\ntransformers for feature representation learning, while using CNNs for cost\naggregation will lead to faster convergence, higher accuracy and better\ngeneralization than other options. Our extensive experiments on Sceneflow,\nSCARED2019 and dVPN datasets demonstrate the superior performance of our\nHybridStereoNet.",
    "published": "2022-07-25T12:54:32Z",
    "updated": "2022-07-25T12:54:32Z",
    "authors": [
      "Xuelian Cheng",
      "Yiran Zhong",
      "Mehrtash Harandi",
      "Tom Drummond",
      "Zhiyong Wang",
      "Zongyuan Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.06083v2",
    "title": "Token Transformer: Can class token help window-based transformer build\n  better long-range interactions?",
    "summary": "Compared with the vanilla transformer, the window-based transformer offers a\nbetter trade-off between accuracy and efficiency. Although the window-based\ntransformer has made great progress, its long-range modeling capabilities are\nlimited due to the size of the local window and the window connection scheme.\nTo address this problem, we propose a novel Token Transformer (TT). The core\nmechanism of TT is the addition of a Class (CLS) token for summarizing window\ninformation in each local window. We refer to this type of token interaction as\nCLS Attention. These CLS tokens will interact spatially with the tokens in each\nwindow to enable long-range modeling. In order to preserve the hierarchical\ndesign of the window-based transformer, we designed Feature Inheritance Module\n(FIM) in each phase of TT to deliver the local window information from the\nprevious phase to the CLS token in the next phase. In addition, we have\ndesigned a Spatial-Channel Feedforward Network (SCFFN) in TT, which can mix CLS\ntokens and embedded tokens on the spatial domain and channel domain without\nadditional parameters. Extensive experiments have shown that our TT achieves\ncompetitive results with low parameters in image classification and downstream\ntasks.",
    "published": "2022-11-11T09:36:47Z",
    "updated": "2023-01-03T09:05:43Z",
    "authors": [
      "Jiawei Mao",
      "Yuanqi Chang",
      "Xuesong Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.10043v1",
    "title": "Vision Transformers in Medical Imaging: A Review",
    "summary": "Transformer, a model comprising attention-based encoder-decoder architecture,\nhave gained prevalence in the field of natural language processing (NLP) and\nrecently influenced the computer vision (CV) space. The similarities between\ncomputer vision and medical imaging, reviewed the question among researchers if\nthe impact of transformers on computer vision be translated to medical imaging?\nIn this paper, we attempt to provide a comprehensive and recent review on the\napplication of transformers in medical imaging by; describing the transformer\nmodel comparing it with a diversity of convolutional neural networks (CNNs),\ndetailing the transformer based approaches for medical image classification,\nsegmentation, registration and reconstruction with a focus on the image\nmodality, comparing the performance of state-of-the-art transformer\narchitectures to best performing CNNs on standard medical datasets.",
    "published": "2022-11-18T05:52:37Z",
    "updated": "2022-11-18T05:52:37Z",
    "authors": [
      "Emerald U. Henry",
      "Onyeka Emebob",
      "Conrad Asotie Omonhinmin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.09539v4",
    "title": "Block-State Transformers",
    "summary": "State space models (SSMs) have shown impressive results on tasks that require\nmodeling long-range dependencies and efficiently scale to long sequences owing\nto their subquadratic runtime complexity. Originally designed for continuous\nsignals, SSMs have shown superior performance on a plethora of tasks, in vision\nand audio; however, SSMs still lag Transformer performance in Language Modeling\ntasks. In this work, we propose a hybrid layer named Block-State Transformer\n(BST), that internally combines an SSM sublayer for long-range\ncontextualization, and a Block Transformer sublayer for short-term\nrepresentation of sequences. We study three different, and completely\nparallelizable, variants that integrate SSMs and block-wise attention. We show\nthat our model outperforms similar Transformer-based architectures on language\nmodeling perplexity and generalizes to longer sequences. In addition, the\nBlock-State Transformer demonstrates more than tenfold increase in speed at the\nlayer level compared to the Block-Recurrent Transformer when model\nparallelization is employed.",
    "published": "2023-06-15T22:48:08Z",
    "updated": "2023-10-30T15:44:04Z",
    "authors": [
      "Mahan Fathi",
      "Jonathan Pilault",
      "Orhan Firat",
      "Christopher Pal",
      "Pierre-Luc Bacon",
      "Ross Goroshin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.04305v1",
    "title": "Automatic Piano Transcription with Hierarchical Frequency-Time\n  Transformer",
    "summary": "Taking long-term spectral and temporal dependencies into account is essential\nfor automatic piano transcription. This is especially helpful when determining\nthe precise onset and offset for each note in the polyphonic piano content. In\nthis case, we may rely on the capability of self-attention mechanism in\nTransformers to capture these long-term dependencies in the frequency and time\naxes. In this work, we propose hFT-Transformer, which is an automatic music\ntranscription method that uses a two-level hierarchical frequency-time\nTransformer architecture. The first hierarchy includes a convolutional block in\nthe time axis, a Transformer encoder in the frequency axis, and a Transformer\ndecoder that converts the dimension in the frequency axis. The output is then\nfed into the second hierarchy which consists of another Transformer encoder in\nthe time axis. We evaluated our method with the widely used MAPS and MAESTRO\nv3.0.0 datasets, and it demonstrated state-of-the-art performance on all the\nF1-scores of the metrics among Frame, Note, Note with Offset, and Note with\nOffset and Velocity estimations.",
    "published": "2023-07-10T02:04:43Z",
    "updated": "2023-07-10T02:04:43Z",
    "authors": [
      "Keisuke Toyama",
      "Taketo Akama",
      "Yukara Ikemiya",
      "Yuhta Takida",
      "Wei-Hsiang Liao",
      "Yuki Mitsufuji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.11378v2",
    "title": "Inspecting Explainability of Transformer Models with Additional\n  Statistical Information",
    "summary": "Transformer becomes more popular in the vision domain in recent years so\nthere is a need for finding an effective way to interpret the Transformer model\nby visualizing it. In recent work, Chefer et al. can visualize the Transformer\non vision and multi-modal tasks effectively by combining attention layers to\nshow the importance of each image patch. However, when applying to other\nvariants of Transformer such as the Swin Transformer, this method can not focus\non the predicted object. Our method, by considering the statistics of tokens in\nlayer normalization layers, shows a great ability to interpret the\nexplainability of Swin Transformer and ViT.",
    "published": "2023-11-19T17:22:50Z",
    "updated": "2025-04-19T08:01:14Z",
    "authors": [
      "Hoang C. Nguyen",
      "Haeil Lee",
      "Junmo Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.14218v1",
    "title": "AutoAugment Input Transformation for Highly Transferable Targeted\n  Attacks",
    "summary": "Deep Neural Networks (DNNs) are widely acknowledged to be susceptible to\nadversarial examples, wherein imperceptible perturbations are added to clean\nexamples through diverse input transformation attacks. However, these methods\noriginally designed for non-targeted attacks exhibit low success rates in\ntargeted attacks. Recent targeted adversarial attacks mainly pay attention to\ngradient optimization, attempting to find the suitable perturbation direction.\nHowever, few of them are dedicated to input transformation.In this work, we\nobserve a positive correlation between the logit/probability of the target\nclass and diverse input transformation methods in targeted attacks. To this\nend, we propose a novel targeted adversarial attack called AutoAugment Input\nTransformation (AAIT). Instead of relying on hand-made strategies, AAIT\nsearches for the optimal transformation policy from a transformation space\ncomprising various operations. Then, AAIT crafts adversarial examples using the\nfound optimal transformation policy to boost the adversarial transferability in\ntargeted attacks. Extensive experiments conducted on CIFAR-10 and\nImageNet-Compatible datasets demonstrate that the proposed AAIT surpasses other\ntransfer-based targeted attacks significantly.",
    "published": "2023-12-21T12:49:36Z",
    "updated": "2023-12-21T12:49:36Z",
    "authors": [
      "Haobo Lu",
      "Xin Liu",
      "Kun He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.16039v2",
    "title": "MoEUT: Mixture-of-Experts Universal Transformers",
    "summary": "Previous work on Universal Transformers (UTs) has demonstrated the importance\nof parameter sharing across layers. By allowing recurrence in depth, UTs have\nadvantages over standard Transformers in learning compositional\ngeneralizations, but layer-sharing comes with a practical limitation of\nparameter-compute ratio: it drastically reduces the parameter count compared to\nthe non-shared model with the same dimensionality. Naively scaling up the layer\nsize to compensate for the loss of parameters makes its computational resource\nrequirements prohibitive. In practice, no previous work has succeeded in\nproposing a shared-layer Transformer design that is competitive in parameter\ncount-dominated tasks such as language modeling. Here we propose MoEUT\n(pronounced \"moot\"), an effective mixture-of-experts (MoE)-based shared-layer\nTransformer architecture, which combines several recent advances in MoEs for\nboth feedforward and attention layers of standard Transformers together with\nnovel layer-normalization and grouping schemes that are specific and crucial to\nUTs. The resulting UT model, for the first time, slightly outperforms standard\nTransformers on language modeling tasks such as BLiMP and PIQA, while using\nsignificantly less compute and memory.",
    "published": "2024-05-25T03:24:32Z",
    "updated": "2024-10-13T04:46:00Z",
    "authors": [
      "RÃ³bert CsordÃ¡s",
      "Kazuki Irie",
      "JÃ¼rgen Schmidhuber",
      "Christopher Potts",
      "Christopher D. Manning"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.12805v1",
    "title": "Dark Transformer: A Video Transformer for Action Recognition in the Dark",
    "summary": "Recognizing human actions in adverse lighting conditions presents significant\nchallenges in computer vision, with wide-ranging applications in visual\nsurveillance and nighttime driving. Existing methods tackle action recognition\nand dark enhancement separately, limiting the potential for end-to-end learning\nof spatiotemporal representations for video action classification. This paper\nintroduces Dark Transformer, a novel video transformer-based approach for\naction recognition in low-light environments. Dark Transformer leverages\nspatiotemporal self-attention mechanisms in cross-domain settings to enhance\ncross-domain action recognition. By extending video transformers to learn\ncross-domain knowledge, Dark Transformer achieves state-of-the-art performance\non benchmark action recognition datasets, including InFAR, XD145, and ARID. The\nproposed approach demonstrates significant promise in addressing the challenges\nof action recognition in adverse lighting conditions, offering practical\nimplications for real-world applications.",
    "published": "2024-06-25T01:25:12Z",
    "updated": "2024-06-25T01:25:12Z",
    "authors": [
      "Anwaar Ulhaq"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.00871v3",
    "title": "MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential\n  with Masked Autoregressive Pretraining",
    "summary": "Hybrid Mamba-Transformer networks have recently garnered broad attention.\nThese networks can leverage the scalability of Transformers while capitalizing\non Mamba's strengths in long-context modeling and computational efficiency.\nHowever, the challenge of effectively pretraining such hybrid networks remains\nan open question. Existing methods, such as Masked Autoencoders (MAE) or\nautoregressive (AR) pretraining, primarily focus on single-type network\narchitectures. In contrast, pretraining strategies for hybrid architectures\nmust be effective for both Mamba and Transformer components. Based on this, we\npropose Masked Autoregressive Pretraining (MAP) to pretrain a hybrid\nMamba-Transformer vision backbone network. This strategy combines the strengths\nof both MAE and Autoregressive pretraining, improving the performance of Mamba\nand Transformer modules within a unified paradigm. Experimental results show\nthat the hybrid Mamba-Transformer vision backbone network pretrained with MAP\nsignificantly outperforms other pretraining strategies, achieving\nstate-of-the-art performance. We validate the method's effectiveness on both 2D\nand 3D datasets and provide detailed ablation studies to support the design\nchoices for each component. The code and checkpoints are available at\nhttps://github.com/yunzeliu/MAP",
    "published": "2024-10-01T17:05:08Z",
    "updated": "2025-05-23T06:42:28Z",
    "authors": [
      "Yunze Liu",
      "Li Yi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.20439v1",
    "title": "TEAFormers: TEnsor-Augmented Transformers for Multi-Dimensional Time\n  Series Forecasting",
    "summary": "Multi-dimensional time series data, such as matrix and tensor-variate time\nseries, are increasingly prevalent in fields such as economics, finance, and\nclimate science. Traditional Transformer models, though adept with sequential\ndata, do not effectively preserve these multi-dimensional structures, as their\ninternal operations in effect flatten multi-dimensional observations into\nvectors, thereby losing critical multi-dimensional relationships and patterns.\nTo address this, we introduce the Tensor-Augmented Transformer (TEAFormer), a\nnovel method that incorporates tensor expansion and compression within the\nTransformer framework to maintain and leverage the inherent multi-dimensional\nstructures, thus reducing computational costs and improving prediction\naccuracy. The core feature of the TEAFormer, the Tensor-Augmentation (TEA)\nmodule, utilizes tensor expansion to enhance multi-view feature learning and\ntensor compression for efficient information aggregation and reduced\ncomputational load. The TEA module is not just a specific model architecture\nbut a versatile component that is highly compatible with the attention\nmechanism and the encoder-decoder structure of Transformers, making it\nadaptable to existing Transformer architectures. Our comprehensive experiments,\nwhich integrate the TEA module into three popular time series Transformer\nmodels across three real-world benchmarks, show significant performance\nenhancements, highlighting the potential of TEAFormers for cutting-edge time\nseries forecasting.",
    "published": "2024-10-27T13:32:12Z",
    "updated": "2024-10-27T13:32:12Z",
    "authors": [
      "Linghang Kong",
      "Elynn Chen",
      "Yuzhou Chen",
      "Yuefeng Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.13419v1",
    "title": "Exploring Transformer-Augmented LSTM for Temporal and Spatial Feature\n  Learning in Trajectory Prediction",
    "summary": "Accurate vehicle trajectory prediction is crucial for ensuring safe and\nefficient autonomous driving. This work explores the integration of Transformer\nbased model with Long Short-Term Memory (LSTM) based technique to enhance\nspatial and temporal feature learning in vehicle trajectory prediction. Here, a\nhybrid model that combines LSTMs for temporal encoding with a Transformer\nencoder for capturing complex interactions between vehicles is proposed.\nSpatial trajectory features of the neighboring vehicles are processed and goes\nthrough a masked scatter mechanism in a grid based environment, which is then\ncombined with temporal trajectory of the vehicles. This combined trajectory\ndata are learned by sequential LSTM encoding and Transformer based attention\nlayers. The proposed model is benchmarked against predecessor LSTM based\nmethods, including STA-LSTM, SA-LSTM, CS-LSTM, and NaiveLSTM. Our results,\nwhile not outperforming it's predecessor, demonstrate the potential of\nintegrating Transformers with LSTM based technique to build interpretable\ntrajectory prediction model. Future work will explore alternative architectures\nusing Transformer applications to further enhance performance. This study\nprovides a promising direction for improving trajectory prediction models by\nleveraging transformer based architectures, paving the way for more robust and\ninterpretable vehicle trajectory prediction system.",
    "published": "2024-12-18T01:31:08Z",
    "updated": "2024-12-18T01:31:08Z",
    "authors": [
      "Chandra Raskoti",
      "Weizi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.08638v1",
    "title": "Transformer Learns Optimal Variable Selection in Group-Sparse\n  Classification",
    "summary": "Transformers have demonstrated remarkable success across various\napplications. However, the success of transformers have not been understood in\ntheory. In this work, we give a case study of how transformers can be trained\nto learn a classic statistical model with \"group sparsity\", where the input\nvariables form multiple groups, and the label only depends on the variables\nfrom one of the groups. We theoretically demonstrate that, a one-layer\ntransformer trained by gradient descent can correctly leverage the attention\nmechanism to select variables, disregarding irrelevant ones and focusing on\nthose beneficial for classification. We also demonstrate that a well-pretrained\none-layer transformer can be adapted to new downstream tasks to achieve good\nprediction accuracy with a limited number of samples. Our study sheds light on\nhow transformers effectively learn structured data.",
    "published": "2025-04-11T15:39:44Z",
    "updated": "2025-04-11T15:39:44Z",
    "authors": [
      "Chenyang Zhang",
      "Xuran Meng",
      "Yuan Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20821v1",
    "title": "The When and How of Target Variable Transformations",
    "summary": "The machine learning pipeline typically involves the iterative process of (1)\ncollecting the data, (2) preparing the data, (3) learning a model, and (4)\nevaluating a model. Practitioners recognize the importance of the data\npreparation phase in terms of its impact on the ability to learn accurate\nmodels. In this regard, significant attention is often paid to manipulating the\nfeature set (e.g., selection, transformations, dimensionality reduction). A\npoint that is less well appreciated is that transformations on the target\nvariable can also have a large impact on whether it is possible to learn a\nsuitable model. These transformations may include accounting for\nsubject-specific biases (e.g., in how someone uses a rating scale), contexts\n(e.g., population size effects), and general trends (e.g., inflation). However,\nthis point has received a much more cursory treatment in the existing\nliterature. The goal of this paper is three-fold. First, we aim to highlight\nthe importance of this problem by showing when transforming the target variable\nhas been useful in practice. Second, we will provide a set of generic ``rules\nof thumb'' that indicate situations when transforming the target variable may\nbe needed. Third, we will discuss which transformations should be considered in\na given situation.",
    "published": "2025-04-29T14:40:21Z",
    "updated": "2025-04-29T14:40:21Z",
    "authors": [
      "Loren Nuyts",
      "Jesse Davis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.00473v1",
    "title": "Interpretable Spatial-Temporal Fusion Transformers: Multi-Output\n  Prediction for Parametric Dynamical Systems with Time-Varying Inputs",
    "summary": "We explore the promising performance of a transformer model in predicting\noutputs of parametric dynamical systems with external time-varying input\nsignals. The outputs of such systems vary not only with physical parameters but\nalso with external time-varying input signals. Accurately catching the dynamics\nof such systems is challenging. We have adapted and extended an existing\ntransformer model for single output prediction to a multiple-output transformer\nthat is able to predict multiple output responses of these systems. The\nmultiple-output transformer generalizes the interpretability of the original\ntransformer. The generalized interpretable attention weight matrix explores not\nonly the temporal correlations in the sequence, but also the interactions\nbetween the multiple outputs, providing explanation for the spatial correlation\nin the output domain. This multiple-output transformer accurately predicts the\nsequence of multiple outputs, regardless of the nonlinearity of the system and\nthe dimensionality of the parameter space.",
    "published": "2025-05-01T11:55:42Z",
    "updated": "2025-05-01T11:55:42Z",
    "authors": [
      "Shuwen Sun",
      "Lihong Feng",
      "Peter Benner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14533v1",
    "title": "Energy-Efficient Deep Reinforcement Learning with Spiking Transformers",
    "summary": "Agent-based Transformers have been widely adopted in recent reinforcement\nlearning advances due to their demonstrated ability to solve complex tasks.\nHowever, the high computational complexity of Transformers often results in\nsignificant energy consumption, limiting their deployment in real-world\nautonomous systems. Spiking neural networks (SNNs), with their biologically\ninspired structure, offer an energy-efficient alternative for machine learning.\nIn this paper, a novel Spike-Transformer Reinforcement Learning (STRL)\nalgorithm that combines the energy efficiency of SNNs with the powerful\ndecision-making capabilities of reinforcement learning is developed.\nSpecifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons\nand attention mechanisms capable of processing spatio-temporal patterns over\nmultiple time steps is designed. The architecture is further enhanced with\nstate, action, and reward encodings to create a Transformer-like structure\noptimized for reinforcement learning tasks. Comprehensive numerical experiments\nconducted on state-of-the-art benchmarks demonstrate that the proposed SNN\nTransformer achieves significantly improved policy performance compared to\nconventional agent-based Transformers. With both enhanced energy efficiency and\npolicy optimality, this work highlights a promising direction for deploying\nbio-inspired, low-cost machine learning models in complex real-world\ndecision-making scenarios.",
    "published": "2025-05-20T15:52:43Z",
    "updated": "2025-05-20T15:52:43Z",
    "authors": [
      "Mohammad Irfan Uddin",
      "Nishad Tasnim",
      "Md Omor Faruk",
      "Zejian Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24717v1",
    "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics\n  Simulations",
    "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for\nsurrogate modeling of physics simulations on regular grids. We combine recent\narchitectural improvements of diffusion transformers with adjustments specific\nfor large-scale simulations to yield a more scalable and versatile\ngeneral-purpose transformer architecture, which can be used as the backbone for\nbuilding large-scale foundation models in physical sciences. We demonstrate\nthat our proposed architecture outperforms state-of-the-art transformer\narchitectures for computer vision on a large dataset of 16 different types of\nPDEs. We propose to embed different physical channels individually as\nspatio-temporal tokens, which interact via channel-wise self-attention. This\nhelps to maintain a consistent information density of tokens when learning\nmultiple types of PDEs simultaneously. We demonstrate that our pre-trained\nmodels achieve improved performance on several challenging downstream tasks\ncompared to training from scratch and also beat other foundation model\narchitectures for physics simulations.",
    "published": "2025-05-30T15:39:54Z",
    "updated": "2025-05-30T15:39:54Z",
    "authors": [
      "Benjamin Holzschuh",
      "Qiang Liu",
      "Georg Kohl",
      "Nils Thuerey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08652v1",
    "title": "JoFormer (Journey-based Transformer): Theory and Empirical Analysis on\n  the Tiny Shakespeare Dataset",
    "summary": "Transformers have demonstrated remarkable success in sequence modeling, yet\neffectively incorporating positional information remains a challenging and\nactive area of research. In this paper, we introduce JoFormer, a journey-based\nTransformer architecture grounded in a recently proposed non-commutative\nalgebra for composing transformations across positions. JoFormer represents\nrelative positions through learnable directional transforms that are\nsequentially composed along the input, thereby extending and generalizing\nexisting approaches based on relative position representations. We derive the\nJoFormer attention mechanism from first principles and show that it subsumes\nstandard methods such as rotary transformations as special cases. To evaluate\nits effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny\nShakespeare character-level language modeling task. Our results demonstrate\nthat\n  JoFormer consistently achieves lower perplexity and faster convergence,\nhighlighting the advantages of its more expressive, journey-based treatment of\nposition. Notably, the per-token JoFormer is still a primitive, conceptual\nvariant with layer-independent angles, yet it already demonstrates strong\nperformance-underscoring its promise as a proof of concept for more expressive\narchitectures. We conclude by discussing how JoFormer offers a principled\napproach to integrating positional structure into Transformer architectures.\nThe code used in this work is available at\nhttps://github.com/mahesh-godavarti/joformer.",
    "published": "2025-06-10T10:05:29Z",
    "updated": "2025-06-10T10:05:29Z",
    "authors": [
      "Mahesh Godavarti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.22084v1",
    "title": "Transformers are Graph Neural Networks",
    "summary": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.",
    "published": "2025-06-27T10:15:33Z",
    "updated": "2025-06-27T10:15:33Z",
    "authors": [
      "Chaitanya K. Joshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.11834v1",
    "title": "Recent Advances in Transformer and Large Language Models for UAV\n  Applications",
    "summary": "The rapid advancement of Transformer-based models has reshaped the landscape\nof uncrewed aerial vehicle (UAV) systems by enhancing perception,\ndecision-making, and autonomy. This review paper systematically categorizes and\nevaluates recent developments in Transformer architectures applied to UAVs,\nincluding attention mechanisms, CNN-Transformer hybrids, reinforcement learning\nTransformers, and large language models (LLMs). Unlike previous surveys, this\nwork presents a unified taxonomy of Transformer-based UAV models, highlights\nemerging applications such as precision agriculture and autonomous navigation,\nand provides comparative analyses through structured tables and performance\nbenchmarks. The paper also reviews key datasets, simulators, and evaluation\nmetrics used in the field. Furthermore, it identifies existing gaps in the\nliterature, outlines critical challenges in computational efficiency and\nreal-time deployment, and offers future research directions. This comprehensive\nsynthesis aims to guide researchers and practitioners in understanding and\nadvancing Transformer-driven UAV technologies.",
    "published": "2025-08-15T22:56:37Z",
    "updated": "2025-08-15T22:56:37Z",
    "authors": [
      "Hamza Kheddar",
      "Yassine Habchi",
      "Mohamed Chahine Ghanem",
      "Mustapha Hemis",
      "Dusit Niyato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.02758v1",
    "title": "Finite free probability and $S$ transforms of Jacobi processes",
    "summary": "In this paper, we study the $S$ transforms of Jacobi processes in the\nframeworks of free and finite free probability theories. We begin by deriving a\npartial differential equation satisfied by the free $S$ transform of the free\nJacobi process, and we provide a detailed analysis of its characteristic\ncurves. We turn next our attention to the averaged characteristic polynomial of\nthe Hermitian Jacobi process and to the dynamic of its roots, referred to as\nthe frozen Jacobi process. In particular, we prove, for a specific set of\nparameters, that the former aligns up to a Szeg\\\"o variable transformation with\nthe Hermite unitary polynomial. We also provide an expansion of the averaged\ncharacteristic polynomial of the Hermitian process in the basis of Jacobi\npolynomials. Finally, we establish the convergence of the frozen Jacobi process\nto the free Jacobi process in high dimensions by using the finite free S\ntransform. In doing so, we prove a general result, interesting in its own, on\nthe convergence of the finite differences of the finite free $S$ transform.",
    "published": "2025-11-04T17:36:27Z",
    "updated": "2025-11-04T17:36:27Z",
    "authors": [
      "Nizar Demni",
      "Nicolas Gilliers",
      "Tarek Hamdi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.03854v4",
    "title": "inTformer: A Time-Embedded Attention-Based Transformer for Crash\n  Likelihood Prediction at Intersections Using Connected Vehicle Data",
    "summary": "The real-time crash likelihood prediction model is an essential component of\nthe proactive traffic safety management system. Over the years, numerous\nstudies have attempted to construct a crash likelihood prediction model in\norder to enhance traffic safety, but mostly on freeways. In the majority of the\nexisting studies, researchers have primarily employed a deep learning-based\nframework to identify crash potential. Lately, Transformer has emerged as a\npotential deep neural network that fundamentally operates through\nattention-based mechanisms. Transformer has several functional benefits over\nextant deep learning models such as LSTM, CNN, etc. Firstly, Transformer can\nreadily handle long-term dependencies in a data sequence. Secondly,\nTransformers can parallelly process all elements in a data sequence during\ntraining. Finally, a Transformer does not have the vanishing gradient issue.\nRealizing the immense possibility of Transformers, this paper proposes\ninTersection-Transformer (inTformer), a time-embedded attention-based\nTransformer model that can effectively predict intersection crash likelihood in\nreal-time. The proposed model was evaluated using connected vehicle data\nextracted from Signal Analytics Platform. Acknowledging the complex traffic\noperation mechanism at intersection, this study developed zone-specific models\nby dividing the intersection region into two distinct zones:\nwithin-intersection and approach zone. The best inTformer models in\n'within-intersection,' and 'approach' zone achieved a sensitivity of 73%, and\n70%, respectively. The zone-level models were also compared to earlier studies\non crash likelihood prediction at intersections and with several established\ndeep learning models trained on the same connected vehicle dataset.",
    "published": "2023-07-07T22:00:31Z",
    "updated": "2023-08-29T15:51:05Z",
    "authors": [
      "B M Tazbiul Hassan Anik",
      "Zubayer Islam",
      "Mohamed Abdel-Aty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.07781v1",
    "title": "3D TransUNet: Advancing Medical Image Segmentation through Vision\n  Transformers",
    "summary": "Medical image segmentation plays a crucial role in advancing healthcare\nsystems for disease diagnosis and treatment planning. The u-shaped\narchitecture, popularly known as U-Net, has proven highly successful for\nvarious medical image segmentation tasks. However, U-Net's convolution-based\noperations inherently limit its ability to model long-range dependencies\neffectively. To address these limitations, researchers have turned to\nTransformers, renowned for their global self-attention mechanisms, as\nalternative architectures. One popular network is our previous TransUNet, which\nleverages Transformers' self-attention to complement U-Net's localized\ninformation with the global context. In this paper, we extend the 2D TransUNet\narchitecture to a 3D network by building upon the state-of-the-art nnU-Net\narchitecture, and fully exploring Transformers' potential in both the encoder\nand decoder design. We introduce two key components: 1) A Transformer encoder\nthat tokenizes image patches from a convolution neural network (CNN) feature\nmap, enabling the extraction of global contexts, and 2) A Transformer decoder\nthat adaptively refines candidate regions by utilizing cross-attention between\ncandidate proposals and U-Net features. Our investigations reveal that\ndifferent medical tasks benefit from distinct architectural designs. The\nTransformer encoder excels in multi-organ segmentation, where the relationship\namong organs is crucial. On the other hand, the Transformer decoder proves more\nbeneficial for dealing with small and challenging segmented targets such as\ntumor segmentation. Extensive experiments showcase the significant potential of\nintegrating a Transformer-based encoder and decoder into the u-shaped medical\nimage segmentation architecture. TransUNet outperforms competitors in various\nmedical applications.",
    "published": "2023-10-11T18:07:19Z",
    "updated": "2023-10-11T18:07:19Z",
    "authors": [
      "Jieneng Chen",
      "Jieru Mei",
      "Xianhang Li",
      "Yongyi Lu",
      "Qihang Yu",
      "Qingyue Wei",
      "Xiangde Luo",
      "Yutong Xie",
      "Ehsan Adeli",
      "Yan Wang",
      "Matthew Lungren",
      "Lei Xing",
      "Le Lu",
      "Alan Yuille",
      "Yuyin Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.08412v2",
    "title": "Multi-branch Cascaded Swin Transformers with Attention to k-space\n  Sampling Pattern for Accelerated MRI Reconstruction",
    "summary": "Global correlations are widely seen in human anatomical structures due to\nsimilarity across tissues and bones. These correlations are reflected in\nmagnetic resonance imaging (MRI) scans as a result of close-range proton\ndensity and T1/T2 parameters. Furthermore, to achieve accelerated MRI, k-space\ndata are undersampled which causes global aliasing artifacts. Convolutional\nneural network (CNN) models are widely utilized for accelerated MRI\nreconstruction, but those models are limited in capturing global correlations\ndue to the intrinsic locality of the convolution operation. The\nself-attention-based transformer models are capable of capturing global\ncorrelations among image features, however, the current contributions of\ntransformer models for MRI reconstruction are minute. The existing\ncontributions mostly provide CNN-transformer hybrid solutions and rarely\nleverage the physics of MRI. In this paper, we propose a physics-based\nstand-alone (convolution free) transformer model titled, the Multi-head\nCascaded Swin Transformers (McSTRA) for accelerated MRI reconstruction. McSTRA\ncombines several interconnected MRI physics-related concepts with the\ntransformer networks: it exploits global MR features via the shifted window\nself-attention mechanism; it extracts MR features belonging to different\nspectral components separately using a multi-head setup; it iterates between\nintermediate de-aliasing and k-space correction via a cascaded network with\ndata consistency in k-space and intermediate loss computations; furthermore, we\npropose a novel positional embedding generation mechanism to guide\nself-attention utilizing the point spread function corresponding to the\nundersampling mask. Our model significantly outperforms state-of-the-art MRI\nreconstruction methods both visually and quantitatively while depicting\nimproved resolution and removal of aliasing artifacts.",
    "published": "2022-07-18T07:21:56Z",
    "updated": "2022-12-21T06:11:44Z",
    "authors": [
      "Mevan Ekanayake",
      "Kamlesh Pawar",
      "Mehrtash Harandi",
      "Gary Egan",
      "Zhaolin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.14246v3",
    "title": "Robust representations of oil wells' intervals via sparse attention\n  mechanism",
    "summary": "Transformer-based neural network architectures achieve state-of-the-art\nresults in different domains, from natural language processing (NLP) to\ncomputer vision (CV). The key idea of Transformers, the attention mechanism,\nhas already led to significant breakthroughs in many areas. The attention has\nfound their implementation for time series data as well. However, due to the\nquadratic complexity of the attention calculation regarding input sequence\nlength, the application of Transformers is limited by high resource demands.\nMoreover, their modifications for industrial time series need to be robust to\nmissing or noised values, which complicates the expansion of the horizon of\ntheir application. To cope with these issues, we introduce the class of\nefficient Transformers named Regularized Transformers (Reguformers). We\nimplement the regularization technique inspired by the dropout ideas to improve\nrobustness and reduce computational expenses. The focus in our experiments is\non oil&gas data, namely, well logs, a prominent example of multivariate time\nseries. The goal is to solve the problems of similarity and representation\nlearning for them. To evaluate our models for such problems, we work with an\nindustry-scale open dataset consisting of well logs of more than 20 wells. The\nexperiments show that all variations of Reguformers outperform the previously\ndeveloped RNNs, classical Transformer model, and robust modifications of it\nlike Informer and Performer in terms of well-intervals' classification and the\nquality of the obtained well-intervals' representations. Moreover, the\nsustainability to missing and incorrect data in our models exceeds that of\nothers by a significant margin. The best result that the Reguformer achieves on\nwell-interval similarity task is the mean PR~AUC score equal to 0.983, which is\ncomparable to the classical Transformer and outperforms the previous models.",
    "published": "2022-12-29T09:56:33Z",
    "updated": "2023-11-06T10:04:49Z",
    "authors": [
      "Alina Ermilova",
      "Nikita Baramiia",
      "Valerii Kornilov",
      "Sergey Petrakov",
      "Alexey Zaytsev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.02945v1",
    "title": "Optimizing the Deployment of Tiny Transformers on Low-Power MCUs",
    "summary": "Transformer networks are rapidly becoming SotA in many fields, such as NLP\nand CV. Similarly to CNN, there is a strong push for deploying Transformer\nmodels at the extreme edge, ultimately fitting the tiny power budget and memory\nfootprint of MCUs. However, the early approaches in this direction are mostly\nad-hoc, platform, and model-specific. This work aims to enable and optimize the\nflexible, multi-platform deployment of encoder Tiny Transformers on commercial\nMCUs. We propose a complete framework to perform end-to-end deployment of\nTransformer models onto single and multi-core MCUs. Our framework provides an\noptimized library of kernels to maximize data reuse and avoid unnecessary data\nmarshaling operations into the crucial attention block. A novel MHSA inference\nschedule, named Fused-Weight Self-Attention, is introduced, fusing the linear\nprojection weights offline to further reduce the number of operations and\nparameters. Furthermore, to mitigate the memory peak reached by the computation\nof the attention map, we present a Depth-First Tiling scheme for MHSA. We\nevaluate our framework on three different MCU classes exploiting ARM and RISC-V\nISA, namely the STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2). We reach an\naverage of 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN\n(ARM) and PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA\ndepth-first tiling scheme reduces the memory peak by up to 6.19x, while the\nfused-weight attention can reduce the runtime by 1.53x, and number of\nparameters by 25%. We report significant improvements across several Tiny\nTransformers: for instance, when executing a transformer block for the task of\nradar-based hand-gesture recognition on GAP9, we achieve a latency of 0.14ms\nand energy consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN\nlibrary on the same platform.",
    "published": "2024-04-03T14:14:08Z",
    "updated": "2024-04-03T14:14:08Z",
    "authors": [
      "Victor J. B. Jung",
      "Alessio Burrello",
      "Moritz Scherer",
      "Francesco Conti",
      "Luca Benini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1907.00235v3",
    "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer\n  on Time Series Forecasting",
    "summary": "Time series forecasting is an important problem across many domains,\nincluding predictions of solar plant energy output, electricity consumption,\nand traffic jam situation. In this paper, we propose to tackle such forecasting\nproblem with Transformer [1]. Although impressed by its performance in our\npreliminary study, we found its two major weaknesses: (1) locality-agnostics:\nthe point-wise dot-product self-attention in canonical Transformer architecture\nis insensitive to local context, which can make the model prone to anomalies in\ntime series; (2) memory bottleneck: space complexity of canonical Transformer\ngrows quadratically with sequence length $L$, making directly modeling long\ntime series infeasible. In order to solve these two issues, we first propose\nconvolutional self-attention by producing queries and keys with causal\nconvolution so that local context can be better incorporated into attention\nmechanism. Then, we propose LogSparse Transformer with only $O(L(\\log L)^{2})$\nmemory cost, improving forecasting accuracy for time series with fine\ngranularity and strong long-term dependencies under constrained memory budget.\nOur experiments on both synthetic data and real-world datasets show that it\ncompares favorably to the state-of-the-art.",
    "published": "2019-06-29T16:36:04Z",
    "updated": "2020-01-03T05:10:50Z",
    "authors": [
      "Shiyang Li",
      "Xiaoyong Jin",
      "Yao Xuan",
      "Xiyou Zhou",
      "Wenhu Chen",
      "Yu-Xiang Wang",
      "Xifeng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.15972v1",
    "title": "Contextual Learning in Fourier Complex Field for VHR Remote Sensing\n  Images",
    "summary": "Very high-resolution (VHR) remote sensing (RS) image classification is the\nfundamental task for RS image analysis and understanding. Recently,\ntransformer-based models demonstrated outstanding potential for learning\nhigh-order contextual relationships from natural images with general resolution\n(224x224 pixels) and achieved remarkable results on general image\nclassification tasks. However, the complexity of the naive transformer grows\nquadratically with the increase in image size, which prevents transformer-based\nmodels from VHR RS image (500x500 pixels) classification and other\ncomputationally expensive downstream tasks. To this end, we propose to\ndecompose the expensive self-attention (SA) into real and imaginary parts via\ndiscrete Fourier transform (DFT) and therefore propose an efficient complex\nself-attention (CSA) mechanism. Benefiting from the conjugated symmetric\nproperty of DFT, CSA is capable to model the high-order contextual information\nwith less than half computations of naive SA. To overcome the gradient\nexplosion in Fourier complex field, we replace the Softmax function with the\ncarefully designed Logmax function to normalize the attention map of CSA and\nstabilize the gradient propagation. By stacking various layers of CSA blocks,\nwe propose the Fourier Complex Transformer (FCT) model to learn global\ncontextual information from VHR aerial images following the hierarchical\nmanners. Universal experiments conducted on commonly used RS classification\ndata sets demonstrate the effectiveness and efficiency of FCT, especially on\nvery high-resolution RS images.",
    "published": "2022-10-28T08:13:33Z",
    "updated": "2022-10-28T08:13:33Z",
    "authors": [
      "Yan Zhang",
      "Xiyuan Gao",
      "Qingyan Duan",
      "Jiaxu Leng",
      "Xiao Pu",
      "Xinbo Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.03688v2",
    "title": "AxFormer: Accuracy-driven Approximation of Transformers for Faster,\n  Smaller and more Accurate NLP Models",
    "summary": "Transformers have greatly advanced the state-of-the-art in Natural Language\nProcessing (NLP) in recent years, but present very large computation and\nstorage requirements. We observe that the design process of Transformers\n(pre-train a foundation model on a large dataset in a self-supervised manner,\nand subsequently fine-tune it for different downstream tasks) leads to\ntask-specific models that are highly over-parameterized, adversely impacting\nboth accuracy and inference efficiency. We propose AxFormer, a systematic\nframework that applies accuracy-driven approximations to create optimized\ntransformer models for a given downstream task. AxFormer combines two key\noptimizations -- accuracy-driven pruning and selective hard attention.\nAccuracy-driven pruning identifies and removes parts of the fine-tuned\ntransformer that hinder performance on the given downstream task. Sparse\nhard-attention optimizes attention blocks in selected layers by eliminating\nirrelevant word aggregations, thereby helping the model focus only on the\nrelevant parts of the input. In effect, AxFormer leads to models that are more\naccurate, while also being faster and smaller. Our experiments on GLUE and\nSQUAD tasks show that AxFormer models are up to 4.5% more accurate, while also\nbeing up to 2.5X faster and up to 3.2X smaller than conventional fine-tuned\nmodels. In addition, we demonstrate that AxFormer can be combined with previous\nefforts such as distillation or quantization to achieve further efficiency\ngains.",
    "published": "2020-10-07T23:29:34Z",
    "updated": "2022-06-10T01:02:42Z",
    "authors": [
      "Amrit Nagarajan",
      "Sanchari Sen",
      "Jacob R. Stevens",
      "Anand Raghunathan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.07662v1",
    "title": "SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
    "summary": "Recently, transformers have shown great potential in image classification and\nestablished state-of-the-art results on the ImageNet benchmark. However,\ncompared to CNNs, transformers converge slowly and are prone to overfitting in\nlow-data regimes due to the lack of spatial inductive biases. Such spatial\ninductive biases can be especially beneficial since the 2D structure of an\ninput image is not well preserved in transformers. In this work, we present\nSpatial Prior-enhanced Self-Attention (SP-SA), a novel variant of vanilla\nSelf-Attention (SA) tailored for vision transformers. Spatial Priors (SPs) are\nour proposed family of inductive biases that highlight certain groups of\nspatial relations. Unlike convolutional inductive biases, which are forced to\nfocus exclusively on hard-coded local regions, our proposed SPs are learned by\nthe model itself and take a variety of spatial relations into account.\nSpecifically, the attention score is calculated with emphasis on certain kinds\nof spatial relations at each head, and such learned spatial foci can be\ncomplementary to each other. Based on SP-SA we propose the SP-ViT family, which\nconsistently outperforms other ViT models with similar GFlops or parameters.\nOur largest model SP-ViT-L achieves a record-breaking 86.3% Top-1 accuracy with\na reduction in the number of parameters by almost 50% compared to previous\nstate-of-the-art model (150M for SP-ViT-L vs 271M for CaiT-M-36) among all\nImageNet-1K models trained on 224x224 and fine-tuned on 384x384 resolution w/o\nextra data.",
    "published": "2022-06-15T16:54:02Z",
    "updated": "2022-06-15T16:54:02Z",
    "authors": [
      "Yuxuan Zhou",
      "Wangmeng Xiang",
      "Chao Li",
      "Biao Wang",
      "Xihan Wei",
      "Lei Zhang",
      "Margret Keuper",
      "Xiansheng Hua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.08225v1",
    "title": "Graph Transformer GANs for Graph-Constrained House Generation",
    "summary": "We present a novel graph Transformer generative adversarial network (GTGAN)\nto learn effective graph node relations in an end-to-end fashion for the\nchallenging graph-constrained house generation task. The proposed\ngraph-Transformer-based generator includes a novel graph Transformer encoder\nthat combines graph convolutions and self-attentions in a Transformer to model\nboth local and global interactions across connected and non-connected graph\nnodes. Specifically, the proposed connected node attention (CNA) and\nnon-connected node attention (NNA) aim to capture the global relations across\nconnected nodes and non-connected nodes in the input graph, respectively. The\nproposed graph modeling block (GMB) aims to exploit local vertex interactions\nbased on a house layout topology. Moreover, we propose a new node\nclassification-based discriminator to preserve the high-level semantic and\ndiscriminative node features for different house components. Finally, we\npropose a novel graph-based cycle-consistency loss that aims at maintaining the\nrelative spatial relationships between ground truth and predicted graphs.\nExperiments on two challenging graph-constrained house generation tasks (i.e.,\nhouse layout and roof generation) with two public datasets demonstrate the\neffectiveness of GTGAN in terms of objective quantitative scores and subjective\nvisual realism. New state-of-the-art results are established by large margins\non both tasks.",
    "published": "2023-03-14T20:35:45Z",
    "updated": "2023-03-14T20:35:45Z",
    "authors": [
      "Hao Tang",
      "Zhenyu Zhang",
      "Humphrey Shi",
      "Bo Li",
      "Ling Shao",
      "Nicu Sebe",
      "Radu Timofte",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.11950v1",
    "title": "Learning A Sparse Transformer Network for Effective Image Deraining",
    "summary": "Transformers-based methods have achieved significant performance in image\nderaining as they can model the non-local information which is vital for\nhigh-quality image reconstruction. In this paper, we find that most existing\nTransformers usually use all similarities of the tokens from the query-key\npairs for the feature aggregation. However, if the tokens from the query are\ndifferent from those of the key, the self-attention values estimated from these\ntokens also involve in feature aggregation, which accordingly interferes with\nthe clear image restoration. To overcome this problem, we propose an effective\nDeRaining network, Sparse Transformer (DRSformer) that can adaptively keep the\nmost useful self-attention values for feature aggregation so that the\naggregated features better facilitate high-quality image reconstruction.\nSpecifically, we develop a learnable top-k selection operator to adaptively\nretain the most crucial attention scores from the keys for each query for\nbetter feature aggregation. Simultaneously, as the naive feed-forward network\nin Transformers does not model the multi-scale information that is important\nfor latent clear image restoration, we develop an effective mixed-scale\nfeed-forward network to generate better features for image deraining. To learn\nan enriched set of hybrid features, which combines local context from CNN\noperators, we equip our model with mixture of experts feature compensator to\npresent a cooperation refinement deraining scheme. Extensive experimental\nresults on the commonly used benchmarks demonstrate that the proposed method\nachieves favorable performance against state-of-the-art approaches. The source\ncode and trained models are available at\nhttps://github.com/cschenxiang/DRSformer.",
    "published": "2023-03-21T15:41:57Z",
    "updated": "2023-03-21T15:41:57Z",
    "authors": [
      "Xiang Chen",
      "Hao Li",
      "Mingqiang Li",
      "Jinshan Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1912.02615v1",
    "title": "Audiovisual Transformer Architectures for Large-Scale Classification and\n  Synchronization of Weakly Labeled Audio Events",
    "summary": "We tackle the task of environmental event classification by drawing\ninspiration from the transformer neural network architecture used in machine\ntranslation. We modify this attention-based feedforward structure in such a way\nthat allows the resulting model to use audio as well as video to compute sound\nevent predictions. We perform extensive experiments with these adapted\ntransformers on an audiovisual data set, obtained by appending relevant visual\ninformation to an existing large-scale weakly labeled audio collection. The\nemployed multi-label data contains clip-level annotation indicating the\npresence or absence of 17 classes of environmental sounds, and does not include\ntemporal information. We show that the proposed modified transformers strongly\nimprove upon previously introduced models and in fact achieve state-of-the-art\nresults. We also make a compelling case for devoting more attention to research\nin multimodal audiovisual classification by proving the usefulness of visual\ninformation for the task at hand,namely audio event recognition. In addition,\nwe visualize internal attention patterns of the audiovisual transformers and in\ndoing so demonstrate their potential for performing multimodal synchronization.",
    "published": "2019-12-02T15:26:37Z",
    "updated": "2019-12-02T15:26:37Z",
    "authors": [
      "Wim Boes",
      "Hugo Van hamme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.14405v1",
    "title": "Learning Source Phrase Representations for Neural Machine Translation",
    "summary": "The Transformer translation model (Vaswani et al., 2017) based on a\nmulti-head attention mechanism can be computed effectively in parallel and has\nsignificantly pushed forward the performance of Neural Machine Translation\n(NMT). Though intuitively the attentional network can connect distant words via\nshorter network paths than RNNs, empirical analysis demonstrates that it still\nhas difficulty in fully capturing long-distance dependencies (Tang et al.,\n2018). Considering that modeling phrases instead of words has significantly\nimproved the Statistical Machine Translation (SMT) approach through the use of\nlarger translation blocks (\"phrases\") and its reordering ability, modeling NMT\nat phrase level is an intuitive proposal to help the model capture\nlong-distance relationships. In this paper, we first propose an attentive\nphrase representation generation mechanism which is able to generate phrase\nrepresentations from corresponding token representations. In addition, we\nincorporate the generated phrase representations into the Transformer\ntranslation model to enhance its ability to capture long-distance\nrelationships. In our experiments, we obtain significant improvements on the\nWMT 14 English-German and English-French tasks on top of the strong Transformer\nbaseline, which shows the effectiveness of our approach. Our approach helps\nTransformer Base models perform at the level of Transformer Big models, and\neven significantly better for long sentences, but with substantially fewer\nparameters and training steps. The fact that phrase representations help even\nin the big setting further supports our conjecture that they make a valuable\ncontribution to long-distance relations.",
    "published": "2020-06-25T13:43:11Z",
    "updated": "2020-06-25T13:43:11Z",
    "authors": [
      "Hongfei Xu",
      "Josef van Genabith",
      "Deyi Xiong",
      "Qiuhui Liu",
      "Jingyi Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.13677v5",
    "title": "ResT: An Efficient Transformer for Visual Recognition",
    "summary": "This paper presents an efficient multi-scale vision Transformer, called ResT,\nthat capably served as a general-purpose backbone for image recognition. Unlike\nexisting Transformer methods, which employ standard Transformer blocks to\ntackle raw images with a fixed resolution, our ResT have several advantages:\n(1) A memory-efficient multi-head self-attention is built, which compresses the\nmemory by a simple depth-wise convolution, and projects the interaction across\nthe attention-heads dimension while keeping the diversity ability of\nmulti-heads; (2) Position encoding is constructed as spatial attention, which\nis more flexible and can tackle with input images of arbitrary size without\ninterpolation or fine-tune; (3) Instead of the straightforward tokenization at\nthe beginning of each stage, we design the patch embedding as a stack of\noverlapping convolution operation with stride on the 2D-reshaped token map. We\ncomprehensively validate ResT on image classification and downstream tasks.\nExperimental results show that the proposed ResT can outperform the recently\nstate-of-the-art backbones by a large margin, demonstrating the potential of\nResT as strong backbones. The code and models will be made publicly available\nat https://github.com/wofmanaf/ResT.",
    "published": "2021-05-28T08:53:54Z",
    "updated": "2021-10-14T08:43:50Z",
    "authors": [
      "Qinglong Zhang",
      "Yubin Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.02034v2",
    "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token\n  Sparsification",
    "summary": "Attention is sparse in vision transformers. We observe the final prediction\nin vision transformers is only based on a subset of most informative tokens,\nwhich is sufficient for accurate image recognition. Based on this observation,\nwe propose a dynamic token sparsification framework to prune redundant tokens\nprogressively and dynamically based on the input. Specifically, we devise a\nlightweight prediction module to estimate the importance score of each token\ngiven the current features. The module is added to different layers to prune\nredundant tokens hierarchically. To optimize the prediction module in an\nend-to-end manner, we propose an attention masking strategy to differentiably\nprune a token by blocking its interactions with other tokens. Benefiting from\nthe nature of self-attention, the unstructured sparse tokens are still hardware\nfriendly, which makes our framework easy to achieve actual speed-up. By\nhierarchically pruning 66% of the input tokens, our method greatly reduces\n31%~37% FLOPs and improves the throughput by over 40% while the drop of\naccuracy is within 0.5% for various vision transformers. Equipped with the\ndynamic token sparsification framework, DynamicViT models can achieve very\ncompetitive complexity/accuracy trade-offs compared to state-of-the-art CNNs\nand vision transformers on ImageNet. Code is available at\nhttps://github.com/raoyongming/DynamicViT",
    "published": "2021-06-03T17:57:41Z",
    "updated": "2021-10-26T13:37:53Z",
    "authors": [
      "Yongming Rao",
      "Wenliang Zhao",
      "Benlin Liu",
      "Jiwen Lu",
      "Jie Zhou",
      "Cho-Jui Hsieh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.06411v3",
    "title": "Zero-Shot Controlled Generation with Encoder-Decoder Transformers",
    "summary": "Controlling neural network-based models for natural language generation (NLG)\nhas broad applications in numerous areas such as machine translation, document\nsummarization, and dialog systems. Approaches that enable such control in a\nzero-shot manner would be of great importance as, among other reasons, they\nremove the need for additional annotated data and training. In this work, we\npropose novel approaches for controlling encoder-decoder transformer-based NLG\nmodels in zero-shot. This is done by introducing three control knobs, namely,\nattention biasing, decoder mixing, and context augmentation, that are applied\nto these models at generation time. These knobs control the generation process\nby directly manipulating trained NLG models (e.g., biasing cross-attention\nlayers) to realize the desired attributes in the generated outputs. We show\nthat not only are these NLG models robust to such manipulations, but also their\nbehavior could be controlled without an impact on their generation performance.\nThese results, to the best of our knowledge, are the first of their kind.\nThrough these control knobs, we also investigate the role of transformer\ndecoder's self-attention module and show strong evidence that its primary role\nis maintaining fluency of sentences generated by these models. Based on this\nhypothesis, we show that alternative architectures for transformer decoders\ncould be viable options. We also study how this hypothesis could lead to more\nefficient ways for training encoder-decoder transformer models.",
    "published": "2021-06-11T14:07:19Z",
    "updated": "2022-04-06T19:03:26Z",
    "authors": [
      "Devamanyu Hazarika",
      "Mahdi Namazifar",
      "Dilek Hakkani-TÃ¼r"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.09681v2",
    "title": "XCiT: Cross-Covariance Image Transformers",
    "summary": "Following their success in natural language processing, transformers have\nrecently shown much promise for computer vision. The self-attention operation\nunderlying transformers yields global interactions between all tokens ,i.e.\nwords or image patches, and enables flexible modelling of image data beyond the\nlocal interactions of convolutions. This flexibility, however, comes with a\nquadratic complexity in time and memory, hindering application to long\nsequences and high-resolution images. We propose a \"transposed\" version of\nself-attention that operates across feature channels rather than tokens, where\nthe interactions are based on the cross-covariance matrix between keys and\nqueries. The resulting cross-covariance attention (XCA) has linear complexity\nin the number of tokens, and allows efficient processing of high-resolution\nimages. Our cross-covariance image transformer (XCiT) is built upon XCA. It\ncombines the accuracy of conventional transformers with the scalability of\nconvolutional architectures. We validate the effectiveness and generality of\nXCiT by reporting excellent results on multiple vision benchmarks, including\nimage classification and self-supervised feature learning on ImageNet-1k,\nobject detection and instance segmentation on COCO, and semantic segmentation\non ADE20k.",
    "published": "2021-06-17T17:33:35Z",
    "updated": "2021-06-18T15:33:31Z",
    "authors": [
      "Alaaeldin El-Nouby",
      "Hugo Touvron",
      "Mathilde Caron",
      "Piotr Bojanowski",
      "Matthijs Douze",
      "Armand Joulin",
      "Ivan Laptev",
      "Natalia Neverova",
      "Gabriel Synnaeve",
      "Jakob Verbeek",
      "HervÃ© Jegou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.02239v4",
    "title": "Vision Xformers: Efficient Attention for Image Classification",
    "summary": "Although transformers have become the neural architectures of choice for\nnatural language processing, they require orders of magnitude more training\ndata, GPU memory, and computations in order to compete with convolutional\nneural networks for computer vision. The attention mechanism of transformers\nscales quadratically with the length of the input sequence, and unrolled images\nhave long sequence lengths. Plus, transformers lack an inductive bias that is\nappropriate for images. We tested three modifications to vision transformer\n(ViT) architectures that address these shortcomings. Firstly, we alleviate the\nquadratic bottleneck by using linear attention mechanisms, called X-formers\n(such that, X in {Performer, Linformer, Nystr\\\"omformer}), thereby creating\nVision X-formers (ViXs). This resulted in up to a seven times reduction in the\nGPU memory requirement. We also compared their performance with FNet and\nmulti-layer perceptron mixers, which further reduced the GPU memory\nrequirement. Secondly, we introduced an inductive bias for images by replacing\nthe initial linear embedding layer by convolutional layers in ViX, which\nsignificantly increased classification accuracy without increasing the model\nsize. Thirdly, we replaced the learnable 1D position embeddings in ViT with\nRotary Position Embedding (RoPE), which increases the classification accuracy\nfor the same model size. We believe that incorporating such changes can\ndemocratize transformers by making them accessible to those with limited data\nand computing resources.",
    "published": "2021-07-05T19:24:23Z",
    "updated": "2021-10-01T15:08:54Z",
    "authors": [
      "Pranav Jeevan",
      "Amit Sethi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.13845v1",
    "title": "Raw Produce Quality Detection with Shifted Window Self-Attention",
    "summary": "Global food insecurity is expected to worsen in the coming decades with the\naccelerated rate of climate change and the rapidly increasing population. In\nthis vein, it is important to remove inefficiencies at every level of food\nproduction. The recent advances in deep learning can help reduce such\ninefficiencies, yet their application has not yet become mainstream throughout\nthe industry, inducing economic costs at a massive scale. To this point, modern\ntechniques such as CNNs (Convolutional Neural Networks) have been applied to\nRPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's\nsuccessful debut in the vision among other modalities led us to expect a better\nperformance with these Transformer-based models in RPQD. In this work, we\nexclusively investigate the recent state-of-the-art Swin (Shifted Windows)\nTransformer which computes self-attention in both intra- and inter-window\nfashion. We compare Swin Transformer against CNN models on four RPQD image\ndatasets, each containing different kinds of raw produce: fruits and\nvegetables, fish, pork, and beef. We observe that Swin Transformer not only\nachieves better or competitive performance but also is data- and\ncompute-efficient, making it ideal for actual deployment in real-world setting.\nTo the best of our knowledge, this is the first large-scale empirical study on\nRPQD task, which we hope will gain more attention in future works.",
    "published": "2021-12-24T10:16:28Z",
    "updated": "2021-12-24T10:16:28Z",
    "authors": [
      "Oh Joon Kwon",
      "Byungsoo Kim",
      "Youngduck Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.10247v2",
    "title": "HIPA: Hierarchical Patch Transformer for Single Image Super Resolution",
    "summary": "Transformer-based architectures start to emerge in single image super\nresolution (SISR) and have achieved promising performance. Most existing Vision\nTransformers divide images into the same number of patches with a fixed size,\nwhich may not be optimal for restoring patches with different levels of texture\nrichness. This paper presents HIPA, a novel Transformer architecture that\nprogressively recovers the high resolution image using a hierarchical patch\npartition. Specifically, we build a cascaded model that processes an input\nimage in multiple stages, where we start with tokens with small patch sizes and\ngradually merge to the full resolution. Such a hierarchical patch mechanism not\nonly explicitly enables feature aggregation at multiple resolutions but also\nadaptively learns patch-aware features for different image regions, e.g., using\na smaller patch for areas with fine details and a larger patch for textureless\nregions. Meanwhile, a new attention-based position encoding scheme for\nTransformer is proposed to let the network focus on which tokens should be paid\nmore attention by assigning different weights to different tokens, which is the\nfirst time to our best knowledge. Furthermore, we also propose a new\nmulti-reception field attention module to enlarge the convolution reception\nfield from different branches. The experimental results on several public\ndatasets demonstrate the superior performance of the proposed HIPA over\nprevious methods quantitatively and qualitatively.",
    "published": "2022-03-19T05:09:34Z",
    "updated": "2023-06-07T01:39:31Z",
    "authors": [
      "Qing Cai",
      "Yiming Qian",
      "Jinxing Li",
      "Jun Lv",
      "Yee-Hong Yang",
      "Feng Wu",
      "David Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.15380v4",
    "title": "SepViT: Separable Vision Transformer",
    "summary": "Vision Transformers have witnessed prevailing success in a series of vision\ntasks. However, these Transformers often rely on extensive computational costs\nto achieve high performance, which is burdensome to deploy on\nresource-constrained devices. To alleviate this issue, we draw lessons from\ndepthwise separable convolution and imitate its ideology to design an efficient\nTransformer backbone, i.e., Separable Vision Transformer, abbreviated as\nSepViT. SepViT helps to carry out the local-global information interaction\nwithin and among the windows in sequential order via a depthwise separable\nself-attention. The novel window token embedding and grouped self-attention are\nemployed to compute the attention relationship among windows with negligible\ncost and establish long-range visual interactions across multiple windows,\nrespectively. Extensive experiments on general-purpose vision benchmarks\ndemonstrate that SepViT can achieve a state-of-the-art trade-off between\nperformance and latency. Among them, SepViT achieves 84.2% top-1 accuracy on\nImageNet-1K classification while decreasing the latency by 40%, compared to the\nones with similar accuracy (e.g., CSWin). Furthermore, SepViT achieves 51.0%\nmIoU on ADE20K semantic segmentation task, 47.9 AP on the RetinaNet-based COCO\ndetection task, 49.4 box AP and 44.6 mask AP on Mask R-CNN-based COCO object\ndetection and instance segmentation tasks.",
    "published": "2022-03-29T09:20:01Z",
    "updated": "2023-06-15T16:37:26Z",
    "authors": [
      "Wei Li",
      "Xing Wang",
      "Xin Xia",
      "Jie Wu",
      "Jiashi Li",
      "Xuefeng Xiao",
      "Min Zheng",
      "Shiping Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.16896v1",
    "title": "CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow",
    "summary": "Optical flow estimation aims to find the 2D motion field by identifying\ncorresponding pixels between two images. Despite the tremendous progress of\ndeep learning-based optical flow methods, it remains a challenge to accurately\nestimate large displacements with motion blur. This is mainly because the\ncorrelation volume, the basis of pixel matching, is computed as the dot product\nof the convolutional features of the two images. The locality of convolutional\nfeatures makes the computed correlations susceptible to various noises. On\nlarge displacements with motion blur, noisy correlations could cause severe\nerrors in the estimated flow. To overcome this challenge, we propose a new\narchitecture \"CRoss-Attentional Flow Transformer\" (CRAFT), aiming to revitalize\nthe correlation volume computation. In CRAFT, a Semantic Smoothing Transformer\nlayer transforms the features of one frame, making them more global and\nsemantically stable. In addition, the dot-product correlations are replaced\nwith transformer Cross-Frame Attention. This layer filters out feature noises\nthrough the Query and Key projections, and computes more accurate correlations.\nOn Sintel (Final) and KITTI (foreground) benchmarks, CRAFT has achieved new\nstate-of-the-art performance. Moreover, to test the robustness of different\nmodels on large motions, we designed an image shifting attack that shifts input\nimages to generate large artificial motions. Under this attack, CRAFT performs\nmuch more robustly than two representative methods, RAFT and GMA. The code of\nCRAFT is is available at https://github.com/askerlee/craft.",
    "published": "2022-03-31T09:05:00Z",
    "updated": "2022-03-31T09:05:00Z",
    "authors": [
      "Xiuchao Sui",
      "Shaohua Li",
      "Xue Geng",
      "Yan Wu",
      "Xinxing Xu",
      "Yong Liu",
      "Rick Goh",
      "Hongyuan Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.10447v2",
    "title": "Weakly Supervised Object Localization via Transformer with Implicit\n  Spatial Calibration",
    "summary": "Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Recent studies leverage the advantage\nof self-attention in visual Transformer for long-range dependency to re-active\nsemantic regions, aiming to avoid partial activation in traditional class\nactivation mapping (CAM). However, the long-range modeling in Transformer\nneglects the inherent spatial coherence of the object, and it usually diffuses\nthe semantic-aware regions far from the object boundary, making localization\nresults significantly larger or far smaller. To address such an issue, we\nintroduce a simple yet effective Spatial Calibration Module (SCM) for accurate\nWSOL, incorporating semantic similarities of patch tokens and their spatial\nrelationships into a unified diffusion model. Specifically, we introduce a\nlearnable parameter to dynamically adjust the semantic correlations and spatial\ncontext intensities for effective information propagation. In practice, SCM is\ndesigned as an external module of Transformer, and can be removed during\ninference to reduce the computation cost. The object-sensitive localization\nability is implicitly embedded into the Transformer encoder through\noptimization in the training phase. It enables the generated attention maps to\ncapture the sharper object boundaries and filter the object-irrelevant\nbackground area. Extensive experimental results demonstrate the effectiveness\nof the proposed method, which significantly outperforms its counterpart TS-CAM\non both CUB-200 and ImageNet-1K benchmarks. The code is available at\nhttps://github.com/164140757/SCM.",
    "published": "2022-07-21T12:37:15Z",
    "updated": "2023-03-10T09:53:56Z",
    "authors": [
      "Haotian Bai",
      "Ruimao Zhang",
      "Jiong Wang",
      "Xiang Wan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.07384v2",
    "title": "Language models are good pathologists: using attention-based sequence\n  reduction and text-pretrained transformers for efficient WSI classification",
    "summary": "In digital pathology, Whole Slide Image (WSI) analysis is usually formulated\nas a Multiple Instance Learning (MIL) problem. Although transformer-based\narchitectures have been used for WSI classification, these methods require\nmodifications to adapt them to specific challenges of this type of image data.\nAmong these challenges is the amount of memory and compute required by deep\ntransformer models to process long inputs, such as the thousands of image\npatches that can compose a WSI at $\\times 10$ or $\\times 20$ magnification. We\nintroduce \\textit{SeqShort}, a multi-head attention-based sequence shortening\nlayer to summarize each WSI in a fixed- and short-sized sequence of instances,\nthat allows us to reduce the computational costs of self-attention on long\nsequences, and to include positional information that is unavailable in other\nMIL approaches. Furthermore, we show that WSI classification performance can be\nimproved when the downstream transformer architecture has been pre-trained on a\nlarge corpus of text data, and only fine-tuning less than 0.1\\% of its\nparameters. We demonstrate the effectiveness of our method in lymph node\nmetastases classification and cancer subtype classification tasks, without the\nneed of designing a WSI-specific transformer nor doing in-domain pre-training,\nkeeping a reduced compute budget and low number of trainable parameters.",
    "published": "2022-11-14T14:11:31Z",
    "updated": "2023-09-30T21:26:44Z",
    "authors": [
      "Juan I. Pisula",
      "Katarzyna Bozek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.10048v2",
    "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
    "summary": "Transformers have been widely used for video processing owing to the\nmulti-head self attention (MHSA) mechanism. However, the MHSA mechanism\nencounters an intrinsic difficulty for video inpainting, since the features\nassociated with the corrupted regions are degraded and incur inaccurate self\nattention. This problem, termed query degradation, may be mitigated by first\ncompleting optical flows and then using the flows to guide the self attention,\nwhich was verified in our previous work - flow-guided transformer (FGT). We\nfurther exploit the flow guidance and propose FGT++ to pursue more effective\nand efficient video inpainting. First, we design a lightweight flow completion\nnetwork by using local aggregation and edge loss. Second, to address the query\ndegradation, we propose a flow guidance feature integration module, which uses\nthe motion discrepancy to enhance the features, together with a flow-guided\nfeature propagation module that warps the features according to the flows.\nThird, we decouple the transformer along the temporal and spatial dimensions,\nwhere flows are used to select the tokens through a temporally deformable MHSA\nmechanism, and global tokens are combined with the inner-window local tokens\nthrough a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to\nbe outperforming the existing video inpainting networks qualitatively and\nquantitatively.",
    "published": "2023-01-24T14:44:44Z",
    "updated": "2024-03-19T04:02:28Z",
    "authors": [
      "Kaidong Zhang",
      "Jialun Peng",
      "Jingjing Fu",
      "Dong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.19585v1",
    "title": "LAIT: Efficient Multi-Segment Encoding in Transformers with\n  Layer-Adjustable Interaction",
    "summary": "Transformer encoders contextualize token representations by attending to all\nother tokens at each layer, leading to quadratic increase in compute effort\nwith the input length. In practice, however, the input text of many NLP tasks\ncan be seen as a sequence of related segments (e.g., the sequence of sentences\nwithin a passage, or the hypothesis and premise in NLI). While attending across\nthese segments is highly beneficial for many tasks, we hypothesize that this\ninteraction can be delayed until later encoding stages.\n  To this end, we introduce Layer-Adjustable Interactions in Transformers\n(LAIT). Within LAIT, segmented inputs are first encoded independently, and then\njointly. This partial two-tower architecture bridges the gap between a Dual\nEncoder's ability to pre-compute representations for segments and a fully\nself-attentive Transformer's capacity to model cross-segment attention. The\nLAIT framework effectively leverages existing pretrained Transformers and\nconverts them into the hybrid of the two aforementioned architectures, allowing\nfor easy and intuitive control over the performance-efficiency tradeoff.\nExperimenting on a wide range of NLP tasks, we find LAIT able to reduce 30-50%\nof the attention FLOPs on many tasks, while preserving high accuracy; in some\npractical settings, LAIT could reduce actual latency by orders of magnitude.",
    "published": "2023-05-31T06:09:59Z",
    "updated": "2023-05-31T06:09:59Z",
    "authors": [
      "Jeremiah Milbauer",
      "Annie Louis",
      "Mohammad Javad Hosseini",
      "Alex Fabrikant",
      "Donald Metzler",
      "Tal Schuster"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.12098v1",
    "title": "MSW-Transformer: Multi-Scale Shifted Windows Transformer Networks for\n  12-Lead ECG Classification",
    "summary": "Automatic classification of electrocardiogram (ECG) signals plays a crucial\nrole in the early prevention and diagnosis of cardiovascular diseases. While\nECG signals can be used for the diagnosis of various diseases, their\npathological characteristics exhibit minimal variations, posing a challenge to\nautomatic classification models. Existing methods primarily utilize\nconvolutional neural networks to extract ECG signal features for\nclassification, which may not fully capture the pathological feature\ndifferences of different diseases. Transformer networks have advantages in\nfeature extraction for sequence data, but the complete network is complex and\nrelies on large-scale datasets. To address these challenges, we propose a\nsingle-layer Transformer network called Multi-Scale Shifted Windows Transformer\nNetworks (MSW-Transformer), which uses a multi-window sliding attention\nmechanism at different scales to capture features in different dimensions. The\nself-attention is restricted to non-overlapping local windows via shifted\nwindows, and different window scales have different receptive fields. A\nlearnable feature fusion method is then proposed to integrate features from\ndifferent windows to further enhance model performance. Furthermore, we\nvisualize the attention mechanism of the multi-window shifted mechanism to\nachieve better clinical interpretation in the ECG classification task. The\nproposed model achieves state-of-the-art performance on five classification\ntasks of the PTBXL-2020 12-lead ECG dataset, which includes 5 diagnostic\nsuperclasses, 23 diagnostic subclasses, 12 rhythm classes, 17 morphology\nclasses, and 44 diagnosis classes, with average macro-F1 scores of 77.85%,\n47.57%, 66.13%, 34.60%, and 34.29%, and average sample-F1 scores of 81.26%,\n68.27%, 91.32%, 50.07%, and 63.19%, respectively.",
    "published": "2023-06-21T08:27:26Z",
    "updated": "2023-06-21T08:27:26Z",
    "authors": [
      "Renjie Cheng",
      "Zhemin Zhuang",
      "Shuxin Zhuang",
      "Lei Xie",
      "Jingfeng Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.13501v1",
    "title": "Knowledge-Infused Self Attention Transformers",
    "summary": "Transformer-based language models have achieved impressive success in various\nnatural language processing tasks due to their ability to capture complex\ndependencies and contextual information using self-attention mechanisms.\nHowever, they are not without limitations. These limitations include\nhallucinations, where they produce incorrect outputs with high confidence, and\nalignment issues, where they generate unhelpful and unsafe outputs for human\nusers. These limitations stem from the absence of implicit and missing context\nin the data alone. To address this, researchers have explored augmenting these\nmodels with external knowledge from knowledge graphs to provide the necessary\nadditional context. However, the ad-hoc nature of existing methods makes it\ndifficult to properly analyze the effects of knowledge infusion on the many\nmoving parts or components of a transformer. This paper introduces a systematic\nmethod for infusing knowledge into different components of a transformer-based\nmodel. A modular framework is proposed to identify specific components within\nthe transformer architecture, such as the self-attention mechanism, encoder\nlayers, or the input embedding layer, where knowledge infusion can be applied.\nAdditionally, extensive experiments are conducted on the General Language\nUnderstanding Evaluation (GLUE) benchmark tasks, and the findings are reported.\nThis systematic approach aims to facilitate more principled approaches to\nincorporating knowledge into language model architectures.",
    "published": "2023-06-23T13:55:01Z",
    "updated": "2023-06-23T13:55:01Z",
    "authors": [
      "Kaushik Roy",
      "Yuxin Zi",
      "Vignesh Narayanan",
      "Manas Gaur",
      "Amit Sheth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.02557v2",
    "title": "Attention-free Spikformer: Mixing Spike Sequences with Simple Linear\n  Transforms",
    "summary": "By integrating the self-attention capability and the biological properties of\nSpiking Neural Networks (SNNs), Spikformer applies the flourishing Transformer\narchitecture to SNNs design. It introduces a Spiking Self-Attention (SSA)\nmodule to mix sparse visual features using spike-form Query, Key, and Value,\nresulting in the State-Of-The-Art (SOTA) performance on numerous datasets\ncompared to previous SNN-like frameworks. In this paper, we demonstrate that\nthe Spikformer architecture can be accelerated by replacing the SSA with an\nunparameterized Linear Transform (LT) such as Fourier and Wavelet transforms.\nThese transforms are utilized to mix spike sequences, reducing the quadratic\ntime complexity to log-linear time complexity. They alternate between the\nfrequency and time domains to extract sparse visual features, showcasing\npowerful performance and efficiency. We conduct extensive experiments on image\nclassification using both neuromorphic and static datasets. The results\nindicate that compared to the SOTA Spikformer with SSA, Spikformer with LT\nachieves higher Top-1 accuracy on neuromorphic datasets (i.e., CIFAR10-DVS and\nDVS128 Gesture) and comparable Top-1 accuracy on static datasets (i.e.,\nCIFAR-10 and CIFAR-100). Furthermore, Spikformer with LT achieves approximately\n29-51% improvement in training speed, 61-70% improvement in inference speed,\nand reduces memory usage by 4-26% due to not requiring learnable parameters.",
    "published": "2023-08-02T11:41:54Z",
    "updated": "2023-08-17T06:12:24Z",
    "authors": [
      "Qingyu Wang",
      "Duzhen Zhang",
      "Tielin Zhang",
      "Bo Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.14036v2",
    "title": "MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor\n  Formula for Image Dehazing",
    "summary": "In recent years, Transformer networks are beginning to replace pure\nconvolutional neural networks (CNNs) in the field of computer vision due to\ntheir global receptive field and adaptability to input. However, the quadratic\ncomputational complexity of softmax-attention limits the wide application in\nimage dehazing task, especially for high-resolution images. To address this\nissue, we propose a new Transformer variant, which applies the Taylor expansion\nto approximate the softmax-attention and achieves linear computational\ncomplexity. A multi-scale attention refinement module is proposed as a\ncomplement to correct the error of the Taylor expansion. Furthermore, we\nintroduce a multi-branch architecture with multi-scale patch embedding to the\nproposed Transformer, which embeds features by overlapping deformable\nconvolution of different scales. The design of multi-scale patch embedding is\nbased on three key ideas: 1) various sizes of the receptive field; 2)\nmulti-level semantic information; 3) flexible shapes of the receptive field.\nOur model, named Multi-branch Transformer expanded by Taylor formula\n(MB-TaylorFormer), can embed coarse to fine features more flexibly at the patch\nembedding stage and capture long-distance pixel interactions with limited\ncomputational cost. Experimental results on several dehazing benchmarks show\nthat MB-TaylorFormer achieves state-of-the-art (SOTA) performance with a light\ncomputational burden. The source code and pre-trained models are available at\nhttps://github.com/FVL2020/ICCV-2023-MB-TaylorFormer.",
    "published": "2023-08-27T08:10:23Z",
    "updated": "2023-08-30T13:27:35Z",
    "authors": [
      "Yuwei Qiu",
      "Kaihao Zhang",
      "Chenxi Wang",
      "Wenhan Luo",
      "Hongdong Li",
      "Zhi Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.04157v3",
    "title": "A Simple Interpretable Transformer for Fine-Grained Image Classification\n  and Analysis",
    "summary": "We present a novel usage of Transformers to make image classification\ninterpretable. Unlike mainstream classifiers that wait until the last fully\nconnected layer to incorporate class information to make predictions, we\ninvestigate a proactive approach, asking each class to search for itself in an\nimage. We realize this idea via a Transformer encoder-decoder inspired by\nDEtection TRansformer (DETR). We learn \"class-specific\" queries (one for each\nclass) as input to the decoder, enabling each class to localize its patterns in\nan image via cross-attention. We name our approach INterpretable TRansformer\n(INTR), which is fairly easy to implement and exhibits several compelling\nproperties. We show that INTR intrinsically encourages each class to attend\ndistinctively; the cross-attention weights thus provide a faithful\ninterpretation of the prediction. Interestingly, via \"multi-head\"\ncross-attention, INTR could identify different \"attributes\" of a class, making\nit particularly suitable for fine-grained classification and analysis, which we\ndemonstrate on eight datasets. Our code and pre-trained models are publicly\naccessible at the Imageomics Institute GitHub site:\nhttps://github.com/Imageomics/INTR.",
    "published": "2023-11-07T17:32:55Z",
    "updated": "2024-06-14T17:28:14Z",
    "authors": [
      "Dipanjyoti Paul",
      "Arpita Chowdhury",
      "Xinqi Xiong",
      "Feng-Ju Chang",
      "David Carlyn",
      "Samuel Stevens",
      "Kaiya L. Provost",
      "Anuj Karpatne",
      "Bryan Carstens",
      "Daniel Rubenstein",
      "Charles Stewart",
      "Tanya Berger-Wolf",
      "Yu Su",
      "Wei-Lun Chao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.07128v5",
    "title": "MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image\n  Segmentation",
    "summary": "Although transformer is preferred in natural language processing, some\nstudies has only been applied to the field of medical imaging in recent years.\nFor its long-term dependency, the transformer is expected to contribute to\nunconventional convolution neural net conquer their inherent spatial induction\nbias. The lately suggested transformer-based segmentation method only uses the\ntransformer as an auxiliary module to help encode the global context into a\nconvolutional representation. How to optimally integrate self-attention with\nconvolution has not been investigated in depth. To solve the problem, this\npaper proposes MS-Twins (Multi-Scale Twins), which is a powerful segmentation\nmodel on account of the bond of self-attention and convolution. MS-Twins can\nbetter capture semantic and fine-grained information by combining different\nscales and cascading features. Compared with the existing network structure,\nMS-Twins has made progress on the previous method based on the transformer of\ntwo in common use data sets, Synapse and ACDC. In particular, the performance\nof MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet,\nthe best entirely convoluted medical image segmentation network, the\nperformance of MS-Twins on Synapse and ACDC still has a bit advantage.",
    "published": "2023-12-12T10:04:11Z",
    "updated": "2024-09-16T17:40:31Z",
    "authors": [
      "Jing Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.11303v3",
    "title": "FViT: A Focal Vision Transformer with Gabor Filter",
    "summary": "Vision transformers have achieved encouraging progress in various computer\nvision tasks. A common belief is that this is attributed to the capability of\nself-attention in modeling the global dependencies among feature tokens.\nHowever, self-attention still faces several challenges in dense prediction\ntasks, including high computational complexity and absence of desirable\ninductive bias. To alleviate these issues, the potential advantages of\ncombining vision transformers with Gabor filters are revisited, and a learnable\nGabor filter (LGF) using convolution is proposed. The LGF does not rely on\nself-attention, and it is used to simulate the response of fundamental cells in\nthe biological visual system to the input images. This encourages vision\ntransformers to focus on discriminative feature representations of targets\nacross different scales and orientations. In addition, a Bionic Focal Vision\n(BFV) block is designed based on the LGF. This block draws inspiration from\nneuroscience and introduces a Dual-Path Feed Forward Network (DPFFN) to emulate\nthe parallel and cascaded information processing scheme of the biological\nvisual cortex. Furthermore, a unified and efficient family of pyramid backbone\nnetworks called Focal Vision Transformers (FViTs) is developed by stacking BFV\nblocks. Experimental results indicate that FViTs demonstrate superior\nperformance in various vision tasks. In terms of computational efficiency and\nscalability, FViTs show significant advantages compared with other\ncounterparts.",
    "published": "2024-02-17T15:03:25Z",
    "updated": "2025-01-21T14:40:56Z",
    "authors": [
      "Yulong Shi",
      "Mingwei Sun",
      "Yongshuai Wang",
      "Zengqiang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.11582v2",
    "title": "SLAB: Efficient Transformers with Simplified Linear Attention and\n  Progressive Re-parameterized Batch Normalization",
    "summary": "Transformers have become foundational architectures for both natural language\nand computer vision tasks. However, the high computational cost makes it quite\nchallenging to deploy on resource-constraint devices. This paper investigates\nthe computational bottleneck modules of efficient transformer, i.e.,\nnormalization layers and attention modules. LayerNorm is commonly used in\ntransformer architectures but is not computational friendly due to statistic\ncalculation during inference. However, replacing LayerNorm with more efficient\nBatchNorm in transformer often leads to inferior performance and collapse in\ntraining. To address this problem, we propose a novel method named PRepBN to\nprogressively replace LayerNorm with re-parameterized BatchNorm in training.\nMoreover, we propose a simplified linear attention (SLA) module that is simple\nyet effective to achieve strong performance. Extensive experiments on image\nclassification as well as object detection demonstrate the effectiveness of our\nproposed method. For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on\nImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of\nFlatten-Swin with $0.1\\%$ higher accuracy. We also evaluated our method for\nlanguage modeling task and obtain comparable performance and lower\nlatency.Codes are publicly available at https://github.com/xinghaochen/SLAB and\nhttps://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.",
    "published": "2024-05-19T15:22:25Z",
    "updated": "2024-06-17T05:59:24Z",
    "authors": [
      "Jialong Guo",
      "Xinghao Chen",
      "Yehui Tang",
      "Yunhe Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15953v3",
    "title": "Activator: GLU Activation Function as the Core Component of a Vision\n  Transformer",
    "summary": "The transformer architecture has driven many successes in a variety of tasks\nwithin the field of deep learning, in particular the recent advances in natural\nlanguage processing (NLP) culminating with large language models (LLM). Adding\nto that success, transformer architecture has found widespread interest from\ncomputer vision (CV) researchers and practitioners, allowing for many\nadvancements in vision-related tasks and opening the door for multitask and\nmulti-modal deep learning architectures that share the same principle of\noperation. One drawback to these architectures is their reliance on the scaled\ndot product attention mechanism with the softmax activation function, which is\ncomputationally expensive and requires large compute capabilities for both\ntraining and inference. This paper investigates substituting the MLP and\nattention mechanism usually adopted for transformer architecture with an\narchitecture based on incorporating a gated linear unit (GLU) activation\nfunction structure with the aim of reducing the computational cost. The\nequalized experimental assessments conducted in this work show that the\nproposed modification with the targeted reductions in computational complexity\noffers competitive performance compared to the selected baseline architectures.\nThe results are significantly in support of the aims of this work, in which the\nfocus was to extensively utilize GLU-based MLPs, establishing a more efficient\nbut capable alternative to the traditional MLP and the attention mechanism as\nthe core component in the design of transformer architectures.",
    "published": "2024-05-24T21:46:52Z",
    "updated": "2025-07-26T21:37:44Z",
    "authors": [
      "Abdullah Nazhat Abdullah",
      "Tarkan Aydin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.20008v2",
    "title": "Sharing Key Semantics in Transformer Makes Efficient Image Restoration",
    "summary": "Image Restoration (IR), a classic low-level vision task, has witnessed\nsignificant advancements through deep models that effectively model global\ninformation. Notably, the emergence of Vision Transformers (ViTs) has further\npropelled these advancements. When computing, the self-attention mechanism, a\ncornerstone of ViTs, tends to encompass all global cues, even those from\nsemantically unrelated objects or regions. This inclusivity introduces\ncomputational inefficiencies, particularly noticeable with high input\nresolution, as it requires processing irrelevant information, thereby impeding\nefficiency. Additionally, for IR, it is commonly noted that small segments of a\ndegraded image, particularly those closely aligned semantically, provide\nparticularly relevant information to aid in the restoration process, as they\ncontribute essential contextual cues crucial for accurate reconstruction. To\naddress these challenges, we propose boosting IR's performance by sharing the\nkey semantics via Transformer for IR (\\ie, SemanIR) in this paper.\nSpecifically, SemanIR initially constructs a sparse yet comprehensive\nkey-semantic dictionary within each transformer stage by establishing essential\nsemantic connections for every degraded patch. Subsequently, this dictionary is\nshared across all subsequent transformer blocks within the same stage. This\nstrategy optimizes attention calculation within each block by focusing\nexclusively on semantically related components stored in the key-semantic\ndictionary. As a result, attention calculation achieves linear computational\ncomplexity within each window. Extensive experiments across 6 IR tasks confirm\nthe proposed SemanIR's state-of-the-art performance, quantitatively and\nqualitatively showcasing advancements. The visual results, code, and trained\nmodels are available at https://github.com/Amazingren/SemanIR.",
    "published": "2024-05-30T12:45:34Z",
    "updated": "2024-12-18T13:54:02Z",
    "authors": [
      "Bin Ren",
      "Yawei Li",
      "Jingyun Liang",
      "Rakesh Ranjan",
      "Mengyuan Liu",
      "Rita Cucchiara",
      "Luc Van Gool",
      "Ming-Hsuan Yang",
      "Nicu Sebe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.20799v3",
    "title": "Rough Transformers: Lightweight and Continuous Time Series Modelling\n  through Signature Patching",
    "summary": "Time-series data in real-world settings typically exhibit long-range\ndependencies and are observed at non-uniform intervals. In these settings,\ntraditional sequence-based recurrent models struggle. To overcome this,\nresearchers often replace recurrent architectures with Neural ODE-based models\nto account for irregularly sampled data and use Transformer-based architectures\nto account for long-range dependencies. Despite the success of these two\napproaches, both incur very high computational costs for input sequences of\neven moderate length. To address this challenge, we introduce the Rough\nTransformer, a variation of the Transformer model that operates on\ncontinuous-time representations of input sequences and incurs significantly\nlower computational costs. In particular, we propose multi-view signature\nattention, which uses path signatures to augment vanilla attention and to\ncapture both local and global (multi-scale) dependencies in the input data,\nwhile remaining robust to changes in the sequence length and sampling frequency\nand yielding improved spatial processing. We find that, on a variety of\ntime-series-related tasks, Rough Transformers consistently outperform their\nvanilla attention counterparts while obtaining the representational benefits of\nNeural ODE-based models, all at a fraction of the computational time and memory\nresources.",
    "published": "2024-05-31T14:00:44Z",
    "updated": "2025-01-11T12:46:55Z",
    "authors": [
      "Fernando Moreno-Pino",
      "Ãlvaro Arroyo",
      "Harrison Waldon",
      "Xiaowen Dong",
      "Ãlvaro Cartea"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.07247v1",
    "title": "Dynamical Mean-Field Theory of Self-Attention Neural Networks",
    "summary": "Transformer-based models have demonstrated exceptional performance across\ndiverse domains, becoming the state-of-the-art solution for addressing\nsequential machine learning problems. Even though we have a general\nunderstanding of the fundamental components in the transformer architecture,\nlittle is known about how they operate or what are their expected dynamics.\nRecently, there has been an increasing interest in exploring the relationship\nbetween attention mechanisms and Hopfield networks, promising to shed light on\nthe statistical physics of transformer networks. However, to date, the\ndynamical regimes of transformer-like models have not been studied in depth. In\nthis paper, we address this gap by using methods for the study of asymmetric\nHopfield networks in nonequilibrium regimes --namely path integral methods over\ngenerating functionals, yielding dynamics governed by concurrent mean-field\nvariables. Assuming 1-bit tokens and weights, we derive analytical\napproximations for the behavior of large self-attention neural networks coupled\nto a softmax output, which become exact in the large limit size. Our findings\nreveal nontrivial dynamical phenomena, including nonequilibrium phase\ntransitions associated with chaotic bifurcations, even for very simple\nconfigurations with a few encoded features and a very short context window.\nFinally, we discuss the potential of our analytic approach to improve our\nunderstanding of the inner workings of transformer models, potentially reducing\ncomputational training costs and enhancing model interpretability.",
    "published": "2024-06-11T13:29:34Z",
    "updated": "2024-06-11T13:29:34Z",
    "authors": [
      "Ãngel Poc-LÃ³pez",
      "Miguel Aguilera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.17335v2",
    "title": "Non-asymptotic Convergence of Training Transformers for Next-token\n  Prediction",
    "summary": "Transformers have achieved extraordinary success in modern machine learning\ndue to their excellent ability to handle sequential data, especially in\nnext-token prediction (NTP) tasks. However, the theoretical understanding of\ntheir performance in NTP is limited, with existing studies focusing mainly on\nasymptotic performance. This paper provides a fine-grained non-asymptotic\nanalysis of the training dynamics of a one-layer transformer consisting of a\nself-attention module followed by a feed-forward layer. We first characterize\nthe essential structural properties of training datasets for NTP using a\nmathematical framework based on partial orders. Then, we design a two-stage\ntraining algorithm, where the pre-processing stage for training the\nfeed-forward layer and the main stage for training the attention layer exhibit\nfast convergence performance. Specifically, both layers converge sub-linearly\nto the direction of their corresponding max-margin solutions. We also show that\nthe cross-entropy loss enjoys a linear convergence rate. Furthermore, we show\nthat the trained transformer presents non-trivial prediction ability with\ndataset shift, which sheds light on the remarkable generalization performance\nof transformers. Our analysis technique involves the development of novel\nproperties on the attention gradient and further in-depth analysis of how these\nproperties contribute to the convergence of the training process. Our\nexperiments further validate our theoretical findings.",
    "published": "2024-09-25T20:22:06Z",
    "updated": "2024-09-29T21:08:33Z",
    "authors": [
      "Ruiquan Huang",
      "Yingbin Liang",
      "Jing Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.15495v2",
    "title": "SEA: State-Exchange Attention for High-Fidelity Physics Based\n  Transformers",
    "summary": "Current approaches using sequential networks have shown promise in estimating\nfield variables for dynamical systems, but they are often limited by high\nrollout errors. The unresolved issue of rollout error accumulation results in\nunreliable estimations as the network predicts further into the future, with\neach step's error compounding and leading to an increase in inaccuracy. Here,\nwe introduce the State-Exchange Attention (SEA) module, a novel\ntransformer-based module enabling information exchange between encoded fields\nthrough multi-head cross-attention. The cross-field multidirectional\ninformation exchange design enables all state variables in the system to\nexchange information with one another, capturing physical relationships and\nsymmetries between fields. Additionally, we introduce an efficient ViT-like\nmesh autoencoder to generate spatially coherent mesh embeddings for a large\nnumber of meshing cells. The SEA integrated transformer demonstrates the\nstate-of-the-art rollout error compared to other competitive baselines.\nSpecifically, we outperform PbGMR-GMUS Transformer-RealNVP and GMR-GMUS\nTransformer, with a reduction in error of 88% and 91%, respectively.\nFurthermore, we demonstrate that the SEA module alone can reduce errors by 97%\nfor state variables that are highly dependent on other states of the system.\nThe repository for this work is available at:\nhttps://github.com/ParsaEsmati/SEA",
    "published": "2024-10-20T20:25:01Z",
    "updated": "2024-10-29T20:13:48Z",
    "authors": [
      "Parsa Esmati",
      "Amirhossein Dadashzadeh",
      "Vahid Goodarzi",
      "Nicolas Larrosa",
      "NicolÃ² Grilli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.22922v1",
    "title": "High-Fidelity Document Stain Removal via A Large-Scale Real-World\n  Dataset and A Memory-Augmented Transformer",
    "summary": "Document images are often degraded by various stains, significantly impacting\ntheir readability and hindering downstream applications such as document\ndigitization and analysis. The absence of a comprehensive stained document\ndataset has limited the effectiveness of existing document enhancement methods\nin removing stains while preserving fine-grained details. To address this\nchallenge, we construct StainDoc, the first large-scale, high-resolution\n($2145\\times2245$) dataset specifically designed for document stain removal.\nStainDoc comprises over 5,000 pairs of stained and clean document images across\nmultiple scenes. This dataset encompasses a diverse range of stain types,\nseverities, and document backgrounds, facilitating robust training and\nevaluation of document stain removal algorithms. Furthermore, we propose\nStainRestorer, a Transformer-based document stain removal approach.\nStainRestorer employs a memory-augmented Transformer architecture that captures\nhierarchical stain representations at part, instance, and semantic levels via\nthe DocMemory module. The Stain Removal Transformer (SRTransformer) leverages\nthese feature representations through a dual attention mechanism: an enhanced\nspatial attention with an expanded receptive field, and a channel attention\ncaptures channel-wise feature importance. This combination enables precise\nstain removal while preserving document content integrity. Extensive\nexperiments demonstrate StainRestorer's superior performance over\nstate-of-the-art methods on the StainDoc dataset and its variants\nStainDoc\\_Mark and StainDoc\\_Seal, establishing a new benchmark for document\nstain removal. Our work highlights the potential of memory-augmented\nTransformers for this task and contributes a valuable dataset to advance future\nresearch.",
    "published": "2024-10-30T11:27:06Z",
    "updated": "2024-10-30T11:27:06Z",
    "authors": [
      "Mingxian Li",
      "Hao Sun",
      "Yingtie Lei",
      "Xiaofeng Zhang",
      "Yihang Dong",
      "Yilin Zhou",
      "Zimeng Li",
      "Xuhang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.12992v1",
    "title": "MemoryFormer: Minimize Transformer Computation by Removing\n  Fully-Connected Layers",
    "summary": "In order to reduce the computational complexity of large language models,\ngreat efforts have been made to to improve the efficiency of transformer models\nsuch as linear attention and flash-attention. However, the model size and\ncorresponding computational complexity are constantly scaled up in pursuit of\nhigher performance. In this work, we present MemoryFormer, a novel transformer\narchitecture which significantly reduces the computational complexity (FLOPs)\nfrom a new perspective. We eliminate nearly all the computations of the\ntransformer model except for the necessary computation required by the\nmulti-head attention operation. This is made possible by utilizing an\nalternative method for feature transformation to replace the linear projection\nof fully-connected layers. Specifically, we first construct a group of\nin-memory lookup tables that store a large amount of discrete vectors to\nreplace the weight matrix used in linear projection. We then use a hash\nalgorithm to retrieve a correlated subset of vectors dynamically based on the\ninput embedding. The retrieved vectors combined together will form the output\nembedding, which provides an estimation of the result of matrix multiplication\noperation in a fully-connected layer. Compared to conducting matrix\nmultiplication, retrieving data blocks from memory is a much cheaper operation\nwhich requires little computations. We train MemoryFormer from scratch and\nconduct extensive experiments on various benchmarks to demonstrate the\neffectiveness of the proposed model.",
    "published": "2024-11-20T02:41:53Z",
    "updated": "2024-11-20T02:41:53Z",
    "authors": [
      "Ning Ding",
      "Yehui Tang",
      "Haochen Qin",
      "Zhenli Zhou",
      "Chao Xu",
      "Lin Li",
      "Kai Han",
      "Heng Liao",
      "Yunhe Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03379v2",
    "title": "MTVNet: Mapping using Transformers for Volumes -- Network for\n  Super-Resolution with Long-Range Interactions",
    "summary": "Until now, it has been difficult for volumetric super-resolution to utilize\nthe recent advances in transformer-based models seen in 2D super-resolution.\nThe memory required for self-attention in 3D volumes limits the receptive\nfield. Therefore, long-range interactions are not used in 3D to the extent done\nin 2D and the strength of transformers is not realized. We propose a\nmulti-scale transformer-based model based on hierarchical attention blocks\ncombined with carrier tokens at multiple scales to overcome this. Here\ninformation from larger regions at coarse resolution is sequentially carried on\nto finer-resolution regions to predict the super-resolved image. Using\ntransformer layers at each resolution, our coarse-to-fine modeling limits the\nnumber of tokens at each scale and enables attention over larger regions than\nwhat has previously been possible. We experimentally compare our method,\nMTVNet, against state-of-the-art volumetric super-resolution models on five 3D\ndatasets demonstrating the advantage of an increased receptive field. This\nadvantage is especially pronounced for images that are larger than what is seen\nin popularly used 3D datasets. Our code is available at\nhttps://github.com/AugustHoeg/MTVNet",
    "published": "2024-12-04T15:06:39Z",
    "updated": "2024-12-09T10:06:22Z",
    "authors": [
      "August Leander HÃ¸eg",
      "Sophia W. Bardenfleth",
      "Hans Martin Kjer",
      "Tim B. Dyrby",
      "Vedrana Andersen Dahl",
      "Anders Dahl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.16446v1",
    "title": "Sensitive Image Classification by Vision Transformers",
    "summary": "When it comes to classifying child sexual abuse images, managing similar\ninter-class correlations and diverse intra-class correlations poses a\nsignificant challenge. Vision transformer models, unlike conventional deep\nconvolutional network models, leverage a self-attention mechanism to capture\nglobal interactions among contextual local elements. This allows them to\nnavigate through image patches effectively, avoiding incorrect correlations and\nreducing ambiguity in attention maps, thus proving their efficacy in computer\nvision tasks. Rather than directly analyzing child sexual abuse data, we\nconstructed two datasets: one comprising clean and pornographic images and\nanother with three classes, which additionally include images indicative of\npornography, sourced from Reddit and Google Open Images data. In our\nexperiments, we also employ an adult content image benchmark dataset. These\ndatasets served as a basis for assessing the performance of vision transformer\nmodels in pornographic image classification. In our study, we conducted a\ncomparative analysis between various popular vision transformer models and\ntraditional pre-trained ResNet models. Furthermore, we compared them with\nestablished methods for sensitive image detection such as attention and metric\nlearning based CNN and Bumble. The findings demonstrated that vision\ntransformer networks surpassed the benchmark pre-trained models, showcasing\ntheir superior classification and detection capabilities in this task.",
    "published": "2024-12-21T02:34:24Z",
    "updated": "2024-12-21T02:34:24Z",
    "authors": [
      "Hanxian He",
      "Campbell Wilson",
      "Thanh Thi Nguyen",
      "Janis Dalins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.02270v2",
    "title": "Exact Sequence Interpolation with Transformers",
    "summary": "We prove that transformers can exactly interpolate datasets of finite input\nsequences in $\\mathbb{R}^d$, $d\\geq 2$, with corresponding output sequences of\nsmaller or equal length. Specifically, given $N$ sequences of arbitrary but\nfinite lengths in $\\mathbb{R}^d$ and output sequences of lengths $m^1, \\dots,\nm^N \\in \\mathbb{N}$, we construct a transformer with $\\mathcal{O}(\\sum_{j=1}^N\nm^j)$ blocks and $\\mathcal{O}(d \\sum_{j=1}^N m^j)$ parameters that exactly\ninterpolates the dataset. Our construction provides complexity estimates that\nare independent of the input sequence length, by alternating feed-forward and\nself-attention layers and by capitalizing on the clustering effect inherent to\nthe latter. Our novel constructive method also uses low-rank parameter matrices\nin the self-attention mechanism, a common feature of practical transformer\nimplementations. These results are first established in the hardmax\nself-attention setting, where the geometric structure permits an explicit and\nquantitative analysis, and are then extended to the softmax setting. Finally,\nwe demonstrate the applicability of our exact interpolation construction to\nlearning problems, in particular by providing convergence guarantees to a\nglobal minimizer under regularized training strategies. Our analysis\ncontributes to the theoretical understanding of transformer models, offering an\nexplanation for their excellent performance in exact sequence-to-sequence\ninterpolation tasks.",
    "published": "2025-02-04T12:31:00Z",
    "updated": "2025-10-29T17:15:10Z",
    "authors": [
      "Albert Alcalde",
      "Giovanni Fantuzzi",
      "Enrique Zuazua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.04830v2",
    "title": "DualX-VSR: Dual Axial Spatial$\\times$Temporal Transformer for Real-World\n  Video Super-Resolution without Motion Compensation",
    "summary": "Transformer-based models like ViViT and TimeSformer have advanced video\nunderstanding by effectively modeling spatiotemporal dependencies. Recent video\ngeneration models, such as Sora and Vidu, further highlight the power of\ntransformers in long-range feature extraction and holistic spatiotemporal\nmodeling. However, directly applying these models to real-world video\nsuper-resolution (VSR) is challenging, as VSR demands pixel-level precision,\nwhich can be compromised by tokenization and sequential attention mechanisms.\nWhile recent transformer-based VSR models attempt to address these issues using\nsmaller patches and local attention, they still face limitations such as\nrestricted receptive fields and dependence on optical flow-based alignment,\nwhich can introduce inaccuracies in real-world settings. To overcome these\nissues, we propose Dual Axial Spatial$\\times$Temporal Transformer for\nReal-World Video Super-Resolution (DualX-VSR), which introduces a novel dual\naxial spatial$\\times$temporal attention mechanism that integrates spatial and\ntemporal information along orthogonal directions. DualX-VSR eliminates the need\nfor motion compensation, offering a simplified structure that provides a\ncohesive representation of spatiotemporal information. As a result, DualX-VSR\nachieves high fidelity and superior performance in real-world VSR task.",
    "published": "2025-06-05T09:53:44Z",
    "updated": "2025-06-13T03:20:44Z",
    "authors": [
      "Shuo Cao",
      "Yihao Liu",
      "Xiaohui Li",
      "Yuanting Gao",
      "Yu Zhou",
      "Chao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.05249v3",
    "title": "On the Convergence of Gradient Descent on Learning Transformers with\n  Residual Connections",
    "summary": "Transformer models have emerged as fundamental tools across various\nscientific and engineering disciplines, owing to their outstanding performance\nin diverse applications. Despite this empirical success, the theoretical\nfoundations of Transformers remain relatively underdeveloped, particularly in\nunderstanding their training dynamics. Existing research predominantly examines\nisolated components--such as self-attention mechanisms and feedforward\nnetworks--without thoroughly investigating the interdependencies between these\ncomponents, especially when residual connections are present. In this paper, we\naim to bridge this gap by analyzing the convergence behavior of a structurally\ncomplete yet single-layer Transformer, comprising self-attention, a feedforward\nnetwork, and residual connections. We demonstrate that, under appropriate\ninitialization, gradient descent exhibits a linear convergence rate, where the\nconvergence speed is determined by the minimum and maximum singular values of\nthe output matrix from the attention layer. Moreover, our analysis reveals that\nresidual connections serve to ameliorate the ill-conditioning of this output\nmatrix, an issue stemming from the low-rank structure imposed by the softmax\noperation, thereby promoting enhanced optimization stability. We also extend\nour theoretical findings to a multi-layer Transformer architecture, confirming\nthe linear convergence rate of gradient descent under suitable initialization.\nEmpirical results corroborate our theoretical insights, illustrating the\nbeneficial role of residual connections in promoting convergence stability.",
    "published": "2025-06-05T17:10:22Z",
    "updated": "2025-07-24T16:56:37Z",
    "authors": [
      "Zhen Qin",
      "Jinxin Zhou",
      "Zhihui Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.13678v4",
    "title": "A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction",
    "summary": "Human activity intensity prediction is crucial to many location-based\nservices. Despite tremendous progress in modeling dynamics of human activity,\nmost existing methods overlook physical constraints of spatial interaction,\nleading to uninterpretable spatial correlations and over-smoothing phenomenon.\nTo address these limitations, this work proposes a physics-informed deep\nlearning framework, namely Gravity-informed Spatiotemporal Transformer\n(Gravityformer) by integrating the universal law of gravitation to refine\ntransformer attention. Specifically, it (1) estimates two spatially explicit\nmass parameters based on spatiotemporal embedding feature, (2) models the\nspatial interaction in end-to-end neural network using proposed adaptive\ngravity model to learn the physical constraint, and (3) utilizes the learned\nspatial interaction to guide and mitigate the over-smoothing phenomenon in\ntransformer attention. Moreover, a parallel spatiotemporal graph convolution\ntransformer is proposed for achieving a balance between coupled spatial and\ntemporal learning. Systematic experiments on six real-world large-scale\nactivity datasets demonstrate the quantitative and qualitative superiority of\nour model over state-of-the-art benchmarks. Additionally, the learned gravity\nattention matrix can be not only disentangled and interpreted based on\ngeographical laws, but also improved the generalization in zero-shot\ncross-region inference. This work provides a novel insight into integrating\nphysical laws with deep learning for spatiotemporal prediction.",
    "published": "2025-06-16T16:32:51Z",
    "updated": "2025-10-24T17:36:52Z",
    "authors": [
      "Yi Wang",
      "Zhenghong Wang",
      "Fan Zhang",
      "Chaogui Kang",
      "Sijie Ruan",
      "Di Zhu",
      "Chengling Tang",
      "Zhongfu Ma",
      "Weiyu Zhang",
      "Yu Zheng",
      "Philip S. Yu",
      "Yu Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21723v1",
    "title": "Detection Transformers Under the Knife: A Neuroscience-Inspired Approach\n  to Ablations",
    "summary": "In recent years, Explainable AI has gained traction as an approach to\nenhancing model interpretability and transparency, particularly in complex\nmodels such as detection transformers. Despite rapid advancements, a\nsubstantial research gap remains in understanding the distinct roles of\ninternal components - knowledge that is essential for improving transparency\nand efficiency. Inspired by neuroscientific ablation studies, which investigate\nthe functions of brain regions through selective impairment, we systematically\nanalyze the impact of ablating key components in three state-of-the-art\ndetection transformer models: Detection transformer (DETR), deformable\ndetection transformer (DDETR), and DETR with improved denoising anchor boxes\n(DINO). The ablations target query embeddings, encoder and decoder multi-head\nself-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA)\nlayers. We evaluate the effects of these ablations on the performance metrics\ngIoU and F1-score, quantifying effects on both the classification and\nregression sub-tasks on the COCO dataset. To facilitate reproducibility and\nfuture research, we publicly release the DeepDissect library. Our findings\nreveal model-specific resilience patterns: while DETR is particularly sensitive\nto ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable\nattention enhances robustness, and DINO exhibits the greatest resilience due to\nits look-forward twice update rule, which helps distributing knowledge across\nblocks. These insights also expose structural redundancies, particularly in\nDDETR's and DINO's decoder MHCA layers, highlighting opportunities for model\nsimplification without sacrificing performance. This study advances XAI for\nDETRs by clarifying the contributions of internal components to model\nperformance, offering insights to optimize and improve transparency and\nefficiency in critical applications.",
    "published": "2025-07-29T12:00:08Z",
    "updated": "2025-07-29T12:00:08Z",
    "authors": [
      "Nils HÃ¼tten",
      "Florian HÃ¶lken",
      "Hasan Tercan",
      "Tobias Meisen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24093v1",
    "title": "Clebsch-Gordan Transformer: Fast and Global Equivariant Attention",
    "summary": "The global attention mechanism is one of the keys to the success of\ntransformer architecture, but it incurs quadratic computational costs in\nrelation to the number of tokens. On the other hand, equivariant models, which\nleverage the underlying geometric structures of problem instance, often achieve\nsuperior accuracy in physical, biochemical, computer vision, and robotic tasks,\nat the cost of additional compute requirements. As a result, existing\nequivariant transformers only support low-order equivariant features and local\ncontext windows, limiting their expressiveness and performance. This work\nproposes Clebsch-Gordan Transformer, achieving efficient global attention by a\nnovel Clebsch-Gordon Convolution on $\\SO(3)$ irreducible representations. Our\nmethod enables equivariant modeling of features at all orders while achieving\n${O}(N \\log N)$ input token complexity. Additionally, the proposed method\nscales well with high-order irreducible features, by exploiting the sparsity of\nthe Clebsch-Gordon matrix. Lastly, we also incorporate optional token\npermutation equivariance through either weight sharing or data augmentation. We\nbenchmark our method on a diverse set of benchmarks including n-body\nsimulation, QM9, ModelNet point cloud classification and a robotic grasping\ndataset, showing clear gains over existing equivariant transformers in GPU\nmemory size, speed, and accuracy.",
    "published": "2025-09-28T22:09:36Z",
    "updated": "2025-09-28T22:09:36Z",
    "authors": [
      "Owen Lewis Howell",
      "Linfeng Zhao",
      "Xupeng Zhu",
      "Yaoyao Qian",
      "Haojie Huang",
      "Lingfeng Sun",
      "Wil Thomason",
      "Robert Platt",
      "Robin Walters"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06680v1",
    "title": "TimeFormer: Transformer with Attention Modulation Empowered by Temporal\n  Characteristics for Time Series Forecasting",
    "summary": "Although Transformers excel in natural language processing, their extension\nto time series forecasting remains challenging due to insufficient\nconsideration of the differences between textual and temporal modalities. In\nthis paper, we develop a novel Transformer architecture designed for time\nseries data, aiming to maximize its representational capacity. We identify two\nkey but often overlooked characteristics of time series: (1) unidirectional\ninfluence from the past to the future, and (2) the phenomenon of decaying\ninfluence over time. These characteristics are introduced to enhance the\nattention mechanism of Transformers. We propose TimeFormer, whose core\ninnovation is a self-attention mechanism with two modulation terms (MoSA),\ndesigned to capture these temporal priors of time series under the constraints\nof the Hawkes process and causal masking. Additionally, TimeFormer introduces a\nframework based on multi-scale and subsequence analysis to capture semantic\ndependencies at different temporal scales, enriching the temporal dependencies.\nExtensive experiments conducted on multiple real-world datasets show that\nTimeFormer significantly outperforms state-of-the-art methods, achieving up to\na 7.45% reduction in MSE compared to the best baseline and setting new\nbenchmarks on 94.04\\% of evaluation metrics. Moreover, we demonstrate that the\nMoSA mechanism can be broadly applied to enhance the performance of other\nTransformer-based models.",
    "published": "2025-10-08T06:07:30Z",
    "updated": "2025-10-08T06:07:30Z",
    "authors": [
      "Zhipeng Liu",
      "Peibo Duan",
      "Xuan Tang",
      "Baixin Li",
      "Yongsheng Huang",
      "Mingyang Geng",
      "Changsheng Zhang",
      "Bin Zhang",
      "Binwu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14336v2",
    "title": "DARTS-GT: Differentiable Architecture Search for Graph Transformers with\n  Quantifiable Instance-Specific Interpretability Analysis",
    "summary": "Graph Transformers (GTs) have emerged as powerful architectures for\ngraph-structured data, yet remain constrained by rigid designs and lack\nquantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN\ntypes across all layers, missing potential benefits of depth-specific component\nselection, while their complex architectures become opaque where performance\ngains cannot be distinguished between meaningful patterns and spurious\ncorrelations. We redesign GT attention through asymmetry, decoupling structural\nencoding from feature representation: queries derive from node features while\nkeys and values come from GNN transformations. Within this framework, we use\nDifferentiable ARchiTecture Search (DARTS) to select optimal GNN operators at\neach layer, enabling depth-wise heterogeneity inside transformer attention\nitself (DARTS-GT). To understand discovered architectures, we develop the first\nquantitative interpretability framework for GTs through causal ablation. Our\nmetrics (Head-deviation, Specialization, and Focus), identify which heads and\nnodes drive predictions while enabling model comparison. Experiments across\neight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while\nremaining competitive on others, with discovered architectures revealing\ndataset-specific patterns. Our interpretability analysis reveals that visual\nattention salience and causal importance do not always correlate, indicating\nwidely used visualization approaches may miss components that actually matter.\nCrucially, heterogeneous architectures found by DARTS-GT consistently produced\nmore interpretable models than baselines, establishing that Graph Transformers\nneed not choose between performance and interpretability.",
    "published": "2025-10-16T06:15:42Z",
    "updated": "2025-10-30T21:46:00Z",
    "authors": [
      "Shruti Sarika Chakraborty",
      "Peter Minary"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1312.6110v3",
    "title": "Learning Generative Models with Visual Attention",
    "summary": "Attention has long been proposed by psychologists as important for\neffectively dealing with the enormous sensory stimulus available in the\nneocortex. Inspired by the visual attention models in computational\nneuroscience and the need of object-centric data for generative models, we\ndescribe for generative learning framework using attentional mechanisms.\nAttentional mechanisms can propagate signals from region of interest in a scene\nto an aligned canonical representation, where generative modeling takes place.\nBy ignoring background clutter, generative models can concentrate their\nresources on the object of interest. Our model is a proper graphical model\nwhere the 2D Similarity transformation is a part of the top-down process. A\nConvNet is employed to provide good initializations during posterior inference\nwhich is based on Hamiltonian Monte Carlo. Upon learning images of faces, our\nmodel can robustly attend to face regions of novel test subjects. More\nimportantly, our model can learn generative models of new faces from a novel\ndataset of large images where the face locations are not known.",
    "published": "2013-12-20T20:50:43Z",
    "updated": "2015-02-21T22:21:15Z",
    "authors": [
      "Yichuan Tang",
      "Nitish Srivastava",
      "Ruslan Salakhutdinov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1610.04308v4",
    "title": "Recurrent 3D Attentional Networks for End-to-End Active Object\n  Recognition",
    "summary": "Active vision is inherently attention-driven: The agent actively selects\nviews to attend in order to fast achieve the vision task while improving its\ninternal representation of the scene being observed. Inspired by the recent\nsuccess of attention-based models in 2D vision tasks based on single RGB\nimages, we propose to address the multi-view depth-based active object\nrecognition using attention mechanism, through developing an end-to-end\nrecurrent 3D attentional network. The architecture takes advantage of a\nrecurrent neural network (RNN) to store and update an internal representation.\nOur model, trained with 3D shape datasets, is able to iteratively attend to the\nbest views targeting an object of interest for recognizing it. To realize 3D\nview selection, we derive a 3D spatial transformer network which is\ndifferentiable for training with backpropagation, achieving much faster\nconvergence than the reinforcement learning employed by most existing\nattention-based models. Experiments show that our method, with only depth\ninput, achieves state-of-the-art next-best-view performance in time efficiency\nand recognition accuracy.",
    "published": "2016-10-14T01:25:09Z",
    "updated": "2022-01-11T12:37:22Z",
    "authors": [
      "Min Liu",
      "Yifei Shi",
      "Lintao Zheng",
      "Kai Xu",
      "Hui Huang",
      "Dinesh Manocha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1809.11155v2",
    "title": "SALSA-TEXT : self attentive latent space based adversarial text\n  generation",
    "summary": "Inspired by the success of self attention mechanism and Transformer\narchitecture in sequence transduction and image generation applications, we\npropose novel self attention-based architectures to improve the performance of\nadversarial latent code- based schemes in text generation. Adversarial latent\ncode-based text generation has recently gained a lot of attention due to their\npromising results. In this paper, we take a step to fortify the architectures\nused in these setups, specifically AAE and ARAE. We benchmark two latent\ncode-based methods (AAE and ARAE) designed based on adversarial setups. In our\nexperiments, the Google sentence compression dataset is utilized to compare our\nmethod with these methods using various objective and subjective measures. The\nexperiments demonstrate the proposed (self) attention-based models outperform\nthe state-of-the-art in adversarial code-based text generation.",
    "published": "2018-09-28T17:38:36Z",
    "updated": "2018-10-08T16:42:59Z",
    "authors": [
      "Jules Gagnon-Marchand",
      "Hamed Sadeghi",
      "Md. Akmal Haidar",
      "Mehdi Rezagholizadeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.04731v2",
    "title": "Advancing Speech Synthesis using EEG",
    "summary": "In this paper we introduce attention-regression model to demonstrate\npredicting acoustic features from electroencephalography (EEG) features\nrecorded in parallel with spoken sentences. First we demonstrate predicting\nacoustic features directly from EEG features using our attention model and then\nwe demonstrate predicting acoustic features from EEG features using a two-step\napproach where in the first step we use our attention model to predict\narticulatory features from EEG features and then in second step another\nattention-regression model is trained to transform the predicted articulatory\nfeatures to acoustic features. Our proposed attention-regression model\ndemonstrates superior performance compared to the regression model introduced\nby authors in [1] when tested using their data set for majority of the subjects\nduring test time. The results presented in this paper further advances the work\ndescribed by authors in [1].",
    "published": "2020-04-09T23:58:40Z",
    "updated": "2020-05-03T20:33:36Z",
    "authors": [
      "Gautam Krishna",
      "Co Tran",
      "Mason Carnahan",
      "Ahmed Tewfik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.09394v3",
    "title": "Enhancing Monotonic Multihead Attention for Streaming ASR",
    "summary": "We investigate a monotonic multihead attention (MMA) by extending hard\nmonotonic attention to Transformer-based automatic speech recognition (ASR) for\nonline streaming applications. For streaming inference, all monotonic attention\n(MA) heads should learn proper alignments because the next token is not\ngenerated until all heads detect the corresponding token boundaries. However,\nwe found not all MA heads learn alignments with a na\\\"ive implementation. To\nencourage every head to learn alignments properly, we propose HeadDrop\nregularization by masking out a part of heads stochastically during training.\nFurthermore, we propose to prune redundant heads to improve consensus among\nheads for boundary detection and prevent delayed token generation caused by\nsuch heads. Chunkwise attention on each MA head is extended to the multihead\ncounterpart. Finally, we propose head-synchronous beam search decoding to\nguarantee stable streaming inference.",
    "published": "2020-05-19T12:39:38Z",
    "updated": "2020-09-30T12:20:25Z",
    "authors": [
      "Hirofumi Inaguma",
      "Masato Mimura",
      "Tatsuya Kawahara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.14642v1",
    "title": "Multiple Structural Priors Guided Self Attention Network for Language\n  Understanding",
    "summary": "Self attention networks (SANs) have been widely utilized in recent NLP\nstudies. Unlike CNNs or RNNs, standard SANs are usually position-independent,\nand thus are incapable of capturing the structural priors between sequences of\nwords. Existing studies commonly apply one single mask strategy on SANs for\nincorporating structural priors while failing at modeling more abundant\nstructural information of texts. In this paper, we aim at introducing multiple\ntypes of structural priors into SAN models, proposing the Multiple Structural\nPriors Guided Self Attention Network (MS-SAN) that transforms different\nstructural priors into different attention heads by using a novel multi-mask\nbased multi-head attention mechanism. In particular, we integrate two\ncategories of structural priors, including the sequential order and the\nrelative position of words. For the purpose of capturing the latent\nhierarchical structure of the texts, we extract these information not only from\nthe word contexts but also from the dependency syntax trees. Experimental\nresults on two tasks show that MS-SAN achieves significant improvements against\nother strong baselines.",
    "published": "2020-12-29T07:30:03Z",
    "updated": "2020-12-29T07:30:03Z",
    "authors": [
      "Le Qi",
      "Yu Zhang",
      "Qingyu Yin",
      "Ting Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1804.10844v1",
    "title": "CRAM: Clued Recurrent Attention Model",
    "summary": "To overcome the poor scalability of convolutional neural network, recurrent\nattention model(RAM) selectively choose what and where to look on the image. By\ndirecting recurrent attention model how to look the image, RAM can be even more\nsuccessful in that the given clue narrow down the scope of the possible focus\nzone. In this perspective, this work proposes clued recurrent attention model\n(CRAM) which add clue or constraint on the RAM better problem solving. CRAM\nfollows encoder-decoder framework, encoder utilizes recurrent attention model\nwith spatial transformer network and decoder which varies depending on the\ntask. To ensure the performance, CRAM tackles two computer vision task. One is\nthe image classification task, with clue given as the binary image saliency\nwhich indicates the approximate location of object. The other is the inpainting\ntask, with clue given as binary mask which indicates the occluded part. In both\ntasks, CRAM shows better performance than existing methods showing the\nsuccessful extension of RAM.",
    "published": "2018-04-28T19:27:43Z",
    "updated": "2018-04-28T19:27:43Z",
    "authors": [
      "Minki Chung",
      "Sungzoon Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.03454v4",
    "title": "DABERT: Dual Attention Enhanced BERT for Semantic Matching",
    "summary": "Transformer-based pre-trained language models such as BERT have achieved\nremarkable results in Semantic Sentence Matching. However, existing models\nstill suffer from insufficient ability to capture subtle differences. Minor\nnoise like word addition, deletion, and modification of sentences may cause\nflipped predictions. To alleviate this problem, we propose a novel Dual\nAttention Enhanced BERT (DABERT) to enhance the ability of BERT to capture\nfine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention\nmodule, which measures soft word matches by introducing a new dual channel\nalignment mechanism to model affinity and difference attention. (2) Adaptive\nFusion module, this module uses attention to learn the aggregation of\ndifference and affinity features, and generates a vector describing the\nmatching details of sentence pairs. We conduct extensive experiments on\nwell-studied semantic matching and robustness test datasets, and the\nexperimental results show the effectiveness of our proposed method.",
    "published": "2022-10-07T10:54:49Z",
    "updated": "2023-04-14T07:14:17Z",
    "authors": [
      "Sirui Wang",
      "Di Liang",
      "Jian Song",
      "Yuntao Li",
      "Wei Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.05261v3",
    "title": "Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair\n  Modeling",
    "summary": "Transformer-based models have achieved great success on sentence pair\nmodeling tasks, such as answer selection and natural language inference (NLI).\nThese models generally perform cross-attention over input pairs, leading to\nprohibitive computational costs. Recent studies propose dual-encoder and late\ninteraction architectures for faster computation. However, the balance between\nthe expressive of cross-attention and computation speedup still needs better\ncoordinated. To this end, this paper introduces a novel paradigm MixEncoder for\nefficient sentence pair modeling. MixEncoder involves a light-weight\ncross-attention mechanism. It conducts query encoding only once while modeling\nthe query-candidate interaction in parallel. Extensive experiments conducted on\nfour tasks demonstrate that our MixEncoder can speed up sentence pairing by\nover 113x while achieving comparable performance as the more expensive\ncross-attention models.",
    "published": "2022-10-11T08:44:03Z",
    "updated": "2023-10-22T10:38:48Z",
    "authors": [
      "Yuanhang Yang",
      "Shiyi Qi",
      "Chuanyi Liu",
      "Qifan Wang",
      "Cuiyun Gao",
      "Zenglin Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1904.03107v1",
    "title": "Convolutional Self-Attention Networks",
    "summary": "Self-attention networks (SANs) have drawn increasing interest due to their\nhigh parallelization in computation and flexibility in modeling dependencies.\nSANs can be further enhanced with multi-head attention by allowing the model to\nattend to information from different representation subspaces. In this work, we\npropose novel convolutional self-attention networks, which offer SANs the\nabilities to 1) strengthen dependencies among neighboring elements, and 2)\nmodel the interaction between features extracted by multiple attention heads.\nExperimental results of machine translation on different language pairs and\nmodel settings show that our approach outperforms both the strong Transformer\nbaseline and other existing models on enhancing the locality of SANs. Comparing\nwith prior studies, the proposed model is parameter free in terms of\nintroducing no more parameters.",
    "published": "2019-04-05T15:02:26Z",
    "updated": "2019-04-05T15:02:26Z",
    "authors": [
      "Baosong Yang",
      "Longyue Wang",
      "Derek Wong",
      "Lidia S. Chao",
      "Zhaopeng Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.09271v1",
    "title": "Wavelet-Attention CNN for Image Classification",
    "summary": "The feature learning methods based on convolutional neural network (CNN) have\nsuccessfully produced tremendous achievements in image classification tasks.\nHowever, the inherent noise and some other factors may weaken the effectiveness\nof the convolutional feature statistics. In this paper, we investigate Discrete\nWavelet Transform (DWT) in the frequency domain and design a new\nWavelet-Attention (WA) block to only implement attention in the high-frequency\ndomain. Based on this, we propose a Wavelet-Attention convolutional neural\nnetwork (WA-CNN) for image classification. Specifically, WA-CNN decomposes the\nfeature maps into low-frequency and high-frequency components for storing the\nstructures of the basic objects, as well as the detailed information and noise,\nrespectively. Then, the WA block is leveraged to capture the detailed\ninformation in the high-frequency domain with different attention factors but\nreserves the basic object structures in the low-frequency domain. Experimental\nresults on CIFAR-10 and CIFAR-100 datasets show that our proposed WA-CNN\nachieves significant improvements in classification accuracy compared to other\nrelated networks. Specifically, based on MobileNetV2 backbones, WA-CNN achieves\n1.26% Top-1 accuracy improvement on the CIFAR-10 benchmark and 1.54% Top-1\naccuracy improvement on the CIFAR-100 benchmark.",
    "published": "2022-01-23T14:00:33Z",
    "updated": "2022-01-23T14:00:33Z",
    "authors": [
      "Zhao Xiangyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1810.13320v2",
    "title": "Convolutional Self-Attention Network",
    "summary": "Self-attention network (SAN) has recently attracted increasing interest due\nto its fully parallelized computation and flexibility in modeling dependencies.\nIt can be further enhanced with multi-headed attention mechanism by allowing\nthe model to jointly attend to information from different representation\nsubspaces at different positions (Vaswani et al., 2017). In this work, we\npropose a novel convolutional self-attention network (CSAN), which offers SAN\nthe abilities to 1) capture neighboring dependencies, and 2) model the\ninteraction between multiple attention heads. Experimental results on WMT14\nEnglish-to-German translation task demonstrate that the proposed approach\noutperforms both the strong Transformer baseline and other existing works on\nenhancing the locality of SAN. Comparing with previous work, our model does not\nintroduce any new parameters.",
    "published": "2018-10-31T14:58:30Z",
    "updated": "2019-04-08T09:15:30Z",
    "authors": [
      "Baosong Yang",
      "Longyue Wang",
      "Derek F. Wong",
      "Lidia S. Chao",
      "Zhaopeng Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1902.05766v1",
    "title": "Context-Aware Self-Attention Networks",
    "summary": "Self-attention model have shown its flexibility in parallel computation and\nthe effectiveness on modeling both long- and short-term dependencies. However,\nit calculates the dependencies between representations without considering the\ncontextual information, which have proven useful for modeling dependencies\namong neural representations in various natural language tasks. In this work,\nwe focus on improving self-attention networks through capturing the richness of\ncontext. To maintain the simplicity and flexibility of the self-attention\nnetworks, we propose to contextualize the transformations of the query and key\nlayers, which are used to calculates the relevance between elements.\nSpecifically, we leverage the internal representations that embed both global\nand deep contexts, thus avoid relying on external resources. Experimental\nresults on WMT14 English-German and WMT17 Chinese-English translation tasks\ndemonstrate the effectiveness and universality of the proposed methods.\nFurthermore, we conducted extensive analyses to quantity how the context\nvectors participate in the self-attention model.",
    "published": "2019-02-15T11:03:52Z",
    "updated": "2019-02-15T11:03:52Z",
    "authors": [
      "Baosong Yang",
      "Jian Li",
      "Derek Wong",
      "Lidia S. Chao",
      "Xing Wang",
      "Zhaopeng Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1906.03674v1",
    "title": "Attention-based Conditioning Methods for External Knowledge Integration",
    "summary": "In this paper, we present a novel approach for incorporating external\nknowledge in Recurrent Neural Networks (RNNs). We propose the integration of\nlexicon features into the self-attention mechanism of RNN-based architectures.\nThis form of conditioning on the attention distribution, enforces the\ncontribution of the most salient words for the task at hand. We introduce three\nmethods, namely attentional concatenation, feature-based gating and affine\ntransformation. Experiments on six benchmark datasets show the effectiveness of\nour methods. Attentional feature-based gating yields consistent performance\nimprovement across tasks. Our approach is implemented as a simple add-on module\nfor RNN-based models with minimal computational overhead and can be adapted to\nany deep neural architecture.",
    "published": "2019-06-09T17:06:28Z",
    "updated": "2019-06-09T17:06:28Z",
    "authors": [
      "Katerina Margatina",
      "Christos Baziotis",
      "Alexandros Potamianos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.05150v1",
    "title": "Attentional Speech Recognition Models Misbehave on Out-of-domain\n  Utterances",
    "summary": "We discuss the problem of echographic transcription in autoregressive\nsequence-to-sequence attentional architectures for automatic speech\nrecognition, where a model produces very long sequences of repetitive outputs\nwhen presented with out-of-domain utterances. We decode audio from the British\nNational Corpus with an attentional encoder-decoder model trained solely on the\nLibriSpeech corpus. We observe that there are many 5-second recordings that\nproduce more than 500 characters of decoding output (i.e. more than 100\ncharacters per second). A frame-synchronous hybrid (DNN-HMM) model trained on\nthe same data does not produce these unusually long transcripts. These decoding\nissues are reproducible in a speech transformer model from ESPnet, and to a\nlesser extent in a self-attention CTC model, suggesting that these issues are\nintrinsic to the use of the attention mechanism. We create a separate length\nprediction model to predict the correct number of wordpieces in the output,\nwhich allows us to identify and truncate problematic decoding results without\nincreasing word error rates on the LibriSpeech task.",
    "published": "2020-02-12T18:53:56Z",
    "updated": "2020-02-12T18:53:56Z",
    "authors": [
      "Phillip Keung",
      "Wei Niu",
      "Yichao Lu",
      "Julian Salazar",
      "Vikas Bhardwaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.07028v1",
    "title": "Low-Rank Bottleneck in Multi-head Attention Models",
    "summary": "Attention based Transformer architecture has enabled significant advances in\nthe field of natural language processing. In addition to new pre-training\ntechniques, recent improvements crucially rely on working with a relatively\nlarger embedding dimension for tokens. Unfortunately, this leads to models that\nare prohibitively large to be employed in the downstream tasks. In this paper\nwe identify one of the important factors contributing to the large embedding\nsize requirement. In particular, our analysis highlights that the scaling\nbetween the number of heads and the size of each head in the current\narchitecture gives rise to a low-rank bottleneck in attention heads, causing\nthis limitation. We further validate this in our experiments. As a solution we\npropose to set the head size of an attention unit to input sequence length, and\nindependent of the number of heads, resulting in multi-head attention layers\nwith provably more expressive power. We empirically show that this allows us to\ntrain models with a relatively smaller embedding dimension and with better\nperformance scaling.",
    "published": "2020-02-17T16:16:40Z",
    "updated": "2020-02-17T16:16:40Z",
    "authors": [
      "Srinadh Bhojanapalli",
      "Chulhee Yun",
      "Ankit Singh Rawat",
      "Sashank J. Reddi",
      "Sanjiv Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2002.09693v1",
    "title": "Interpretable Crowd Flow Prediction with Spatial-Temporal Self-Attention",
    "summary": "Crowd flow prediction has been increasingly investigated in intelligent urban\ncomputing field as a fundamental component of urban management system. The most\nchallenging part of predicting crowd flow is to measure the complicated\nspatial-temporal dependencies. A prevalent solution employed in current methods\nis to divide and conquer the spatial and temporal information by various\narchitectures (e.g., CNN/GCN, LSTM). However, this strategy has two\ndisadvantages: (1) the sophisticated dependencies are also divided and\ntherefore partially isolated; (2) the spatial-temporal features are transformed\ninto latent representations when passing through different architectures,\nmaking it hard to interpret the predicted crowd flow. To address these issues,\nwe propose a Spatial-Temporal Self-Attention Network (STSAN) with an ST\nencoding gate that calculates the entire spatial-temporal representation with\npositional and time encodings and therefore avoids dividing the dependencies.\nFurthermore, we develop a Multi-aspect attention mechanism that applies scaled\ndot-product attention over spatial-temporal information and measures the\nattention weights that explicitly indicate the dependencies. Experimental\nresults on traffic and mobile data demonstrate that the proposed method reduces\ninflow and outflow RMSE by 16% and 8% on the Taxi-NYC dataset compared to the\nSOTA baselines.",
    "published": "2020-02-22T12:43:11Z",
    "updated": "2020-02-22T12:43:11Z",
    "authors": [
      "Haoxing Lin",
      "Weijia Jia",
      "Yongjian You",
      "Yiping Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.04253v3",
    "title": "Motion-Attentive Transition for Zero-Shot Video Object Segmentation",
    "summary": "In this paper, we present a novel Motion-Attentive Transition Network\n(MATNet) for zero-shot video object segmentation, which provides a new way of\nleveraging motion information to reinforce spatio-temporal object\nrepresentation. An asymmetric attention block, called Motion-Attentive\nTransition (MAT), is designed within a two-stream encoder, which transforms\nappearance features into motion-attentive representations at each convolutional\nstage. In this way, the encoder becomes deeply interleaved, allowing for\nclosely hierarchical interactions between object motion and appearance. This is\nsuperior to the typical two-stream architecture, which treats motion and\nappearance separately in each stream and often suffers from overfitting to\nappearance information. Additionally, a bridge network is proposed to obtain a\ncompact, discriminative and scale-sensitive representation for multi-level\nencoder features, which is further fed into a decoder to achieve segmentation\nresults. Extensive experiments on three challenging public benchmarks (i.e.\nDAVIS-16, FBMS and Youtube-Objects) show that our model achieves compelling\nperformance against the state-of-the-arts.",
    "published": "2020-03-09T16:58:42Z",
    "updated": "2020-07-09T17:34:32Z",
    "authors": [
      "Tianfei Zhou",
      "Shunzhou Wang",
      "Yi Zhou",
      "Yazhou Yao",
      "Jianwu Li",
      "Ling Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.10503v3",
    "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks",
    "summary": "We introduce the SE(3)-Transformer, a variant of the self-attention module\nfor 3D point clouds and graphs, which is equivariant under continuous 3D\nroto-translations. Equivariance is important to ensure stable and predictable\nperformance in the presence of nuisance transformations of the data input. A\npositive corollary of equivariance is increased weight-tying within the model.\nThe SE(3)-Transformer leverages the benefits of self-attention to operate on\nlarge point clouds and graphs with varying number of points, while guaranteeing\nSE(3)-equivariance for robustness. We evaluate our model on a toy N-body\nparticle simulation dataset, showcasing the robustness of the predictions under\nrotations of the input. We further achieve competitive performance on two\nreal-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms\na strong, non-equivariant attention baseline and an equivariant model without\nattention.",
    "published": "2020-06-18T13:23:01Z",
    "updated": "2020-11-24T19:03:03Z",
    "authors": [
      "Fabian B. Fuchs",
      "Daniel E. Worrall",
      "Volker Fischer",
      "Max Welling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.04266v1",
    "title": "BERT-JAM: Boosting BERT-Enhanced Neural Machine Translation with Joint\n  Attention",
    "summary": "BERT-enhanced neural machine translation (NMT) aims at leveraging\nBERT-encoded representations for translation tasks. A recently proposed\napproach uses attention mechanisms to fuse Transformer's encoder and decoder\nlayers with BERT's last-layer representation and shows enhanced performance.\nHowever, their method doesn't allow for the flexible distribution of attention\nbetween the BERT representation and the encoder/decoder representation. In this\nwork, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves\nupon existing models from two aspects: 1) BERT-JAM uses joint-attention modules\nto allow the encoder/decoder layers to dynamically allocate attention between\ndifferent representations, and 2) BERT-JAM allows the encoder/decoder layers to\nmake use of BERT's intermediate representations by composing them using a gated\nlinear unit (GLU). We train BERT-JAM with a novel three-phase optimization\nstrategy that progressively unfreezes different components of BERT-JAM. Our\nexperiments show that BERT-JAM achieves SOTA BLEU scores on multiple\ntranslation tasks.",
    "published": "2020-11-09T09:30:37Z",
    "updated": "2020-11-09T09:30:37Z",
    "authors": [
      "Zhebin Zhang",
      "Sai Wu",
      "Dawei Jiang",
      "Gang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.05484v1",
    "title": "4D Attention-based Neural Network for EEG Emotion Recognition",
    "summary": "Electroencephalograph (EEG) emotion recognition is a significant task in the\nbrain-computer interface field. Although many deep learning methods are\nproposed recently, it is still challenging to make full use of the information\ncontained in different domains of EEG signals. In this paper, we present a\nnovel method, called four-dimensional attention-based neural network (4D-aNN)\nfor EEG emotion recognition. First, raw EEG signals are transformed into 4D\nspatial-spectral-temporal representations. Then, the proposed 4D-aNN adopts\nspectral and spatial attention mechanisms to adaptively assign the weights of\ndifferent brain regions and frequency bands, and a convolutional neural network\n(CNN) is utilized to deal with the spectral and spatial information of the 4D\nrepresentations. Moreover, a temporal attention mechanism is integrated into a\nbidirectional Long Short-Term Memory (LSTM) to explore temporal dependencies of\nthe 4D representations. Our model achieves state-of-the-art performance on the\nSEED dataset under intra-subject splitting. The experimental results have shown\nthe effectiveness of the attention mechanisms in different domains for EEG\nemotion recognition.",
    "published": "2021-01-14T07:41:48Z",
    "updated": "2021-01-14T07:41:48Z",
    "authors": [
      "Guowen Xiao",
      "Mengwen Ye",
      "Bowen Xu",
      "Zhendi Chen",
      "Quansheng Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.08602v1",
    "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention",
    "summary": "We present lambda layers -- an alternative framework to self-attention -- for\ncapturing long-range interactions between an input and structured contextual\ninformation (e.g. a pixel surrounded by other pixels). Lambda layers capture\nsuch interactions by transforming available contexts into linear functions,\ntermed lambdas, and applying these linear functions to each input separately.\nSimilar to linear attention, lambda layers bypass expensive attention maps, but\nin contrast, they model both content and position-based interactions which\nenables their application to large structured inputs such as images. The\nresulting neural network architectures, LambdaNetworks, significantly\noutperform their convolutional and attentional counterparts on ImageNet\nclassification, COCO object detection and COCO instance segmentation, while\nbeing more computationally efficient. Additionally, we design LambdaResNets, a\nfamily of hybrid architectures across different scales, that considerably\nimproves the speed-accuracy tradeoff of image classification models.\nLambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x\nfaster than the popular EfficientNets on modern machine learning accelerators.\nWhen training with an additional 130M pseudo-labeled images, LambdaResNets\nachieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.",
    "published": "2021-02-17T06:33:47Z",
    "updated": "2021-02-17T06:33:47Z",
    "authors": [
      "Irwan Bello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.02205v1",
    "title": "Attention Head Masking for Inference Time Content Selection in\n  Abstractive Summarization",
    "summary": "How can we effectively inform content selection in Transformer-based\nabstractive summarization models? In this work, we present a\nsimple-yet-effective attention head masking technique, which is applied on\nencoder-decoder attentions to pinpoint salient content at inference time. Using\nattention head masking, we are able to reveal the relation between\nencoder-decoder attentions and content selection behaviors of summarization\nmodels. We then demonstrate its effectiveness on three document summarization\ndatasets based on both in-domain and cross-domain settings. Importantly, our\nmodels outperform prior state-of-the-art models on CNN/Daily Mail and New York\nTimes datasets. Moreover, our inference-time masking technique is also\ndata-efficient, requiring only 20% of the training samples to outperform BART\nfine-tuned on the full CNN/DailyMail dataset.",
    "published": "2021-04-06T00:49:25Z",
    "updated": "2021-04-06T00:49:25Z",
    "authors": [
      "Shuyang Cao",
      "Lu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.06129v2",
    "title": "SAFIN: Arbitrary Style Transfer With Self-Attentive Factorized Instance\n  Normalization",
    "summary": "Artistic style transfer aims to transfer the style characteristics of one\nimage onto another image while retaining its content. Existing approaches\ncommonly leverage various normalization techniques, although these face\nlimitations in adequately transferring diverse textures to different spatial\nlocations. Self-Attention-based approaches have tackled this issue with partial\nsuccess but suffer from unwanted artifacts. Motivated by these observations,\nthis paper aims to combine the best of both worlds: self-attention and\nnormalization. That yields a new plug-and-play module that we name\nSelf-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially\na spatially adaptive normalization module whose parameters are inferred through\nattention on the content and style image. We demonstrate that plugging SAFIN\ninto the base network of another state-of-the-art method results in enhanced\nstylization. We also develop a novel base network composed of Wavelet Transform\nfor multi-scale style transfer, which when combined with SAFIN, produces\nvisually appealing results with lesser unwanted textures.",
    "published": "2021-05-13T08:01:01Z",
    "updated": "2021-05-20T05:44:54Z",
    "authors": [
      "Aaditya Singh",
      "Shreeshail Hingane",
      "Xinyu Gong",
      "Zhangyang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14719v1",
    "title": "Noise Classification Aided Attention-Based Neural Network for Monaural\n  Speech Enhancement",
    "summary": "This paper proposes an noise type classification aided attention-based neural\nnetwork approach for monaural speech enhancement. The network is constructed\nbased on a previous work by introducing a noise classification subnetwork into\nthe structure and taking the classification embedding into the attention\nmechanism for guiding the network to make better feature extraction.\nSpecifically, to make the network an end-to-end way, an audio encoder and\ndecoder constructed by temporal convolution is used to make transformation\nbetween waveform and spectrogram. Additionally, our model is composed of two\nlong short term memory (LSTM) based encoders, two attention mechanism, a noise\nclassifier and a speech mask generator. Experiments show that, compared with\nOM-LSA and the previous work, the proposed noise classification aided\nattention-based approach can achieve better performance in terms of speech\nquality (PESQ). More promisingly, our approach has better generalization\nability to unseen noise conditions.",
    "published": "2021-05-31T06:30:07Z",
    "updated": "2021-05-31T06:30:07Z",
    "authors": [
      "Lu Ma",
      "Song Yang",
      "Yaguang Gong",
      "Zhongqin Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.15338v2",
    "title": "Probabilistic Attention for Interactive Segmentation",
    "summary": "We provide a probabilistic interpretation of attention and show that the\nstandard dot-product attention in transformers is a special case of Maximum A\nPosteriori (MAP) inference. The proposed approach suggests the use of\nExpectation Maximization algorithms for online adaptation of key and value\nmodel parameters. This approach is useful for cases in which external agents,\ne.g., annotators, provide inference-time information about the correct values\nof some tokens, e.g, the semantic category of some pixels, and we need for this\nnew information to propagate to other tokens in a principled manner. We\nillustrate the approach on an interactive semantic segmentation task in which\nannotators and models collaborate online to improve annotation efficiency.\nUsing standard benchmarks, we observe that key adaptation boosts model\nperformance ($\\sim10\\%$ mIoU) in the low feedback regime and value propagation\nimproves model responsiveness in the high feedback regime. A PyTorch layer\nimplementation of our probabilistic attention model will be made publicly\navailable here: https://github.com/apple/ml-probabilistic-attention.",
    "published": "2021-06-23T00:19:43Z",
    "updated": "2021-07-02T20:42:33Z",
    "authors": [
      "Prasad Gabbur",
      "Manjot Bilkhu",
      "Javier Movellan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.05132v1",
    "title": "The Center of Attention: Center-Keypoint Grouping via Attention for\n  Multi-Person Pose Estimation",
    "summary": "We introduce CenterGroup, an attention-based framework to estimate human\nposes from a set of identity-agnostic keypoints and person center predictions\nin an image. Our approach uses a transformer to obtain context-aware embeddings\nfor all detected keypoints and centers and then applies multi-head attention to\ndirectly group joints into their corresponding person centers. While most\nbottom-up methods rely on non-learnable clustering at inference, CenterGroup\nuses a fully differentiable attention mechanism that we train end-to-end\ntogether with our keypoint detector. As a result, our method obtains\nstate-of-the-art performance with up to 2.5x faster inference time than\ncompeting bottom-up methods. Our code is available at\nhttps://github.com/dvl-tum/center-group .",
    "published": "2021-10-11T10:22:04Z",
    "updated": "2021-10-11T10:22:04Z",
    "authors": [
      "Guillem BrasÃ³",
      "Nikita Kister",
      "Laura Leal-TaixÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.06393v1",
    "title": "Attention-guided Generative Models for Extractive Question Answering",
    "summary": "We propose a novel method for applying Transformer models to extractive\nquestion answering (QA) tasks. Recently, pretrained generative\nsequence-to-sequence (seq2seq) models have achieved great success in question\nanswering. Contributing to the success of these models are internal attention\nmechanisms such as cross-attention. We propose a simple strategy to obtain an\nextractive answer span from the generative model by leveraging the decoder\ncross-attention patterns. Viewing cross-attention as an architectural prior, we\napply joint training to further improve QA performance. Empirical results show\nthat on open-domain question answering datasets like NaturalQuestions and\nTriviaQA, our method approaches state-of-the-art performance on both generative\nand extractive inference, all while using much fewer parameters. Furthermore,\nthis strategy allows us to perform hallucination-free inference while\nconferring significant improvements to the model's ability to rerank relevant\npassages.",
    "published": "2021-10-12T23:02:35Z",
    "updated": "2021-10-12T23:02:35Z",
    "authors": [
      "Peng Xu",
      "Davis Liang",
      "Zhiheng Huang",
      "Bing Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.05642v3",
    "title": "Parameter-Free Attentive Scoring for Speaker Verification",
    "summary": "This paper presents a novel study of parameter-free attentive scoring for\nspeaker verification. Parameter-free scoring provides the flexibility of\ncomparing speaker representations without the need of an accompanying\nparametric scoring model. Inspired by the attention component in Transformer\nneural networks, we propose a variant of the scaled dot product attention\nmechanism to compare enrollment and test segment representations. In addition,\nthis work explores the effect on performance of (i) different types of\nnormalization, (ii) independent versus tied query/key estimation, (iii) varying\nthe number of key-value pairs and (iv) pooling multiple enrollment utterance\nstatistics. Experimental results for a 4 task average show that a simple\nparameter-free attentive scoring mechanism can improve the average EER by 10%\nover the best cosine similarity baseline.",
    "published": "2022-03-10T21:11:37Z",
    "updated": "2023-03-06T17:57:00Z",
    "authors": [
      "Jason Pelecanos",
      "Quan Wang",
      "Yiling Huang",
      "Ignacio Lopez Moreno"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.08305v2",
    "title": "A Study of the Attention Abnormality in Trojaned BERTs",
    "summary": "Trojan attacks raise serious security concerns. In this paper, we investigate\nthe underlying mechanism of Trojaned BERT models. We observe the attention\nfocus drifting behavior of Trojaned models, i.e., when encountering an poisoned\ninput, the trigger token hijacks the attention focus regardless of the context.\nWe provide a thorough qualitative and quantitative analysis of this phenomenon,\nrevealing insights into the Trojan mechanism. Based on the observation, we\npropose an attention-based Trojan detector to distinguish Trojaned models from\nclean ones. To the best of our knowledge, this is the first paper to analyze\nthe Trojan mechanism and to develop a Trojan detector based on the\ntransformer's attention.",
    "published": "2022-05-13T16:48:37Z",
    "updated": "2022-07-04T17:13:15Z",
    "authors": [
      "Weimin Lyu",
      "Songzhu Zheng",
      "Tengfei Ma",
      "Chao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.11701v1",
    "title": "Perceiver-VL: Efficient Vision-and-Language Modeling with Iterative\n  Latent Attention",
    "summary": "We present Perceiver-VL, a vision-and-language framework that efficiently\nhandles high-dimensional multimodal inputs such as long videos and text.\nPowered by the iterative latent cross-attention of Perceiver, our framework\nscales with linear complexity, in contrast to the quadratic complexity of\nself-attention used in many state-of-the-art transformer-based models. To\nfurther improve the efficiency of our framework, we also study applying\nLayerDrop on cross-attention layers and introduce a mixed-stream architecture\nfor cross-modal retrieval. We evaluate Perceiver-VL on diverse video-text and\nimage-text benchmarks, where Perceiver-VL achieves the lowest GFLOPs and\nlatency while maintaining competitive performance. In addition, we also provide\ncomprehensive analyses of various aspects of our framework, including\npretraining data, scalability of latent size and input size, dropping\ncross-attention layers at inference to reduce latency, modality aggregation\nstrategy, positional encoding, and weight initialization strategy. Our code and\ncheckpoints are available at: https://github.com/zinengtang/Perceiver_VL",
    "published": "2022-11-21T18:22:39Z",
    "updated": "2022-11-21T18:22:39Z",
    "authors": [
      "Zineng Tang",
      "Jaemin Cho",
      "Jie Lei",
      "Mohit Bansal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.12220v1",
    "title": "A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken\n  Language Understanding",
    "summary": "Multi-Intent Spoken Language Understanding (SLU), a novel and more complex\nscenario of SLU, is attracting increasing attention. Unlike traditional SLU,\neach intent in this scenario has its specific scope. Semantic information\noutside the scope even hinders the prediction, which tremendously increases the\ndifficulty of intent detection. More seriously, guiding slot filling with these\ninaccurate intent labels suffers error propagation problems, resulting in\nunsatisfied overall performance. To solve these challenges, in this paper, we\npropose a novel Scope-Sensitive Result Attention Network (SSRAN) based on\nTransformer, which contains a Scope Recognizer (SR) and a Result Attention\nNetwork (RAN). Scope Recognizer assignments scope information to each token,\nreducing the distraction of out-of-scope tokens. Result Attention Network\neffectively utilizes the bidirectional interaction between results of slot\nfilling and intent detection, mitigating the error propagation problem.\nExperiments on two public datasets indicate that our model significantly\nimproves SLU performance (5.4\\% and 2.1\\% on Overall accuracy) over the\nstate-of-the-art baseline.",
    "published": "2022-11-22T12:24:22Z",
    "updated": "2022-11-22T12:24:22Z",
    "authors": [
      "Lizhi Cheng",
      "Wenmian Yang",
      "Weijia Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.07407v1",
    "title": "TAME: Attention Mechanism Based Feature Fusion for Generating\n  Explanation Maps of Convolutional Neural Networks",
    "summary": "The apparent ``black box'' nature of neural networks is a barrier to adoption\nin applications where explainability is essential. This paper presents TAME\n(Trainable Attention Mechanism for Explanations), a method for generating\nexplanation maps with a multi-branch hierarchical attention mechanism. TAME\ncombines a target model's feature maps from multiple layers using an attention\nmechanism, transforming them into an explanation map. TAME can easily be\napplied to any convolutional neural network (CNN) by streamlining the\noptimization of the attention mechanism's training method and the selection of\ntarget model's feature maps. After training, explanation maps can be computed\nin a single forward pass. We apply TAME to two widely used models, i.e. VGG-16\nand ResNet-50, trained on ImageNet and show improvements over previous\ntop-performing methods. We also provide a comprehensive ablation study\ncomparing the performance of different variations of TAME's architecture. TAME\nsource code is made publicly available at https://github.com/bmezaris/TAME",
    "published": "2023-01-18T10:05:28Z",
    "updated": "2023-01-18T10:05:28Z",
    "authors": [
      "Mariano Ntrougkas",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.09190v3",
    "title": "Apples and Oranges? Assessing Image Quality over Content Recognition",
    "summary": "Image recognition and quality assessment are two important viewing tasks,\nwhile potentially following different visual mechanisms. This paper\ninvestigates if the two tasks can be performed in a multitask learning manner.\nA sequential spatial-channel attention module is proposed to simulate the\nvisual attention and contrast sensitivity mechanisms that are crucial for\ncontent recognition and quality assessment. Spatial attention is shared between\ncontent recognition and quality assessment, while channel attention is solely\nfor quality assessment. Such attention module is integrated into Transformer to\nbuild a uniform model for the two viewing tasks. The experimental results have\ndemonstrated that the proposed uniform model can achieve promising performance\nfor both quality assessment and content recognition tasks.",
    "published": "2023-01-22T19:51:22Z",
    "updated": "2023-01-31T11:05:59Z",
    "authors": [
      "Junyong You",
      "Zheng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.09422v2",
    "title": "Neural Attention Memory",
    "summary": "We propose a novel perspective of the attention mechanism by reinventing it\nas a memory architecture for neural networks, namely Neural Attention Memory\n(NAM). NAM is a memory structure that is both readable and writable via\ndifferentiable linear algebra operations. We explore three use cases of NAM:\nmemory-augmented neural network (MANN), few-shot learning, and efficient\nlong-range attention. First, we design two NAM-based MANNs of Long Short-term\nMemory (LSAM) and NAM Turing Machine (NAM-TM) that show better computational\npowers in algorithmic zero-shot generalization tasks compared to other\nbaselines such as differentiable neural computer (DNC). Next, we apply NAM to\nthe N-way K-shot learning task and show that it is more effective at reducing\nfalse positives compared to the baseline cosine classifier. Finally, we\nimplement an efficient Transformer with NAM and evaluate it with long-range\narena tasks to show that NAM can be an efficient and effective alternative for\nscaled dot-product attention.",
    "published": "2023-02-18T21:19:21Z",
    "updated": "2023-10-14T04:36:47Z",
    "authors": [
      "Hyoungwook Nam",
      "Seung Byum Seo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.12973v1",
    "title": "Attention-based Spatial-Temporal Graph Convolutional Recurrent Networks\n  for Traffic Forecasting",
    "summary": "Traffic forecasting is one of the most fundamental problems in transportation\nscience and artificial intelligence. The key challenge is to effectively model\ncomplex spatial-temporal dependencies and correlations in modern traffic data.\nExisting methods, however, cannot accurately model both long-term and\nshort-term temporal correlations simultaneously, limiting their expressive\npower on complex spatial-temporal patterns. In this paper, we propose a novel\nspatial-temporal neural network framework: Attention-based Spatial-Temporal\nGraph Convolutional Recurrent Network (ASTGCRN), which consists of a graph\nconvolutional recurrent module (GCRN) and a global attention module. In\nparticular, GCRN integrates gated recurrent units and adaptive graph\nconvolutional networks for dynamically learning graph structures and capturing\nspatial dependencies and local temporal relationships. To effectively extract\nglobal temporal dependencies, we design a temporal attention layer and\nimplement it as three independent modules based on multi-head self-attention,\ntransformer, and informer respectively. Extensive experiments on five real\ntraffic datasets have demonstrated the excellent predictive performance of all\nour three models with all their average MAE, RMSE and MAPE across the test\ndatasets lower than the baseline methods.",
    "published": "2023-02-25T03:37:00Z",
    "updated": "2023-02-25T03:37:00Z",
    "authors": [
      "Haiyang Liu",
      "Chunjiang Zhu",
      "Detian Zhang",
      "Qing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.04701v2",
    "title": "Differentially Private Attention Computation",
    "summary": "Large language models (LLMs), especially those based on the Transformer\narchitecture, have had a profound impact on various aspects of daily life, such\nas natural language processing, content generation, research methodologies, and\nmore. Nevertheless, a crucial concern regarding the inference results of large\nlanguage models is the issue of security and privacy. Given that large language\nmodels can generate results that may leak sensitive confidential or copyright\ninformation in many scenarios, it is crucial to compute the attention matrix\nwith provable privacy guarantees, as attention is all you need.\n  In this work, we propose a novel and efficient algorithm for approximating\nthe attention matrix while providing differential privacy (DP) guarantees. To\nachieve this, we build on recent advancements in fast attention computation and\ndifferentially private matrix publishing.",
    "published": "2023-05-08T13:32:41Z",
    "updated": "2024-10-14T15:52:31Z",
    "authors": [
      "Yeqi Gao",
      "Zhao Song",
      "Xin Yang",
      "Yufa Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.14380v2",
    "title": "Finding the Pillars of Strength for Multi-Head Attention",
    "summary": "Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g.,\nredundancy and over-parameterization. Specifically, the heads of MHA were\noriginally designed to attend to information from different representation\nsubspaces, whereas prior studies found that some attention heads likely learn\nsimilar features and can be pruned without harming performance. Inspired by the\nminimum-redundancy feature selection, we assume that focusing on the most\nrepresentative and distinctive features with minimum resources can mitigate the\nabove issues and lead to more effective and efficient MHAs. In particular, we\npropose Grouped Head Attention, trained with a self-supervised group constraint\nthat group attention heads, where each group focuses on an essential but\ndistinctive feature subset. We additionally propose a Voting-to-Stay procedure\nto remove redundant heads, thus achieving a transformer with lighter weights.\nMoreover, our method achieves significant performance gains on three\nwell-established tasks while considerably compressing parameters.",
    "published": "2023-05-22T03:44:44Z",
    "updated": "2023-10-15T04:10:13Z",
    "authors": [
      "Jinjie Ni",
      "Rui Mao",
      "Zonglin Yang",
      "Han Lei",
      "Erik Cambria"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.05861v1",
    "title": "Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive\n  Feature Learning in Speech Enhancement",
    "summary": "Current speech enhancement (SE) research has largely neglected channel\nattention and spatial attention, and encoder-decoder architecture-based\nnetworks have not adequately considered how to provide efficient inputs to the\nintermediate enhancement layer. To address these issues, this paper proposes a\ntime-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved\ndensely connected blocks, dual-path modules, convolution-augmented transformers\n(conformers), channel attention, and spatial attention. Compared with previous\nmodels, our proposed model has a more efficient encoder-decoder and can learn\ncomprehensive features. Experimental results on the VCTK+DEMAND dataset\ndemonstrate that our method outperforms existing techniques in SE performance.\nFurthermore, the improved densely connected block and two dimensions attention\nmodule developed in this work are highly adaptable and easily integrated into\nexisting networks.",
    "published": "2023-06-09T12:52:01Z",
    "updated": "2023-06-09T12:52:01Z",
    "authors": [
      "Junyu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.01990v1",
    "title": "Unsupervised Spectral Demosaicing with Lightweight Spectral Attention\n  Networks",
    "summary": "This paper presents a deep learning-based spectral demosaicing technique\ntrained in an unsupervised manner. Many existing deep learning-based techniques\nrelying on supervised learning with synthetic images, often underperform on\nreal-world images especially when the number of spectral bands increases.\nAccording to the characteristics of the spectral mosaic image, this paper\nproposes a mosaic loss function, the corresponding model structure, a\ntransformation strategy, and an early stopping strategy, which form a complete\nunsupervised spectral demosaicing framework. A challenge in real-world spectral\ndemosaicing is inconsistency between the model parameters and the computational\nresources of the imager. We reduce the complexity and parameters of the\nspectral attention module by dividing the spectral attention tensor into\nspectral attention matrices in the spatial dimension and spectral attention\nvector in the channel dimension, which is more suitable for unsupervised\nframework. This paper also presents Mosaic25, a real 25-band hyperspectral\nmosaic image dataset of various objects, illuminations, and materials for\nbenchmarking. Extensive experiments on synthetic and real-world datasets\ndemonstrate that the proposed method outperforms conventional unsupervised\nmethods in terms of spatial distortion suppression, spectral fidelity,\nrobustness, and computational cost.",
    "published": "2023-07-05T02:45:44Z",
    "updated": "2023-07-05T02:45:44Z",
    "authors": [
      "Kai Feng",
      "Yongqiang Zhao",
      "Seong G. Kong",
      "Haijin Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08145v1",
    "title": "Self-Attention Based Generative Adversarial Networks For Unsupervised\n  Video Summarization",
    "summary": "In this paper, we study the problem of producing a comprehensive video\nsummary following an unsupervised approach that relies on adversarial learning.\nWe build on a popular method where a Generative Adversarial Network (GAN) is\ntrained to create representative summaries, indistinguishable from the\noriginals. The introduction of the attention mechanism into the architecture\nfor the selection, encoding and decoding of video frames, shows the efficacy of\nself-attention and transformer in modeling temporal relationships for video\nsummarization. We propose the SUM-GAN-AED model that uses a self-attention\nmechanism for frame selection, combined with LSTMs for encoding and decoding.\nWe evaluate the performance of the SUM-GAN-AED model on the SumMe, TVSum and\nCOGNIMUSE datasets. Experimental results indicate that using a self-attention\nmechanism as the frame selection mechanism outperforms the state-of-the-art on\nSumMe and leads to comparable to state-of-the-art performance on TVSum and\nCOGNIMUSE.",
    "published": "2023-07-16T19:56:13Z",
    "updated": "2023-07-16T19:56:13Z",
    "authors": [
      "Maria Nektaria Minaidi",
      "Charilaos Papaioannou",
      "Alexandros Potamianos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.15144v2",
    "title": "TKwinFormer: Top k Window Attention in Vision Transformers for Feature\n  Matching",
    "summary": "Local feature matching remains a challenging task, primarily due to\ndifficulties in matching sparse keypoints and low-texture regions. The key to\nsolving this problem lies in effectively and accurately integrating global and\nlocal information. To achieve this goal, we introduce an innovative local\nfeature matching method called TKwinFormer. Our approach employs a multi-stage\nmatching strategy to optimize the efficiency of information interaction.\nFurthermore, we propose a novel attention mechanism called Top K Window\nAttention, which facilitates global information interaction through window\ntokens prior to patch-level matching, resulting in improved matching accuracy.\nAdditionally, we design an attention block to enhance attention between\nchannels. Experimental results demonstrate that TKwinFormer outperforms\nstate-of-the-art methods on various benchmarks. Code is available at:\nhttps://github.com/LiaoYun0x0/TKwinFormer.",
    "published": "2023-08-29T09:22:42Z",
    "updated": "2025-03-31T02:56:26Z",
    "authors": [
      "Yun Liao",
      "Yide Di",
      "Hao Zhou",
      "Kaijun Zhu",
      "Mingyu Lu",
      "Yijia Zhang",
      "Qing Duan",
      "Junhui Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.07765v2",
    "title": "Echotune: A Modular Extractor Leveraging the Variable-Length Nature of\n  Speech in ASR Tasks",
    "summary": "The Transformer architecture has proven to be highly effective for Automatic\nSpeech Recognition (ASR) tasks, becoming a foundational component for a\nplethora of research in the domain. Historically, many approaches have leaned\non fixed-length attention windows, which becomes problematic for varied speech\nsamples in duration and complexity, leading to data over-smoothing and neglect\nof essential long-term connectivity. Addressing this limitation, we introduce\nEcho-MSA, a nimble module equipped with a variable-length attention mechanism\nthat accommodates a range of speech sample complexities and durations. This\nmodule offers the flexibility to extract speech features across various\ngranularities, spanning from frames and phonemes to words and discourse. The\nproposed design captures the variable length feature of speech and addresses\nthe limitations of fixed-length attention. Our evaluation leverages a parallel\nattention architecture complemented by a dynamic gating mechanism that\namalgamates traditional attention with the Echo-MSA module output. Empirical\nevidence from our study reveals that integrating Echo-MSA into the primary\nmodel's training regime significantly enhances the word error rate (WER)\nperformance, all while preserving the intrinsic stability of the original\nmodel.",
    "published": "2023-09-14T14:51:51Z",
    "updated": "2024-04-08T03:30:34Z",
    "authors": [
      "Sizhou Chen",
      "Songyang Gao",
      "Sen Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.12680v2",
    "title": "On the Optimization and Generalization of Multi-head Attention",
    "summary": "The training and generalization dynamics of the Transformer's core mechanism,\nnamely the Attention mechanism, remain under-explored. Besides, existing\nanalyses primarily focus on single-head attention. Inspired by the demonstrated\nbenefits of overparameterization when training fully-connected networks, we\ninvestigate the potential optimization and generalization advantages of using\nmultiple attention heads. Towards this goal, we derive convergence and\ngeneralization guarantees for gradient-descent training of a single-layer\nmulti-head self-attention model, under a suitable realizability condition on\nthe data. We then establish primitive conditions on the initialization that\nensure realizability holds. Finally, we demonstrate that these conditions are\nsatisfied for a simple tokenized-mixture model. We expect the analysis can be\nextended to various data-model and architecture variations.",
    "published": "2023-10-19T12:18:24Z",
    "updated": "2024-10-12T04:12:31Z",
    "authors": [
      "Puneesh Deora",
      "Rouzbeh Ghaderi",
      "Hossein Taheri",
      "Christos Thrampoulidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.19084v1",
    "title": "Roles of Scaling and Instruction Tuning in Language Perception: Model\n  vs. Human Attention",
    "summary": "Recent large language models (LLMs) have revealed strong abilities to\nunderstand natural language. Since most of them share the same basic structure,\ni.e. the transformer block, possible contributors to their success in the\ntraining process are scaling and instruction tuning. However, how these factors\naffect the models' language perception is unclear. This work compares the\nself-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different\nsizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human\nreading attention, to assess the effect of scaling and instruction tuning on\nlanguage perception. Results show that scaling enhances the human resemblance\nand improves the effective attention by reducing the trivial pattern reliance,\nwhile instruction tuning does not. However, instruction tuning significantly\nenhances the models' sensitivity to instructions. We also find that current\nLLMs are consistently closer to non-native than native speakers in attention,\nsuggesting a sub-optimal language perception of all models. Our code and data\nused in the analysis is available on GitHub.",
    "published": "2023-10-29T17:16:40Z",
    "updated": "2023-10-29T17:16:40Z",
    "authors": [
      "Changjiang Gao",
      "Shujian Huang",
      "Jixing Li",
      "Jiajun Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.20305v1",
    "title": "Bilateral Network with Residual U-blocks and Dual-Guided Attention for\n  Real-time Semantic Segmentation",
    "summary": "When some application scenarios need to use semantic segmentation technology,\nlike automatic driving, the primary concern comes to real-time performance\nrather than extremely high segmentation accuracy. To achieve a good trade-off\nbetween speed and accuracy, two-branch architecture has been proposed in recent\nyears. It treats spatial information and semantics information separately which\nallows the model to be composed of two networks both not heavy. However, the\nprocess of fusing features with two different scales becomes a performance\nbottleneck for many nowaday two-branch models. In this research, we design a\nnew fusion mechanism for two-branch architecture which is guided by attention\ncomputation. To be precise, we use the Dual-Guided Attention (DGA) module we\nproposed to replace some multi-scale transformations with the calculation of\nattention which means we only use several attention layers of near linear\ncomplexity to achieve performance comparable to frequently-used multi-layer\nfusion. To ensure that our module can be effective, we use Residual U-blocks\n(RSU) to build one of the two branches in our networks which aims to obtain\nbetter multi-scale features. Extensive experiments on Cityscapes and CamVid\ndataset show the effectiveness of our method.",
    "published": "2023-10-31T09:20:59Z",
    "updated": "2023-10-31T09:20:59Z",
    "authors": [
      "Liang Liao",
      "Liang Wan",
      "Mingsheng Liu",
      "Shusheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.07507v2",
    "title": "NAC-TCN: Temporal Convolutional Networks with Causal Dilated\n  Neighborhood Attention for Emotion Understanding",
    "summary": "In the task of emotion recognition from videos, a key improvement has been to\nfocus on emotions over time rather than a single frame. There are many\narchitectures to address this task such as GRUs, LSTMs, Self-Attention,\nTransformers, and Temporal Convolutional Networks (TCNs). However, these\nmethods suffer from high memory usage, large amounts of operations, or poor\ngradients. We propose a method known as Neighborhood Attention with\nConvolutions TCN (NAC-TCN) which incorporates the benefits of attention and\nTemporal Convolutional Networks while ensuring that causal relationships are\nunderstood which results in a reduction in computation and memory cost. We\naccomplish this by introducing a causal version of Dilated Neighborhood\nAttention while incorporating it with convolutions. Our model achieves\ncomparable, better, or state-of-the-art performance over TCNs, TCAN, LSTMs, and\nGRUs while requiring fewer parameters on standard emotion recognition datasets.\nWe publish our code online for easy reproducibility and use in other projects.",
    "published": "2023-12-12T18:41:30Z",
    "updated": "2024-01-06T05:18:44Z",
    "authors": [
      "Alexander Mehta",
      "William Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.10242v1",
    "title": "A Survey of Classical And Quantum Sequence Models",
    "summary": "Our primary objective is to conduct a brief survey of various classical and\nquantum neural net sequence models, which includes self-attention and recurrent\nneural networks, with a focus on recent quantum approaches proposed to work\nwith near-term quantum devices, while exploring some basic enhancements for\nthese quantum models. We re-implement a key representative set of these\nexisting methods, adapting an image classification approach using quantum\nself-attention to create a quantum hybrid transformer that works for text and\nimage classification, and applying quantum self-attention and quantum recurrent\nneural networks to natural language processing tasks. We also explore different\nencoding techniques and introduce positional encoding into quantum\nself-attention neural networks leading to improved accuracy and faster\nconvergence in text and image classification experiments. This paper also\nperforms a comparative analysis of classical self-attention models and their\nquantum counterparts, helping shed light on the differences in these models and\ntheir performance.",
    "published": "2023-12-15T22:21:26Z",
    "updated": "2023-12-15T22:21:26Z",
    "authors": [
      "I-Chi Chen",
      "Harshdeep Singh",
      "V L Anukruti",
      "Brian Quanz",
      "Kavitha Yogaraj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.03322v1",
    "title": "Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly\n  Detection",
    "summary": "This paper introduces a hybrid attention and autoencoder (AE) model for\nunsupervised online anomaly detection in time series. The autoencoder captures\nlocal structural patterns in short embeddings, while the attention model learns\nlong-term features, facilitating parallel computing with positional encoding.\nUnique in its approach, our proposed hybrid model combines attention and\nautoencoder for the first time in time series anomaly detection. It employs an\nattention-based mechanism, akin to the deep transformer model, with key\narchitectural modifications for predicting the next time step window in the\nautoencoder's latent space. The model utilizes a threshold from the validation\ndataset for anomaly detection and introduces an alternative method based on\nanalyzing the first statistical moment of error, improving accuracy without\ndependence on a validation dataset. Evaluation on diverse real-world benchmark\ndatasets and comparing with other well-established models, confirms the\neffectiveness of our proposed model in anomaly detection.",
    "published": "2024-01-06T22:55:02Z",
    "updated": "2024-01-06T22:55:02Z",
    "authors": [
      "Seyed Amirhossein Najafi",
      "Mohammad Hassan Asemani",
      "Peyman Setoodeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.04145v1",
    "title": "Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep\n  Reinforcement Learning Method for Global Path Planning",
    "summary": "Deep reinforcement learning (DRL) methods have recently shown promise in path\nplanning tasks. However, when dealing with global planning tasks, these methods\nface serious challenges such as poor convergence and generalization. To this\nend, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan\nArbitrarily) in this paper. Firstly, we analyze the reasons of these problems\nfrom the perspective of DRL's observation, revealing that the traditional\ndesign causes DRL to be interfered by irrelevant map information. Secondly, we\ndevelop the LOPA which utilizes a novel attention-enhanced mechanism to attain\nan improved attention capability towards the key information of the\nobservation. Such a mechanism is realized by two steps: (1) an attention model\nis built to transform the DRL's observation into two dynamic views: local and\nglobal, significantly guiding the LOPA to focus on the key information on the\ngiven maps; (2) a dual-channel network is constructed to process these two\nviews and integrate them to attain an improved reasoning capability. The LOPA\nis validated via multi-objective global path planning experiments. The result\nsuggests the LOPA has improved convergence and generalization performance as\nwell as great path planning efficiency.",
    "published": "2024-01-08T02:27:14Z",
    "updated": "2024-01-08T02:27:14Z",
    "authors": [
      "Guoming Huang",
      "Mingxin Hou",
      "Xiaofang Yuan",
      "Shuqiao Huang",
      "Yaonan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.11143v4",
    "title": "Density Adaptive Attention is All You Need: Robust Parameter-Efficient\n  Fine-Tuning Across Multiple Modalities",
    "summary": "We propose the Multi-Head Density Adaptive Attention Mechanism (DAAM), a\nnovel probabilistic attention framework that can be used for\nParameter-Efficient Fine-tuning (PEFT), and the Density Adaptive Transformer\n(DAT), designed to enhance information aggregation across multiple modalities,\nincluding Speech, Text, and Vision. DAAM integrates learnable mean and variance\ninto its attention mechanism, implemented in a multi-head framework, enabling\nit to collectively model any probability distribution for dynamic recalibration\nof feature significance. This method demonstrates significant improvements,\nespecially with highly non-stationary data, surpassing the state-of-the-art\nattention techniques in model performance, up to approximately +20% (abs.) in\naccuracy. Empirically, DAAM exhibits superior adaptability and efficacy across\na diverse range of tasks, including emotion recognition in speech, image\nclassification, and text classification, thereby establishing its robustness\nand versatility in handling data across multiple modalities. Furthermore, we\nintroduce the Importance Factor, a new learning-based metric that enhances the\nexplainability of models trained with DAAM-based methods.",
    "published": "2024-01-20T06:42:32Z",
    "updated": "2024-09-29T00:45:46Z",
    "authors": [
      "Georgios Ioannides",
      "Aman Chadha",
      "Aaron Elkins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.13055v2",
    "title": "Identifying Semantic Induction Heads to Understand In-Context Learning",
    "summary": "Although large language models (LLMs) have demonstrated remarkable\nperformance, the lack of transparency in their inference logic raises concerns\nabout their trustworthiness. To gain a better understanding of LLMs, we conduct\na detailed analysis of the operations of attention heads and aim to better\nunderstand the in-context learning of LLMs. Specifically, we investigate\nwhether attention heads encode two types of relationships between tokens\npresent in natural languages: the syntactic dependency parsed from sentences\nand the relation within knowledge graphs. We find that certain attention heads\nexhibit a pattern where, when attending to head tokens, they recall tail tokens\nand increase the output logits of those tail tokens. More crucially, the\nformulation of such semantic induction heads has a close correlation with the\nemergence of the in-context learning ability of language models. The study of\nsemantic attention heads advances our understanding of the intricate operations\nof attention heads in transformers, and further provides new insights into the\nin-context learning of LLMs.",
    "published": "2024-02-20T14:43:39Z",
    "updated": "2024-07-25T08:07:39Z",
    "authors": [
      "Jie Ren",
      "Qipeng Guo",
      "Hang Yan",
      "Dongrui Liu",
      "Quanshi Zhang",
      "Xipeng Qiu",
      "Dahua Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.16470v1",
    "title": "Unveiling Vulnerability of Self-Attention",
    "summary": "Pre-trained language models (PLMs) are shown to be vulnerable to minor word\nchanges, which poses a big threat to real-world systems. While previous studies\ndirectly focus on manipulating word inputs, they are limited by their means of\ngenerating adversarial samples, lacking generalization to versatile real-world\nattack. This paper studies the basic structure of transformer-based PLMs, the\nself-attention (SA) mechanism. (1) We propose a powerful perturbation technique\n\\textit{HackAttend}, which perturbs the attention scores within the SA matrices\nvia meticulously crafted attention masks. We show that state-of-the-art PLMs\nfall into heavy vulnerability that minor attention perturbations $(1\\%)$ can\nproduce a very high attack success rate $(98\\%)$. Our paper expands the\nconventional text attack of word perturbations to more general structural\nperturbations. (2) We introduce \\textit{S-Attend}, a novel smoothing technique\nthat effectively makes SA robust via structural perturbations. We empirically\ndemonstrate that this simple yet effective technique achieves robust\nperformance on par with adversarial training when facing various text\nattackers. Code is publicly available at \\url{github.com/liongkj/HackAttend}.",
    "published": "2024-02-26T10:31:45Z",
    "updated": "2024-02-26T10:31:45Z",
    "authors": [
      "Khai Jiet Liong",
      "Hongqiu Wu",
      "Hai Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.08058v2",
    "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
    "summary": "Large Language Models (LLMs) with hundreds of billions of parameters have\ntransformed the field of machine learning. However, serving these models at\ninference time is both compute and memory intensive, where a single request can\nrequire multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is\none of the key components of LLMs, which can account for over 50% of LLMs\nmemory and compute requirement. We observe that there is a high amount of\nredundancy across heads on which tokens they pay attention to. Based on this\ninsight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a\nhigh amount of correlation for self-attention at runtime, thus reducing both\nmemory and compute. In our experiments, we show that CHAI is able to reduce the\nmemory requirements for storing K,V cache by up to 21.4% and inference time\nlatency by up to 1.73x without any fine-tuning required. CHAI achieves this\nwith a maximum 3.2% deviation in accuracy across 3 different models (i.e.\nOPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.",
    "published": "2024-03-12T20:10:04Z",
    "updated": "2024-04-27T22:45:39Z",
    "authors": [
      "Saurabh Agarwal",
      "Bilge Acun",
      "Basil Hosmer",
      "Mostafa Elhoushi",
      "Yejin Lee",
      "Shivaram Venkataraman",
      "Dimitris Papailiopoulos",
      "Carole-Jean Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.11311v1",
    "title": "A Dual Power Grid Cascading Failure Model for the Vulnerability Analysis",
    "summary": "Considering the attacks against the power grid, one of the most effective\napproaches could be the attack to the transmission lines that leads to large\ncascading failures. Hence, the problem of locating the most critical or\nvulnerable transmission lines for a Power Grid Cascading Failure (PGCF) has\ndrawn much attention from the research society. There exists many deterministic\nsolutions and stochastic approximation algorithms aiming to analyze the power\ngrid vulnerability. However, it has been challenging to reveal the correlations\nbetween the transmission lines to identify the critical ones. In this paper, we\npropose a novel approach of learning such correlations via attention mechanism\ninspired by the Transformer based models that were initially designated to\nlearn the correlation of words in sentences. Multiple modifications and\nadjustments are proposed to support the attention mechanism producing an\ninformative correlation matrix, the Attention Matrix. With the Attention\nRanking algorithm, we are able to identify the most critical lines. The\nproposed Dual PGCF model provide a novel and effective analysis to improve the\npower grid resilience against cascading failure, which is proved by extensive\nexperiment results.",
    "published": "2024-05-18T15:04:44Z",
    "updated": "2024-05-18T15:04:44Z",
    "authors": [
      "Tianxin Zhou",
      "Xiang Li",
      "Haibing Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.17957v1",
    "title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic\n  Alignment",
    "summary": "Large Language Model (LLM) based text-to-speech (TTS) systems have\ndemonstrated remarkable capabilities in handling large speech datasets and\ngenerating natural speech for new speakers. However, LLM-based TTS models are\nnot robust as the generated output can contain repeating words, missing words\nand mis-aligned speech (referred to as hallucinations or attention errors),\nespecially when the text contains multiple occurrences of the same token. We\nexamine these challenges in an encoder-decoder transformer model and find that\ncertain cross-attention heads in such models implicitly learn the text and\nspeech alignment when trained for predicting speech tokens for a given text. To\nmake the alignment more robust, we propose techniques utilizing CTC loss and\nattention priors that encourage monotonic cross-attention over the text tokens.\nOur guided attention training technique does not introduce any new learnable\nparameters and significantly improves robustness of LLM-based TTS models.",
    "published": "2024-06-25T22:18:52Z",
    "updated": "2024-06-25T22:18:52Z",
    "authors": [
      "Paarth Neekhara",
      "Shehzeen Hussain",
      "Subhankar Ghosh",
      "Jason Li",
      "Rafael Valle",
      "Rohan Badlani",
      "Boris Ginsburg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.02423v2",
    "title": "On the Anatomy of Attention",
    "summary": "We introduce a category-theoretic diagrammatic formalism in order to\nsystematically relate and reason about machine learning models. Our diagrams\npresent architectures intuitively but without loss of essential detail, where\nnatural relationships between models are captured by graphical transformations,\nand important differences and similarities can be identified at a glance. In\nthis paper, we focus on attention mechanisms: translating folklore into\nmathematical derivations, and constructing a taxonomy of attention variants in\nthe literature. As a first example of an empirical investigation underpinned by\nour formalism, we identify recurring anatomical components of attention, which\nwe exhaustively recombine to explore a space of variations on the attention\nmechanism.",
    "published": "2024-07-02T16:50:26Z",
    "updated": "2024-07-07T17:03:05Z",
    "authors": [
      "Nikhil Khatri",
      "Tuomas Laakkonen",
      "Jonathon Liu",
      "Vincent Wang-MaÅcianica"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.06498v1",
    "title": "Enhancing spatial auditory attention decoding with neuroscience-inspired\n  prototype training",
    "summary": "The spatial auditory attention decoding (Sp-AAD) technology aims to determine\nthe direction of auditory attention in multi-talker scenarios via neural\nrecordings. Despite the success of recent Sp-AAD algorithms, their performance\nis hindered by trial-specific features in EEG data. This study aims to improve\ndecoding performance against these features. Studies in neuroscience indicate\nthat spatial auditory attention can be reflected in the topological\ndistribution of EEG energy across different frequency bands. This insight\nmotivates us to propose Prototype Training, a neuroscience-inspired method for\nSp-AAD. This method constructs prototypes with enhanced energy distribution\nrepresentations and reduced trial-specific characteristics, enabling the model\nto better capture auditory attention features. To implement prototype training,\nan EEGWaveNet that employs the wavelet transform of EEG is further proposed.\nDetailed experiments indicate that the EEGWaveNet with prototype training\noutperforms other competitive models on various datasets, and the effectiveness\nof the proposed method is also validated. As a training method independent of\nmodel architecture, prototype training offers new insights into the field of\nSp-AAD.",
    "published": "2024-07-09T02:03:19Z",
    "updated": "2024-07-09T02:03:19Z",
    "authors": [
      "Zelin Qiu",
      "Jianjun Gu",
      "Dingding Yao",
      "Junfeng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.08608v2",
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and\n  Low-precision",
    "summary": "Attention, as a core layer of the ubiquitous Transformer architecture, is the\nbottleneck for large language models and long-context applications.\nFlashAttention elaborated an approach to speed up attention on GPUs through\nminimizing memory reads/writes. However, it has yet to take advantage of new\ncapabilities present in recent hardware, with FlashAttention-2 achieving only\n35% utilization on the H100 GPU. We develop three main techniques to speed up\nattention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to\n(1) overlap overall computation and data movement via warp-specialization and\n(2) interleave block-wise matmul and softmax operations, and (3) block\nquantization and incoherent processing that leverages hardware support for FP8\nlow-precision. We demonstrate that our method, FlashAttention-3, achieves\nspeedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s\n(75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate\nthat FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a\nbaseline FP8 attention.",
    "published": "2024-07-11T15:44:48Z",
    "updated": "2024-07-12T22:15:02Z",
    "authors": [
      "Jay Shah",
      "Ganesh Bikshandi",
      "Ying Zhang",
      "Vijay Thakkar",
      "Pradeep Ramani",
      "Tri Dao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.13885v1",
    "title": "Attention in SRAM on Tenstorrent Grayskull",
    "summary": "When implementations of the Transformer's self-attention layer utilize SRAM\ninstead of DRAM, they can achieve significant speedups. The Tenstorrent\nGrayskull architecture provides a large SRAM, distributed across a grid of\ncores. This work presents a fused kernel for Grayskull, that exclusively\nutilizes its large SRAM by combining matrix multiplication, attention score\nscaling and Softmax operations. Additionally, a dedicated Softmax kernel\nutilizing the SRAM and a CPU implementation serving as a baseline are\npresented. The Softmax operation consumes most of the runtime in the\ncomputation of attention weights from queries and keys on Grayskull. The\nspeedup of the dedicated Softmax kernel compared to the CPU implementation is\nup to $10 \\times$, and the Softmax implementation inside the fused kernel is\napproximately $1.8 \\times$ faster than the dedicated Softmax kernel. The time\nand memory complexity of all implementations is quadratic in sequence length.\nCurrently, the Grayskull e150 is approximately $30 \\times$ cheaper for the\ngeneral public than an Nvidia H100 PCIe (a state-of-the-art GPU) and offers\napproximately $1.5 \\times$ more SRAM.",
    "published": "2024-07-18T20:19:36Z",
    "updated": "2024-07-18T20:19:36Z",
    "authors": [
      "Moritz ThÃ¼ning"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.07995v2",
    "title": "Depth Matters: Exploring Deep Interactions of RGB-D for Semantic\n  Segmentation in Traffic Scenes",
    "summary": "RGB-D has gradually become a crucial data source for understanding complex\nscenes in assisted driving. However, existing studies have paid insufficient\nattention to the intrinsic spatial properties of depth maps. This oversight\nsignificantly impacts the attention representation, leading to prediction\nerrors caused by attention shift issues. To this end, we propose a novel\nlearnable Depth interaction Pyramid Transformer (DiPFormer) to explore the\neffectiveness of depth. Firstly, we introduce Depth Spatial-Aware Optimization\n(Depth SAO) as offset to represent real-world spatial relationships. Secondly,\nthe similarity in the feature space of RGB-D is learned by Depth Linear\nCross-Attention (Depth LCA) to clarify spatial differences at the pixel level.\nFinally, an MLP Decoder is utilized to effectively fuse multi-scale features\nfor meeting real-time requirements. Comprehensive experiments demonstrate that\nthe proposed DiPFormer significantly addresses the issue of attention\nmisalignment in both road detection (+7.5%) and semantic segmentation (+4.9% /\n+1.5%) tasks. DiPFormer achieves state-of-the-art performance on the KITTI\n(97.57% F-score on KITTI road and 68.74% mIoU on KITTI-360) and Cityscapes\n(83.4% mIoU) datasets.",
    "published": "2024-09-12T12:39:34Z",
    "updated": "2025-07-01T15:38:14Z",
    "authors": [
      "Siyu Chen",
      "Ting Han",
      "Changshe Zhang",
      "Weiquan Liu",
      "Jinhe Su",
      "Zongyue Wang",
      "Guorong Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.17560v1",
    "title": "Dynamic Subframe Splitting and Spatio-Temporal Motion Entangled Sparse\n  Attention for RGB-E Tracking",
    "summary": "Event-based bionic camera asynchronously captures dynamic scenes with high\ntemporal resolution and high dynamic range, offering potential for the\nintegration of events and RGB under conditions of illumination degradation and\nfast motion. Existing RGB-E tracking methods model event characteristics\nutilising attention mechanism of Transformer before integrating both\nmodalities. Nevertheless, these methods involve aggregating the event stream\ninto a single event frame, lacking the utilisation of the temporal information\ninherent in the event stream.Moreover, the traditional attention mechanism is\nwell-suited for dense semantic features, while the attention mechanism for\nsparse event features require revolution. In this paper, we propose a dynamic\nevent subframe splitting strategy to split the event stream into more\nfine-grained event clusters, aiming to capture spatio-temporal features that\ncontain motion cues. Based on this, we design an event-based sparse attention\nmechanism to enhance the interaction of event features in temporal and spatial\ndimensions. The experimental results indicate that our method outperforms\nexisting state-of-the-art methods on the FE240 and COESOT datasets, providing\nan effective processing manner for the event data.",
    "published": "2024-09-26T06:12:08Z",
    "updated": "2024-09-26T06:12:08Z",
    "authors": [
      "Pengcheng Shao",
      "Tianyang Xu",
      "Xuefeng Zhu",
      "Xiaojun Wu",
      "Josef Kittler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.09040v1",
    "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention\n  Manipulation",
    "summary": "This paper studies the vulnerabilities of transformer-based Large Language\nModels (LLMs) to jailbreaking attacks, focusing specifically on the\noptimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe\na positive correlation between the effectiveness of attacks and the internal\nbehaviors of the models. For instance, attacks tend to be less effective when\nmodels pay more attention to system prompts designed to ensure LLM safety\nalignment. Building on this discovery, we introduce an enhanced method that\nmanipulates models' attention scores to facilitate LLM jailbreaking, which we\nterm AttnGCG. Empirically, AttnGCG shows consistent improvements in attack\nefficacy across diverse LLMs, achieving an average increase of ~7% in the\nLlama-2 series and ~10% in the Gemma series. Our strategy also demonstrates\nrobust attack transferability against both unseen harmful goals and black-box\nLLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score\nvisualization is more interpretable, allowing us to gain better insights into\nhow our targeted attention manipulation facilitates more effective\njailbreaking. We release the code at\nhttps://github.com/UCSC-VLAA/AttnGCG-attack.",
    "published": "2024-10-11T17:55:09Z",
    "updated": "2024-10-11T17:55:09Z",
    "authors": [
      "Zijun Wang",
      "Haoqin Tu",
      "Jieru Mei",
      "Bingchen Zhao",
      "Yisen Wang",
      "Cihang Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.19574v2",
    "title": "KV Shifting Attention Enhances Language Modeling",
    "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
    "published": "2024-11-29T09:42:38Z",
    "updated": "2024-12-05T12:19:38Z",
    "authors": [
      "Mingyu Xu",
      "Wei Cheng",
      "Bingning Wang",
      "Weipeng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.09828v1",
    "title": "MSC: Multi-Scale Spatio-Temporal Causal Attention for Autoregressive\n  Video Diffusion",
    "summary": "Diffusion transformers enable flexible generative modeling for video.\nHowever, it is still technically challenging and computationally expensive to\ngenerate high-resolution videos with rich semantics and complex motion. Similar\nto languages, video data are also auto-regressive by nature, so it is\ncounter-intuitive to use attention mechanism with bi-directional dependency in\nthe model. Here we propose a Multi-Scale Causal (MSC) framework to address\nthese problems. Specifically, we introduce multiple resolutions in the spatial\ndimension and high-low frequencies in the temporal dimension to realize\nefficient attention calculation. Furthermore, attention blocks on multiple\nscales are combined in a controlled way to allow causal conditioning on noisy\nimage frames for diffusion training, based on the idea that noise destroys\ninformation at different rates on different resolutions. We theoretically show\nthat our approach can greatly reduce the computational complexity and enhance\nthe efficiency of training. The causal attention diffusion framework can also\nbe used for auto-regressive long video generation, without violating the\nnatural order of frame sequences.",
    "published": "2024-12-13T03:39:09Z",
    "updated": "2024-12-13T03:39:09Z",
    "authors": [
      "Xunnong Xu",
      "Mengying Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.01406v1",
    "title": "nnY-Net: Swin-NeXt with Cross-Attention for 3D Medical Images\n  Segmentation",
    "summary": "This paper provides a novel 3D medical image segmentation model structure\ncalled nnY-Net. This name comes from the fact that our model adds a\ncross-attention module at the bottom of the U-net structure to form a Y\nstructure. We integrate the advantages of the two latest SOTA models, MedNeXt\nand SwinUNETR, and use Swin Transformer as the encoder and ConvNeXt as the\ndecoder to innovatively design the Swin-NeXt structure. Our model uses the\nlowest-level feature map of the encoder as Key and Value and uses patient\nfeatures such as pathology and treatment information as Query to calculate the\nattention weights in a Cross Attention module. Moreover, we simplify some pre-\nand post-processing as well as data enhancement methods in 3D image\nsegmentation based on the dynUnet and nnU-net frameworks. We integrate our\nproposed Swin-NeXt with Cross-Attention framework into this framework. Last, we\nconstruct a DiceFocalCELoss to improve the training efficiency for the uneven\ndata convergence of voxel classification.",
    "published": "2025-01-02T18:46:41Z",
    "updated": "2025-01-02T18:46:41Z",
    "authors": [
      "Haixu Liu",
      "Zerui Tao",
      "Wenzhen Dong",
      "Qiuzhuang Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.02040v2",
    "title": "A Separable Self-attention Inspired by the State Space Model for\n  Computer Vision",
    "summary": "Mamba is an efficient State Space Model (SSM) with linear computational\ncomplexity. Although SSMs are not suitable for handling non-causal data, Vision\nMamba (ViM) methods still demonstrate good performance in tasks such as image\nclassification and object detection. Recent studies have shown that there is a\nrich theoretical connection between state space models and attention variants.\nWe propose a novel separable self attention method, for the first time\nintroducing some excellent design concepts of Mamba into separable\nself-attention. To ensure a fair comparison with ViMs, we introduce VMINet, a\nsimple yet powerful prototype architecture, constructed solely by stacking our\nnovel attention modules with the most basic down-sampling layers. Notably,\nVMINet differs significantly from the conventional Transformer architecture.\nOur experiments demonstrate that VMINet has achieved competitive results on\nimage classification and high-resolution dense prediction tasks.Code is\navailable at: https://github.com/yws-wxs/VMINet.",
    "published": "2025-01-03T15:23:36Z",
    "updated": "2025-05-20T01:01:55Z",
    "authors": [
      "Juntao Zhang",
      "Shaogeng Liu",
      "Kun Bian",
      "You Zhou",
      "Pei Zhang",
      "Jianning Liu",
      "Jun Zhou",
      "Bingyan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.15021v1",
    "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
    "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
    "published": "2025-01-25T02:01:56Z",
    "updated": "2025-01-25T02:01:56Z",
    "authors": [
      "Zunhai Su",
      "Wang Shen",
      "Linge Li",
      "Zhe Chen",
      "Hanyu Wei",
      "Huangqi Yu",
      "Kehong Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.18984v2",
    "title": "Context Matters: Query-aware Dynamic Long Sequence Modeling of Gigapixel\n  Images",
    "summary": "Whole slide image (WSI) analysis presents significant computational\nchallenges due to the massive number of patches in gigapixel images. While\ntransformer architectures excel at modeling long-range correlations through\nself-attention, their quadratic computational complexity makes them impractical\nfor computational pathology applications. Existing solutions like local-global\nor linear self-attention reduce computational costs but compromise the strong\nmodeling capabilities of full self-attention. In this work, we propose Querent,\ni.e., the query-aware long contextual dynamic modeling framework, which\nachieves a theoretically bounded approximation of full self-attention while\ndelivering practical efficiency. Our method adaptively predicts which\nsurrounding regions are most relevant for each patch, enabling focused yet\nunrestricted attention computation only with potentially important contexts. By\nusing efficient region-wise metadata computation and importance estimation, our\napproach dramatically reduces computational overhead while preserving global\nperception to model fine-grained patch correlations. Through comprehensive\nexperiments on biomarker prediction, gene mutation prediction, cancer\nsubtyping, and survival analysis across over 10 WSI datasets, our method\ndemonstrates superior performance compared to the state-of-the-art approaches.\nCodes are available at https://github.com/dddavid4real/Querent.",
    "published": "2025-01-31T09:29:21Z",
    "updated": "2025-05-24T06:42:23Z",
    "authors": [
      "Zhengrui Guo",
      "Qichen Sun",
      "Jiabo Ma",
      "Lishuang Feng",
      "Jinzhuo Wang",
      "Hao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01578v3",
    "title": "ReGLA: Refining Gated Linear Attention",
    "summary": "Recent advancements in Large Language Models (LLMs) have set themselves apart\nwith their exceptional performance in complex language modelling tasks.\nHowever, these models are also known for their significant computational and\nstorage requirements, primarily due to the quadratic computation complexity of\nsoftmax attention. To mitigate this issue, linear attention has been designed\nto reduce the quadratic space-time complexity that is inherent in standard\ntransformers. In this work, we embarked on a comprehensive exploration of three\nkey components that substantially impact the performance of the Gated Linear\nAttention module: feature maps, normalization, and the gating mechanism. We\ndeveloped a feature mapping function to address some crucial issues that\nprevious suggestions overlooked. Then we offered further rationale for the\nintegration of normalization layers to stabilize the training process.\nMoreover, we explored the saturation phenomenon of the gating mechanism and\naugmented it with a refining module. We conducted extensive experiments and\nshowed our architecture outperforms previous Gated Linear Attention mechanisms\nin extensive tasks including training from scratch and post-linearization with\ncontinual pre-training.",
    "published": "2025-02-03T18:03:13Z",
    "updated": "2025-08-08T18:42:33Z",
    "authors": [
      "Peng Lu",
      "Ivan Kobyzev",
      "Mehdi Rezagholizadeh",
      "Boxing Chen",
      "Philippe Langlais"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.03805v1",
    "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
    "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
    "published": "2025-02-06T06:31:47Z",
    "updated": "2025-02-06T06:31:47Z",
    "authors": [
      "Yuan Feng",
      "Junlin Lv",
      "Yukun Cao",
      "Xike Xie",
      "S Kevin Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.04369v1",
    "title": "HSI: A Holistic Style Injector for Arbitrary Style Transfer",
    "summary": "Attention-based arbitrary style transfer methods have gained significant\nattention recently due to their impressive ability to synthesize style details.\nHowever, the point-wise matching within the attention mechanism may overly\nfocus on local patterns such that neglect the remarkable global features of\nstyle images. Additionally, when processing large images, the quadratic\ncomplexity of the attention mechanism will bring high computational load. To\nalleviate above problems, we propose Holistic Style Injector (HSI), a novel\nattention-style transformation module to deliver artistic expression of target\nstyle. Specifically, HSI performs stylization only based on global style\nrepresentation that is more in line with the characteristics of style transfer,\nto avoid generating local disharmonious patterns in stylized images. Moreover,\nwe propose a dual relation learning mechanism inside the HSI to dynamically\nrender images by leveraging semantic similarity in content and style, ensuring\nthe stylized images preserve the original content and improve style fidelity.\nNote that the proposed HSI achieves linear computational complexity because it\nestablishes feature mapping through element-wise multiplication rather than\nmatrix multiplication. Qualitative and quantitative results demonstrate that\nour method outperforms state-of-the-art approaches in both effectiveness and\nefficiency.",
    "published": "2025-02-05T09:36:24Z",
    "updated": "2025-02-05T09:36:24Z",
    "authors": [
      "Shuhao Zhang",
      "Hui Kang",
      "Yang Liu",
      "Fang Mei",
      "Hongjuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08246v2",
    "title": "Inference-time sparse attention with asymmetric indexing",
    "summary": "Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compatible vector search algorithms based on standard\npartitioning methods such as k-means. However, such partitioning methods yield\npoor results in this context because (1) the keys and queries follow different\ndistributions, and (2) the RoPE positional encoding hinders the bucket\nassignment.\n  This paper introduces Saap (Self-Attention with Asymmetric Partitions), which\novercomes these problems. It is an asymmetrical indexing technique that employs\ndistinct partitions for keys and queries, thereby approximating self-attention\nwith a data-adaptive sparsity pattern. It works on pretrained language models\nand only requires to train (offline) a small query classifier. On a long\ncontext Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens,\nSaap typically reduces by a factor of 20 the fraction of memory that needs to\nbe looked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2.",
    "published": "2025-02-12T09:39:54Z",
    "updated": "2025-06-03T10:07:35Z",
    "authors": [
      "Pierre-Emmanuel MazarÃ©",
      "Gergely Szilvasy",
      "Maria Lomeli",
      "Francisco Massa",
      "Naila Murray",
      "HervÃ© JÃ©gou",
      "Matthijs Douze"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.02732v4",
    "title": "Why do LLMs attend to the first token?",
    "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training.",
    "published": "2025-04-03T16:17:55Z",
    "updated": "2025-08-05T16:43:21Z",
    "authors": [
      "Federico Barbero",
      "Ãlvaro Arroyo",
      "Xiangming Gu",
      "Christos Perivolaropoulos",
      "Michael Bronstein",
      "Petar VeliÄkoviÄ",
      "Razvan Pascanu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.04335v2",
    "title": "Hallucinated Span Detection with Multi-View Attention Features",
    "summary": "This study addresses the problem of hallucinated span detection in the\noutputs of large language models. It has received less attention than\noutput-level hallucination detection despite its practical importance. Prior\nwork has shown that attentions often exhibit irregular patterns when\nhallucinations occur. Motivated by these findings, we extract features from the\nattention matrix that provide complementary views capturing (a) whether certain\ntokens are influential or ignored, (b) whether attention is biased toward\nspecific subsets, and (c) whether a token is generated referring to a narrow or\nbroad context, in the generation. These features are input to a\nTransformer-based classifier to conduct sequential labelling to identify\nhallucinated spans. Experimental results indicate that the proposed method\noutperforms strong baselines on hallucinated span detection with longer input\ncontexts, such as data-to-text and summarisation tasks.",
    "published": "2025-04-06T03:00:58Z",
    "updated": "2025-09-15T04:21:37Z",
    "authors": [
      "Yuya Ogasa",
      "Yuki Arase"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.04199v1",
    "title": "Enhanced SCanNet with CBAM and Dice Loss for Semantic Change Detection",
    "summary": "Semantic Change Detection (SCD) in remote sensing imagery requires accurately\nidentifying land-cover changes across multi-temporal image pairs. Despite\nsubstantial advancements, including the introduction of transformer-based\narchitectures, current SCD models continue to struggle with challenges such as\nnoisy inputs, subtle class boundaries, and significant class imbalance. In this\nstudy, we propose enhancing the Semantic Change Network (SCanNet) by\nintegrating the Convolutional Block Attention Module (CBAM) and employing Dice\nloss during training. CBAM sequentially applies channel attention to highlight\nfeature maps with the most meaningful content, followed by spatial attention to\npinpoint critical regions within these maps. This sequential approach ensures\nprecise suppression of irrelevant features and spatial noise, resulting in more\naccurate and robust detection performance compared to attention mechanisms that\napply both processes simultaneously or independently. Dice loss, designed\nexplicitly for handling class imbalance, further boosts sensitivity to minority\nchange classes. Quantitative experiments conducted on the SECOND dataset\ndemonstrate consistent improvements. Qualitative analysis confirms these\nimprovements, showing clearer segmentation boundaries and more accurate\nrecovery of small-change regions. These findings highlight the effectiveness of\nattention mechanisms and Dice loss in improving feature representation and\naddressing class imbalance in semantic change detection tasks.",
    "published": "2025-05-07T07:54:03Z",
    "updated": "2025-05-07T07:54:03Z",
    "authors": [
      "Athulya Ratnayake",
      "Buddhi Wijenayake",
      "Praveen Sumanasekara",
      "Roshan Godaliyadda",
      "Vijitha Herath",
      "Parakrama Ekanayake"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07377v1",
    "title": "Examining the Role of LLM-Driven Interactions on Attention and Cognitive\n  Engagement in Virtual Classrooms",
    "summary": "Transforming educational technologies through the integration of large\nlanguage models (LLMs) and virtual reality (VR) offers the potential for\nimmersive and interactive learning experiences. However, the effects of LLMs on\nuser engagement and attention in educational environments remain open\nquestions. In this study, we utilized a fully LLM-driven virtual learning\nenvironment, where peers and teachers were LLM-driven, to examine how students\nbehaved in such settings. Specifically, we investigate how peer question-asking\nbehaviors influenced student engagement, attention, cognitive load, and\nlearning outcomes and found that, in conditions where LLM-driven peer learners\nasked questions, students exhibited more targeted visual scanpaths, with their\nattention directed toward the learning content, particularly in complex\nsubjects. Our results suggest that peer questions did not introduce extraneous\ncognitive load directly, as the cognitive load is strongly correlated with\nincreased attention to the learning material. Considering these findings, we\nprovide design recommendations for optimizing VR learning spaces.",
    "published": "2025-05-12T09:21:19Z",
    "updated": "2025-05-12T09:21:19Z",
    "authors": [
      "Suleyman Ozdel",
      "Can Sarpkaya",
      "Efe Bozkir",
      "Hong Gao",
      "Enkelejda Kasneci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17425v1",
    "title": "Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads",
    "summary": "Multimodal models like CLIP have gained significant attention due to their\nremarkable zero-shot performance across various tasks. However, studies have\nrevealed that CLIP can inadvertently learn spurious associations between target\nvariables and confounding factors. To address this, we introduce\n\\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies\nspurious attention heads in Vision Transformers via mechanistic insights and\nmitigates them through targeted ablation. Furthermore, LTC identifies salient,\ntask-relevant attention heads, enabling the integration of discriminative\nfeatures through orthogonal projection to improve classification performance.\nWe evaluate LTC on benchmarks with inherent background and gender biases,\nachieving over a $>50\\%$ gain in worst-group accuracy compared to non-training\npost-hoc baselines. Additionally, we visualize the representation of selected\nheads and find that the presented interpretation corroborates our contrastive\nmechanism for identifying both spurious and salient attention heads. Code\navailable at https://github.com/wj210/CLIP_LTC.",
    "published": "2025-05-23T03:13:42Z",
    "updated": "2025-05-23T03:13:42Z",
    "authors": [
      "Wei Jie Yeo",
      "Rui Mao",
      "Moloud Abdar",
      "Erik Cambria",
      "Ranjan Satapathy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19531v1",
    "title": "Minimalist Softmax Attention Provably Learns Constrained Boolean\n  Functions",
    "summary": "We study the computational limits of learning $k$-bit Boolean functions\n(specifically, $\\mathrm{AND}$, $\\mathrm{OR}$, and their noisy variants), using\na minimalist single-head softmax-attention mechanism, where $k=\\Theta(d)$\nrelevant bits are selected from $d$ inputs. We show that these simple\n$\\mathrm{AND}$ and $\\mathrm{OR}$ functions are unsolvable with a single-head\nsoftmax-attention mechanism alone. However, with teacher forcing, the same\nminimalist attention is capable of solving them. These findings offer two key\ninsights: Architecturally, solving these Boolean tasks requires only minimalist\nattention, without deep Transformer blocks or FFNs. Methodologically, one\ngradient descent update with supervision suffices and replaces the multi-step\nChain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for\nsolving Boolean problems. Together, the bounds expose a fundamental gap between\nwhat this minimal architecture achieves under ideal supervision and what is\nprovably impossible under standard training.",
    "published": "2025-05-26T05:33:26Z",
    "updated": "2025-05-26T05:33:26Z",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Xiwen Zhang",
      "Maojiang Su",
      "Zhao Song",
      "Han Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.00691v4",
    "title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for\n  Accelerated Convergence in Permutation-Invariant Neural Networks for\n  Reinforcement Learning",
    "summary": "Training reinforcement learning (RL) agents often requires significant\ncomputational resources and prolonged training durations. To address this\nchallenge, we build upon prior work that introduced a neural architecture with\npermutation-invariant sensory processing. We propose a modified attention\nmechanism that applies a non-linear transformation to the key vectors (K),\nproducing enriched representations (K') through a custom mapping function. This\nNonlinear Attention (NLA) mechanism enhances the representational capacity of\nthe attention layer, enabling the agent to learn more expressive feature\ninteractions. As a result, our model achieves significantly faster convergence\nand improved training efficiency, while maintaining performance on par with the\nbaseline. These results highlight the potential of nonlinear attention\nmechanisms to accelerate reinforcement learning without sacrificing\neffectiveness.",
    "published": "2025-05-31T19:50:37Z",
    "updated": "2025-06-23T08:46:29Z",
    "authors": [
      "Junaid Muzaffar",
      "Khubaib Ahmed",
      "Ingo Frommholz",
      "Zeeshan Pervez",
      "Ahsan ul Haq"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.00846v2",
    "title": "Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor\n  Programs",
    "summary": "In modern theoretical analyses of neural networks, the infinite-width limit\nis often invoked to justify Gaussian approximations of neuron preactivations\n(e.g., via neural network Gaussian processes or Tensor Programs). However,\nthese Gaussian-based asymptotic theories have so far been unable to capture the\nbehavior of attention layers, except under special regimes such as infinitely\nmany heads or tailored scaling schemes. In this paper, leveraging the Tensor\nPrograms framework, we rigorously identify the infinite-width limit\ndistribution of variables within a single attention layer under realistic\narchitectural dimensionality and standard $1/\\sqrt{n}$-scaling with $n$\ndimensionality. We derive the exact form of this limit law without resorting to\ninfinite-head approximations or tailored scalings, demonstrating that it\ndeparts fundamentally from Gaussianity. This limiting distribution exhibits\nnon-Gaussianity from a hierarchical structure, being Gaussian conditional on\nthe random similarity scores. Numerical experiments validate our theoretical\npredictions, confirming the effectiveness of our theory at finite width and\naccurate description of finite-head attentions. Beyond characterizing a\nstandalone attention layer, our findings lay the groundwork for developing a\nunified theory of deep Transformer architectures in the infinite-width regime.",
    "published": "2025-06-01T05:53:47Z",
    "updated": "2025-10-26T06:38:29Z",
    "authors": [
      "Mana Sakai",
      "Ryo Karakida",
      "Masaaki Imaizumi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01582v1",
    "title": "Bayes optimal learning of attention-indexed models",
    "summary": "We introduce the attention-indexed model (AIM), a theoretical framework for\nanalyzing learning in deep attention layers. Inspired by multi-index models,\nAIM captures how token-level outputs emerge from layered bilinear interactions\nover high-dimensional embeddings. Unlike prior tractable attention models, AIM\nallows full-width key and query matrices, aligning more closely with practical\ntransformers. Using tools from statistical mechanics and random matrix theory,\nwe derive closed-form predictions for Bayes-optimal generalization error and\nidentify sharp phase transitions as a function of sample complexity, model\nwidth, and sequence length. We propose a matching approximate message passing\nalgorithm and show that gradient descent can reach optimal performance. AIM\noffers a solvable playground for understanding learning in modern attention\narchitectures.",
    "published": "2025-06-02T12:11:26Z",
    "updated": "2025-06-02T12:11:26Z",
    "authors": [
      "Fabrizio Boncoraglio",
      "Emanuele Troiani",
      "Vittorio Erba",
      "Lenka ZdeborovÃ¡"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16640v3",
    "title": "Long-Context Generalization with Sparse Attention",
    "summary": "Transformer-based architectures traditionally employ softmax to compute\nattention weights, which produces dense distributions over all tokens in a\nsequence. While effective in many settings, this density has been shown to be\ndetrimental for tasks that demand precise focus on fixed-size patterns: as\nsequence length increases, non-informative tokens accumulate attention\nprobability mass, leading to dispersion and representational collapse. We show\nin this paper that dynamically sparse attention mechanisms using\n$\\alpha$-entmax can avoid these issues, due to their ability to assign exact\nzeros to irrelevant tokens. Furthermore, we introduce Adaptive-Scalable Entmax\n(ASEntmax), which endows $\\alpha$-entmax with a learnable temperature\nparameter, allowing the attention distribution to interpolate between sparse\n(pattern-focused) and dense (softmax-like) regimes. Our empirical evaluation on\nsynthetic tasks and language modeling demonstrates that ASEntmax substantially\noutperforms softmax, scalable softmax, and fixed-temperature $\\alpha$-entmax\nbaselines, achieving up to 1000$\\times$ length extrapolation on synthetic\nbenchmarks and superior long-context generalization on language modeling while\npreserving short-context performance, including better perplexity trends and\nhigher retrieval accuracies at 8$\\times$ training length.",
    "published": "2025-06-19T22:43:25Z",
    "updated": "2025-09-27T01:15:13Z",
    "authors": [
      "Pavlo Vasylenko",
      "Hugo Pitorro",
      "AndrÃ© F. T. Martins",
      "Marcos Treviso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02666v1",
    "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for\n  Self-Supervised Representation Learning",
    "summary": "In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications.",
    "published": "2025-07-03T14:29:43Z",
    "updated": "2025-07-03T14:29:43Z",
    "authors": [
      "Junyu Wang",
      "Tianrui Wang",
      "Meng Ge",
      "Longbiao Wang",
      "Jianwu Dang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.07120v1",
    "title": "Faster VGGT with Block-Sparse Global Attention",
    "summary": "Efficient and accurate feed-forward multi-view reconstruction has long been\nan important task in computer vision. Recent transformer-based models like VGGT\nand $\\pi^3$ have achieved impressive results with simple architectures, yet\nthey face an inherent runtime bottleneck, due to the quadratic complexity of\nthe global attention layers, that limits the scalability to large image sets.\nIn this paper, we empirically analyze the global attention matrix of these\nmodels and observe that probability mass concentrates on a small subset of\npatch-patch interactions that correspond to cross-view geometric matches.\nMotivated by the structured attention and inspired by recent advancement in\nlarge language models, we propose a replacement for the dense global attention\noperation based on highly optimized block-sparse kernels, yielding up to\n$4\\times$ faster inference with comparable task performance. Our retrofit\nrequires no retraining of the backbone, extends to both VGGT and $\\pi^3$, and\nsupports large image collections. Evaluations on a comprehensive suite of\nmulti-view benchmarks demonstrate the effectiveness of our approach.",
    "published": "2025-09-08T18:16:09Z",
    "updated": "2025-09-08T18:16:09Z",
    "authors": [
      "Chung-Shien Brian Wang",
      "Christian Schmidt",
      "Jens Piekenbrinck",
      "Bastian Leibe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16955v1",
    "title": "Quantum Adaptive Self-Attention for Financial Rebalancing: An Empirical\n  Study on Automated Market Makers in Decentralized Finance",
    "summary": "We formulate automated market maker (AMM) \\emph{rebalancing} as a binary\ndetection problem and study a hybrid quantum--classical self-attention block,\n\\textbf{Quantum Adaptive Self-Attention (QASA)}. QASA constructs quantum\nqueries/keys/values via variational quantum circuits (VQCs) and applies\nstandard softmax attention over Pauli-$Z$ expectation vectors, yielding a\ndrop-in attention module for financial time-series decision making. Using daily\ndata for \\textbf{BTCUSDC} over \\textbf{Jan-2024--Jan-2025} with a 70/15/15\ntime-series split, we compare QASA against classical ensembles, a transformer,\nand pure quantum baselines under Return, Sharpe, and Max Drawdown. The\n\\textbf{QASA-Sequence} variant attains the \\emph{best single-model\nrisk-adjusted performance} (\\textbf{13.99\\%} return; \\textbf{Sharpe 1.76}),\nwhile hybrid models average \\textbf{11.2\\%} return (vs.\\ 9.8\\% classical; 4.4\\%\npure quantum), indicating a favorable performance--stability--cost trade-off.",
    "published": "2025-09-21T07:27:14Z",
    "updated": "2025-09-21T07:27:14Z",
    "authors": [
      "Chi-Sheng Chen",
      "Aidan Hung-Wen Tsai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26538v1",
    "title": "HilbertA: Hilbert Attention for Image Generation with Diffusion Models",
    "summary": "Designing sparse attention for diffusion transformers requires reconciling\ntwo-dimensional spatial locality with GPU efficiency, a trade-off that current\nmethods struggle to achieve. Existing approaches enforce two-dimensional\nspatial locality but often incur uncoalesced memory access. We present\nHilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA\nreorders image tokens along Hilbert curves to achieve a contiguous memory\nlayout while preserving spatial neighborhoods, and employs a sliding schedule\nacross layers to enable long-range information propagation without repeated or\nuncoalesced memory access. To further enhance cross-tile communication and\npositional awareness, HilbertA introduces a small central shared region.\nImplemented in Triton, HilbertA delivers comparable image quality with\nsignificant acceleration over prior methods on Flux.1-dev, demonstrating the\nfeasibility of hardware-aligned two-dimensional sparse attention for\nhigh-resolution image generation. HilbertA delivers attention speedups of\n$2.3\\times$ when generating $1024\\times 1024$ images, and up to $4.17\\times$ at\n$2048\\times 2048$, while achieving image quality comparable to or surpassing\nbaselines.",
    "published": "2025-09-30T17:13:22Z",
    "updated": "2025-09-30T17:13:22Z",
    "authors": [
      "Shaoyi Zheng",
      "Wenbo Lu",
      "Yuxuan Xia",
      "Haomin Liu",
      "Shengjie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02219v1",
    "title": "Contrastive Retrieval Heads Improve Attention-Based Re-Ranking",
    "summary": "The strong zero-shot and long-context capabilities of recent Large Language\nModels (LLMs) have paved the way for highly effective re-ranking systems.\nAttention-based re-rankers leverage attention weights from transformer heads to\nproduce relevance scores, but not all heads are created equally: many\ncontribute noise and redundancy, thus limiting performance. To address this, we\nintroduce CoRe heads, a small set of retrieval heads identified via a\ncontrastive scoring metric that explicitly rewards high attention heads that\ncorrelate with relevant documents, while downplaying nodes with higher\nattention that correlate with irrelevant documents. This relative ranking\ncriterion isolates the most discriminative heads for re-ranking and yields a\nstate-of-the-art list-wise re-ranker. Extensive experiments with three LLMs\nshow that aggregated signals from CoRe heads, constituting less than 1% of all\nheads, substantially improve re-ranking accuracy over strong baselines. We\nfurther find that CoRe heads are concentrated in middle layers, and pruning the\ncomputation of final 50% of model layers preserves accuracy while significantly\nreducing inference time and memory usage.",
    "published": "2025-10-02T17:03:09Z",
    "updated": "2025-10-02T17:03:09Z",
    "authors": [
      "Linh Tran",
      "Yulong Li",
      "Radu Florian",
      "Wei Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10802v2",
    "title": "MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral\n  Cloud Segmentation",
    "summary": "Clouds remain a critical challenge in optical satellite imagery, hindering\nreliable analysis for environmental monitoring, land cover mapping, and climate\nresearch. To overcome this, we propose MSCloudCAM, a Cross-Attention with\nMulti-Scale Context Network tailored for multispectral and multi-sensor cloud\nsegmentation. Our framework exploits the spectral richness of Sentinel-2\n(CloudSEN12) and Landsat-8 (L8Biome) data to classify four semantic categories:\nclear sky, thin cloud, thick cloud, and cloud shadow. MSCloudCAM combines a\nSwin Transformer backbone for hierarchical feature extraction with multi-scale\ncontext modules ASPP and PSP for enhanced scale-aware learning. A\nCross-Attention block enables effective multisensor and multispectral feature\nfusion, while the integration of an Efficient Channel Attention Block (ECAB)\nand a Spatial Attention Module adaptively refine feature representations.\nComprehensive experiments on CloudSEN12 and L8Biome demonstrate that MSCloudCAM\ndelivers state-of-the-art segmentation accuracy, surpassing leading baseline\narchitectures while maintaining competitive parameter efficiency and FLOPs.\nThese results underscore the model's effectiveness and practicality, making it\nwell-suited for large-scale Earth observation tasks and real-world\napplications.",
    "published": "2025-10-12T20:40:22Z",
    "updated": "2025-10-16T21:22:55Z",
    "authors": [
      "Md Abdullah Al Mazid",
      "Liangdong Deng",
      "Naphtali Rishe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14726v1",
    "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object\n  Detection",
    "summary": "Recent object detection methods have made remarkable progress by leveraging\nattention mechanisms to improve feature discriminability. However, most\nexisting approaches are confined to refining single-layer or fusing dual-layer\nfeatures, overlooking the rich inter-layer dependencies across multi-scale\nrepresentations. This limits their ability to capture comprehensive contextual\ninformation essential for detecting objects with large scale variations. In\nthis paper, we propose a novel Cross-Layer Feature Self-Attention Module\n(CFSAM), which holistically models both local and global dependencies within\nmulti-scale feature maps. CFSAM consists of three key components: a\nconvolutional local feature extractor, a Transformer-based global modeling unit\nthat efficiently captures cross-layer interactions, and a feature fusion\nmechanism to restore and enhance the original representations. When integrated\ninto the SSD300 framework, CFSAM significantly boosts detection performance,\nachieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO\n(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the\nmodule accelerates convergence during training without introducing substantial\ncomputational overhead. Our work highlights the importance of explicit\ncross-layer attention modeling in advancing multi-scale object detection.",
    "published": "2025-10-16T14:25:21Z",
    "updated": "2025-10-16T14:25:21Z",
    "authors": [
      "Dingzhou Xie",
      "Rushi Lan",
      "Cheng Pang",
      "Enhao Ning",
      "Jiahao Zeng",
      "Wei Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.26912v1",
    "title": "Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall\n  and Language Modeling",
    "summary": "Hybrid models that combine state space models (SSMs) with attention\nmechanisms have shown strong performance by leveraging the efficiency of SSMs\nand the high recall ability of attention. However, the architectural design\nchoices behind these hybrid models remain insufficiently understood. In this\nwork, we analyze hybrid architectures through the lens of memory utilization\nand overall performance, and propose a complementary method to further enhance\ntheir effectiveness. We first examine the distinction between sequential and\nparallel integration of SSM and attention layers. Our analysis reveals several\ninteresting findings, including that sequential hybrids perform better on\nshorter contexts, whereas parallel hybrids are more effective for longer\ncontexts. We also introduce a data-centric approach of continually training on\ndatasets augmented with paraphrases, which further enhances recall while\npreserving other capabilities. It generalizes well across different base models\nand outperforms architectural modifications aimed at enhancing recall. Our\nfindings provide a deeper understanding of hybrid SSM-attention models and\noffer practical guidance for designing architectures tailored to various use\ncases. Our findings provide a deeper understanding of hybrid SSM-attention\nmodels and offer practical guidance for designing architectures tailored to\nvarious use cases.",
    "published": "2025-10-30T18:19:52Z",
    "updated": "2025-10-30T18:19:52Z",
    "authors": [
      "Hyunji Lee",
      "Wenhao Yu",
      "Hongming Zhang",
      "Kaixin Ma",
      "Jiyeon Kim",
      "Dong Yu",
      "Minjoon Seo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.00336v1",
    "title": "Transformers with Competitive Ensembles of Independent Mechanisms",
    "summary": "An important development in deep learning from the earliest MLPs has been a\nmove towards architectures with structural inductive biases which enable the\nmodel to keep distinct sources of information and routes of processing\nwell-separated. This structure is linked to the notion of independent\nmechanisms from the causality literature, in which a mechanism is able to\nretain the same processing as irrelevant aspects of the world are changed. For\nexample, convnets enable separation over positions, while attention-based\narchitectures (especially Transformers) learn which combination of positions to\nprocess dynamically. In this work we explore a way in which the Transformer\narchitecture is deficient: it represents each position with a large monolithic\nhidden representation and a single set of parameters which are applied over the\nentire hidden representation. This potentially throws unrelated sources of\ninformation together, and limits the Transformer's ability to capture\nindependent mechanisms. To address this, we propose Transformers with\nIndependent Mechanisms (TIM), a new Transformer layer which divides the hidden\nrepresentation and parameters into multiple mechanisms, which only exchange\ninformation through attention. Additionally, we propose a competition mechanism\nwhich encourages these mechanisms to specialize over time steps, and thus be\nmore independent. We study TIM on a large-scale BERT model, on the Image\nTransformer, and on speech enhancement and find evidence for semantically\nmeaningful specialization as well as improved performance.",
    "published": "2021-02-27T21:48:46Z",
    "updated": "2021-02-27T21:48:46Z",
    "authors": [
      "Alex Lamb",
      "Di He",
      "Anirudh Goyal",
      "Guolin Ke",
      "Chien-Feng Liao",
      "Mirco Ravanelli",
      "Yoshua Bengio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.10743v2",
    "title": "Multi-Unit Transformers for Neural Machine Translation",
    "summary": "Transformer models achieve remarkable success in Neural Machine Translation.\nMany efforts have been devoted to deepening the Transformer by stacking several\nunits (i.e., a combination of Multihead Attentions and FFN) in a cascade, while\nthe investigation over multiple parallel units draws little attention. In this\npaper, we propose the Multi-Unit Transformers (MUTE), which aim to promote the\nexpressiveness of the Transformer by introducing diverse and complementary\nunits. Specifically, we use several parallel units and show that modeling with\nmultiple units improves model performance and introduces diversity. Further, to\nbetter leverage the advantage of the multi-unit setting, we design biased\nmodule and sequential dependency that guide and encourage complementariness\namong different units. Experimental results on three machine translation tasks,\nthe NIST Chinese-to-English, WMT'14 English-to-German and WMT'18\nChinese-to-English, show that the MUTE models significantly outperform the\nTransformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild\ndrop in inference speed (about 3.1%). In addition, our methods also surpass the\nTransformer-Big model, with only 54\\% of its parameters. These results\ndemonstrate the effectiveness of the MUTE, as well as its efficiency in both\nthe inference process and parameter usage.",
    "published": "2020-10-21T03:41:49Z",
    "updated": "2020-10-23T11:33:45Z",
    "authors": [
      "Jianhao Yan",
      "Fandong Meng",
      "Jie Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14173v3",
    "title": "FoveaTer: Foveated Transformer for Image Classification",
    "summary": "Many animals and humans process the visual field with a varying spatial\nresolution (foveated vision) and use peripheral processing to make eye\nmovements and point the fovea to acquire high-resolution information about\nobjects of interest. This architecture results in computationally efficient\nrapid scene exploration. Recent progress in self-attention-based Vision\nTransformers, an alternative to the traditionally convolution-reliant computer\nvision systems. However, the Transformer models do not explicitly model the\nfoveated properties of the visual system nor the interaction between eye\nmovements and the classification task. We propose Foveated Transformer\n(FoveaTer) model, which uses pooling regions and eye movements to perform\nobject classification tasks using a Vision Transformer architecture. Using\nsquare pooling regions or biologically-inspired radial-polar pooling regions,\nour proposed model pools the image features from the convolution backbone and\nuses the pooled features as an input to transformer layers. It decides on\nsubsequent fixation location based on the attention assigned by the Transformer\nto various locations from past and present fixations. It dynamically allocates\nmore fixation/computational resources to more challenging images before making\nthe final image category decision. Using five ablation studies, we evaluate the\ncontribution of different components of the Foveated model. We perform a\npsychophysics scene categorization task and use the experimental data to find a\nsuitable radial-polar pooling region combination. We also show that the\nFoveated model better explains the human decisions in a scene categorization\ntask than a Baseline model. We demonstrate our model's robustness against PGD\nadversarial attacks with both types of pooling regions, where we see the\nFoveated model outperform the Baseline model.",
    "published": "2021-05-29T01:54:33Z",
    "updated": "2022-10-02T19:59:49Z",
    "authors": [
      "Aditya Jonnalagadda",
      "William Yang Wang",
      "B. S. Manjunath",
      "Miguel P. Eckstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.06915v3",
    "title": "Object-Region Video Transformers",
    "summary": "Recently, video transformers have shown great success in video understanding,\nexceeding CNN performance; yet existing video transformer models do not\nexplicitly model objects, although objects can be essential for recognizing\nactions. In this work, we present Object-Region Video Transformers (ORViT), an\n\\emph{object-centric} approach that extends video transformer layers with a\nblock that directly incorporates object representations. The key idea is to\nfuse object-centric representations starting from early layers and propagate\nthem into the transformer-layers, thus affecting the spatio-temporal\nrepresentations throughout the network. Our ORViT block consists of two\nobject-level streams: appearance and dynamics. In the appearance stream, an\n\"Object-Region Attention\" module applies self-attention over the patches and\n\\emph{object regions}. In this way, visual object regions interact with uniform\npatch tokens and enrich them with contextualized object information. We further\nmodel object dynamics via a separate \"Object-Dynamics Module\", which captures\ntrajectory interactions, and show how to integrate the two streams. We evaluate\nour model on four tasks and five datasets: compositional and few-shot action\nrecognition on SomethingElse, spatio-temporal action detection on AVA, and\nstandard action recognition on Something-Something V2, Diving48 and\nEpic-Kitchen100. We show strong performance improvement across all tasks and\ndatasets considered, demonstrating the value of a model that incorporates\nobject representations into a transformer architecture. For code and pretrained\nmodels, visit the project page at \\url{https://roeiherz.github.io/ORViT/}",
    "published": "2021-10-13T17:51:46Z",
    "updated": "2022-06-09T20:48:45Z",
    "authors": [
      "Roei Herzig",
      "Elad Ben-Avraham",
      "Karttikeya Mangalam",
      "Amir Bar",
      "Gal Chechik",
      "Anna Rohrbach",
      "Trevor Darrell",
      "Amir Globerson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.02387v3",
    "title": "An Empirical Study of Training End-to-End Vision-and-Language\n  Transformers",
    "summary": "Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.",
    "published": "2021-11-03T17:55:36Z",
    "updated": "2022-03-18T03:29:10Z",
    "authors": [
      "Zi-Yi Dou",
      "Yichong Xu",
      "Zhe Gan",
      "Jianfeng Wang",
      "Shuohang Wang",
      "Lijuan Wang",
      "Chenguang Zhu",
      "Pengchuan Zhang",
      "Lu Yuan",
      "Nanyun Peng",
      "Zicheng Liu",
      "Michael Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.04093v2",
    "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned\n  Transformer",
    "summary": "Attention-based Transformer models have been increasingly employed for\nautomatic music generation. To condition the generation process of such a model\nwith a user-specified sequence, a popular approach is to take that conditioning\nsequence as a priming sequence and ask a Transformer decoder to generate a\ncontinuation. However, this prompt-based conditioning cannot guarantee that the\nconditioning sequence would develop or even simply repeat itself in the\ngenerated continuation. In this paper, we propose an alternative conditioning\napproach, called theme-based conditioning, that explicitly trains the\nTransformer to treat the conditioning sequence as a thematic material that has\nto manifest itself multiple times in its generation result. This is achieved\nwith two main technical contributions. First, we propose a deep learning-based\napproach that uses contrastive representation learning and clustering to\nautomatically retrieve thematic materials from music pieces in the training\ndata. Second, we propose a novel gated parallel attention module to be used in\na sequence-to-sequence (seq2seq) encoder/decoder architecture to more\neffectively account for a given conditioning thematic material in the\ngeneration process of the Transformer decoder. We report on objective and\nsubjective evaluations of variants of the proposed Theme Transformer and the\nconventional prompt-based baseline, showing that our best model can generate,\nto some extent, polyphonic pop piano music with repetition and plausible\nvariations of a given condition.",
    "published": "2021-11-07T13:55:39Z",
    "updated": "2022-03-21T07:01:50Z",
    "authors": [
      "Yi-Jen Shih",
      "Shih-Lun Wu",
      "Frank Zalkow",
      "Meinard MÃ¼ller",
      "Yi-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.15637v2",
    "title": "Building extraction with vision transformer",
    "summary": "As an important carrier of human productive activities, the extraction of\nbuildings is not only essential for urban dynamic monitoring but also necessary\nfor suburban construction inspection. Nowadays, accurate building extraction\nfrom remote sensing images remains a challenge due to the complex background\nand diverse appearances of buildings. The convolutional neural network (CNN)\nbased building extraction methods, although increased the accuracy\nsignificantly, are criticized for their inability for modelling global\ndependencies. Thus, this paper applies the Vision Transformer for building\nextraction. However, the actual utilization of the Vision Transformer often\ncomes with two limitations. First, the Vision Transformer requires more GPU\nmemory and computational costs compared to CNNs. This limitation is further\nmagnified when encountering large-sized inputs like fine-resolution remote\nsensing images. Second, spatial details are not sufficiently preserved during\nthe feature extraction of the Vision Transformer, resulting in the inability\nfor fine-grained building segmentation. To handle these issues, we propose a\nnovel Vision Transformer (BuildFormer), with a dual-path structure.\nSpecifically, we design a spatial-detailed context path to encode rich spatial\ndetails and a global context path to capture global dependencies. Besides, we\ndevelop a window-based linear multi-head self-attention to make the complexity\nof the multi-head self-attention linear with the window size, which strengthens\nthe global context extraction by using large windows and greatly improves the\npotential of the Vision Transformer in processing large-sized remote sensing\nimages. The proposed method yields state-of-the-art performance (75.74% IoU) on\nthe Massachusetts building dataset. Code will be available.",
    "published": "2021-11-29T11:23:52Z",
    "updated": "2022-04-13T09:34:42Z",
    "authors": [
      "Libo Wang",
      "Shenghui Fang",
      "Rui Li",
      "Xiaoliang Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.16952v2",
    "title": "Multimodal Fusion Transformer for Remote Sensing Image Classification",
    "summary": "Vision transformers (ViTs) have been trending in image classification tasks\ndue to their promising performance when compared to convolutional neural\nnetworks (CNNs). As a result, many researchers have tried to incorporate ViTs\nin hyperspectral image (HSI) classification tasks. To achieve satisfactory\nperformance, close to that of CNNs, transformers need fewer parameters. ViTs\nand other similar transformers use an external classification (CLS) token which\nis randomly initialized and often fails to generalize well, whereas other\nsources of multimodal datasets, such as light detection and ranging (LiDAR)\noffer the potential to improve these models by means of a CLS. In this paper,\nwe introduce a new multimodal fusion transformer (MFT) network which comprises\na multihead cross patch attention (mCrossPA) for HSI land-cover classification.\nOur mCrossPA utilizes other sources of complementary information in addition to\nthe HSI in the transformer encoder to achieve better generalization. The\nconcept of tokenization is used to generate CLS and HSI patch tokens, helping\nto learn a {distinctive representation} in a reduced and hierarchical feature\nspace. Extensive experiments are carried out on {widely used benchmark}\ndatasets {i.e.,} the University of Houston, Trento, University of Southern\nMississippi Gulfpark (MUUFL), and Augsburg. We compare the results of the\nproposed MFT model with other state-of-the-art transformers, classical CNNs,\nand conventional classifiers models. The superior performance achieved by the\nproposed model is due to the use of multihead cross patch attention. The source\ncode will be made available publicly at\n\\url{https://github.com/AnkurDeria/MFT}.}",
    "published": "2022-03-31T11:18:41Z",
    "updated": "2023-06-20T17:58:25Z",
    "authors": [
      "Swalpa Kumar Roy",
      "Ankur Deria",
      "Danfeng Hong",
      "Behnood Rasti",
      "Antonio Plaza",
      "Jocelyn Chanussot"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.14701v1",
    "title": "Modeling Beats and Downbeats with a Time-Frequency Transformer",
    "summary": "Transformer is a successful deep neural network (DNN) architecture that has\nshown its versatility not only in natural language processing but also in music\ninformation retrieval (MIR). In this paper, we present a novel\nTransformer-based approach to tackle beat and downbeat tracking. This approach\nemploys SpecTNT (Spectral-Temporal Transformer in Transformer), a variant of\nTransformer that models both spectral and temporal dimensions of a\ntime-frequency input of music audio. A SpecTNT model uses a stack of blocks,\nwhere each consists of two levels of Transformer encoders. The lower-level (or\nspectral) encoder handles the spectral features and enables the model to pay\nattention to harmonic components of each frame. Since downbeats indicate bar\nboundaries and are often accompanied by harmonic changes, this step may help\ndownbeat modeling. The upper-level (or temporal) encoder aggregates useful\nlocal spectral information to pay attention to beat/downbeat positions. We also\npropose an architecture that combines SpecTNT with a state-of-the-art model,\nTemporal Convolutional Networks (TCN), to further improve the performance.\nExtensive experiments demonstrate that our approach can significantly\noutperform TCN in downbeat tracking while maintaining comparable result in beat\ntracking.",
    "published": "2022-05-29T16:01:39Z",
    "updated": "2022-05-29T16:01:39Z",
    "authors": [
      "Yun-Ning Hung",
      "Ju-Chiang Wang",
      "Xuchen Song",
      "Wei-Tsung Lu",
      "Minz Won"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.08494v2",
    "title": "Rethinking Alignment in Video Super-Resolution Transformers",
    "summary": "The alignment of adjacent frames is considered an essential operation in\nvideo super-resolution (VSR). Advanced VSR models, including the latest VSR\nTransformers, are generally equipped with well-designed alignment modules.\nHowever, the progress of the self-attention mechanism may violate this common\nsense. In this paper, we rethink the role of alignment in VSR Transformers and\nmake several counter-intuitive observations. Our experiments show that: (i) VSR\nTransformers can directly utilize multi-frame information from unaligned\nvideos, and (ii) existing alignment methods are sometimes harmful to VSR\nTransformers. These observations indicate that we can further improve the\nperformance of VSR Transformers simply by removing the alignment module and\nadopting a larger attention window. Nevertheless, such designs will\ndramatically increase the computational burden, and cannot deal with large\nmotions. Therefore, we propose a new and efficient alignment method called\npatch alignment, which aligns image patches instead of pixels. VSR Transformers\nequipped with patch alignment could demonstrate state-of-the-art performance on\nmultiple benchmarks. Our work provides valuable insights on how multi-frame\ninformation is used in VSR and how to select alignment methods for different\nnetworks/datasets. Codes and models will be released at\nhttps://github.com/XPixelGroup/RethinkVSRAlignment.",
    "published": "2022-07-18T10:20:23Z",
    "updated": "2022-10-09T08:58:05Z",
    "authors": [
      "Shuwei Shi",
      "Jinjin Gu",
      "Liangbin Xie",
      "Xintao Wang",
      "Yujiu Yang",
      "Chao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.10322v1",
    "title": "Deep Transformers without Shortcuts: Modifying Self-attention for\n  Faithful Signal Propagation",
    "summary": "Skip connections and normalisation layers form two standard architectural\ncomponents that are ubiquitous for the training of Deep Neural Networks (DNNs),\nbut whose precise roles are poorly understood. Recent approaches such as Deep\nKernel Shaping have made progress towards reducing our reliance on them, using\ninsights from wide NN kernel theory to improve signal propagation in vanilla\nDNNs (which we define as networks without skips or normalisation). However,\nthese approaches are incompatible with the self-attention layers present in\ntransformers, whose kernels are intrinsically more complicated to analyse and\ncontrol. And so the question remains: is it possible to train deep vanilla\ntransformers? We answer this question in the affirmative by designing several\napproaches that use combinations of parameter initialisations, bias matrices\nand location-dependent rescaling to achieve faithful signal propagation in\nvanilla transformers. Our methods address various intricacies specific to\nsignal propagation in transformers, including the interaction with positional\nencoding and causal masking. In experiments on WikiText-103 and C4, our\napproaches enable deep transformers without normalisation to train at speeds\nmatching their standard counterparts, and deep vanilla transformers to reach\nthe same performance as standard ones after about 5 times more iterations.",
    "published": "2023-02-20T21:26:25Z",
    "updated": "2023-02-20T21:26:25Z",
    "authors": [
      "Bobby He",
      "James Martens",
      "Guodong Zhang",
      "Aleksandar Botev",
      "Andrew Brock",
      "Samuel L Smith",
      "Yee Whye Teh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.01831v1",
    "title": "DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation",
    "summary": "Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful\neffectiveness in generating high-quality 2D images. However, it is still being\ndetermined whether the Transformer architecture performs equally well in 3D\nshape generation, as previous 3D diffusion methods mostly adopted the U-Net\narchitecture. To bridge this gap, we propose a novel Diffusion Transformer for\n3D shape generation, namely DiT-3D, which can directly operate the denoising\nprocess on voxelized point clouds using plain Transformers. Compared to\nexisting U-Net approaches, our DiT-3D is more scalable in model size and\nproduces much higher quality generations. Specifically, the DiT-3D adopts the\ndesign philosophy of DiT but modifies it by incorporating 3D positional and\npatch embeddings to adaptively aggregate input from voxelized point clouds. To\nreduce the computational cost of self-attention in 3D shape generation, we\nincorporate 3D window attention into Transformer blocks, as the increased 3D\ntoken length resulting from the additional dimension of voxels can lead to high\ncomputation. Finally, linear and devoxelization layers are used to predict the\ndenoised point clouds. In addition, our transformer architecture supports\nefficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on\nImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on\nthe ShapeNet dataset demonstrate that the proposed DiT-3D achieves\nstate-of-the-art performance in high-fidelity and diverse 3D point cloud\ngeneration. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy\nof the state-of-the-art method by 4.59 and increases the Coverage metric by\n3.51 when evaluated on Chamfer Distance.",
    "published": "2023-07-04T17:15:46Z",
    "updated": "2023-07-04T17:15:46Z",
    "authors": [
      "Shentong Mo",
      "Enze Xie",
      "Ruihang Chu",
      "Lewei Yao",
      "Lanqing Hong",
      "Matthias NieÃner",
      "Zhenguo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.10131v1",
    "title": "Deep Prompt Tuning for Graph Transformers",
    "summary": "Graph transformers have gained popularity in various graph-based tasks by\naddressing challenges faced by traditional Graph Neural Networks. However, the\nquadratic complexity of self-attention operations and the extensive layering in\ngraph transformer architectures present challenges when applying them to graph\nbased prediction tasks. Fine-tuning, a common approach, is resource-intensive\nand requires storing multiple copies of large models. We propose a novel\napproach called deep graph prompt tuning as an alternative to fine-tuning for\nleveraging large graph transformer models in downstream graph based prediction\ntasks. Our method introduces trainable feature nodes to the graph and pre-pends\ntask-specific tokens to the graph transformer, enhancing the model's expressive\npower. By freezing the pre-trained parameters and only updating the added\ntokens, our approach reduces the number of free parameters and eliminates the\nneed for multiple model copies, making it suitable for small datasets and\nscalable to large graphs. Through extensive experiments on various-sized\ndatasets, we demonstrate that deep graph prompt tuning achieves comparable or\neven superior performance to fine-tuning, despite utilizing significantly fewer\ntask-specific parameters. Our contributions include the introduction of prompt\ntuning for graph transformers, its application to both graph transformers and\nmessage passing graph neural networks, improved efficiency and resource\nutilization, and compelling experimental results. This work brings attention to\na promising approach to leverage pre-trained models in graph based prediction\ntasks and offers new opportunities for exploring and advancing graph\nrepresentation learning.",
    "published": "2023-09-18T20:12:17Z",
    "updated": "2023-09-18T20:12:17Z",
    "authors": [
      "Reza Shirkavand",
      "Heng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.03016v1",
    "title": "Understanding In-Context Learning in Transformers and LLMs by Learning\n  to Learn Discrete Functions",
    "summary": "In order to understand the in-context learning phenomenon, recent works have\nadopted a stylized experimental framework and demonstrated that Transformers\ncan learn gradient-based learning algorithms for various classes of real-valued\nfunctions. However, the limitations of Transformers in implementing learning\nalgorithms, and their ability to learn other forms of algorithms are not well\nunderstood. Additionally, the degree to which these capabilities are confined\nto attention-based models is unclear. Furthermore, it remains to be seen\nwhether the insights derived from these stylized settings can be extrapolated\nto pretrained Large Language Models (LLMs). In this work, we take a step\ntowards answering these questions by demonstrating the following: (a) On a\ntest-bed with a variety of Boolean function classes, we find that Transformers\ncan nearly match the optimal learning algorithm for 'simpler' tasks, while\ntheir performance deteriorates on more 'complex' tasks. Additionally, we find\nthat certain attention-free models perform (almost) identically to Transformers\non a range of tasks. (b) When provided a teaching sequence, i.e. a set of\nexamples that uniquely identifies a function in a class, we show that\nTransformers learn more sample-efficiently. Interestingly, our results show\nthat Transformers can learn to implement two distinct algorithms to solve a\nsingle task, and can adaptively select the more sample-efficient algorithm\ndepending on the sequence of in-context examples. (c) Lastly, we show that\nextant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines\non prediction tasks that are guaranteed to not be in their training set.",
    "published": "2023-10-04T17:57:33Z",
    "updated": "2023-10-04T17:57:33Z",
    "authors": [
      "Satwik Bhattamishra",
      "Arkil Patel",
      "Phil Blunsom",
      "Varun Kanade"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.04180v1",
    "title": "Degradation-Aware Self-Attention Based Transformer for Blind Image\n  Super-Resolution",
    "summary": "Compared to CNN-based methods, Transformer-based methods achieve impressive\nimage restoration outcomes due to their abilities to model remote dependencies.\nHowever, how to apply Transformer-based methods to the field of blind\nsuper-resolution (SR) and further make an SR network adaptive to degradation\ninformation is still an open problem. In this paper, we propose a new\ndegradation-aware self-attention-based Transformer model, where we incorporate\ncontrastive learning into the Transformer network for learning the degradation\nrepresentations of input images with unknown noise. In particular, we integrate\nboth CNN and Transformer components into the SR network, where we first use the\nCNN modulated by the degradation information to extract local features, and\nthen employ the degradation-aware Transformer to extract global semantic\nfeatures. We apply our proposed model to several popular large-scale benchmark\ndatasets for testing, and achieve the state-of-the-art performance compared to\nexisting methods. In particular, our method yields a PSNR of 32.43 dB on the\nUrban100 dataset at $\\times$2 scale, 0.94 dB higher than DASR, and 26.62 dB on\nthe Urban100 dataset at $\\times$4 scale, 0.26 dB improvement over KDSR, setting\na new benchmark in this area. Source code is available at:\nhttps://github.com/I2-Multimedia-Lab/DSAT/tree/main.",
    "published": "2023-10-06T11:52:31Z",
    "updated": "2023-10-06T11:52:31Z",
    "authors": [
      "Qingguo Liu",
      "Pan Gao",
      "Kang Han",
      "Ningzhong Liu",
      "Wei Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.02382v2",
    "title": "Ultra-Long Sequence Distributed Transformer",
    "summary": "Transformer models trained on long sequences often achieve higher accuracy\nthan short sequences. Unfortunately, conventional transformers struggle with\nlong sequence training due to the overwhelming computation and memory\nrequirements. Existing methods for long sequence training offer limited speedup\nand memory reduction, and may compromise accuracy. This paper presents a novel\nand efficient distributed training method, the Long Short-Sequence Transformer\n(LSS Transformer), for training transformer with long sequences. It distributes\na long sequence into segments among GPUs, with each GPU computing a partial\nself-attention for its segment. Then, it uses a fused communication and a novel\ndouble gradient averaging technique to avoid the need to aggregate partial\nself-attention and minimize communication overhead. We evaluated the\nperformance between LSS Transformer and the state-of-the-art Nvidia sequence\nparallelism on a Wikipedia enwik8 dataset. Results show that our proposed\nmethod lead to 5.6x faster and 10.2x more memory-efficient implementation\ncompared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs.\nMoreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456\nGPUs, achieving 161% super-linear parallel efficiency and a throughput of 32\npetaflops.",
    "published": "2023-11-04T11:38:53Z",
    "updated": "2023-11-08T17:04:27Z",
    "authors": [
      "Xiao Wang",
      "Isaac Lyngaas",
      "Aristeidis Tsaris",
      "Peng Chen",
      "Sajal Dash",
      "Mayanka Chandra Shekar",
      "Tao Luo",
      "Hong-Jun Yoon",
      "Mohamed Wahib",
      "John Gouley"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.06423v2",
    "title": "CurveFormer++: 3D Lane Detection by Curve Propagation with Temporal\n  Curve Queries and Attention",
    "summary": "In autonomous driving, accurate 3D lane detection using monocular cameras is\nimportant for downstream tasks. Recent CNN and Transformer approaches usually\napply a two-stage model design. The first stage transforms the image feature\nfrom a front image into a bird's-eye-view (BEV) representation. Subsequently, a\nsub-network processes the BEV feature to generate the 3D detection results.\nHowever, these approaches heavily rely on a challenging image feature\ntransformation module from a perspective view to a BEV representation. In our\nwork, we present CurveFormer++, a single-stage Transformer-based method that\ndoes not require the view transform module and directly infers 3D lane results\nfrom the perspective image features. Specifically, our approach models the 3D\nlane detection task as a curve propagation problem, where each lane is\nrepresented by a curve query with a dynamic and ordered anchor point set. By\nemploying a Transformer decoder, the model can iteratively refine the 3D lane\nresults. A curve cross-attention module is introduced to calculate similarities\nbetween image features and curve queries. To handle varying lane lengths, we\nemploy context sampling and anchor point restriction techniques to compute more\nrelevant image features. Furthermore, we apply a temporal fusion module that\nincorporates selected informative sparse curve queries and their corresponding\nanchor point sets to leverage historical information. In the experiments, we\nevaluate our approach on two publicly real-world datasets. The results\ndemonstrate that our method provides outstanding performance compared with both\nCNN and Transformer based methods. We also conduct ablation studies to analyze\nthe impact of each component.",
    "published": "2024-02-09T14:13:40Z",
    "updated": "2025-03-16T14:40:20Z",
    "authors": [
      "Yifeng Bai",
      "Zhirong Chen",
      "Pengpeng Liang",
      "Bo Song",
      "Erkang Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.15607v3",
    "title": "How Do Nonlinear Transformers Learn and Generalize in In-Context\n  Learning?",
    "summary": "Transformer-based large language models have displayed impressive in-context\nlearning capabilities, where a pre-trained model can handle new tasks without\nfine-tuning by simply augmenting the query with some input-output examples from\nthat task. Despite the empirical success, the mechanics of how to train a\nTransformer to achieve ICL and the corresponding ICL capacity is mostly elusive\ndue to the technical challenges of analyzing the nonconvex training problems\nresulting from the nonlinear self-attention and nonlinear activation in\nTransformers. To the best of our knowledge, this paper provides the first\ntheoretical analysis of the training dynamics of Transformers with nonlinear\nself-attention and nonlinear MLP, together with the ICL generalization\ncapability of the resulting model. Focusing on a group of binary classification\ntasks, we train Transformers using data from a subset of these tasks and\nquantify the impact of various factors on the ICL generalization performance on\nthe remaining unseen tasks with and without data distribution shifts. We also\nanalyze how different components in the learned Transformers contribute to the\nICL performance. Furthermore, we provide the first theoretical analysis of how\nmodel pruning affects ICL performance and prove that proper magnitude-based\npruning can have a minimal impact on ICL while reducing inference costs. These\ntheoretical findings are justified through numerical experiments.",
    "published": "2024-02-23T21:07:20Z",
    "updated": "2024-06-16T04:02:28Z",
    "authors": [
      "Hongkang Li",
      "Meng Wang",
      "Songtao Lu",
      "Xiaodong Cui",
      "Pin-Yu Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.11138v5",
    "title": "Spiking Wavelet Transformer",
    "summary": "Spiking neural networks (SNNs) offer an energy-efficient alternative to\nconventional deep learning by emulating the event-driven processing manner of\nthe brain. Incorporating Transformers with SNNs has shown promise for accuracy.\nHowever, they struggle to learn high-frequency patterns, such as moving edges\nand pixel-level brightness changes, because they rely on the global\nself-attention mechanism. Learning these high-frequency representations is\nchallenging but essential for SNN-based event-driven vision. To address this\nissue, we propose the Spiking Wavelet Transformer (SWformer), an attention-free\narchitecture that effectively learns comprehensive spatial-frequency features\nin a spike-driven manner by leveraging the sparse wavelet transform. The\ncritical component is a Frequency-Aware Token Mixer (FATM) with three branches:\n1) spiking wavelet learner for spatial-frequency domain learning, 2)\nconvolution-based learner for spatial feature extraction, and 3) spiking\npointwise convolution for cross-channel information aggregation - with negative\nspike dynamics incorporated in 1) to enhance frequency representation. The FATM\nenables the SWformer to outperform vanilla Spiking Transformers in capturing\nhigh-frequency visual components, as evidenced by our empirical results.\nExperiments on both static and neuromorphic datasets demonstrate SWformer's\neffectiveness in capturing spatial-frequency patterns in a multiplication-free\nand event-driven fashion, outperforming state-of-the-art SNNs. SWformer\nachieves a 22.03% reduction in parameter count, and a 2.52% performance\nimprovement on the ImageNet dataset compared to vanilla Spiking Transformers.\nThe code is available at: https://github.com/bic-L/Spiking-Wavelet-Transformer.",
    "published": "2024-03-17T08:41:48Z",
    "updated": "2024-09-04T08:57:24Z",
    "authors": [
      "Yuetong Fang",
      "Ziqing Wang",
      "Lingfeng Zhang",
      "Jiahang Cao",
      "Honglei Chen",
      "Renjing Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15827v2",
    "title": "Efficient Point Transformer with Dynamic Token Aggregating for LiDAR\n  Point Cloud Processing",
    "summary": "Recently, LiDAR point cloud processing and analysis have made great progress\ndue to the development of 3D Transformers. However, existing 3D Transformer\nmethods usually are computationally expensive and inefficient due to their huge\nand redundant attention maps. They also tend to be slow due to requiring\ntime-consuming point cloud sampling and grouping processes. To address these\nissues, we propose an efficient point TransFormer with Dynamic Token\nAggregating (DTA-Former) for point cloud representation and processing.\nFirstly, we propose an efficient Learnable Token Sparsification (LTS) block,\nwhich considers both local and global semantic information for the adaptive\nselection of key tokens. Secondly, to achieve the feature aggregation for\nsparsified tokens, we present the first Dynamic Token Aggregating (DTA) block\nin the 3D Transformer paradigm, providing our model with strong aggregated\nfeatures while preventing information loss. After that, a dual-attention\nTransformer-based Global Feature Enhancement (GFE) block is used to improve the\nrepresentation capability of the model. Equipped with LTS, DTA, and GFE blocks,\nDTA-Former achieves excellent classification results via hierarchical feature\nlearning. Lastly, a novel Iterative Token Reconstruction (ITR) block is\nintroduced for dense prediction whereby the semantic features of tokens and\ntheir semantic relationships are gradually optimized during iterative\nreconstruction. Based on ITR, we propose a new W-net architecture, which is\nmore suitable for Transformer-based feature learning than the common U-net\ndesign.",
    "published": "2024-05-23T20:50:50Z",
    "updated": "2024-12-25T06:20:14Z",
    "authors": [
      "Dening Lu",
      "Jun Zhou",
      " Kyle",
      " Gao",
      "Linlin Xu",
      "Jonathan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.08159v1",
    "title": "SDformer: Efficient End-to-End Transformer for Depth Completion",
    "summary": "Depth completion aims to predict dense depth maps with sparse depth\nmeasurements from a depth sensor. Currently, Convolutional Neural Network (CNN)\nbased models are the most popular methods applied to depth completion tasks.\nHowever, despite the excellent high-end performance, they suffer from a limited\nrepresentation area. To overcome the drawbacks of CNNs, a more effective and\npowerful method has been presented: the Transformer, which is an adaptive\nself-attention setting sequence-to-sequence model. While the standard\nTransformer quadratically increases the computational cost from the key-query\ndot-product of input resolution which improperly employs depth completion\ntasks. In this work, we propose a different window-based Transformer\narchitecture for depth completion tasks named Sparse-to-Dense Transformer\n(SDformer). The network consists of an input module for the depth map and RGB\nimage features extraction and concatenation, a U-shaped encoder-decoder\nTransformer for extracting deep features, and a refinement module.\nSpecifically, we first concatenate the depth map features with the RGB image\nfeatures through the input model. Then, instead of calculating self-attention\nwith the whole feature maps, we apply different window sizes to extract the\nlong-range depth dependencies. Finally, we refine the predicted features from\nthe input module and the U-shaped encoder-decoder Transformer module to get the\nenriching depth features and employ a convolution layer to obtain the dense\ndepth map. In practice, the SDformer obtains state-of-the-art results against\nthe CNN-based depth completion models with lower computing loads and parameters\non the NYU Depth V2 and KITTI DC datasets.",
    "published": "2024-09-12T15:52:08Z",
    "updated": "2024-09-12T15:52:08Z",
    "authors": [
      "Jian Qian",
      "Miao Sun",
      "Ashley Lee",
      "Jie Li",
      "Shenglong Zhuo",
      "Patrick Yin Chiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.03810v3",
    "title": "Exploring the Limitations of Mamba in COPY and CoT Reasoning",
    "summary": "Transformers have become the backbone of modern Large Language Models (LLMs);\nhowever, their inference overhead grows linearly with the sequence length,\nposing challenges for modeling long sequences. In light of this, Mamba has\nattracted attention for maintaining a constant inference size, with empirical\nevidence demonstrating that it can match Transformer performance in sequence\nmodeling while significantly reducing computational costs. However, an open\nquestion remains: can Mamba always bring savings while achieving performance\ncomparable to Transformers? In this paper, we focus on analyzing the expressive\nability of Mamba to perform our defined COPY operation and Chain of Thought\n(CoT) reasoning. First, inspired by the connection between Mamba and linear\nattention, we show that constant-sized Mamba may struggle to perform COPY\noperations while Transformers can handle them more easily. However, when the\nsize of Mamba grows linearly with the input sequence length, it can accurately\nperform COPY, but in this case, Mamba no longer provides overhead savings.\nBased on this observation, we further analyze Mamba's ability to tackle CoT\ntasks, which can be described by the Dynamic Programming (DP) problems. Our\nfindings suggest that to solve arbitrary DP problems, the total cost of Mamba\nis still comparable to standard Transformers. However, similar to efficient\nTransformers, when facing DP problems with favorable properties such as\nlocality, Mamba can provide savings in overhead. Our experiments on the copy\nand CoT tasks further demonstrate Mamba's limitations compared to Transformers\nin learning these tasks.",
    "published": "2024-10-04T13:31:24Z",
    "updated": "2025-05-29T03:19:51Z",
    "authors": [
      "Ruifeng Ren",
      "Zhicong Li",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.00503v3",
    "title": "Homeostasis and Sparsity in Transformer",
    "summary": "The transformer architecture has become an integral part of the field of\nmodern neural networks, playing a crucial role in a variety of tasks, such as\ntext generation, machine translation, image and audio processing, among others.\nThere is also an alternative approach to building intelligent systems, proposed\nby Jeff Hawkins and inspired by the processes occurring in the neocortex. In\nour article we want to combine some of these ideas and to propose the use of\nhomeostasis mechanisms, such as RFB-kWTA and \"Smart\" Inhibition, in the\nattention mechanism of the transformer and at the output of the transformer\nblock, as well as conducting an experiment involving the introduction of sparse\ndistributed representations of the transformer at various points. RFB-kWTA\nutilizes statistics of layer activations across time to adjust the entire\nlayer, enhancing the values of rare activations while reducing those of\nfrequent ones. \"Smart\" Inhibition also uses activation statistics to sample\nsparsity masks, with rarer activation times are more likely to be activated.\nOur proposed mechanisms significantly outperform the classical transformer\n0.2768 BLEU and a model that only makes use of dropout in the attention\nmechanism and output of the transformer block 0.3007 BLEU, achieving a score of\n0.3062 on the Multi30K dataset.",
    "published": "2024-11-30T15:03:41Z",
    "updated": "2024-12-16T14:59:05Z",
    "authors": [
      "Leonid Kotyuzanskiy",
      "Artem Klimov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.18929v1",
    "title": "Revisiting Transformers through the Lens of Low Entropy and Dynamic\n  Sparsity",
    "summary": "Compression has been a critical lens to understand the success of\nTransformers. In the past, we have typically taken the target distribution as a\ncriterion to evaluate a model's compression performance. Nevertheless,it often\nremains challenging to precisely assess how well the model achieves compression\nand to compare the information content of the learned distribution with that of\nthe target distribution during compression,as the target distribution is\ntypically unknown and entropy computation often incurs exponential cost. In\nthis work, we explore these issues under a controlled experimental setup. We\nfind that Transformers exhibit a unique inductive bias in data compression:\nbeyond approaching the target distribution, they tend to favor learning\nlower-entropy distributions, with this tendency becoming more pronounced as the\nmodel size increases. This preference prevents Transformers from perfectly\naligning with the target distribution, instead further compressing its\ninformation content. Furthermore, we show that the FFN module plays a critical\nrole in driving this bias. In addition, while models remove informational\nredundancy from data during compression, they also exhibit redundancy within\ntheir parameters, which enables compression and can be characterized through\ndynamic sparsity. However, the dynamic sparsity patterns in Transformers,\nparticularly in attention and FFN modules, demand further exploration. As for\nthis, we show that larger Transformers show stronger preferences for bypassing\nattention computations via residual connections and have lower proportion of\nactive neurons. Interestingly, we also find that training instability in larger\nmodels strongly correlates with sudden increases in dead neurons. Our work\ncontributes to a deeper understanding of Transformers from the lens of entropy\nand dynamic sparsity.",
    "published": "2025-04-26T14:02:07Z",
    "updated": "2025-04-26T14:02:07Z",
    "authors": [
      "Ruifeng Ren",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15009v3",
    "title": "How Transformers Learn In-Context Recall Tasks? Optimality, Training\n  Dynamics and Generalization",
    "summary": "We study the approximation capabilities, convergence speeds and\non-convergence behaviors of transformers trained on in-context recall tasks --\nwhich requires to recognize the \\emph{positional} association between a pair of\ntokens from in-context examples. Existing theoretical results only focus on the\nin-context reasoning behavior of transformers after being trained for the\n\\emph{one} gradient descent step. It remains unclear what is the on-convergence\nbehavior of transformers being trained by gradient descent and how fast the\nconvergence rate is. In addition, the generalization of transformers in\none-step in-context reasoning has not been formally investigated. This work\naddresses these gaps. We first show that a class of transformers with either\nlinear, ReLU or softmax attentions, is provably Bayes-optimal for an in-context\nrecall task. When being trained with gradient descent, we show via a\nfinite-sample analysis that the expected loss converges at linear rate to the\nBayes risks. Moreover, we show that the trained transformers exhibit\nout-of-distribution (OOD) generalization, i.e., generalizing to samples outside\nof the population distribution. Our theoretical findings are further supported\nby extensive empirical validations, showing that \\emph{without} proper\nparameterization, models with larger expressive power surprisingly \\emph{fail}\nto generalize OOD after being trained by gradient descent.",
    "published": "2025-05-21T01:26:44Z",
    "updated": "2025-10-21T17:34:30Z",
    "authors": [
      "Quan Nguyen",
      "Thanh Nguyen-Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27004v1",
    "title": "Mixture-of-Transformers Learn Faster: A Theoretical Study on\n  Classification Problems",
    "summary": "Mixture-of-Experts (MoE) models improve transformer efficiency but lack a\nunified theoretical explanation, especially when both feed-forward and\nattention layers are allowed to specialize. To this end, we study the\nMixture-of-Transformers (MoT), a tractable theoretical framework in which each\ntransformer block acts as an expert governed by a continuously trained gating\nnetwork. This design allows us to isolate and study the core learning dynamics\nof expert specialization and attention alignment. In particular, we develop a\nthree-stage training algorithm with continuous training of the gating network,\nand show that each transformer expert specializes in a distinct class of tasks\nand that the gating network accurately routes data samples to the correct\nexpert. Our analysis shows how expert specialization reduces gradient conflicts\nand makes each subtask strongly convex. We prove that the training drives the\nexpected prediction loss to near zero in $O(\\log(\\epsilon^{-1}))$ iteration\nsteps, significantly improving over the $O(\\epsilon^{-1})$ rate for a single\ntransformer. We further validate our theoretical findings through extensive\nreal-data experiments, demonstrating the practical effectiveness of MoT.\nTogether, these results offer the first unified theoretical account of\ntransformer-level specialization and learning dynamics, providing practical\nguidance for designing efficient large-scale models.",
    "published": "2025-10-30T21:07:36Z",
    "updated": "2025-10-30T21:07:36Z",
    "authors": [
      "Hongbo Li",
      "Qinhang Wu",
      "Sen Lin",
      "Yingbin Liang",
      "Ness B. Shroff"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.12406v1",
    "title": "A survey on attention mechanisms for medical applications: are we moving\n  towards better algorithms?",
    "summary": "The increasing popularity of attention mechanisms in deep learning algorithms\nfor computer vision and natural language processing made these models\nattractive to other research domains. In healthcare, there is a strong need for\ntools that may improve the routines of the clinicians and the patients.\nNaturally, the use of attention-based algorithms for medical applications\noccurred smoothly. However, being healthcare a domain that depends on\nhigh-stake decisions, the scientific community must ponder if these\nhigh-performing algorithms fit the needs of medical applications. With this\nmotto, this paper extensively reviews the use of attention mechanisms in\nmachine learning (including Transformers) for several medical applications.\nThis work distinguishes itself from its predecessors by proposing a critical\nanalysis of the claims and potentialities of attention mechanisms presented in\nthe literature through an experimental case study on medical image\nclassification with three different use cases. These experiments focus on the\nintegrating process of attention mechanisms into established deep learning\narchitectures, the analysis of their predictive power, and a visual assessment\nof their saliency maps generated by post-hoc explanation methods. This paper\nconcludes with a critical analysis of the claims and potentialities presented\nin the literature about attention mechanisms and proposes future research lines\nin medical applications that may benefit from these frameworks.",
    "published": "2022-04-26T16:04:19Z",
    "updated": "2022-04-26T16:04:19Z",
    "authors": [
      "Tiago GonÃ§alves",
      "Isabel Rio-Torto",
      "LuÃ­s F. Teixeira",
      "Jaime S. Cardoso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.04886v3",
    "title": "Lipschitz Normalization for Self-Attention Layers with Application to\n  Graph Neural Networks",
    "summary": "Attention based neural networks are state of the art in a large range of\napplications. However, their performance tends to degrade when the number of\nlayers increases. In this work, we show that enforcing Lipschitz continuity by\nnormalizing the attention scores can significantly improve the performance of\ndeep attention models. First, we show that, for deep graph attention networks\n(GAT), gradient explosion appears during training, leading to poor performance\nof gradient-based training algorithms. To address this issue, we derive a\ntheoretical analysis of the Lipschitz continuity of attention modules and\nintroduce LipschitzNorm, a simple and parameter-free normalization for\nself-attention mechanisms that enforces the model to be Lipschitz continuous.\nWe then apply LipschitzNorm to GAT and Graph Transformers and show that their\nperformance is substantially improved in the deep setting (10 to 30 layers).\nMore specifically, we show that a deep GAT model with LipschitzNorm achieves\nstate of the art results for node label prediction tasks that exhibit\nlong-range dependencies, while showing consistent improvements over their\nunnormalized counterparts in benchmark node classification tasks.",
    "published": "2021-03-08T16:47:16Z",
    "updated": "2021-09-13T14:38:02Z",
    "authors": [
      "George Dasoulas",
      "Kevin Scaman",
      "Aladin Virmaux"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.12256v2",
    "title": "NGAT4Rec: Neighbor-Aware Graph Attention Network For Recommendation",
    "summary": "Learning informative representations (aka. embeddings) of users and items is\nthe core of modern recommender systems. Previous works exploit user-item\nrelationships of one-hop neighbors in the user-item interaction graph to\nimprove the quality of representation. Recently, the research of Graph Neural\nNetwork (GNN) for recommendation considers the implicit collaborative\ninformation of multi-hop neighbors to enrich the representation. However, most\nworks of GNN for recommendation systems do not consider the relational\ninformation which implies the expression differences of different neighbors in\nthe neighborhood explicitly. The influence of each neighboring item to the\nrepresentation of the user's preference can be represented by the correlation\nbetween the item and neighboring items of the user. Symmetrically, for a given\nitem, the correlation between one neighboring user and neighboring users can\nreflect the strength of signal about the item's characteristic. To modeling the\nimplicit correlations of neighbors in graph embedding aggregating, we propose a\nNeighbor-Aware Graph Attention Network for recommendation task, termed\nNGAT4Rec. It employs a novel neighbor-aware graph attention layer that assigns\ndifferent neighbor-aware attention coefficients to different neighbors of a\ngiven node by computing the attention among these neighbors pairwisely. Then\nNGAT4Rec aggregates the embeddings of neighbors according to the corresponding\nneighbor-aware attention coefficients to generate next layer embedding for\nevery node. Furthermore, we combine more neighbor-aware graph attention layer\nto gather the influential signals from multi-hop neighbors. We remove feature\ntransformation and nonlinear activation that proved to be useless on\ncollaborative filtering. Extensive experiments on three benchmark datasets show\nthat our model outperforms various state-of-the-art models consistently.",
    "published": "2020-10-23T09:37:43Z",
    "updated": "2021-03-01T10:13:57Z",
    "authors": [
      "Jinbo Song",
      "Chao Chang",
      "Fei Sun",
      "Xinbo Song",
      "Peng Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.09281v2",
    "title": "SpatialFormer: Semantic and Target Aware Attentions for Few-Shot\n  Learning",
    "summary": "Recent Few-Shot Learning (FSL) methods put emphasis on generating a\ndiscriminative embedding features to precisely measure the similarity between\nsupport and query sets. Current CNN-based cross-attention approaches generate\ndiscriminative representations via enhancing the mutually semantic similar\nregions of support and query pairs. However, it suffers from two problems: CNN\nstructure produces inaccurate attention map based on local features, and\nmutually similar backgrounds cause distraction. To alleviate these problems, we\ndesign a novel SpatialFormer structure to generate more accurate attention\nregions based on global features. Different from the traditional Transformer\nmodeling intrinsic instance-level similarity which causes accuracy degradation\nin FSL, our SpatialFormer explores the semantic-level similarity between pair\ninputs to boost the performance. Then we derive two specific attention modules,\nnamed SpatialFormer Semantic Attention (SFSA) and SpatialFormer Target\nAttention (SFTA), to enhance the target object regions while reduce the\nbackground distraction. Particularly, SFSA highlights the regions with same\nsemantic information between pair features, and SFTA finds potential foreground\nobject regions of novel feature that are similar to base categories. Extensive\nexperiments show that our methods are effective and achieve new\nstate-of-the-art results on few-shot classification benchmarks.",
    "published": "2023-03-15T08:31:48Z",
    "updated": "2024-07-17T03:43:11Z",
    "authors": [
      "Jinxiang Lai",
      "Siqian Yang",
      "Wenlong Wu",
      "Tao Wu",
      "Guannan Jiang",
      "Xi Wang",
      "Jun Liu",
      "Bin-Bin Gao",
      "Wei Zhang",
      "Yuan Xie",
      "Chengjie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.15446v2",
    "title": "SwiftFormer: Efficient Additive Attention for Transformer-based\n  Real-time Mobile Vision Applications",
    "summary": "Self-attention has become a defacto choice for capturing global context in\nvarious vision applications. However, its quadratic computational complexity\nwith respect to image resolution limits its use in real-time applications,\nespecially for deployment on resource-constrained mobile devices. Although\nhybrid approaches have been proposed to combine the advantages of convolutions\nand self-attention for a better speed-accuracy trade-off, the expensive matrix\nmultiplication operations in self-attention remain a bottleneck. In this work,\nwe introduce a novel efficient additive attention mechanism that effectively\nreplaces the quadratic matrix multiplication operations with linear\nelement-wise multiplications. Our design shows that the key-value interaction\ncan be replaced with a linear layer without sacrificing any accuracy. Unlike\nprevious state-of-the-art methods, our efficient formulation of self-attention\nenables its usage at all stages of the network. Using our proposed efficient\nadditive attention, we build a series of models called \"SwiftFormer\" which\nachieves state-of-the-art performance in terms of both accuracy and mobile\ninference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy\nwith only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster\ncompared to MobileViT-v2. Code: https://github.com/Amshaker/SwiftFormer",
    "published": "2023-03-27T17:59:58Z",
    "updated": "2023-07-25T19:56:00Z",
    "authors": [
      "Abdelrahman Shaker",
      "Muhammad Maaz",
      "Hanoona Rasheed",
      "Salman Khan",
      "Ming-Hsuan Yang",
      "Fahad Shahbaz Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.11115v3",
    "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages",
    "summary": "Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.",
    "published": "2021-05-24T06:42:58Z",
    "updated": "2023-03-13T01:47:55Z",
    "authors": [
      "Shunyu Yao",
      "Binghui Peng",
      "Christos Papadimitriou",
      "Karthik Narasimhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.04540v1",
    "title": "Multi-Frequency Information Enhanced Channel Attention Module for\n  Speaker Representation Learning",
    "summary": "Recently, attention mechanisms have been applied successfully in neural\nnetwork-based speaker verification systems. Incorporating the\nSqueeze-and-Excitation block into convolutional neural networks has achieved\nremarkable performance. However, it uses global average pooling (GAP) to simply\naverage the features along time and frequency dimensions, which is incapable of\npreserving sufficient speaker information in the feature maps. In this study,\nwe show that GAP is a special case of a discrete cosine transform (DCT) on\ntime-frequency domain mathematically using only the lowest frequency component\nin frequency decomposition. To strengthen the speaker information extraction\nability, we propose to utilize multi-frequency information and design two novel\nand effective attention modules, called Single-Frequency Single-Channel (SFSC)\nattention module and Multi-Frequency Single-Channel (MFSC) attention module.\nThe proposed attention modules can effectively capture more speaker information\nfrom multiple frequency components on the basis of DCT. We conduct\ncomprehensive experiments on the VoxCeleb datasets and a probe evaluation on\nthe 1st 48-UTD forensic corpus. Experimental results demonstrate that our\nproposed SFSC and MFSC attention modules can efficiently generate more\ndiscriminative speaker representations and outperform ResNet34-SE and\nECAPA-TDNN systems with relative 20.9% and 20.2% reduction in EER, without\nadding extra network parameters.",
    "published": "2022-07-10T21:19:36Z",
    "updated": "2022-07-10T21:19:36Z",
    "authors": [
      "Mufan Sang",
      "John H. L. Hansen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.08283v3",
    "title": "SceneGATE: Scene-Graph based co-Attention networks for TExt visual\n  question answering",
    "summary": "Most TextVQA approaches focus on the integration of objects, scene texts and\nquestion words by a simple transformer encoder. But this fails to capture the\nsemantic relations between different modalities. The paper proposes a Scene\nGraph based co-Attention Network (SceneGATE) for TextVQA, which reveals the\nsemantic relations among the objects, Optical Character Recognition (OCR)\ntokens and the question words. It is achieved by a TextVQA-based scene graph\nthat discovers the underlying semantics of an image. We created a\nguided-attention module to capture the intra-modal interplay between the\nlanguage and the vision as a guidance for inter-modal interactions. To make\nexplicit teaching of the relations between the two modalities, we proposed and\nintegrated two attention modules, namely a scene graph-based semantic\nrelation-aware attention and a positional relation-aware attention. We\nconducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.\nIt is shown that our SceneGATE method outperformed existing ones because of the\nscene graph and its attention modules.",
    "published": "2022-12-16T05:10:09Z",
    "updated": "2023-08-07T08:32:54Z",
    "authors": [
      "Feiqi Cao",
      "Siwen Luo",
      "Felipe Nunez",
      "Zean Wen",
      "Josiah Poon",
      "Caren Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.13899v1",
    "title": "Attentive Deep Neural Networks for Legal Document Retrieval",
    "summary": "Legal text retrieval serves as a key component in a wide range of legal text\nprocessing tasks such as legal question answering, legal case entailment, and\nstatute law retrieval. The performance of legal text retrieval depends, to a\nlarge extent, on the representation of text, both query and legal documents.\nBased on good representations, a legal text retrieval model can effectively\nmatch the query to its relevant documents. Because legal documents often\ncontain long articles and only some parts are relevant to queries, it is quite\na challenge for existing models to represent such documents. In this paper, we\nstudy the use of attentive neural network-based text representation for statute\nlaw document retrieval. We propose a general approach using deep neural\nnetworks with attention mechanisms. Based on it, we develop two hierarchical\narchitectures with sparse attention to represent long sentences and articles,\nand we name them Attentive CNN and Paraformer. The methods are evaluated on\ndatasets of different sizes and characteristics in English, Japanese, and\nVietnamese. Experimental results show that: i) Attentive neural methods\nsubstantially outperform non-neural methods in terms of retrieval performance\nacross datasets and languages; ii) Pretrained transformer-based models achieve\nbetter accuracy on small datasets at the cost of high computational complexity\nwhile lighter weight Attentive CNN achieves better accuracy on large datasets;\nand iii) Our proposed Paraformer outperforms state-of-the-art methods on COLIEE\ndataset, achieving the highest recall and F2 scores in the top-N retrieval\ntask.",
    "published": "2022-12-13T01:37:27Z",
    "updated": "2022-12-13T01:37:27Z",
    "authors": [
      "Ha-Thanh Nguyen",
      "Manh-Kien Phi",
      "Xuan-Bach Ngo",
      "Vu Tran",
      "Le-Minh Nguyen",
      "Minh-Phuong Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.02117v2",
    "title": "Learning to Agree on Vision Attention for Visual Commonsense Reasoning",
    "summary": "Visual Commonsense Reasoning (VCR) remains a significant yet challenging\nresearch problem in the realm of visual reasoning. A VCR model generally aims\nat answering a textual question regarding an image, followed by the rationale\nprediction for the preceding answering process. Though these two processes are\nsequential and intertwined, existing methods always consider them as two\nindependent matching-based instances. They, therefore, ignore the pivotal\nrelationship between the two processes, leading to sub-optimal model\nperformance. This paper presents a novel visual attention alignment method to\nefficaciously handle these two processes in a unified framework. To achieve\nthis, we first design a re-attention module for aggregating the vision\nattention map produced in each process. Thereafter, the resultant two sets of\nattention maps are carefully aligned to guide the two processes to make\ndecisions based on the same image regions. We apply this method to both\nconventional attention and the recent Transformer models and carry out\nextensive experiments on the VCR benchmark dataset. The results demonstrate\nthat with the attention alignment module, our method achieves a considerable\nimprovement over the baseline methods, evidently revealing the feasibility of\nthe coupling of the two processes as well as the effectiveness of the proposed\nmethod.",
    "published": "2023-02-04T07:02:29Z",
    "updated": "2023-02-19T06:44:39Z",
    "authors": [
      "Zhenyang Li",
      "Yangyang Guo",
      "Kejie Wang",
      "Fan Liu",
      "Liqiang Nie",
      "Mohan Kankanhalli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.03071v2",
    "title": "OrthoNets: Orthogonal Channel Attention Networks",
    "summary": "Designing an effective channel attention mechanism implores one to find a\nlossy-compression method allowing for optimal feature representation. Despite\nrecent progress in the area, it remains an open problem. FcaNet, the current\nstate-of-the-art channel attention mechanism, attempted to find such an\ninformation-rich compression using Discrete Cosine Transforms (DCTs). One\ndrawback of FcaNet is that there is no natural choice of the DCT frequencies.\nTo circumvent this issue, FcaNet experimented on ImageNet to find optimal\nfrequencies. We hypothesize that the choice of frequency plays only a\nsupporting role and the primary driving force for the effectiveness of their\nattention filters is the orthogonality of the DCT kernels. To test this\nhypothesis, we construct an attention mechanism using randomly initialized\northogonal filters. Integrating this mechanism into ResNet, we create OrthoNet.\nWe compare OrthoNet to FcaNet (and other attention mechanisms) on Birds,\nMS-COCO, and Places356 and show superior performance. On the ImageNet dataset,\nour method competes with or surpasses the current state-of-the-art. Our results\nimply that an optimal choice of filter is elusive and generalization can be\nachieved with a sufficiently large number of orthogonal filters. We further\ninvestigate other general principles for implementing channel attention, such\nas its position in the network and channel groupings. Our code is publicly\navailable at https://github.com/hady1011/OrthoNets/",
    "published": "2023-11-06T12:54:20Z",
    "updated": "2023-11-07T02:23:30Z",
    "authors": [
      "Hadi Salman",
      "Caleb Parks",
      "Matthew Swan",
      "John Gauch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.06614v1",
    "title": "AttenScribble: Attentive Similarity Learning for Scribble-Supervised\n  Medical Image Segmentation",
    "summary": "The success of deep networks in medical image segmentation relies heavily on\nmassive labeled training data. However, acquiring dense annotations is a\ntime-consuming process. Weakly-supervised methods normally employ less\nexpensive forms of supervision, among which scribbles started to gain\npopularity lately thanks to its flexibility. However, due to lack of shape and\nboundary information, it is extremely challenging to train a deep network on\nscribbles that generalizes on unlabeled pixels. In this paper, we present a\nstraightforward yet effective scribble supervised learning framework. Inspired\nby recent advances of transformer based segmentation, we create a pluggable\nspatial self-attention module which could be attached on top of any internal\nfeature layers of arbitrary fully convolutional network (FCN) backbone. The\nmodule infuses global interaction while keeping the efficiency of convolutions.\nDescended from this module, we construct a similarity metric based on\nnormalized and symmetrized attention. This attentive similarity leads to a\nnovel regularization loss that imposes consistency between segmentation\nprediction and visual affinity. This attentive similarity loss optimizes the\nalignment of FCN encoders, attention mapping and model prediction. Ultimately,\nthe proposed FCN+Attention architecture can be trained end-to-end guided by a\ncombination of three learning objectives: partial segmentation loss, a\ncustomized masked conditional random fields and the proposed attentive\nsimilarity loss. Extensive experiments on public datasets (ACDC and CHAOS)\nshowed that our framework not just out-performs existing state-of-the-art, but\nalso delivers close performance to fully-supervised benchmark. Code will be\navailable upon publication.",
    "published": "2023-12-11T18:42:18Z",
    "updated": "2023-12-11T18:42:18Z",
    "authors": [
      "Mu Tian",
      "Qinzhu Yang",
      "Yi Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.08866v3",
    "title": "MCANet: Medical Image Segmentation with Multi-Scale Cross-Axis Attention",
    "summary": "Efficiently capturing multi-scale information and building long-range\ndependencies among pixels are essential for medical image segmentation because\nof the various sizes and shapes of the lesion regions or organs. In this paper,\nwe present Multi-scale Cross-axis Attention (MCA) to solve the above\nchallenging issues based on the efficient axial attention. Instead of simply\nconnecting axial attention along the horizontal and vertical directions\nsequentially, we propose to calculate dual cross attentions between two\nparallel axial attentions to capture global information better. To process the\nsignificant variations of lesion regions or organs in individual sizes and\nshapes, we also use multiple convolutions of strip-shape kernels with different\nkernel sizes in each axial attention path to improve the efficiency of the\nproposed MCA in encoding spatial information. We build the proposed MCA upon\nthe MSCAN backbone, yielding our network, termed MCANet. Our MCANet with only\n4M+ parameters performs even better than most previous works with heavy\nbackbones (e.g., Swin Transformer) on four challenging tasks, including skin\nlesion segmentation, nuclei segmentation, abdominal multi-organ segmentation,\nand polyp segmentation. Code is available at\nhttps://github.com/haoshao-nku/medical_seg.",
    "published": "2023-12-14T12:41:08Z",
    "updated": "2025-04-17T07:04:50Z",
    "authors": [
      "Hao Shao",
      "Quansheng Zeng",
      "Qibin Hou",
      "Jufeng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.12804v2",
    "title": "Multi-stages attention Breast cancer classification based on nonlinear\n  spiking neural P neurons with autapses",
    "summary": "Breast cancer(BC) is a prevalent type of malignant tumor in women. Early\ndiagnosis and treatment are vital for enhancing the patients' survival rate.\nDownsampling in deep networks may lead to loss of information, so for\ncompensating the detail and edge information and allowing convolutional neural\nnetworks to pay more attention to seek the lesion region, we propose a\nmulti-stages attention architecture based on NSNP neurons with autapses. First,\nunlike the single-scale attention acquisition methods of existing methods, we\nset up spatial attention acquisition at each feature map scale of the\nconvolutional network to obtain an fusion global information on attention\nguidance. Then we introduce a new type of NSNP variants called NSNP neurons\nwith autapses. Specifically, NSNP systems are modularized as feature encoders,\nrecoding the features extracted from convolutional neural network as well as\nthe fusion of attention information and preserve the key characteristic\nelements in feature maps. This ensures the retention of valuable data while\ngradually transforming high-dimensional complicated info into low-dimensional\nones. The proposed method is evaluated on the public dataset BreakHis at\nvarious magnifications and classification tasks. It achieves a classification\naccuracy of 96.32% at all magnification cases, outperforming state-of-the-art\nmethods. Ablation studies are also performed, verifying the proposed model's\nefficacy. The source code is available at\nXhuBobYoung/Breast-cancer-Classification.",
    "published": "2023-12-20T06:52:38Z",
    "updated": "2024-01-04T09:28:50Z",
    "authors": [
      "Bo Yang",
      "Hong Peng",
      "Xiaohui Luo",
      "Jun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.02336v1",
    "title": "Brand Visibility in Packaging: A Deep Learning Approach for Logo\n  Detection, Saliency-Map Prediction, and Logo Placement Analysis",
    "summary": "In the highly competitive area of product marketing, the visibility of brand\nlogos on packaging plays a crucial role in shaping consumer perception,\ndirectly influencing the success of the product. This paper introduces a\ncomprehensive framework to measure the brand logo's attention on a packaging\ndesign. The proposed method consists of three steps. The first step leverages\nYOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500\nand LogoDet-3K. The second step involves modeling the user's visual attention\nwith a novel saliency prediction model tailored for the packaging context. The\nproposed saliency model combines the visual elements with text maps employing a\ntransformers-based architecture to predict user attention maps. In the third\nstep, by integrating logo detection with a saliency map generation, the\nframework provides a comprehensive brand attention score. The effectiveness of\nthe proposed method is assessed module by module, ensuring a thorough\nevaluation of each component. Comparing logo detection and saliency map\nprediction with state-of-the-art models shows the superiority of the proposed\nmethods. To investigate the robustness of the proposed brand attention score,\nwe collected a unique dataset to examine previous psychophysical hypotheses\nrelated to brand visibility. the results show that the brand attention score is\nin line with all previous studies. Also, we introduced seven new hypotheses to\ncheck the impact of position, orientation, presence of person, and other visual\nelements on brand attention. This research marks a significant stride in the\nintersection of cognitive psychology, computer vision, and marketing, paving\nthe way for advanced, consumer-centric packaging designs.",
    "published": "2024-03-04T18:58:53Z",
    "updated": "2024-03-04T18:58:53Z",
    "authors": [
      "Alireza Hosseini",
      "Kiana Hooshanfar",
      "Pouria Omrani",
      "Reza Toosi",
      "Ramin Toosi",
      "Zahra Ebrahimian",
      "Mohammad Ali Akhaee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.09347v4",
    "title": "BurstAttention: An Efficient Distributed Attention Framework for\n  Extremely Long Sequences",
    "summary": "Effective attention modules have played a crucial role in the success of\nTransformer-based large language models (LLMs), but the quadratic time and\nmemory complexities of these attention modules also pose a challenge when\nprocessing long sequences. One potential solution for the long sequence problem\nis to utilize distributed clusters to parallelize the computation of attention\nmodules across multiple devices (e.g., GPUs). However, adopting a distributed\napproach inevitably introduces extra memory overheads to store local attention\nresults and incurs additional communication costs to aggregate local results\ninto global ones. In this paper, we propose a distributed attention framework\nnamed ``BurstAttention'' to optimize memory access and communication operations\nat both the global cluster and local device levels. In our experiments, we\ncompare BurstAttention with other competitive distributed attention solutions\nfor long sequence processing. The experimental results under different length\nsettings demonstrate that BurstAttention offers significant advantages for\nprocessing long sequences compared with these competitive baselines, reducing\n40% communication overheads and achieving 1.37 X speedup during training 128K\nsequence length on 32 X A100.",
    "published": "2024-03-14T12:51:58Z",
    "updated": "2024-06-06T05:43:52Z",
    "authors": [
      "Ao Sun",
      "Weilin Zhao",
      "Xu Han",
      "Cheng Yang",
      "Zhiyuan Liu",
      "Chuan Shi",
      "Maosong Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.17471v1",
    "title": "Medical Image Segmentation Using Directional Window Attention",
    "summary": "Accurate segmentation of medical images is crucial for diagnostic purposes,\nincluding cell segmentation, tumor identification, and organ localization.\nTraditional convolutional neural network (CNN)-based approaches struggled to\nachieve precise segmentation results due to their limited receptive fields,\nparticularly in cases involving multi-organ segmentation with varying shapes\nand sizes. The transformer-based approaches address this limitation by\nleveraging the global receptive field, but they often face challenges in\ncapturing local information required for pixel-precise segmentation. In this\nwork, we introduce DwinFormer, a hierarchical encoder-decoder architecture for\nmedical image segmentation comprising a directional window (Dwin) attention and\nglobal self-attention (GSA) for feature encoding. The focus of our design is\nthe introduction of Dwin block within DwinFormer that effectively captures\nlocal and global information along the horizontal, vertical, and depthwise\ndirections of the input feature map by separately performing attention in each\nof these directional volumes. To this end, our Dwin block introduces a nested\nDwin attention (NDA) that progressively increases the receptive field in\nhorizontal, vertical, and depthwise directions and a convolutional Dwin\nattention (CDA) that captures local contextual information for the attention\ncomputation. While the proposed Dwin block captures local and global\ndependencies at the first two high-resolution stages of DwinFormer, the GSA\nblock encodes global dependencies at the last two lower-resolution stages.\nExperiments over the challenging 3D Synapse Multi-organ dataset and Cell HMS\ndataset demonstrate the benefits of our DwinFormer over the state-of-the-art\napproaches. Our source code will be publicly available at\n\\url{https://github.com/Daniyanaj/DWINFORMER}.",
    "published": "2024-06-25T11:15:56Z",
    "updated": "2024-06-25T11:15:56Z",
    "authors": [
      "Daniya Najiha Abdul Kareem",
      "Mustansar Fiaz",
      "Noa Novershtern",
      "Hisham Cholakkal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.06110v1",
    "title": "FGA: Fourier-Guided Attention Network for Crowd Count Estimation",
    "summary": "Crowd counting is gaining societal relevance, particularly in domains of\nUrban Planning, Crowd Management, and Public Safety. This paper introduces\nFourier-guided attention (FGA), a novel attention mechanism for crowd count\nestimation designed to address the inefficient full-scale global pattern\ncapture in existing works on convolution-based attention networks. FGA\nefficiently captures multi-scale information, including full-scale global\npatterns, by utilizing Fast-Fourier Transformations (FFT) along with spatial\nattention for global features and convolutions with channel-wise attention for\nsemi-global and local features. The architecture of FGA involves a dual-path\napproach: (1) a path for processing full-scale global features through FFT,\nallowing for efficient extraction of information in the frequency domain, and\n(2) a path for processing remaining feature maps for semi-global and local\nfeatures using traditional convolutions and channel-wise attention. This\ndual-path architecture enables FGA to seamlessly integrate frequency and\nspatial information, enhancing its ability to capture diverse crowd patterns.\nWe apply FGA in the last layers of two popular crowd-counting works, CSRNet and\nCANNet, to evaluate the module's performance on benchmark datasets such as\nShanghaiTech-A, ShanghaiTech-B, UCF-CC-50, and JHU++ crowd. The experiments\ndemonstrate a notable improvement across all datasets based on\nMean-Squared-Error (MSE) and Mean-Absolute-Error (MAE) metrics, showing\ncomparable performance to recent state-of-the-art methods. Additionally, we\nillustrate the interpretability using qualitative analysis, leveraging Grad-CAM\nheatmaps, to show the effectiveness of FGA in capturing crowd patterns.",
    "published": "2024-07-08T16:47:19Z",
    "updated": "2024-07-08T16:47:19Z",
    "authors": [
      "Yashwardhan Chaudhuri",
      "Ankit Kumar",
      "Arun Balaji Buduru",
      "Adel Alshamrani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.07654v2",
    "title": "Graph Triple Attention Network: A Decoupled Perspective",
    "summary": "Graph Transformers (GTs) have recently achieved significant success in the\ngraph domain by effectively capturing both long-range dependencies and graph\ninductive biases. However, these methods face two primary challenges: (1)\nmulti-view chaos, which results from coupling multi-view information\n(positional, structural, attribute), thereby impeding flexible usage and the\ninterpretability of the propagation process. (2) local-global chaos, which\narises from coupling local message passing with global attention, leading to\nissues of overfitting and over-globalizing. To address these challenges, we\npropose a high-level decoupled perspective of GTs, breaking them down into\nthree components and two interaction levels: positional attention, structural\nattention, and attribute attention, alongside local and global interaction.\nBased on this decoupled perspective, we design a decoupled graph triple\nattention network named DeGTA, which separately computes multi-view attentions\nand adaptively integrates multi-view local and global information. This\napproach offers three key advantages: enhanced interpretability, flexible\ndesign, and adaptive integration of local and global information. Through\nextensive experiments, DeGTA achieves state-of-the-art performance across\nvarious datasets and tasks, including node classification and graph\nclassification. Comprehensive ablation studies demonstrate that decoupling is\nessential for improving performance and enhancing interpretability. Our code is\navailable at: https://github.com/wangxiaotang0906/DeGTA",
    "published": "2024-08-14T16:29:07Z",
    "updated": "2024-12-31T07:35:26Z",
    "authors": [
      "Xiaotang Wang",
      "Yun Zhu",
      "Haizhou Shi",
      "Yongchao Liu",
      "Chuntao Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.16503v1",
    "title": "Locally Grouped and Scale-Guided Attention for Dense Pest Counting",
    "summary": "This study introduces a new dense pest counting problem to predict densely\ndistributed pests captured by digital traps. Unlike traditional detection-based\ncounting models for sparsely distributed objects, trap-based pest counting must\ndeal with dense pest distributions that pose challenges such as severe\nocclusion, wide pose variation, and similar appearances in colors and textures.\nTo address these problems, it is essential to incorporate the local attention\nmechanism, which identifies locally important and unimportant areas to learn\nlocally grouped features, thereby enhancing discriminative performance.\nAccordingly, this study presents a novel design that integrates locally grouped\nand scale-guided attention into a multiscale CenterNet framework. To group\nlocal features with similar attributes, a straightforward method is introduced\nusing the heatmap predicted by the first hourglass containing pest centroid\ninformation, which eliminates the need for complex clustering models. To\nenhance attentiveness, the pixel attention module transforms the heatmap into a\nlearnable map. Subsequently, scale-guided attention is deployed to make the\nobject and background features more discriminative, achieving multiscale\nfeature fusion. Through experiments, the proposed model is verified to enhance\nobject features based on local grouping and discriminative feature attention\nlearning. Additionally, the proposed model is highly effective in overcoming\nocclusion and pose variation problems, making it more suitable for dense pest\ncounting. In particular, the proposed model outperforms state-of-the-art models\nby a large margin, with a remarkable contribution to dense pest counting.",
    "published": "2024-08-29T13:02:01Z",
    "updated": "2024-08-29T13:02:01Z",
    "authors": [
      "Chang-Hwan Son"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.07860v2",
    "title": "BA-Net: Bridge Attention in Deep Neural Networks",
    "summary": "Attention mechanisms, particularly channel attention, have become highly\ninfluential in numerous computer vision tasks. Despite their effectiveness,\nmany existing methods primarily focus on optimizing performance through complex\nattention modules applied at individual convolutional layers, often overlooking\nthe synergistic interactions that can occur across multiple layers. In response\nto this gap, we introduce bridge attention, a novel approach designed to\nfacilitate more effective integration and information flow between different\nconvolutional layers. Our work extends the original bridge attention model\n(BAv1) by introducing an adaptive selection operator, which reduces information\nredundancy and optimizes the overall information exchange. This enhancement\nresults in the development of BAv2, which achieves substantial performance\nimprovements in the ImageNet classification task, obtaining Top-1 accuracies of\n80.49% and 81.75% when using ResNet50 and ResNet101 as backbone networks,\nrespectively. These results surpass the retrained baselines by 1.61% and 0.77%,\nrespectively. Furthermore, BAv2 outperforms other existing channel attention\ntechniques, such as the classical SENet101, exceeding its retrained performance\nby 0.52% Additionally, integrating BAv2 into advanced convolutional networks\nand vision transformers has led to significant gains in performance across a\nwide range of computer vision tasks, underscoring its broad applicability.",
    "published": "2024-10-10T12:28:20Z",
    "updated": "2024-10-11T03:35:52Z",
    "authors": [
      "Ronghui Zhang",
      "Runzong Zou",
      "Yue Zhao",
      "Zirui Zhang",
      "Junzhou Chen",
      "Yue Cao",
      "Chuan Hu",
      "Houbing Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.11076v3",
    "title": "MoRe: Class Patch Attention Needs Regularization for Weakly Supervised\n  Semantic Segmentation",
    "summary": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels\ntypically uses Class Activation Maps (CAM) to achieve dense predictions.\nRecently, Vision Transformer (ViT) has provided an alternative to generate\nlocalization maps from class-patch attention. However, due to insufficient\nconstraints on modeling such attention, we observe that the Localization\nAttention Maps (LAM) often struggle with the artifact issue, i.e., patch\nregions with minimal semantic relevance are falsely activated by class tokens.\nIn this work, we propose MoRe to address this issue and further explore the\npotential of LAM. Our findings suggest that imposing additional regularization\non class-patch attention is necessary. To this end, we first view the attention\nas a novel directed graph and propose the Graph Category Representation module\nto implicitly regularize the interaction among class-patch entities. It ensures\nthat class tokens dynamically condense the related patch information and\nsuppress unrelated artifacts at a graph level. Second, motivated by the\nobservation that CAM from classification weights maintains smooth localization\nof objects, we devise the Localization-informed Regularization module to\nexplicitly regularize the class-patch attention. It directly mines the token\nrelations from CAM and further supervises the consistency between class and\npatch tokens in a learnable manner. Extensive experiments are conducted on\nPASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact\nissue and achieves state-of-the-art performance, surpassing recent single-stage\nand even multi-stage methods. Code is available at\nhttps://github.com/zwyang6/MoRe.",
    "published": "2024-12-15T06:20:41Z",
    "updated": "2025-01-17T07:21:34Z",
    "authors": [
      "Zhiwei Yang",
      "Yucong Meng",
      "Kexue Fu",
      "Shuo Wang",
      "Zhijian Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.13428v4",
    "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in\n  Large Language Models",
    "summary": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by proposing a new design principle for attention, viewing it as a\ntwo-stage process. We first decompose the Softmax operation into a non-linear\npositivity transformation and an $l_1$-normalisation step, identifying the\nlatter as essential for maintaining model performance. In the first stage, we\nreplace the standard exponential function with the more numerically stable\nSoftplus activation and introduce a dynamic scale factor based on invariance\nentropy, creating a novel attention mechanism that outperforms conventional\nSoftmax attention. In the second stage, we introduce a re-weighting mechanism\nthat sharpens the attention distribution, amplifying significant weights while\ndiminishing weaker ones. This enables the model to concentrate more effectively\non relevant tokens and fundamentally improves length extrapolation. When\ncombined, this two-stage approach ensures numerical stability and dramatically\nimproves length extrapolation, maintaining a nearly constant validation loss at\n16$\\times$ the training length while achieving superior results on challenging\nlong-context retrieval tasks and standard downstream benchmarks.",
    "published": "2025-01-23T07:21:08Z",
    "updated": "2025-08-11T02:00:03Z",
    "authors": [
      "Bo Gao",
      "Michael W. Spratling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.04077v3",
    "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
    "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
    "published": "2025-02-06T13:41:46Z",
    "updated": "2025-10-26T04:25:10Z",
    "authors": [
      "Qingyue Yang",
      "Jie Wang",
      "Xing Li",
      "Zhihai Wang",
      "Chen Chen",
      "Lei Chen",
      "Xianzhi Yu",
      "Wulong Liu",
      "Jianye Hao",
      "Mingxuan Yuan",
      "Bin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.18094v2",
    "title": "FwNet-ECA: A Classification Model Enhancing Window Attention with Global\n  Receptive Fields via Fourier Filtering Operations",
    "summary": "Windowed attention mechanisms were introduced to mitigate the issue of\nexcessive computation inherent in global attention mechanisms. In this paper,\nwe present FwNet-ECA, a novel method that utilizes Fourier transforms paired\nwith learnable weight matrices to enhance the spectral features of images. This\nmethod establishes a global receptive field through Filter Enhancement and\navoids the use of moving window attention. Additionally, we incorporate the\nEfficient Channel Attention (ECA) module to improve communication between\ndifferent channels. Instead of relying on physically shifted windows, our\napproach leverages frequency domain enhancement to implicitly bridge\ninformation across spatial regions. We validate our model on the iCartoonFace\ndataset and conduct downstream tasks on ImageNet, demonstrating that our model\nachieves lower parameter counts and computational overheads compared to shifted\nwindow approaches, while maintaining competitive accuracy. Furthermore, our\nvisualization operations clearly demonstrated that the Filter Enhancement\ntechnique achieves greater effectiveness in the model's shallow layers, where\nfeature maps are relatively larger. This work offers a more efficient and\neffective alternative for leveraging attention mechanisms in visual processing\ntasks, alleviating the challenges associated with windowed attention models.\nCode is available at https://github.com/qingxiaoli/FwNet-ECA",
    "published": "2025-02-25T11:01:53Z",
    "updated": "2025-03-04T00:48:00Z",
    "authors": [
      "Shengtian Mian",
      "Ya Wang",
      "Nannan Gu",
      "Yuping Wang",
      "Xiaoqing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.10148v2",
    "title": "Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for\n  Multi-Instance Synthesis in Diffusion Transformers",
    "summary": "Text-to-image (T2I) generation models often struggle with multi-instance\nsynthesis (MIS), where they must accurately depict multiple distinct instances\nin a single image based on complex prompts detailing individual features.\nTraditional MIS control methods for UNet architectures like SD v1.5/SDXL fail\nto adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated\nattention between image and text tokens rather than text-image cross-attention.\nTo enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT.\nOur token-wise and layer-wise analysis of attention maps reveals a hierarchical\nresponse structure: instance tokens dominate early layers, background tokens in\nmiddle layers, and attribute tokens in later layers. Building on this\nobservation, we propose a training-free approach for enhancing MIS in DiT-based\nmodels with hierarchical and step-layer-wise attention specialty tuning (AST).\nAST amplifies key regions while suppressing irrelevant areas in distinct\nattention maps across layers and steps, guided by the hierarchical structure.\nThis optimizes multimodal interactions by hierarchically decoupling the complex\nprompts with instance-based sketches. We evaluate our approach using upgraded\nsketch-based layouts for the T2I-CompBench and customized complex scenes. Both\nquantitative and qualitative results confirm our method enhances complex layout\ngeneration, ensuring precise instance placement and attribute representation in\nMIS.",
    "published": "2025-04-14T11:59:58Z",
    "updated": "2025-04-21T03:29:53Z",
    "authors": [
      "Chunyang Zhang",
      "Zhenhong Sun",
      "Zhicheng Zhang",
      "Junyan Wang",
      "Yu Zhang",
      "Dong Gong",
      "Huadong Mo",
      "Daoyi Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12203v1",
    "title": "CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and\n  Self-Attention for Enhanced CT Image Reconstruction",
    "summary": "Low-dose CT (LDCT) images are often accompanied by significant noise, which\nnegatively impacts image quality and subsequent diagnostic accuracy. To address\nthe challenges of multi-scale feature fusion and diverse noise distribution\npatterns in LDCT denoising, this paper introduces an innovative model,\nCTLformer, which combines convolutional structures with transformer\narchitecture. Two key innovations are proposed: a multi-scale attention\nmechanism and a dynamic attention control mechanism. The multi-scale attention\nmechanism, implemented through the Token2Token mechanism and self-attention\ninteraction modules, effectively captures both fine details and global\nstructures at different scales, enhancing relevant features and suppressing\nnoise. The dynamic attention control mechanism adapts the attention\ndistribution based on the noise characteristics of the input image, focusing on\nhigh-noise regions while preserving details in low-noise areas, thereby\nenhancing robustness and improving denoising performance. Furthermore,\nCTLformer integrates convolutional layers for efficient feature extraction and\nuses overlapping inference to mitigate boundary artifacts, further\nstrengthening its denoising capability. Experimental results on the 2016\nNational Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset\ndemonstrate that CTLformer significantly outperforms existing methods in both\ndenoising performance and model efficiency, greatly improving the quality of\nLDCT images. The proposed CTLformer not only provides an efficient solution for\nLDCT denoising but also shows broad potential in medical image analysis,\nespecially for clinical applications dealing with complex noise patterns.",
    "published": "2025-05-18T02:37:50Z",
    "updated": "2025-05-18T02:37:50Z",
    "authors": [
      "Zhiting Zheng",
      "Shuqi Wu",
      "Wen Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19668v1",
    "title": "Burst Image Super-Resolution via Multi-Cross Attention Encoding and\n  Multi-Scan State-Space Decoding",
    "summary": "Multi-image super-resolution (MISR) can achieve higher image quality than\nsingle-image super-resolution (SISR) by aggregating sub-pixel information from\nmultiple spatially shifted frames. Among MISR tasks, burst super-resolution\n(BurstSR) has gained significant attention due to its wide range of\napplications. Recent methods have increasingly adopted Transformers over\nconvolutional neural networks (CNNs) in super-resolution tasks, due to their\nsuperior ability to capture both local and global context. However, most\nexisting approaches still rely on fixed and narrow attention windows that\nrestrict the perception of features beyond the local field. This limitation\nhampers alignment and feature aggregation, both of which are crucial for\nhigh-quality super-resolution. To address these limitations, we propose a novel\nfeature extractor that incorporates two newly designed attention mechanisms:\noverlapping cross-window attention and cross-frame attention, enabling more\nprecise and efficient extraction of sub-pixel information across multiple\nframes. Furthermore, we introduce a Multi-scan State-Space Module with the\ncross-frame attention mechanism to enhance feature aggregation. Extensive\nexperiments on both synthetic and real-world benchmarks demonstrate the\nsuperiority of our approach. Additional evaluations on ISO 12233 resolution\ntest charts further confirm its enhanced super-resolution performance.",
    "published": "2025-05-26T08:24:33Z",
    "updated": "2025-05-26T08:24:33Z",
    "authors": [
      "Tengda Huang",
      "Yu Zhang",
      "Tianren Li",
      "Yufu Qu",
      "Fulin Liu",
      "Zhenzhong Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22918v4",
    "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
    "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. Experimental results\non T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that\nRe-ttention requires as few as 3.1% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference.",
    "published": "2025-05-28T22:39:12Z",
    "updated": "2025-10-28T21:55:57Z",
    "authors": [
      "Ruichen Chen",
      "Keith G. Mills",
      "Liyao Jiang",
      "Chao Gao",
      "Di Niu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.08637v1",
    "title": "Scaling Attention to Very Long Sequences in Linear Time with\n  Wavelet-Enhanced Random Spectral Attention (WERSA)",
    "summary": "Transformer models are computationally costly on long sequences since regular\nattention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced\nRandom Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time\ncomplexity that is pivotal to enable successful long-sequence processing\nwithout the performance trade-off. WERSA merges content-adaptive random\nspectral features together with multi-resolution Haar wavelets and learnable\nparameters to selectively attend to informative scales of data while preserving\nlinear efficiency.\n  Large-scale comparisons \\textbf{on single GPU} and across various benchmarks\n(vision, NLP, hierarchical reasoning) and various attention mechanisms (like\nMultiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,\nWaveformer), reveal uniform advantages of WERSA. It achieves best accuracy in\nall tests. On ArXiv classification, WERSA improves accuracy over vanilla\nattention by 1.2\\% (86.2\\% vs 85.0\\%) while cutting training time by 81\\% (296s\nvs 1554s) and FLOPS by 73.4\\% (26.2G vs 98.4G). Significantly, WERSA excels\nwhere vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy\nsequences, it achieves best accuracy (79.1\\%) and AUC (0.979) among viable\nmethods, operating on data that gives Out-Of-Memory errors to quadratic methods\nwhile being \\textbf{twice as fast} as Waveformer, its next-best competitor.\n  By significantly reducing computational loads without compromising accuracy,\nWERSA makes possible more practical, more affordable, long-context models, in\nparticular on low-resource hardware, for more sustainable and more scalable AI\ndevelopment.",
    "published": "2025-07-11T14:40:40Z",
    "updated": "2025-07-11T14:40:40Z",
    "authors": [
      "Vincenzo Dentamaro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11090v1",
    "title": "End-to-End Visual Autonomous Parking via Control-Aided Attention",
    "summary": "Precise parking requires an end-to-end system where perception adaptively\nprovides policy-relevant details-especially in critical areas where fine\ncontrol decisions are essential. End-to-end learning offers a unified framework\nby directly mapping sensor inputs to control actions, but existing approaches\nlack effective synergy between perception and control. We find that\ntransformer-based self-attention, when used alone, tends to produce unstable\nand temporally inconsistent spatial attention, which undermines the reliability\nof downstream policy decisions over time. Instead, we propose CAA-Policy, an\nend-to-end imitation learning system that allows control signal to guide the\nlearning of visual attention via a novel Control-Aided Attention (CAA)\nmechanism. For the first time, we train such an attention module in a\nself-supervised manner, using backpropagated gradients from the control outputs\ninstead of from the training loss. This strategy encourages the attention to\nfocus on visual features that induce high variance in action outputs, rather\nthan merely minimizing the training loss-a shift we demonstrate leads to a more\nrobust and generalizable policy. To further enhance stability, CAA-Policy\nintegrates short-horizon waypoint prediction as an auxiliary task, and\nintroduces a separately trained motion prediction module to robustly track the\ntarget spot over time. Extensive experiments in the CARLA simulator show that\n\\titlevariable~consistently surpasses both the end-to-end learning baseline and\nthe modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,\nrobustness, and interpretability. Code is released at\nhttps://github.com/Joechencc/CAAPolicy.",
    "published": "2025-09-14T04:51:19Z",
    "updated": "2025-09-14T04:51:19Z",
    "authors": [
      "Chao Chen",
      "Shunyu Yao",
      "Yuanwu He",
      "Tao Feng",
      "Ruojing Song",
      "Yuliang Guo",
      "Xinyu Huang",
      "Chenxu Wu",
      "Ren Liu",
      "Chen Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12817v1",
    "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
    "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
    "published": "2025-09-16T08:36:05Z",
    "updated": "2025-09-16T08:36:05Z",
    "authors": [
      "Yuan Cao",
      "Dong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22650v1",
    "title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
    "summary": "Most existing approaches to referring segmentation achieve strong performance\nonly through fine-tuning or by composing multiple pre-trained models, often at\nthe cost of additional training and architectural modifications. Meanwhile,\nlarge-scale generative diffusion models encode rich semantic information,\nmaking them attractive as general-purpose feature extractors. In this work, we\nintroduce a new method that directly exploits features, attention scores, from\ndiffusion transformers for downstream tasks, requiring neither architectural\nmodifications nor additional training. To systematically evaluate these\nfeatures, we extend benchmarks with vision-language grounding tasks spanning\nboth images and videos. Our key insight is that stop words act as attention\nmagnets: they accumulate surplus attention and can be filtered to reduce noise.\nMoreover, we identify global attention sinks (GAS) emerging in deeper layers\nand show that they can be safely suppressed or redirected onto auxiliary\ntokens, leading to sharper and more accurate grounding maps. We further propose\nan attention redistribution strategy, where appended stop words partition\nbackground activations into smaller clusters, yielding sharper and more\nlocalized heatmaps. Building on these findings, we develop RefAM, a simple\ntraining-free grounding framework that combines cross-attention maps, GAS\nhandling, and redistribution. Across zero-shot referring image and video\nsegmentation benchmarks, our approach consistently outperforms prior methods,\nestablishing a new state of the art without fine-tuning or additional\ncomponents.",
    "published": "2025-09-26T17:59:57Z",
    "updated": "2025-09-26T17:59:57Z",
    "authors": [
      "Anna Kukleva",
      "Enis Simsar",
      "Alessio Tonioni",
      "Muhammad Ferjad Naeem",
      "Federico Tombari",
      "Jan Eric Lenssen",
      "Bernt Schiele"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05583v1",
    "title": "When Does Global Attention Help? A Unified Empirical Study on Atomistic\n  Graph Learning",
    "summary": "Graph neural networks (GNNs) are widely used as surrogates for costly\nexperiments and first-principles simulations to study the behavior of compounds\nat atomistic scale, and their architectural complexity is constantly increasing\nto enable the modeling of complex physics. While most recent GNNs combine more\ntraditional message passing neural networks (MPNNs) layers to model short-range\ninteractions with more advanced graph transformers (GTs) with global attention\nmechanisms to model long-range interactions, it is still unclear when global\nattention mechanisms provide real benefits over well-tuned MPNN layers due to\ninconsistent implementations, features, or hyperparameter tuning. We introduce\nthe first unified, reproducible benchmarking framework - built on HydraGNN -\nthat enables seamless switching among four controlled model classes: MPNN, MPNN\nwith chemistry/topology encoders, GPS-style hybrids of MPNN with global\nattention, and fully fused local - global models with encoders. Using seven\ndiverse open-source datasets for benchmarking across regression and\nclassification tasks, we systematically isolate the contributions of message\npassing, global attention, and encoder-based feature augmentation. Our study\nshows that encoder-augmented MPNNs form a robust baseline, while fused\nlocal-global models yield the clearest benefits for properties governed by\nlong-range interaction effects. We further quantify the accuracy - compute\ntrade-offs of attention, reporting its overhead in memory. Together, these\nresults establish the first controlled evaluation of global attention in\natomistic graph learning and provide a reproducible testbed for future model\ndevelopment.",
    "published": "2025-10-07T05:01:19Z",
    "updated": "2025-10-07T05:01:19Z",
    "authors": [
      "Arindam Chowdhury",
      "Massimiliano Lupo Pasini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.09958v1",
    "title": "Toward Transformer-Based Object Detection",
    "summary": "Transformers have become the dominant model in natural language processing,\nowing to their ability to pretrain on massive amounts of data, then transfer to\nsmaller, more specific tasks via fine-tuning. The Vision Transformer was the\nfirst major attempt to apply a pure transformer model directly to images as\ninput, demonstrating that as compared to convolutional networks,\ntransformer-based architectures can achieve competitive results on benchmark\nclassification tasks. However, the computational complexity of the attention\noperator means that we are limited to low-resolution inputs. For more complex\ntasks such as detection or segmentation, maintaining a high input resolution is\ncrucial to ensure that models can properly identify and reflect fine details in\ntheir output. This naturally raises the question of whether or not\ntransformer-based architectures such as the Vision Transformer are capable of\nperforming tasks other than classification. In this paper, we determine that\nVision Transformers can be used as a backbone by a common detection task head\nto produce competitive COCO results. The model that we propose, ViT-FRCNN,\ndemonstrates several known properties associated with transformers, including\nlarge pretraining capacity and fast fine-tuning performance. We also\ninvestigate improvements over a standard detection backbone, including superior\nperformance on out-of-domain images, better performance on large objects, and a\nlessened reliance on non-maximum suppression. We view ViT-FRCNN as an important\nstepping stone toward a pure-transformer solution of complex vision tasks such\nas object detection.",
    "published": "2020-12-17T22:33:14Z",
    "updated": "2020-12-17T22:33:14Z",
    "authors": [
      "Josh Beal",
      "Eric Kim",
      "Eric Tzeng",
      "Dong Huk Park",
      "Andrew Zhai",
      "Dmitry Kislyuk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.07962v1",
    "title": "An Extendable, Efficient and Effective Transformer-based Object Detector",
    "summary": "Transformers have been widely used in numerous vision problems especially for\nvisual recognition and detection. Detection transformers are the first fully\nend-to-end learning systems for object detection, while vision transformers are\nthe first fully transformer-based architecture for image classification. In\nthis paper, we integrate Vision and Detection Transformers (ViDT) to construct\nan effective and efficient object detector. ViDT introduces a reconfigured\nattention module to extend the recent Swin Transformer to be a standalone\nobject detector, followed by a computationally efficient transformer decoder\nthat exploits multi-scale features and auxiliary techniques essential to boost\nthe detection performance without much increase in computational load. In\naddition, we extend it to ViDT+ to support joint-task learning for object\ndetection and instance segmentation. Specifically, we attach an efficient\nmulti-scale feature fusion layer and utilize two more auxiliary training\nlosses, IoU-aware loss and token labeling loss. Extensive evaluation results on\nthe Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP\nand latency trade-off among existing fully transformer-based object detectors,\nand its extended ViDT+ achieves 53.2AP owing to its high scalability for large\nmodels. The source code and trained models are available at\nhttps://github.com/naver-ai/vidt.",
    "published": "2022-04-17T09:27:45Z",
    "updated": "2022-04-17T09:27:45Z",
    "authors": [
      "Hwanjun Song",
      "Deqing Sun",
      "Sanghyuk Chun",
      "Varun Jampani",
      "Dongyoon Han",
      "Byeongho Heo",
      "Wonjae Kim",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.11816v2",
    "title": "Incorporating Convolution Designs into Visual Transformers",
    "summary": "Motivated by the success of Transformers in natural language processing (NLP)\ntasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to\nthe vision domain. However, pure Transformer architectures often require a\nlarge amount of training data or extra supervision to obtain comparable\nperformance with convolutional neural networks (CNNs). To overcome these\nlimitations, we analyze the potential drawbacks when directly borrowing\nTransformer architectures from NLP. Then we propose a new\n\\textbf{Convolution-enhanced image Transformer (CeiT)} which combines the\nadvantages of CNNs in extracting low-level features, strengthening locality,\nand the advantages of Transformers in establishing long-range dependencies.\nThree modifications are made to the original Transformer: \\textbf{1)} instead\nof the straightforward tokenization from raw input images, we design an\n\\textbf{Image-to-Tokens (I2T)} module that extracts patches from generated\nlow-level features; \\textbf{2)} the feed-froward network in each encoder block\nis replaced with a \\textbf{Locally-enhanced Feed-Forward (LeFF)} layer that\npromotes the correlation among neighboring tokens in the spatial dimension;\n\\textbf{3)} a \\textbf{Layer-wise Class token Attention (LCA)} is attached at\nthe top of the Transformer that utilizes the multi-level representations.\n  Experimental results on ImageNet and seven downstream tasks show the\neffectiveness and generalization ability of CeiT compared with previous\nTransformers and state-of-the-art CNNs, without requiring a large amount of\ntraining data and extra CNN teachers. Besides, CeiT models also demonstrate\nbetter convergence with $3\\times$ fewer training iterations, which can reduce\nthe training cost significantly\\footnote{Code and models will be released upon\nacceptance.}.",
    "published": "2021-03-22T13:16:12Z",
    "updated": "2021-04-20T11:03:32Z",
    "authors": [
      "Kun Yuan",
      "Shaopeng Guo",
      "Ziwei Liu",
      "Aojun Zhou",
      "Fengwei Yu",
      "Wei Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1906.00258v2",
    "title": "Enhancing Transformation-based Defenses using a Distribution Classifier",
    "summary": "Adversarial attacks on convolutional neural networks (CNN) have gained\nsignificant attention and there have been active research efforts on defense\nmechanisms. Stochastic input transformation methods have been proposed, where\nthe idea is to recover the image from adversarial attack by random\ntransformation, and to take the majority vote as consensus among the random\nsamples. However, the transformation improves the accuracy on adversarial\nimages at the expense of the accuracy on clean images. While it is intuitive\nthat the accuracy on clean images would deteriorate, the exact mechanism in\nwhich how this occurs is unclear. In this paper, we study the distribution of\nsoftmax induced by stochastic transformations. We observe that with random\ntransformations on the clean images, although the mass of the softmax\ndistribution could shift to the wrong class, the resulting distribution of\nsoftmax could be used to correct the prediction. Furthermore, on the\nadversarial counterparts, with the image transformation, the resulting shapes\nof the distribution of softmax are similar to the distributions from the clean\nimages. With these observations, we propose a method to improve existing\ntransformation-based defenses. We train a separate lightweight distribution\nclassifier to recognize distinct features in the distributions of softmax\noutputs of transformed images. Our empirical studies show that our distribution\nclassifier, by training on distributions obtained from clean images only,\noutperforms majority voting for both clean and adversarial images. Our method\nis generic and can be integrated with existing transformation-based defenses.",
    "published": "2019-06-01T16:59:17Z",
    "updated": "2020-01-30T05:34:00Z",
    "authors": [
      "Connie Kou",
      "Hwee Kuan Lee",
      "Ee-Chien Chang",
      "Teck Khim Ng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.06716v1",
    "title": "DS-TransUNet:Dual Swin Transformer U-Net for Medical Image Segmentation",
    "summary": "Automatic medical image segmentation has made great progress benefit from the\ndevelopment of deep learning. However, most existing methods are based on\nconvolutional neural networks (CNNs), which fail to build long-range\ndependencies and global context connections due to the limitation of receptive\nfield in convolution operation. Inspired by the success of Transformer in\nmodeling the long-range contextual information, some researchers have expended\nconsiderable efforts in designing the robust variants of Transformer-based\nU-Net. Moreover, the patch division used in vision transformers usually ignores\nthe pixel-level intrinsic structural features inside each patch. To alleviate\nthese problems, we propose a novel deep medical image segmentation framework\ncalled Dual Swin Transformer U-Net (DS-TransUNet), which might be the first\nattempt to concurrently incorporate the advantages of hierarchical Swin\nTransformer into both encoder and decoder of the standard U-shaped architecture\nto enhance the semantic segmentation quality of varying medical images. Unlike\nmany prior Transformer-based solutions, the proposed DS-TransUNet first adopts\ndual-scale encoder subnetworks based on Swin Transformer to extract the coarse\nand fine-grained feature representations of different semantic scales. As the\ncore component for our DS-TransUNet, a well-designed Transformer Interactive\nFusion (TIF) module is proposed to effectively establish global dependencies\nbetween features of different scales through the self-attention mechanism.\nFurthermore, we also introduce the Swin Transformer block into decoder to\nfurther explore the long-range contextual information during the up-sampling\nprocess. Extensive experiments across four typical tasks for medical image\nsegmentation demonstrate the effectiveness of DS-TransUNet, and show that our\napproach significantly outperforms the state-of-the-art methods.",
    "published": "2021-06-12T08:37:17Z",
    "updated": "2021-06-12T08:37:17Z",
    "authors": [
      "Ailiang Lin",
      "Bingzhi Chen",
      "Jiayu Xu",
      "Zheng Zhang",
      "Guangming Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.14949v1",
    "title": "HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling",
    "summary": "Recently, masked image modeling (MIM) has offered a new methodology of\nself-supervised pre-training of vision transformers. A key idea of efficient\nimplementation is to discard the masked image patches (or tokens) throughout\nthe target network (encoder), which requires the encoder to be a plain vision\ntransformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin\nTransformer) have potentially better properties in formulating vision inputs.\nIn this paper, we offer a new design of hierarchical vision transformers named\nHiViT (short for Hierarchical ViT) that enjoys both high efficiency and good\nperformance in MIM. The key is to remove the unnecessary \"local inter-unit\noperations\", deriving structurally simple hierarchical vision transformers in\nwhich mask-units can be serialized like plain vision transformers. For this\npurpose, we start with Swin Transformer and (i) set the masking unit size to be\nthe token size in the main stage of Swin Transformer, (ii) switch off\ninter-unit self-attentions before the main stage, and (iii) eliminate all\noperations after the main stage. Empirical studies demonstrate the advantageous\nperformance of HiViT in terms of fully-supervised, self-supervised, and\ntransfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B\nreports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over\nSwin-B, and the performance gain generalizes to downstream tasks of detection\nand segmentation. Code will be made publicly available.",
    "published": "2022-05-30T09:34:44Z",
    "updated": "2022-05-30T09:34:44Z",
    "authors": [
      "Xiaosong Zhang",
      "Yunjie Tian",
      "Wei Huang",
      "Qixiang Ye",
      "Qi Dai",
      "Lingxi Xie",
      "Qi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.21635v1",
    "title": "MART: MultiscAle Relational Transformer Networks for Multi-agent\n  Trajectory Prediction",
    "summary": "Multi-agent trajectory prediction is crucial to autonomous driving and\nunderstanding the surrounding environment. Learning-based approaches for\nmulti-agent trajectory prediction, such as primarily relying on graph neural\nnetworks, graph transformers, and hypergraph neural networks, have demonstrated\noutstanding performance on real-world datasets in recent years. However, the\nhypergraph transformer-based method for trajectory prediction is yet to be\nexplored. Therefore, we present a MultiscAle Relational Transformer (MART)\nnetwork for multi-agent trajectory prediction. MART is a hypergraph transformer\narchitecture to consider individual and group behaviors in transformer\nmachinery. The core module of MART is the encoder, which comprises a Pair-wise\nRelational Transformer (PRT) and a Hyper Relational Transformer (HRT). The\nencoder extends the capabilities of a relational transformer by introducing\nHRT, which integrates hyperedge features into the transformer mechanism,\npromoting attention weights to focus on group-wise relations. In addition, we\npropose an Adaptive Group Estimator (AGE) designed to infer complex group\nrelations in real-world environments. Extensive experiments on three real-world\ndatasets (NBA, SDD, and ETH-UCY) demonstrate that our method achieves\nstate-of-the-art performance, enhancing ADE/FDE by 3.9%/11.8% on the NBA\ndataset. Code is available at https://github.com/gist-ailab/MART.",
    "published": "2024-07-31T14:31:49Z",
    "updated": "2024-07-31T14:31:49Z",
    "authors": [
      "Seongju Lee",
      "Junseok Lee",
      "Yeonguk Yu",
      "Taeri Kim",
      "Kyoobin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18948v1",
    "title": "Exact Expressive Power of Transformers with Padding",
    "summary": "Chain of thought is a natural inference-time method for increasing the\ncomputational power of transformer-based large language models (LLMs), but\ncomes at the cost of sequential decoding. Are there more efficient alternatives\nto expand a transformer's expressive power without adding parameters? We\nconsider transformers with padding tokens as a form of parallelizable test-time\ncompute. We show that averaging-hard-attention, masked-pre-norm transformers\nwith polynomial padding converge to precisely the class $\\mathsf{TC}^0$ of\nextremely parallelizable problems. While the $\\mathsf{TC}^0$ upper bound was\nknown, proving a matching lower bound had been elusive. Further, our novel\nanalysis reveals the precise expanded power of padded transformers when coupled\nwith another form of inference-time compute, namely dynamically increasing\ndepth via looping. Our core technical contribution is to show how padding helps\nbring the notions of complete problems and reductions, which have been a\ncornerstone of classical complexity theory, to the formal study of\ntransformers. Armed with this new tool, we prove that padded transformers with\n$O(\\log^d n)$ looping on inputs of length $n$ recognize exactly the class\n$\\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and\nlooping together systematically expand transformers' expressive power: with\npolylogarithmic looping, padded transformers converge to the class\n$\\mathsf{NC}$, the best that could be expected without losing parallelism\n(unless $\\mathsf{NC} = \\mathsf{P}$). Our results thus motivate further\nexploration of padding and looping as parallelizable alternatives to chain of\nthought.",
    "published": "2025-05-25T02:52:15Z",
    "updated": "2025-05-25T02:52:15Z",
    "authors": [
      "William Merrill",
      "Ashish Sabharwal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20098v2",
    "title": "Transformers in Protein: A Survey",
    "summary": "As protein informatics advances rapidly, the demand for enhanced predictive\naccuracy, structural analysis, and functional understanding has intensified.\nTransformer models, as powerful deep learning architectures, have demonstrated\nunprecedented potential in addressing diverse challenges across protein\nresearch. However, a comprehensive review of Transformer applications in this\nfield remains lacking. This paper bridges this gap by surveying over 100\nstudies, offering an in-depth analysis of practical implementations and\nresearch progress of Transformers in protein-related tasks. Our review\nsystematically covers critical domains, including protein structure prediction,\nfunction prediction, protein-protein interaction analysis, functional\nannotation, and drug discovery/target identification. To contextualize these\nadvancements across various protein domains, we adopt a domain-oriented\nclassification system. We first introduce foundational concepts: the\nTransformer architecture and attention mechanisms, categorize Transformer\nvariants tailored for protein science, and summarize essential protein\nknowledge. For each research domain, we outline its objectives and background,\ncritically evaluate prior methods and their limitations, and highlight\ntransformative contributions enabled by Transformer models. We also curate and\nsummarize pivotal datasets and open-source code resources to facilitate\nreproducibility and benchmarking. Finally, we discuss persistent challenges in\napplying Transformers to protein informatics and propose future research\ndirections. This review aims to provide a consolidated foundation for the\nsynergistic integration of Transformer and protein informatics, fostering\nfurther innovation and expanded applications in the field.",
    "published": "2025-05-26T15:08:18Z",
    "updated": "2025-05-27T10:44:08Z",
    "authors": [
      "Xiaowen Ling",
      "Zhiqiang Li",
      "Yanbin Wang",
      "Zhuhong You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.16380v4",
    "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in\n  1-layer Transformer",
    "summary": "Transformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).",
    "published": "2023-05-25T15:59:13Z",
    "updated": "2023-10-30T17:32:08Z",
    "authors": [
      "Yuandong Tian",
      "Yiping Wang",
      "Beidi Chen",
      "Simon Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.13315v2",
    "title": "Gated Rotary-Enhanced Linear Attention for Long-term Sequential\n  Recommendation",
    "summary": "In Sequential Recommendation Systems (SRSs), Transformer models have\ndemonstrated remarkable performance but face computational and memory cost\nchallenges, especially when modeling long-term user behavior sequences. Due to\nits quadratic complexity, the dot-product attention mechanism in Transformers\nbecomes expensive for processing long sequences. By approximating the\ndot-product attention using elaborate mapping functions, linear attention\nprovides a more efficient option with linear complexity. However, existing\nlinear attention methods face three limitations: 1) they often use learnable\nposition encodings, which incur extra computational costs in long-term sequence\nscenarios, 2) they may not sufficiently account for user's fine-grained local\npreferences (short-lived burst of interest), and 3) they try to capture some\ntemporary activities, but often confuse these with stable and long-term\ninterests. This can result in unclear or less effective recommendations. To\nremedy these drawbacks, we propose a long-term sequential Recommendation model\nwith Gated Rotary Enhanced Linear Attention (RecGRELA). Specifically, we first\npropose a Rotary-Enhanced Linear Attention (RELA) module to efficiently model\nlong-range dependency within the user's historical information using rotary\nposition encodings. Then, we introduce a local short operation to add the local\npreferences of interactions and show the theoretical insight. We further\nintroduce a SiLU-based Gated mechanism for RELA (GRELA) to help the model tell\nif a user behavior shows a short-term, local interest or a real change in their\nlong-term tastes. Experimental results on four public benchmark datasets show\nthat our RecGRELA achieves state-of-the-art performance compared with existing\nSRSs based on Recurrent Neural Networks, Transformer, and Mamba while keeping\nlow memory overhead.",
    "published": "2025-06-16T09:56:10Z",
    "updated": "2025-11-02T13:29:58Z",
    "authors": [
      "Juntao Hu",
      "Wei Zhou",
      "Huayi Shen",
      "Xiao Du",
      "Jie Liao",
      "Min Gao",
      "Jun Zeng",
      "Junhao Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.08829v2",
    "title": "Learning Spatial-Frequency Transformer for Visual Object Tracking",
    "summary": "Recent trackers adopt the Transformer to combine or replace the widely used\nResNet as their new backbone network. Although their trackers work well in\nregular scenarios, however, they simply flatten the 2D features into a sequence\nto better match the Transformer. We believe these operations ignore the spatial\nprior of the target object which may lead to sub-optimal results only. In\naddition, many works demonstrate that self-attention is actually a low-pass\nfilter, which is independent of input features or key/queries. That is to say,\nit may suppress the high-frequency component of the input features and preserve\nor even amplify the low-frequency information. To handle these issues, in this\npaper, we propose a unified Spatial-Frequency Transformer that models the\nGaussian spatial Prior and High-frequency emphasis Attention (GPHA)\nsimultaneously. To be specific, Gaussian spatial prior is generated using dual\nMulti-Layer Perceptrons (MLPs) and injected into the similarity matrix produced\nby multiplying Query and Key features in self-attention. The output will be fed\ninto a Softmax layer and then decomposed into two components, i.e., the direct\nsignal and high-frequency signal. The low- and high-pass branches are rescaled\nand combined to achieve all-pass, therefore, the high-frequency features will\nbe protected well in stacked self-attention layers. We further integrate the\nSpatial-Frequency Transformer into the Siamese tracking framework and propose a\nnovel tracking algorithm, termed SFTransT. The cross-scale fusion based\nSwinTransformer is adopted as the backbone, and also a multi-head\ncross-attention module is used to boost the interaction between search and\ntemplate features. The output will be fed into the tracking head for target\nlocalization. Extensive experiments on both short-term and long-term tracking\nbenchmarks all demonstrate the effectiveness of our proposed framework.",
    "published": "2022-08-18T13:46:12Z",
    "updated": "2023-03-09T10:05:38Z",
    "authors": [
      "Chuanming Tang",
      "Xiao Wang",
      "Yuanchao Bai",
      "Zhe Wu",
      "Jianlin Zhang",
      "Yongmei Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.01572v2",
    "title": "FedTP: Federated Learning by Transformer Personalization",
    "summary": "Federated learning is an emerging learning paradigm where multiple clients\ncollaboratively train a machine learning model in a privacy-preserving manner.\nPersonalized federated learning extends this paradigm to overcome heterogeneity\nacross clients by learning personalized models. Recently, there have been some\ninitial attempts to apply Transformers to federated learning. However, the\nimpacts of federated learning algorithms on self-attention have not yet been\nstudied. This paper investigates this relationship and reveals that federated\naveraging algorithms actually have a negative impact on self-attention where\nthere is data heterogeneity. These impacts limit the capabilities of the\nTransformer model in federated learning settings. Based on this, we propose\nFedTP, a novel Transformer-based federated learning framework that learns\npersonalized self-attention for each client while aggregating the other\nparameters among the clients. Instead of using a vanilla personalization\nmechanism that maintains personalized self-attention layers of each client\nlocally, we develop a learn-to-personalize mechanism to further encourage the\ncooperation among clients and to increase the scablability and generalization\nof FedTP. Specifically, the learn-to-personalize is realized by learning a\nhypernetwork on the server that outputs the personalized projection matrices of\nself-attention layers to generate client-wise queries, keys and values.\nFurthermore, we present the generalization bound for FedTP with the\nlearn-to-personalize mechanism. Notably, FedTP offers a convenient environment\nfor performing a range of image and language tasks using the same federated\nnetwork architecture - all of which benefit from Transformer personalization.\nExtensive experiments verify that FedTP with the learn-to-personalize mechanism\nyields state-of-the-art performance in non-IID scenarios. Our code is available\nonline.",
    "published": "2022-11-03T03:42:11Z",
    "updated": "2023-04-18T07:40:02Z",
    "authors": [
      "Hongxia Li",
      "Zhongyi Cai",
      "Jingya Wang",
      "Jiangnan Tang",
      "Weiping Ding",
      "Chin-Teng Lin",
      "Ye Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.06115v1",
    "title": "VTPNet for 3D deep learning on point cloud",
    "summary": "Recently, Transformer-based methods for point cloud learning have achieved\ngood results on various point cloud learning benchmarks. However, since the\nattention mechanism needs to generate three feature vectors of query, key, and\nvalue to calculate attention features, most of the existing Transformer-based\npoint cloud learning methods usually consume a large amount of computational\ntime and memory resources when calculating global attention. To address this\nproblem, we propose a Voxel-Transformer-Point (VTP) Block for extracting local\nand global features of point clouds. VTP combines the advantages of\nvoxel-based, point-based and Transformer-based methods, which consists of\nVoxel-Based Branch (V branch), Point-Based Transformer Branch (PT branch) and\nPoint-Based Branch (P branch). The V branch extracts the coarse-grained\nfeatures of the point cloud through low voxel resolution; the PT branch obtains\nthe fine-grained features of the point cloud by calculating the self-attention\nin the local neighborhood and the inter-neighborhood cross-attention; the P\nbranch uses a simplified MLP network to generate the global location\ninformation of the point cloud. In addition, to enrich the local features of\npoint clouds at different scales, we set the voxel scale in the V branch and\nthe neighborhood sphere scale in the PT branch to one large and one small\n(large voxel scale \\& small neighborhood sphere scale or small voxel scale \\&\nlarge neighborhood sphere scale). Finally, we use VTP as the feature extraction\nnetwork to construct a VTPNet for point cloud learning, and performs shape\nclassification, part segmentation, and semantic segmentation tasks on the\nModelNet40, ShapeNet Part, and S3DIS datasets. The experimental results\nindicate that VTPNet has good performance in 3D point cloud learning.",
    "published": "2023-05-10T13:07:46Z",
    "updated": "2023-05-10T13:07:46Z",
    "authors": [
      "Wei Zhou",
      "Weiwei Jin",
      "Qian Wang",
      "Yifan Wang",
      "Dekui Wang",
      "Xingxing Hao",
      "Yongxiang Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.22743v1",
    "title": "ConMatFormer: A Multi-attention and Transformer Integrated ConvNext\n  based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification",
    "summary": "Diabetic foot ulcer (DFU) detection is a clinically significant yet\nchallenging task due to the scarcity and variability of publicly available\ndatasets. To solve these problems, we propose ConMatFormer, a new hybrid deep\nlearning architecture that combines ConvNeXt blocks, multiple attention\nmechanisms convolutional block attention module (CBAM) and dual attention\nnetwork (DANet), and transformer modules in a way that works together. This\ndesign facilitates the extraction of better local features and understanding of\nthe global context, which allows us to model small skin patterns across\ndifferent types of DFU very accurately. To address the class imbalance, we used\ndata augmentation methods. A ConvNeXt block was used to obtain detailed local\nfeatures in the initial stages. Subsequently, we compiled the model by adding a\ntransformer module to enhance long-range dependency. This enabled us to\npinpoint the DFU classes that were underrepresented or constituted minorities.\nTests on the DS1 (DFUC2021) and DS2 (diabetic foot ulcer (DFU)) datasets showed\nthat ConMatFormer outperformed state-of-the-art (SOTA) convolutional neural\nnetwork (CNN) and Vision Transformer (ViT) models in terms of accuracy,\nreliability, and flexibility. The proposed method achieved an accuracy of\n0.8961 and a precision of 0.9160 in a single experiment, which is a significant\nimprovement over the current standards for classifying DFUs. In addition, by\n4-fold cross-validation, the proposed model achieved an accuracy of 0.9755 with\na standard deviation of only 0.0031. We further applied explainable artificial\nintelligence (XAI) methods, such as Grad-CAM, Grad-CAM++, and LIME, to\nconsistently monitor the transparency and trustworthiness of the\ndecision-making process.. Our findings set a new benchmark for DFU\nclassification and provide a hybrid attention transformer framework for medical\nimage analysis.",
    "published": "2025-10-26T16:34:43Z",
    "updated": "2025-10-26T16:34:43Z",
    "authors": [
      "Raihan Ahamed Rifat",
      "Fuyad Hasan Bhoyan",
      "Md Humaion Kabir Mehedi",
      "Md Kaviul Hossain",
      "Md. Jakir Hossen",
      "M. F. Mridha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1808.05578v1",
    "title": "LARNN: Linear Attention Recurrent Neural Network",
    "summary": "The Linear Attention Recurrent Neural Network (LARNN) is a recurrent\nattention module derived from the Long Short-Term Memory (LSTM) cell and ideas\nfrom the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The\nLARNN uses attention on its past cell state values for a limited window size\n$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)\ncell and the Transformer Network for its Multi-Head Attention Mechanism. The\nMulti-Head Attention Mechanism is used inside the cell such that it can query\nits own $k$ past values with the attention window. This has the effect of\naugmenting the rank of the tensor with the attention mechanism, such that the\ncell can perform complex queries to question its previous inner memories, which\nshould augment the long short-term effect of the memory. With a clever trick,\nthe LARNN cell with attention can be easily used inside a loop on the cell\nstate, just like how any other Recurrent Neural Network (RNN) cell can be\nlooped linearly through time series. This is due to the fact that its state,\nwhich is looped upon throughout time steps within time series, stores the inner\nstates in a \"first in, first out\" queue which contains the $k$ most recent\nstates and on which it is easily possible to add static positional encoding\nwhen the queue is represented as a tensor. This neural architecture yields\nbetter results than the vanilla LSTM cells. It can obtain results of 91.92% for\nthe test accuracy, compared to the previously attained 91.65% using vanilla\nLSTM cells. Note that this is not to compare to other research, where up to\n93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3\ncells as analyzed here. Finally, an interesting discovery is made, such that\nadding activation within the multi-head attention mechanism's linear layers can\nyield better results in the context researched hereto.",
    "published": "2018-08-16T16:48:56Z",
    "updated": "2018-08-16T16:48:56Z",
    "authors": [
      "Guillaume Chevalier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14840v1",
    "title": "Subquadratic Algorithms and Hardness for Attention with Any Temperature",
    "summary": "Despite the popularity of the Transformer architecture, the standard\nalgorithm for computing Attention suffers from quadratic time complexity in\ncontext length $n$. Alman and Song [NeurIPS 2023] showed that when the head\ndimension $d = \\Theta(\\log n)$, subquadratic Attention is possible if and only\nif the inputs have small entries bounded by $B = o(\\sqrt{\\log n})$ in absolute\nvalues, under the Strong Exponential Time Hypothesis ($\\mathsf{SETH}$).\nEquivalently, subquadratic Attention is possible if and only if the softmax is\napplied with high temperature for $d=\\Theta(\\log n)$. Running times of these\nalgorithms depend exponentially on $B$ and thus they do not lead to even a\npolynomial-time algorithm outside the specific range of $B$.\n  This naturally leads to the question: when can Attention be computed\nefficiently without strong assumptions on temperature? Are there fast attention\nalgorithms that scale polylogarithmically with entry size $B$? In this work, we\nresolve this question and characterize when fast Attention for arbitrary\ntemperatures is possible. First, for all constant $d = O(1)$, we give the first\nsubquadratic $\\tilde{O}(n^{2 - 1/d} \\cdot \\mathrm{polylog}(B))$ time algorithm\nfor Attention with large $B$. Our result holds even for matrices with large\nhead dimension if they have low rank. In this regime, we also give a similar\nrunning time for Attention gradient computation, and therefore for the full LLM\ntraining process. Furthermore, we show that any substantial improvement on our\nalgorithm is unlikely. In particular, we show that even when $d =\n2^{\\Theta(\\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under\n$\\mathsf{SETH}$.\n  Finally, in the regime where $d = \\mathrm{poly}(n)$, we show that the\nstandard algorithm is optimal under popular fine-grained complexity\nassumptions.",
    "published": "2025-05-20T19:12:43Z",
    "updated": "2025-05-20T19:12:43Z",
    "authors": [
      "Shreya Gupta",
      "Boyang Huang",
      "Barna Saha",
      "Yinzhan Xu",
      "Christopher Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.11689v1",
    "title": "Encoding Syntactic Knowledge in Transformer Encoder for Intent Detection\n  and Slot Filling",
    "summary": "We propose a novel Transformer encoder-based architecture with syntactical\nknowledge encoded for intent detection and slot filling. Specifically, we\nencode syntactic knowledge into the Transformer encoder by jointly training it\nto predict syntactic parse ancestors and part-of-speech of each token via\nmulti-task learning. Our model is based on self-attention and feed-forward\nlayers and does not require external syntactic information to be available at\ninference time. Experiments show that on two benchmark datasets, our models\nwith only two Transformer encoder layers achieve state-of-the-art results.\nCompared to the previously best performed model without pre-training, our\nmodels achieve absolute F1 score and accuracy improvement of 1.59% and 0.85%\nfor slot filling and intent detection on the SNIPS dataset, respectively. Our\nmodels also achieve absolute F1 score and accuracy improvement of 0.1% and\n0.34% for slot filling and intent detection on the ATIS dataset, respectively,\nover the previously best performed model. Furthermore, the visualization of the\nself-attention weights illustrates the benefits of incorporating syntactic\ninformation during training.",
    "published": "2020-12-21T21:25:11Z",
    "updated": "2020-12-21T21:25:11Z",
    "authors": [
      "Jixuan Wang",
      "Kai Wei",
      "Martin Radfar",
      "Weiwei Zhang",
      "Clement Chung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.00301v3",
    "title": "$\\infty$-former: Infinite Memory Transformer",
    "summary": "Transformers are unable to model long-term memories effectively, since the\namount of computation they need to perform grows with the context length. While\nvariations of efficient transformers have been proposed, they all have a finite\nmemory capacity and are forced to drop old information. In this paper, we\npropose the $\\infty$-former, which extends the vanilla transformer with an\nunbounded long-term memory. By making use of a continuous-space attention\nmechanism to attend over the long-term memory, the $\\infty$-former's attention\ncomplexity becomes independent of the context length, trading off memory length\nwith precision. In order to control where precision is more important,\n$\\infty$-former maintains \"sticky memories\" being able to model arbitrarily\nlong contexts while keeping the computation budget fixed. Experiments on a\nsynthetic sorting task, language modeling, and document grounded dialogue\ngeneration demonstrate the $\\infty$-former's ability to retain information from\nlong sequences.",
    "published": "2021-09-01T10:51:58Z",
    "updated": "2022-03-25T10:37:54Z",
    "authors": [
      "Pedro Henrique Martins",
      "Zita Marinho",
      "AndrÃ© F. T. Martins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.02860v4",
    "title": "Hierarchical Graph Convolutional Skeleton Transformer for Action\n  Recognition",
    "summary": "Graph convolutional networks (GCNs) have emerged as dominant methods for\nskeleton-based action recognition.\n  However, they still suffer from two problems, namely, neighborhood\nconstraints and entangled spatiotemporal feature representations.\n  Most studies have focused on improving the design of graph topology to solve\nthe first problem but they have yet to fully explore the latter.\n  In this work, we design a disentangled spatiotemporal transformer (DSTT)\nblock to overcome the above limitations of GCNs in three steps: (i) feature\ndisentanglement for spatiotemporal decomposition;(ii) global spatiotemporal\nattention for capturing correlations in the global context; and (iii) local\ninformation enhancement for utilizing more local information.\n  Thereon, we propose a novel architecture, named Hierarchical Graph\nConvolutional skeleton Transformer (HGCT), to employ the complementary\nadvantages of GCN (i.e., local topology, temporal dynamics and hierarchy) and\nTransformer (i.e., global context and dynamic attention).\n  HGCT is lightweight and computationally efficient.\n  Quantitative analysis demonstrates the superiority and good interpretability\nof HGCT.",
    "published": "2021-09-07T04:32:10Z",
    "updated": "2022-01-10T11:02:07Z",
    "authors": [
      "Ruwen Bai",
      "Min Li",
      "Bo Meng",
      "Fengfa Li",
      "Miao Jiang",
      "Junxing Ren",
      "Degang Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.11159v2",
    "title": "OH-Former: Omni-Relational High-Order Transformer for Person\n  Re-Identification",
    "summary": "Transformers have shown preferable performance on many vision tasks. However,\nfor the task of person re-identification (ReID), vanilla transformers leave the\nrich contexts on high-order feature relations under-exploited and deteriorate\nlocal feature details, which are insufficient due to the dramatic variations of\npedestrians. In this work, we propose an Omni-Relational High-Order Transformer\n(OH-Former) to model omni-relational features for ReID. First, to strengthen\nthe capacity of visual representation, instead of obtaining the attention\nmatrix based on pairs of queries and isolated keys at each spatial location, we\ntake a step further to model high-order statistics information for the\nnon-local mechanism. We share the attention weights in the corresponding layer\nof each order with a prior mixing mechanism to reduce the computation cost.\nThen, a convolution-based local relation perception module is proposed to\nextract the local relations and 2D position information. The experimental\nresults of our model are superior promising, which show state-of-the-art\nperformance on Market-1501, DukeMTMC, MSMT17 and Occluded-Duke datasets.",
    "published": "2021-09-23T06:11:38Z",
    "updated": "2021-11-24T07:56:32Z",
    "authors": [
      "Xianing Chen",
      "Chunlin Xu",
      "Qiong Cao",
      "Jialang Xu",
      "Yujie Zhong",
      "Jiale Xu",
      "Zhengxin Li",
      "Jingya Wang",
      "Shenghua Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.13925v2",
    "title": "Fine-tuning Vision Transformers for the Prediction of State Variables in\n  Ising Models",
    "summary": "Transformers are state-of-the-art deep learning models that are composed of\nstacked attention and point-wise, fully connected layers designed for handling\nsequential data. Transformers are not only ubiquitous throughout Natural\nLanguage Processing (NLP), but, recently, they have inspired a new wave of\nComputer Vision (CV) applications research. In this work, a Vision Transformer\n(ViT) is applied to predict the state variables of 2-dimensional Ising model\nsimulations. Our experiments show that ViT outperform state-of-the-art\nConvolutional Neural Networks (CNN) when using a small number of microstate\nimages from the Ising model corresponding to various boundary conditions and\ntemperatures. This work opens the possibility of applying ViT to other\nsimulations, and raises interesting research directions on how attention maps\ncan learn about the underlying physics governing different phenomena.",
    "published": "2021-09-28T00:23:31Z",
    "updated": "2021-11-30T04:27:14Z",
    "authors": [
      "Onur Kara",
      "Arijit Sehanobish",
      "Hector H Corzo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.12957v1",
    "title": "Multi-view 3D Reconstruction with Transformer",
    "summary": "Deep CNN-based methods have so far achieved the state of the art results in\nmulti-view 3D object reconstruction. Despite the considerable progress, the two\ncore modules of these methods - multi-view feature extraction and fusion, are\nusually investigated separately, and the object relations in different views\nare rarely explored. In this paper, inspired by the recent great success in\nself-attention-based Transformer models, we reformulate the multi-view 3D\nreconstruction as a sequence-to-sequence prediction problem and propose a new\nframework named 3D Volume Transformer (VolT) for such a task. Unlike previous\nCNN-based methods using a separate design, we unify the feature extraction and\nview fusion in a single Transformer network. A natural advantage of our design\nlies in the exploration of view-to-view relationships using self-attention\namong multiple unordered inputs. On ShapeNet - a large-scale 3D reconstruction\nbenchmark dataset, our method achieves a new state-of-the-art accuracy in\nmulti-view reconstruction with fewer parameters ($70\\%$ less) than other\nCNN-based methods. Experimental results also suggest the strong scaling\ncapability of our method. Our code will be made publicly available.",
    "published": "2021-03-24T03:14:49Z",
    "updated": "2021-03-24T03:14:49Z",
    "authors": [
      "Dan Wang",
      "Xinrui Cui",
      "Xun Chen",
      "Zhengxia Zou",
      "Tianyang Shi",
      "Septimiu Salcudean",
      "Z. Jane Wang",
      "Rabab Ward"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.01069v1",
    "title": "Dual-former: Hybrid Self-attention Transformer for Efficient Image\n  Restoration",
    "summary": "Recently, image restoration transformers have achieved comparable performance\nwith previous state-of-the-art CNNs. However, how to efficiently leverage such\narchitectures remains an open problem. In this work, we present Dual-former\nwhose critical insight is to combine the powerful global modeling ability of\nself-attention modules and the local modeling ability of convolutions in an\noverall architecture. With convolution-based Local Feature Extraction modules\nequipped in the encoder and the decoder, we only adopt a novel Hybrid\nTransformer Block in the latent layer to model the long-distance dependence in\nspatial dimensions and handle the uneven distribution between channels. Such a\ndesign eliminates the substantial computational complexity in previous image\nrestoration transformers and achieves superior performance on multiple image\nrestoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB\ngain over the state-of-the-art MAXIM method on the Indoor dataset for single\nimage dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image\nderaining, it exceeds the SOTA method by 0.1dB PSNR on the average results of\nfive datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses\nthe latest desnowing method on various datasets, with fewer parameters.",
    "published": "2022-10-03T16:39:21Z",
    "updated": "2022-10-03T16:39:21Z",
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Yun Liu",
      "Erkang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.05794v3",
    "title": "Designing Robust Transformers using Robust Kernel Density Estimation",
    "summary": "Recent advances in Transformer architectures have empowered their empirical\nsuccess in a variety of tasks across different domains. However, existing works\nmainly focus on predictive accuracy and computational cost, without considering\nother practical issues, such as robustness to contaminated samples. Recent work\nby Nguyen et al., (2022) has shown that the self-attention mechanism, which is\nthe center of the Transformer architecture, can be viewed as a non-parametric\nestimator based on kernel density estimation (KDE). This motivates us to\nleverage a set of robust kernel density estimation methods for alleviating the\nissue of data contamination. Specifically, we introduce a series of\nself-attention mechanisms that can be incorporated into different Transformer\narchitectures and discuss the special properties of each method. We then\nperform extensive empirical studies on language modeling and image\nclassification tasks. Our methods demonstrate robust performance in multiple\nscenarios while maintaining competitive results on clean datasets.",
    "published": "2022-10-11T21:39:52Z",
    "updated": "2023-11-08T14:50:22Z",
    "authors": [
      "Xing Han",
      "Tongzheng Ren",
      "Tan Minh Nguyen",
      "Khai Nguyen",
      "Joydeep Ghosh",
      "Nhat Ho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.07124v1",
    "title": "RTFormer: Efficient Design for Real-Time Semantic Segmentation with\n  Transformer",
    "summary": "Recently, transformer-based networks have shown impressive results in\nsemantic segmentation. Yet for real-time semantic segmentation, pure CNN-based\napproaches still dominate in this field, due to the time-consuming computation\nmechanism of transformer. We propose RTFormer, an efficient dual-resolution\ntransformer for real-time semantic segmenation, which achieves better trade-off\nbetween performance and efficiency than CNN-based models. To achieve high\ninference efficiency on GPU-like devices, our RTFormer leverages GPU-Friendly\nAttention with linear complexity and discards the multi-head mechanism.\nBesides, we find that cross-resolution attention is more efficient to gather\nglobal context information for high-resolution branch by spreading the high\nlevel knowledge learned from low-resolution branch. Extensive experiments on\nmainstream benchmarks demonstrate the effectiveness of our proposed RTFormer,\nit achieves state-of-the-art on Cityscapes, CamVid and COCOStuff, and shows\npromising results on ADE20K. Code is available at PaddleSeg:\nhttps://github.com/PaddlePaddle/PaddleSeg.",
    "published": "2022-10-13T16:03:53Z",
    "updated": "2022-10-13T16:03:53Z",
    "authors": [
      "Jian Wang",
      "Chenhui Gou",
      "Qiman Wu",
      "Haocheng Feng",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.14714v1",
    "title": "TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early\n  Intent Prediction",
    "summary": "Human intention prediction is a growing area of research where an activity in\na video has to be anticipated by a vision-based system. To this end, the model\ncreates a representation of the past, and subsequently, it produces future\nhypotheses about upcoming scenarios. In this work, we focus on pedestrians'\nearly intention prediction in which, from a current observation of an urban\nscene, the model predicts the future activity of pedestrians that approach the\nstreet. Our method is based on a multi-modal transformer that encodes past\nobservations and produces multiple predictions at different anticipation times.\nMoreover, we propose to learn the attention masks of our transformer-based\nmodel (Temporal Adaptive Mask Transformer) in order to weigh differently\npresent and past temporal dependencies. We investigate our method on several\npublic benchmarks for early intention prediction, improving the prediction\nperformances at different anticipation times compared to the previous works.",
    "published": "2022-10-26T13:47:23Z",
    "updated": "2022-10-26T13:47:23Z",
    "authors": [
      "Nada Osman",
      "Guglielmo Camporese",
      "Lamberto Ballan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1905.08836v1",
    "title": "Sample Efficient Text Summarization Using a Single Pre-Trained\n  Transformer",
    "summary": "Language model (LM) pre-training has resulted in impressive performance and\nsample efficiency on a variety of language understanding tasks. However, it\nremains unclear how to best use pre-trained LMs for generation tasks such as\nabstractive summarization, particularly to enhance sample efficiency. In these\nsequence-to-sequence settings, prior work has experimented with loading\npre-trained weights into the encoder and/or decoder networks, but used\nnon-pre-trained encoder-decoder attention weights. We instead use a pre-trained\ndecoder-only network, where the same Transformer LM both encodes the source and\ngenerates the summary. This ensures that all parameters in the network,\nincluding those governing attention over source states, have been pre-trained\nbefore the fine-tuning step. Experiments on the CNN/Daily Mail dataset show\nthat our pre-trained Transformer LM substantially improves over pre-trained\nTransformer encoder-decoder networks in limited-data settings. For instance, it\nachieves 13.1 ROUGE-2 using only 1% of the training data (~3000 examples),\nwhile pre-trained encoder-decoder models score 2.3 ROUGE-2.",
    "published": "2019-05-21T19:13:16Z",
    "updated": "2019-05-21T19:13:16Z",
    "authors": [
      "Urvashi Khandelwal",
      "Kevin Clark",
      "Dan Jurafsky",
      "Lukasz Kaiser"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.01791v1",
    "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized\n  Identity Prior",
    "summary": "Traditional (unstructured) pruning methods for a Transformer model focus on\nregularizing the individual weights by penalizing them toward zero. In this\nwork, we explore spectral-normalized identity priors (SNIP), a structured\npruning approach that penalizes an entire residual module in a Transformer\nmodel toward an identity mapping. Our method identifies and discards\nunimportant non-linear mappings in the residual connections by applying a\nthresholding operator on the function norm. It is applicable to any structured\nmodule, including a single attention head, an entire attention block, or a\nfeed-forward subnetwork. Furthermore, we introduce spectral normalization to\nstabilize the distribution of the post-activation values of the Transformer\nlayers, further improving the pruning effectiveness of the proposed\nmethodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to\ndemonstrate that SNIP achieves effective pruning results while maintaining\ncomparable performance. Specifically, we improve the performance over the\nstate-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.",
    "published": "2020-10-05T05:40:56Z",
    "updated": "2020-10-05T05:40:56Z",
    "authors": [
      "Zi Lin",
      "Jeremiah Zhe Liu",
      "Zi Yang",
      "Nan Hua",
      "Dan Roth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.13154v2",
    "title": "Attention is All You Need in Speech Separation",
    "summary": "Recurrent Neural Networks (RNNs) have long been the dominant architecture in\nsequence-to-sequence learning. RNNs, however, are inherently sequential models\nthat do not allow parallelization of their computations. Transformers are\nemerging as a natural alternative to standard RNNs, replacing recurrent\ncomputations with a multi-head attention mechanism. In this paper, we propose\nthe SepFormer, a novel RNN-free Transformer-based neural network for speech\nseparation. The SepFormer learns short and long-term dependencies with a\nmulti-scale approach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It\nreaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on\nWSJ0-3mix. The SepFormer inherits the parallelization advantages of\nTransformers and achieves a competitive performance even when downsampling the\nencoded representation by a factor of 8. It is thus significantly faster and it\nis less memory-demanding than the latest speech separation systems with\ncomparable performance.",
    "published": "2020-10-25T16:28:54Z",
    "updated": "2021-03-08T21:24:43Z",
    "authors": [
      "Cem Subakan",
      "Mirco Ravanelli",
      "Samuele Cornell",
      "Mirko Bronzi",
      "Jianyuan Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2202.13517v1",
    "title": "CTformer: Convolution-free Token2Token Dilated Vision Transformer for\n  Low-dose CT Denoising",
    "summary": "Low-dose computed tomography (LDCT) denoising is an important problem in CT\nresearch. Compared to the normal dose CT (NDCT), LDCT images are subjected to\nsevere noise and artifacts. Recently in many studies, vision transformers have\nshown superior feature representation ability over convolutional neural\nnetworks (CNNs). However, unlike CNNs, the potential of vision transformers in\nLDCT denoising was little explored so far. To fill this gap, we propose a\nConvolution-free Token2Token Dilated Vision Transformer for low-dose CT\ndenoising. The CTformer uses a more powerful token rearrangement to encompass\nlocal contextual information and thus avoids convolution. It also dilates and\nshifts feature maps to capture longer-range interaction. We interpret the\nCTformer by statically inspecting patterns of its internal attention maps and\ndynamically tracing the hierarchical attention flow with an explanatory graph.\nFurthermore, an overlapped inference mechanism is introduced to effectively\neliminate the boundary artifacts that are common for encoder-decoder-based\ndenoising models. Experimental results on Mayo LDCT dataset suggest that the\nCTformer outperforms the state-of-the-art denoising methods with a low\ncomputation overhead.",
    "published": "2022-02-28T02:58:16Z",
    "updated": "2022-02-28T02:58:16Z",
    "authors": [
      "Dayang Wang",
      "Fenglei Fan",
      "Zhan Wu",
      "Rui Liu",
      "Fei Wang",
      "Hengyong Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.13689v2",
    "title": "Tiny-Sepformer: A Tiny Time-Domain Transformer Network for Speech\n  Separation",
    "summary": "Time-domain Transformer neural networks have proven their superiority in\nspeech separation tasks. However, these models usually have a large number of\nnetwork parameters, thus often encountering the problem of GPU memory\nexplosion. In this paper, we proposed Tiny-Sepformer, a tiny version of\nTransformer network for speech separation. We present two techniques to reduce\nthe model parameters and memory consumption: (1) Convolution-Attention (CA)\nblock, spliting the vanilla Transformer to two paths, multi-head attention and\n1D depthwise separable convolution, (2) parameter sharing, sharing the layer\nparameters within the CA block. In our experiments, Tiny-Sepformer could\ngreatly reduce the model size, and achieves comparable separation performance\nwith vanilla Sepformer on WSJ0-2/3Mix datasets.",
    "published": "2022-06-28T01:46:37Z",
    "updated": "2022-06-30T08:33:44Z",
    "authors": [
      "Jian Luo",
      "Jianzong Wang",
      "Ning Cheng",
      "Edward Xiao",
      "Xulong Zhang",
      "Jing Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.11039v2",
    "title": "Flat Multi-modal Interaction Transformer for Named Entity Recognition",
    "summary": "Multi-modal named entity recognition (MNER) aims at identifying entity spans\nand recognizing their categories in social media posts with the aid of images.\nHowever, in dominant MNER approaches, the interaction of different modalities\nis usually carried out through the alternation of self-attention and\ncross-attention or over-reliance on the gating machine, which results in\nimprecise and biased correspondence between fine-grained semantic units of text\nand image. To address this issue, we propose a Flat Multi-modal Interaction\nTransformer (FMIT) for MNER. Specifically, we first utilize noun phrases in\nsentences and general domain words to obtain visual cues. Then, we transform\nthe fine-grained semantic representation of the vision and text into a unified\nlattice structure and design a novel relative position encoding to match\ndifferent modalities in Transformer. Meanwhile, we propose to leverage entity\nboundary detection as an auxiliary task to alleviate visual bias. Experiments\nshow that our methods achieve the new state-of-the-art performance on two\nbenchmark datasets.",
    "published": "2022-08-23T15:25:44Z",
    "updated": "2023-03-09T05:48:21Z",
    "authors": [
      "Junyu Lu",
      "Dixiang Zhang",
      "Pingjian Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.00613v1",
    "title": "Diffusing Graph Attention",
    "summary": "The dominant paradigm for machine learning on graphs uses Message Passing\nGraph Neural Networks (MP-GNNs), in which node representations are updated by\naggregating information in their local neighborhood. Recently, there have been\nincreasingly more attempts to adapt the Transformer architecture to graphs in\nan effort to solve some known limitations of MP-GNN. A challenging aspect of\ndesigning Graph Transformers is integrating the arbitrary graph structure into\nthe architecture. We propose Graph Diffuser (GD) to address this challenge. GD\nlearns to extract structural and positional relationships between distant nodes\nin the graph, which it then uses to direct the Transformer's attention and node\nrepresentation. We demonstrate that existing GNNs and Graph Transformers\nstruggle to capture long-range interactions and how Graph Diffuser does so\nwhile admitting intuitive visualizations. Experiments on eight benchmarks show\nGraph Diffuser to be a highly competitive model, outperforming the\nstate-of-the-art in a diverse set of domains.",
    "published": "2023-03-01T16:11:05Z",
    "updated": "2023-03-01T16:11:05Z",
    "authors": [
      "Daniel Glickman",
      "Eran Yahav"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.09233v3",
    "title": "SwinVFTR: A Novel Volumetric Feature-learning Transformer for 3D OCT\n  Fluid Segmentation",
    "summary": "Accurately segmenting fluid in 3D optical coherence tomography (OCT) images\nis critical for detecting eye diseases but remains challenging. Traditional\nautoencoder-based methods struggle with resolution loss and information\nrecovery. While transformer-based models improve segmentation, they arent\noptimized for 3D OCT volumes, which vary by vendor and extraction technique. To\naddress this, we propose SwinVFTR, a transformer architecture for precise fluid\nsegmentation in 3D OCT images. SwinVFTR employs channel-wise volumetric\nsampling and a shifted window transformer block to improve fluid localization.\nMoreover, a novel volumetric attention block enhances spatial and depth-wise\nattention. Trained using multi-class dice loss, SwinVFTR outperforms existing\nmodels on Spectralis, Cirrus, and Topcon OCT datasets, achieving mean dice\nscores of 0.72, 0.59, and 0.68, respectively, along with superior performance\nin mean intersection-over-union (IOU) and structural similarity (SSIM) metrics.",
    "published": "2023-03-16T11:16:02Z",
    "updated": "2025-01-02T22:11:57Z",
    "authors": [
      "Khondker Fariha Hossain",
      "Sharif Amit Kamran",
      "Alireza Tavakkoli",
      "George Bebis",
      "Sal Baker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1810.09184v1",
    "title": "Learning sparse transformations through backpropagation",
    "summary": "Many transformations in deep learning architectures are sparsely connected.\nWhen such transformations cannot be designed by hand, they can be learned, even\nthrough plain backpropagation, for instance in attention mechanisms. However,\nduring learning, such sparse structures are often represented in a dense form,\nas we do not know beforehand which elements will eventually become non-zero. We\nintroduce the adaptive, sparse hyperlayer, a method for learning a sparse\ntransformation, paramatrized sparsely: as index-tuples with associated values.\nTo overcome the lack of gradients from such a discrete structure, we introduce\na method of randomly sampling connections, and backpropagating over the\nrandomly wired computation graph. To show that this approach allows us to train\na model to competitive performance on real data, we use it to build two\narchitectures. First, an attention mechanism for visual classification. Second,\nwe implement a method for differentiable sorting: specifically, learning to\nsort unlabeled MNIST digits, given only the correct order.",
    "published": "2018-10-22T11:34:32Z",
    "updated": "2018-10-22T11:34:32Z",
    "authors": [
      "Peter Bloem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1909.02273v1",
    "title": "Source Dependency-Aware Transformer with Supervised Self-Attention",
    "summary": "Recently, Transformer has achieved the state-of-the-art performance on many\nmachine translation tasks. However, without syntax knowledge explicitly\nconsidered in the encoder, incorrect context information that violates the\nsyntax structure may be integrated into source hidden states, leading to\nerroneous translations. In this paper, we propose a novel method to incorporate\nsource dependencies into the Transformer. Specifically, we adopt the source\ndependency tree and define two matrices to represent the dependency relations.\nBased on the matrices, two heads in the multi-head self-attention module are\ntrained in a supervised manner and two extra cross entropy losses are\nintroduced into the training objective function. Under this training objective,\nthe model is trained to learn the source dependency relations directly. Without\nrequiring pre-parsed input during inference, our model can generate better\ntranslations with the dependency-aware context information. Experiments on\nbi-directional Chinese-to-English, English-to-Japanese and English-to-German\ntranslation tasks show that our proposed method can significantly improve the\nTransformer baseline.",
    "published": "2019-09-05T09:17:37Z",
    "updated": "2019-09-05T09:17:37Z",
    "authors": [
      "Chengyi Wang",
      "Shuangzhi Wu",
      "Shujie Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1910.00486v3",
    "title": "Dialogue Transformers",
    "summary": "We introduce a dialogue policy based on a transformer architecture, where the\nself-attention mechanism operates over the sequence of dialogue turns. Recent\nwork has used hierarchical recurrent neural networks to encode multiple\nutterances in a dialogue context, but we argue that a pure self-attention\nmechanism is more suitable. By default, an RNN assumes that every item in a\nsequence is relevant for producing an encoding of the full sequence, but a\nsingle conversation can consist of multiple overlapping discourse segments as\nspeakers interleave multiple topics. A transformer picks which turns to include\nin its encoding of the current dialogue state, and is naturally suited to\nselectively ignoring or attending to dialogue history. We compare the\nperformance of the Transformer Embedding Dialogue (TED) policy to an LSTM and\nto the REDP, which was specifically designed to overcome this limitation of\nRNNs.",
    "published": "2019-10-01T15:36:27Z",
    "updated": "2020-05-01T07:43:00Z",
    "authors": [
      "Vladimir Vlasov",
      "Johannes E. M. Mosig",
      "Alan Nichol"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.10220v2",
    "title": "I-BERT: Inductive Generalization of Transformer to Arbitrary Context\n  Lengths",
    "summary": "Self-attention has emerged as a vital component of state-of-the-art\nsequence-to-sequence models for natural language processing in recent years,\nbrought to the forefront by pre-trained bi-directional Transformer models. Its\neffectiveness is partly due to its non-sequential architecture, which promotes\nscalability and parallelism but limits the model to inputs of a bounded length.\nIn particular, such architectures perform poorly on algorithmic tasks, where\nthe model must learn a procedure which generalizes to input lengths unseen in\ntraining, a capability we refer to as inductive generalization. Identifying the\ncomputational limits of existing self-attention mechanisms, we propose I-BERT,\na bi-directional Transformer that replaces positional encodings with a\nrecurrent layer. The model inductively generalizes on a variety of algorithmic\ntasks where state-of-the-art Transformer models fail to do so. We also test our\nmethod on masked language modeling tasks where training and validation sets are\npartitioned to verify inductive generalization. Out of three algorithmic and\ntwo natural language inductive generalization tasks, I-BERT achieves\nstate-of-the-art results on four tasks.",
    "published": "2020-06-18T00:56:12Z",
    "updated": "2020-06-19T20:39:09Z",
    "authors": [
      "Hyoungwook Nam",
      "Seung Byum Seo",
      "Vikram Sharma Mailthody",
      "Noor Michael",
      "Lan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2007.03356v1",
    "title": "Do Transformers Need Deep Long-Range Memory",
    "summary": "Deep attention models have advanced the modelling of sequential data across\nmany domains. For language modelling in particular, the Transformer-XL -- a\nTransformer augmented with a long-range memory of past activations -- has been\nshown to be state-of-the-art across a variety of well-studied benchmarks. The\nTransformer-XL incorporates a long-range memory at every layer of the network,\nwhich renders its state to be thousands of times larger than RNN predecessors.\nHowever it is unclear whether this is necessary. We perform a set of\ninterventions to show that comparable performance can be obtained with 6X fewer\nlong range memories and better performance can be obtained by limiting the\nrange of attention in lower layers of the network.",
    "published": "2020-07-07T11:48:49Z",
    "updated": "2020-07-07T11:48:49Z",
    "authors": [
      "Jack W. Rae",
      "Ali Razavi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.04547v4",
    "title": "Of Non-Linearity and Commutativity in BERT",
    "summary": "In this work we provide new insights into the transformer architecture, and\nin particular, its best-known variant, BERT. First, we propose a method to\nmeasure the degree of non-linearity of different elements of transformers.\nNext, we focus our investigation on the feed-forward networks (FFN) inside\ntransformers, which contain 2/3 of the model parameters and have so far not\nreceived much attention. We find that FFNs are an inefficient yet important\narchitectural element and that they cannot simply be replaced by attention\nblocks without a degradation in performance. Moreover, we study the\ninteractions between layers in BERT and show that, while the layers exhibit\nsome hierarchical structure, they extract features in a fuzzy manner. Our\nresults suggest that BERT has an inductive bias towards layer commutativity,\nwhich we find is mainly due to the skip connections. This provides a\njustification for the strong performance of recurrent and weight-shared\ntransformer models.",
    "published": "2021-01-12T15:29:38Z",
    "updated": "2021-05-07T07:38:46Z",
    "authors": [
      "Sumu Zhao",
      "Damian Pascual",
      "Gino Brunner",
      "Roger Wattenhofer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2101.10804v3",
    "title": "CPTR: Full Transformer Network for Image Captioning",
    "summary": "In this paper, we consider the image captioning task from a new\nsequence-to-sequence prediction perspective and propose CaPtion TransformeR\n(CPTR) which takes the sequentialized raw images as the input to Transformer.\nCompared to the \"CNN+Transformer\" design paradigm, our model can model global\ncontext at every encoder layer from the beginning and is totally\nconvolution-free. Extensive experiments demonstrate the effectiveness of the\nproposed model and we surpass the conventional \"CNN+Transformer\" methods on the\nMSCOCO dataset. Besides, we provide detailed visualizations of the\nself-attention between patches in the encoder and the \"words-to-patches\"\nattention in the decoder thanks to the full Transformer architecture.",
    "published": "2021-01-26T14:29:52Z",
    "updated": "2021-01-28T04:38:38Z",
    "authors": [
      "Wei Liu",
      "Sihan Chen",
      "Longteng Guo",
      "Xinxin Zhu",
      "Jing Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.09943v2",
    "title": "Towards Emotion Recognition in Hindi-English Code-Mixed Data: A\n  Transformer Based Approach",
    "summary": "In the last few years, emotion detection in social-media text has become a\npopular problem due to its wide ranging application in better understanding the\nconsumers, in psychology, in aiding human interaction with computers, designing\nsmart systems etc. Because of the availability of huge amounts of data from\nsocial-media, which is regularly used for expressing sentiments and opinions,\nthis problem has garnered great attention. In this paper, we present a Hinglish\ndataset labelled for emotion detection. We highlight a deep learning based\napproach for detecting emotions in Hindi-English code mixed tweets, using\nbilingual word embeddings derived from FastText and Word2Vec approaches, as\nwell as transformer based models. We experiment with various deep learning\nmodels, including CNNs, LSTMs, Bi-directional LSTMs (with and without\nattention), along with transformers like BERT, RoBERTa, and ALBERT. The\ntransformer based BERT model outperforms all other models giving the best\nperformance with an accuracy of 71.43%.",
    "published": "2021-02-19T14:07:20Z",
    "updated": "2021-02-28T08:43:57Z",
    "authors": [
      "Anshul Wadhawan",
      "Akshita Aggarwal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.13249v2",
    "title": "Chess as a Testbed for Language Model State Tracking",
    "summary": "Transformer language models have made tremendous strides in natural language\nunderstanding tasks. However, the complexity of natural language makes it\nchallenging to ascertain how accurately these models are tracking the world\nstate underlying the text. Motivated by this issue, we consider the task of\nlanguage modeling for the game of chess. Unlike natural language, chess\nnotations describe a simple, constrained, and deterministic domain. Moreover,\nwe observe that the appropriate choice of chess notation allows for directly\nprobing the world state, without requiring any additional probing-related\nmachinery. We find that: (a) With enough training data, transformer language\nmodels can learn to track pieces and predict legal moves with high accuracy\nwhen trained solely on move sequences. (b) For small training sets providing\naccess to board state information during training can yield significant\nimprovements. (c) The success of transformer language models is dependent on\naccess to the entire game history i.e. \"full attention\". Approximating this\nfull attention results in a significant performance drop. We propose this\ntestbed as a benchmark for future work on the development and analysis of\ntransformer language models.",
    "published": "2021-02-26T01:16:23Z",
    "updated": "2022-05-13T21:40:30Z",
    "authors": [
      "Shubham Toshniwal",
      "Sam Wiseman",
      "Karen Livescu",
      "Kevin Gimpel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.01136v2",
    "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
    "summary": "We design a family of image classification architectures that optimize the\ntrade-off between accuracy and efficiency in a high-speed regime. Our work\nexploits recent findings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We revisit principles from\nthe extensive literature on convolutional neural networks to apply them to\ntransformers, in particular activation maps with decreasing resolutions. We\nalso introduce the attention bias, a new way to integrate positional\ninformation in vision transformers. As a result, we propose LeVIT: a hybrid\nneural network for fast inference image classification. We consider different\nmeasures of efficiency on different hardware platforms, so as to best reflect a\nwide range of application scenarios. Our extensive experiments empirically\nvalidate our technical choices and show they are suitable to most\narchitectures. Overall, LeViT significantly outperforms existing convnets and\nvision transformers with respect to the speed/accuracy tradeoff. For example,\nat 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on\nCPU. We release the code at https://github.com/facebookresearch/LeViT",
    "published": "2021-04-02T16:29:57Z",
    "updated": "2021-05-06T15:25:03Z",
    "authors": [
      "Ben Graham",
      "Alaaeldin El-Nouby",
      "Hugo Touvron",
      "Pierre Stock",
      "Armand Joulin",
      "HervÃ© JÃ©gou",
      "Matthijs Douze"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.03964v1",
    "title": "Handwriting Transformers",
    "summary": "We propose a novel transformer-based styled handwritten text image generation\napproach, HWT, that strives to learn both style-content entanglement as well as\nglobal and local writing style patterns. The proposed HWT captures the long and\nshort range relationships within the style examples through a self-attention\nmechanism, thereby encoding both global and local style patterns. Further, the\nproposed transformer-based HWT comprises an encoder-decoder attention that\nenables style-content entanglement by gathering the style representation of\neach query character. To the best of our knowledge, we are the first to\nintroduce a transformer-based generative network for styled handwritten text\ngeneration. Our proposed HWT generates realistic styled handwritten text images\nand significantly outperforms the state-of-the-art demonstrated through\nextensive qualitative, quantitative and human-based evaluations. The proposed\nHWT can handle arbitrary length of text and any desired writing style in a\nfew-shot setting. Further, our HWT generalizes well to the challenging scenario\nwhere both words and writing style are unseen during training, generating\nrealistic styled handwritten text images.",
    "published": "2021-04-08T17:59:43Z",
    "updated": "2021-04-08T17:59:43Z",
    "authors": [
      "Ankan Kumar Bhunia",
      "Salman Khan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Fahad Shahbaz Khan",
      "Mubarak Shah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.04091v1",
    "title": "Grapheme-to-Phoneme Transformer Model for Transfer Learning Dialects",
    "summary": "Grapheme-to-Phoneme (G2P) models convert words to their phonetic\npronunciations. Classic G2P methods include rule-based systems and\npronunciation dictionaries, while modern G2P systems incorporate learning, such\nas, LSTM and Transformer-based attention models. Usually, dictionary-based\nmethods require significant manual effort to build, and have limited adaptivity\non unseen words. And transformer-based models require significant training\ndata, and do not generalize well, especially for dialects with limited data.\n  We propose a novel use of transformer-based attention model that can adapt to\nunseen dialects of English language, while using a small dictionary. We show\nthat our method has potential applications for accent transfer for\ntext-to-speech, and for building robust G2P models for dialects with limited\npronunciation dictionary size.\n  We experiment with two English dialects: Indian and British. A model trained\nfrom scratch using 1000 words from British English dictionary, with 14211 words\nheld out, leads to phoneme error rate (PER) of 26.877%, on a test set generated\nusing the full dictionary. The same model pretrained on CMUDict American\nEnglish dictionary, and fine-tuned on the same dataset leads to PER of 2.469%\non the test set.",
    "published": "2021-04-08T21:36:21Z",
    "updated": "2021-04-08T21:36:21Z",
    "authors": [
      "Eric Engelhart",
      "Mahsa Elyasi",
      "Gaurav Bharaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.06976v1",
    "title": "Pose Recognition with Cascade Transformers",
    "summary": "In this paper, we present a regression-based pose recognition method using\ncascade Transformers. One way to categorize the existing approaches in this\ndomain is to separate them into 1). heatmap-based and 2). regression-based. In\ngeneral, heatmap-based methods achieve higher accuracy but are subject to\nvarious heuristic designs (not end-to-end mostly), whereas regression-based\napproaches attain relatively lower accuracy but they have less intermediate\nnon-differentiable steps. Here we utilize the encoder-decoder structure in\nTransformers to perform regression-based person and keypoint detection that is\ngeneral-purpose and requires less heuristic design compared with the existing\napproaches. We demonstrate the keypoint hypothesis (query) refinement process\nacross different self-attention layers to reveal the recursive self-attention\nmechanism in Transformers. In the experiments, we report competitive results\nfor pose recognition when compared with the competing regression-based methods.",
    "published": "2021-04-14T17:00:22Z",
    "updated": "2021-04-14T17:00:22Z",
    "authors": [
      "Ke Li",
      "Shijie Wang",
      "Xiang Zhang",
      "Yifan Xu",
      "Weijian Xu",
      "Zhuowen Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.03928v2",
    "title": "Which transformer architecture fits my data? A vocabulary bottleneck in\n  self-attention",
    "summary": "After their successful debut in natural language processing, Transformer\narchitectures are now becoming the de-facto standard in many domains. An\nobstacle for their deployment over new modalities is the architectural\nconfiguration: the optimal depth-to-width ratio has been shown to dramatically\nvary across data types (e.g., $10$x larger over images than over language). We\ntheoretically predict the existence of an embedding rank bottleneck that limits\nthe contribution of self-attention width to the Transformer expressivity. We\nthus directly tie the input vocabulary size and rank to the optimal\ndepth-to-width ratio, since a small vocabulary size or rank dictates an added\nadvantage of depth over width. We empirically demonstrate the existence of this\nbottleneck and its implications on the depth-to-width interplay of Transformer\narchitectures, linking the architecture variability across domains to the often\nglossed-over usage of different vocabulary sizes or embedding ranks in\ndifferent domains. As an additional benefit, our rank bottlenecking framework\nallows us to identify size redundancies of $25\\%-50\\%$ in leading NLP models\nsuch as ALBERT and T5.",
    "published": "2021-05-09T13:08:26Z",
    "updated": "2021-06-09T17:18:03Z",
    "authors": [
      "Noam Wies",
      "Yoav Levine",
      "Daniel Jannai",
      "Amnon Shashua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.15516v3",
    "title": "GeoT: A Geometry-aware Transformer for Reliable Molecular Property\n  Prediction and Chemically Interpretable Representation Learning",
    "summary": "In recent years, molecular representation learning has emerged as a key area\nof focus in various chemical tasks. However, many existing models fail to fully\nconsider the geometric information of molecular structures, resulting in less\nintuitive representations. Moreover, the widely used message-passing mechanism\nis limited to provide the interpretation of experimental results from a\nchemical perspective. To address these challenges, we introduce a novel\nTransformer-based framework for molecular representation learning, named the\nGeometry-aware Transformer (GeoT). GeoT learns molecular graph structures\nthrough attention-based mechanisms specifically designed to offer reliable\ninterpretability, as well as molecular property prediction. Consequently, GeoT\ncan generate attention maps of interatomic relationships associated with\ntraining objectives. In addition, GeoT demonstrates comparable performance to\nMPNN-based models while achieving reduced computational complexity. Our\ncomprehensive experiments, including an empirical simulation, reveal that GeoT\neffectively learns the chemical insights into molecular structures, bridging\nthe gap between artificial intelligence and molecular sciences.",
    "published": "2021-06-29T15:47:18Z",
    "updated": "2023-06-28T13:51:49Z",
    "authors": [
      "Bumju Kwak",
      "Jiwon Park",
      "Taewon Kang",
      "Jeonghee Jo",
      "Byunghan Lee",
      "Sungroh Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.08224v1",
    "title": "Transformers predicting the future. Applying attention in next-frame and\n  time series forecasting",
    "summary": "Recurrent Neural Networks were, until recently, one of the best ways to\ncapture the timely dependencies in sequences. However, with the introduction of\nthe Transformer, it has been proven that an architecture with only\nattention-mechanisms without any RNN can improve on the results in various\nsequence processing tasks (e.g. NLP). Multiple studies since then have shown\nthat similar approaches can be applied for images, point clouds, video, audio\nor time series forecasting. Furthermore, solutions such as the Perceiver or the\nInformer have been introduced to expand on the applicability of the\nTransformer. Our main objective is testing and evaluating the effectiveness of\napplying Transformer-like models on time series data, tackling susceptibility\nto anomalies, context awareness and space complexity by fine-tuning the\nhyperparameters, preprocessing the data, applying dimensionality reduction or\nconvolutional encodings, etc. We are also looking at the problem of next-frame\nprediction and exploring ways to modify existing solutions in order to achieve\nhigher performance and learn generalized knowledge.",
    "published": "2021-08-18T16:17:29Z",
    "updated": "2021-08-18T16:17:29Z",
    "authors": [
      "Radostin Cholakov",
      "Todor Kolev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.12275v1",
    "title": "Can the Transformer Be Used as a Drop-in Replacement for RNNs in\n  Text-Generating GANs?",
    "summary": "In this paper we address the problem of fine-tuned text generation with a\nlimited computational budget. For that, we use a well-performing text\ngenerative adversarial network (GAN) architecture - Diversity-Promoting GAN\n(DPGAN), and attempted a drop-in replacement of the LSTM layer with a\nself-attention-based Transformer layer in order to leverage their efficiency.\nThe resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,\nquality and diversity of generated text and stability. Computational\nexperiments suggested that a transformer architecture is unable to drop-in\nreplace the LSTM layer, under-performing during the pre-training phase and\nundergoing a complete mode collapse during the GAN tuning phase. Our results\nsuggest that the transformer architecture need to be adapted before it can be\nused as a replacement for RNNs in text-generating GANs.",
    "published": "2021-08-26T14:15:36Z",
    "updated": "2021-08-26T14:15:36Z",
    "authors": [
      "Kevin Blin",
      "Andrei Kucharavy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.07732v4",
    "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves\n  Systematic Generalization",
    "summary": "Despite progress across a broad range of applications, Transformers have\nlimited success in systematic generalization. The situation is especially\nfrustrating in the case of algorithmic tasks, where they often fail to find\nintuitive solutions that route relevant information to the right node/operation\nat the right time in the grid represented by Transformer columns. To facilitate\nthe learning of useful control flow, we propose two modifications to the\nTransformer architecture, copy gate and geometric attention. Our novel Neural\nData Router (NDR) achieves 100% length generalization accuracy on the classic\ncompositional table lookup task, as well as near-perfect accuracy on the simple\narithmetic task and a new variant of ListOps testing for generalization across\ncomputational depths. NDR's attention and gating patterns tend to be\ninterpretable as an intuitive form of neural routing. Our code is public.",
    "published": "2021-10-14T21:24:27Z",
    "updated": "2022-05-05T10:01:16Z",
    "authors": [
      "RÃ³bert CsordÃ¡s",
      "Kazuki Irie",
      "JÃ¼rgen Schmidhuber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.03250v1",
    "title": "Context-Aware Transformer Transducer for Speech Recognition",
    "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems often have\ndifficulty recognizing uncommon words, that appear infrequently in the training\ndata. One promising method, to improve the recognition accuracy on such rare\nwords, is to latch onto personalized/contextual information at inference. In\nthis work, we present a novel context-aware transformer transducer (CATT)\nnetwork that improves the state-of-the-art transformer-based ASR system by\ntaking advantage of such contextual signals. Specifically, we propose a\nmulti-head attention-based context-biasing network, which is jointly trained\nwith the rest of the ASR sub-networks. We explore different techniques to\nencode contextual data and to create the final attention context vectors. We\nalso leverage both BLSTM and pretrained BERT based models to encode contextual\ndata and guide the network training. Using an in-house far-field dataset, we\nshow that CATT, using a BERT based context encoder, improves the word error\nrate of the baseline transformer transducer and outperforms an existing deep\ncontextual model by 24.2% and 19.4% respectively.",
    "published": "2021-11-05T04:14:35Z",
    "updated": "2021-11-05T04:14:35Z",
    "authors": [
      "Feng-Ju Chang",
      "Jing Liu",
      "Martin Radfar",
      "Athanasios Mouchtaris",
      "Maurizio Omologo",
      "Ariya Rastrow",
      "Siegfried Kunzmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.14576v1",
    "title": "Recurrent Vision Transformer for Solving Visual Reasoning Problems",
    "summary": "Although convolutional neural networks (CNNs) showed remarkable results in\nmany vision tasks, they are still strained by simple yet challenging visual\nreasoning problems. Inspired by the recent success of the Transformer network\nin computer vision, in this paper, we introduce the Recurrent Vision\nTransformer (RViT) model. Thanks to the impact of recurrent connections and\nspatial attention in reasoning tasks, this network achieves competitive results\non the same-different visual reasoning problems from the SVRT dataset. The\nweight-sharing both in spatial and depth dimensions regularizes the model,\nallowing it to learn using far fewer free parameters, using only 28k training\nsamples. A comprehensive ablation study confirms the importance of a hybrid CNN\n+ Transformer architecture and the role of the feedback connections, which\niteratively refine the internal representation until a stable prediction is\nobtained. In the end, this study can lay the basis for a deeper understanding\nof the role of attention and recurrent connections for solving visual abstract\nreasoning tasks.",
    "published": "2021-11-29T15:01:09Z",
    "updated": "2021-11-29T15:01:09Z",
    "authors": [
      "Nicola Messina",
      "Giuseppe Amato",
      "Fabio Carrara",
      "Claudio Gennaro",
      "Fabrizio Falchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.06759v1",
    "title": "Hformer: Hybrid CNN-Transformer for Fringe Order Prediction in Phase\n  Unwrapping of Fringe Projection",
    "summary": "Recently, deep learning has attracted more and more attention in phase\nunwrapping of fringe projection three-dimensional (3D) measurement, with the\naim to improve the performance leveraging the powerful Convolutional Neural\nNetwork (CNN) models. In this paper, for the first time (to the best of our\nknowledge), we introduce the Transformer into the phase unwrapping which is\ndifferent from CNN and propose Hformer model dedicated to phase unwrapping via\nfringe order prediction. The proposed model has a hybrid CNN-Transformer\narchitecture that is mainly composed of backbone, encoder and decoder to take\nadvantage of both CNN and Transformer. Encoder and decoder with cross attention\nare designed for the fringe order prediction. Experimental results show that\nthe proposed Hformer model achieves better performance in fringe order\nprediction compared with the CNN models such as U-Net and DCNN. Moreover,\nablation study on Hformer is made to verify the improved feature pyramid\nnetworks (FPN) and testing strategy with flipping in the predicted fringe\norder. Our work opens an alternative way to deep learning based phase\nunwrapping methods, which are dominated by CNN in fringe projection 3D\nmeasurement.",
    "published": "2021-12-13T16:09:21Z",
    "updated": "2021-12-13T16:09:21Z",
    "authors": [
      "Xinjun Zhu",
      "Zhiqiang Han",
      "Mengkai Yuan",
      "Qinghua Guo",
      "Hongyi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.02833v1",
    "title": "Cross-view Transformers for real-time Map-view Semantic Segmentation",
    "summary": "We present cross-view transformers, an efficient attention-based model for\nmap-view semantic segmentation from multiple cameras. Our architecture\nimplicitly learns a mapping from individual camera views into a canonical\nmap-view representation using a camera-aware cross-view attention mechanism.\nEach camera uses positional embeddings that depend on its intrinsic and\nextrinsic calibration. These embeddings allow a transformer to learn the\nmapping across different views without ever explicitly modeling it\ngeometrically. The architecture consists of a convolutional image encoder for\neach view and cross-view transformer layers to infer a map-view semantic\nsegmentation. Our model is simple, easily parallelizable, and runs in\nreal-time. The presented architecture performs at state-of-the-art on the\nnuScenes dataset, with 4x faster inference speeds. Code is available at\nhttps://github.com/bradyz/cross_view_transformers.",
    "published": "2022-05-05T17:59:33Z",
    "updated": "2022-05-05T17:59:33Z",
    "authors": [
      "Brady Zhou",
      "Philipp KrÃ¤henbÃ¼hl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.05854v1",
    "title": "Entity-aware and Motion-aware Transformers for Language-driven Action\n  Localization in Videos",
    "summary": "Language-driven action localization in videos is a challenging task that\ninvolves not only visual-linguistic matching but also action boundary\nprediction. Recent progress has been achieved through aligning language query\nto video segments, but estimating precise boundaries is still under-explored.\nIn this paper, we propose entity-aware and motion-aware Transformers that\nprogressively localizes actions in videos by first coarsely locating clips with\nentity queries and then finely predicting exact boundaries in a shrunken\ntemporal region with motion queries. The entity-aware Transformer incorporates\nthe textual entities into visual representation learning via cross-modal and\ncross-frame attentions to facilitate attending action-related video clips. The\nmotion-aware Transformer captures fine-grained motion changes at multiple\ntemporal scales via integrating long short-term memory into the self-attention\nmodule to further improve the precision of action boundary prediction.\nExtensive experiments on the Charades-STA and TACoS datasets demonstrate that\nour method achieves better performance than existing methods.",
    "published": "2022-05-12T03:00:40Z",
    "updated": "2022-05-12T03:00:40Z",
    "authors": [
      "Shuo Yang",
      "Xinxiao Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.10662v2",
    "title": "Equivariant Mesh Attention Networks",
    "summary": "Equivariance to symmetries has proven to be a powerful inductive bias in deep\nlearning research. Recent works on mesh processing have concentrated on various\nkinds of natural symmetries, including translations, rotations, scaling, node\npermutations, and gauge transformations. To date, no existing architecture is\nequivariant to all of these transformations. In this paper, we present an\nattention-based architecture for mesh data that is provably equivariant to all\ntransformations mentioned above. Our pipeline relies on the use of relative\ntangential features: a simple, effective, equivariance-friendly alternative to\nraw node positions as inputs. Experiments on the FAUST and TOSCA datasets\nconfirm that our proposed architecture achieves improved performance on these\nbenchmarks and is indeed equivariant, and therefore robust, to a wide variety\nof local/global transformations.",
    "published": "2022-05-21T19:53:14Z",
    "updated": "2022-08-27T16:43:35Z",
    "authors": [
      "Sourya Basu",
      "Jose Gallego-Posada",
      "Francesco ViganÃ²",
      "James Rowbottom",
      "Taco Cohen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.04410v2",
    "title": "CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical\n  Expression Recognition",
    "summary": "The Transformer-based encoder-decoder architecture has recently made\nsignificant advances in recognizing handwritten mathematical expressions.\nHowever, the transformer model still suffers from the lack of coverage problem,\nmaking its expression recognition rate (ExpRate) inferior to its RNN\ncounterpart. Coverage information, which records the alignment information of\nthe past steps, has proven effective in the RNN models. In this paper, we\npropose CoMER, a model that adopts the coverage information in the transformer\ndecoder. Specifically, we propose a novel Attention Refinement Module (ARM) to\nrefine the attention weights with past alignment information without hurting\nits parallelism. Furthermore, we take coverage information to the extreme by\nproposing self-coverage and cross-coverage, which utilize the past alignment\ninformation from the current and previous layers. Experiments show that CoMER\nimproves the ExpRate by 0.61%/2.09%/1.59% compared to the current\nstate-of-the-art model, and reaches 59.33%/59.81%/62.97% on the CROHME\n2014/2016/2019 test sets.",
    "published": "2022-07-10T07:59:23Z",
    "updated": "2022-07-18T02:58:10Z",
    "authors": [
      "Wenqi Zhao",
      "Liangcai Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.08466v1",
    "title": "What does Transformer learn about source code?",
    "summary": "In the field of source code processing, the transformer-based representation\nmodels have shown great powerfulness and have achieved state-of-the-art (SOTA)\nperformance in many tasks. Although the transformer models process the\nsequential source code, pieces of evidence show that they may capture the\nstructural information (\\eg, in the syntax tree, data flow, control flow, \\etc)\nas well. We propose the aggregated attention score, a method to investigate the\nstructural information learned by the transformer. We also put forward the\naggregated attention graph, a new way to extract program graphs from the\npre-trained models automatically. We measure our methods from multiple\nperspectives. Furthermore, based on our empirical findings, we use the\nautomatically extracted graphs to replace those ingenious manual designed\ngraphs in the Variable Misuse task. Experimental results show that the semantic\ngraphs we extracted automatically are greatly meaningful and effective, which\nprovide a new perspective for us to understand and use the information\ncontained in the model.",
    "published": "2022-07-18T09:33:04Z",
    "updated": "2022-07-18T09:33:04Z",
    "authors": [
      "Kechi Zhang",
      "Ge Li",
      "Zhi Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.13820v1",
    "title": "Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery\n  with Transformers",
    "summary": "Transformer encoder architectures have recently achieved state-of-the-art\nresults on monocular 3D human mesh reconstruction, but they require a\nsubstantial number of parameters and expensive computations. Due to the large\nmemory overhead and slow inference speed, it is difficult to deploy such models\nfor practical use. In this paper, we propose a novel transformer\nencoder-decoder architecture for 3D human mesh reconstruction from a single\nimage, called FastMETRO. We identify the performance bottleneck in the\nencoder-based transformers is caused by the token design which introduces high\ncomplexity interactions among input tokens. We disentangle the interactions via\nan encoder-decoder architecture, which allows our model to demand much fewer\nparameters and shorter inference time. In addition, we impose the prior\nknowledge of human body's morphological relationship via attention masking and\nmesh upsampling operations, which leads to faster convergence with higher\naccuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency,\nand clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore,\nwe validate its generalizability on FreiHAND.",
    "published": "2022-07-27T22:54:09Z",
    "updated": "2022-07-27T22:54:09Z",
    "authors": [
      "Junhyeong Cho",
      "Kim Youwang",
      "Tae-Hyun Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.03834v2",
    "title": "Pre-Training a Graph Recurrent Network for Language Representation",
    "summary": "Transformer-based pre-trained models have gained much advance in recent\nyears, becoming one of the most important backbones in natural language\nprocessing. Recent work shows that the attention mechanism inside Transformer\nmay not be necessary, both convolutional neural networks and multi-layer\nperceptron based models have also been investigated as Transformer\nalternatives. In this paper, we consider a graph recurrent network for language\nmodel pre-training, which builds a graph structure for each sequence with local\ntoken-level communications, together with a sentence-level representation\ndecoupled from other tokens. The original model performs well in\ndomain-specific text classification under supervised training, however, its\npotential in learning transfer knowledge by self-supervised way has not been\nfully exploited. We fill this gap by optimizing the architecture and verifying\nits effectiveness in more general language understanding tasks, for both\nEnglish and Chinese languages. As for model efficiency, instead of the\nquadratic complexity in Transformer-based models, our model has linear\ncomplexity and performs more efficiently during inference. Moreover, we find\nthat our model can generate more diverse outputs with less contextualized\nfeature redundancy than existing attention-based models.",
    "published": "2022-09-08T14:12:15Z",
    "updated": "2022-10-26T08:22:52Z",
    "authors": [
      "Yile Wang",
      "Linyi Yang",
      "Zhiyang Teng",
      "Ming Zhou",
      "Yue Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.05032v1",
    "title": "Vision Transformer with Convolutional Encoder-Decoder for Hand Gesture\n  Recognition using 24 GHz Doppler Radar",
    "summary": "Transformers combined with convolutional encoders have been recently used for\nhand gesture recognition (HGR) using micro-Doppler signatures. We propose a\nvision-transformer-based architecture for HGR with multi-antenna\ncontinuous-wave Doppler radar receivers. The proposed architecture consists of\nthree modules: a convolutional encoderdecoder, an attention module with three\ntransformer layers, and a multi-layer perceptron. The novel convolutional\ndecoder helps to feed patches with larger sizes to the attention module for\nimproved feature extraction. Experimental results obtained with a dataset\ncorresponding to a two-antenna continuous-wave Doppler radar receiver operating\nat 24 GHz (published by Skaria et al.) confirm that the proposed architecture\nachieves an accuracy of 98.3% which substantially surpasses the\nstate-of-the-art on the used dataset.",
    "published": "2022-09-12T05:56:35Z",
    "updated": "2022-09-12T05:56:35Z",
    "authors": [
      "Kavinda Kehelella",
      "Gayangana Leelarathne",
      "Dhanuka Marasinghe",
      "Nisal Kariyawasam",
      "Viduneth Ariyarathna",
      "Arjuna Madanayake",
      "Ranga Rodrigo",
      "Chamira U. S. Edussooriya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.06143v1",
    "title": "FAN-Trans: Online Knowledge Distillation for Facial Action Unit\n  Detection",
    "summary": "Due to its importance in facial behaviour analysis, facial action unit (AU)\ndetection has attracted increasing attention from the research community.\nLeveraging the online knowledge distillation framework, we propose the\n``FANTrans\" method for AU detection. Our model consists of a hybrid network of\nconvolution and transformer blocks to learn per-AU features and to model AU\nco-occurrences. The model uses a pre-trained face alignment network as the\nfeature extractor. After further transformation by a small learnable add-on\nconvolutional subnet, the per-AU features are fed into transformer blocks to\nenhance their representation. As multiple AUs often appear together, we propose\na learnable attention drop mechanism in the transformer block to learn the\ncorrelation between the features for different AUs. We also design a classifier\nthat predicts AU presence by considering all AUs' features, to explicitly\ncapture label dependencies. Finally, we make the attempt of adapting online\nknowledge distillation in the training stage for this task, further improving\nthe model's performance. Experiments on the BP4D and DISFA datasets\ndemonstrating the effectiveness of proposed method.",
    "published": "2022-11-11T11:35:33Z",
    "updated": "2022-11-11T11:35:33Z",
    "authors": [
      "Jing Yang",
      "Jie Shen",
      "Yiming Lin",
      "Yordan Hristov",
      "Maja Pantic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.10622v1",
    "title": "Rethinking Batch Sample Relationships for Data Representation: A\n  Batch-Graph Transformer based Approach",
    "summary": "Exploring sample relationships within each mini-batch has shown great\npotential for learning image representations. Existing works generally adopt\nthe regular Transformer to model the visual content relationships, ignoring the\ncues of semantic/label correlations between samples. Also, they generally adopt\nthe \"full\" self-attention mechanism which are obviously redundant and also\nsensitive to the noisy samples. To overcome these issues, in this paper, we\ndesign a simple yet flexible Batch-Graph Transformer (BGFormer) for mini-batch\nsample representations by deeply capturing the relationships of image samples\nfrom both visual and semantic perspectives. BGFormer has three main aspects.\n(1) It employs a flexible graph model, termed Batch Graph to jointly encode the\nvisual and semantic relationships of samples within each mini-batch. (2) It\nexplores the neighborhood relationships of samples by borrowing the idea of\nsparse graph representation which thus performs robustly, w.r.t., noisy\nsamples. (3) It devises a novel Transformer architecture that mainly adopts\ndual structure-constrained self-attention (SSA), together with graph\nnormalization, FFN, etc, to carefully exploit the batch graph information for\nsample tokens (nodes) representations. As an application, we apply BGFormer to\nthe metric learning tasks. Extensive experiments on four popular datasets\ndemonstrate the effectiveness of the proposed model.",
    "published": "2022-11-19T08:46:50Z",
    "updated": "2022-11-19T08:46:50Z",
    "authors": [
      "Xixi Wang",
      "Bo Jiang",
      "Xiao Wang",
      "Bin Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.06026v1",
    "title": "Video Prediction by Efficient Transformers",
    "summary": "Video prediction is a challenging computer vision task that has a wide range\nof applications. In this work, we present a new family of Transformer-based\nmodels for video prediction. Firstly, an efficient local spatial-temporal\nseparation attention mechanism is proposed to reduce the complexity of standard\nTransformers. Then, a full autoregressive model, a partial autoregressive model\nand a non-autoregressive model are developed based on the new efficient\nTransformer. The partial autoregressive model has a similar performance with\nthe full autoregressive model but a faster inference speed. The\nnon-autoregressive model not only achieves a faster inference speed but also\nmitigates the quality degradation problem of the autoregressive counterparts,\nbut it requires additional parameters and loss function for learning. Given the\nsame attention mechanism, we conducted a comprehensive study to compare the\nproposed three video prediction variants. Experiments show that the proposed\nvideo prediction models are competitive with more complex state-of-the-art\nconvolutional-LSTM based models. The source code is available at\nhttps://github.com/XiYe20/VPTR.",
    "published": "2022-12-12T16:46:48Z",
    "updated": "2022-12-12T16:46:48Z",
    "authors": [
      "Xi Ye",
      "Guillaume-Alexandre Bilodeau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.01921v1",
    "title": "Transformers in Action Recognition: A Review on Temporal Modeling",
    "summary": "In vision-based action recognition, spatio-temporal features from different\nmodalities are used for recognizing activities. Temporal modeling is a long\nchallenge of action recognition. However, there are limited methods such as\npre-computed motion features, three-dimensional (3D) filters, and recurrent\nneural networks (RNN) for modeling motion information in deep-based approaches.\nRecently, transformers success in modeling long-range dependencies in natural\nlanguage processing (NLP) tasks has gotten great attention from other domains;\nincluding speech, image, and video, to rely entirely on self-attention without\nusing sequence-aligned RNNs or convolutions. Although the application of\ntransformers to action recognition is relatively new, the amount of research\nproposed on this topic within the last few years is astounding. This paper\nespecially reviews recent progress in deep learning methods for modeling\ntemporal variations. It focuses on action recognition methods that use\ntransformers for temporal modeling, discussing their main features, used\nmodalities, and identifying opportunities and challenges for future research.",
    "published": "2022-12-29T11:03:19Z",
    "updated": "2022-12-29T11:03:19Z",
    "authors": [
      "Elham Shabaninia",
      "Hossein Nezamabadi-pour",
      "Fatemeh Shafizadegan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.09462v1",
    "title": "MedViT: A Robust Vision Transformer for Generalized Medical Image\n  Classification",
    "summary": "Convolutional Neural Networks (CNNs) have advanced existing medical systems\nfor automatic disease diagnosis. However, there are still concerns about the\nreliability of deep medical diagnosis systems against the potential threats of\nadversarial attacks since inaccurate diagnosis could lead to disastrous\nconsequences in the safety realm. In this study, we propose a highly robust yet\nefficient CNN-Transformer hybrid model which is equipped with the locality of\nCNNs as well as the global connectivity of vision Transformers. To mitigate the\nhigh quadratic complexity of the self-attention mechanism while jointly\nattending to information in various representation subspaces, we construct our\nattention mechanism by means of an efficient convolution operation. Moreover,\nto alleviate the fragility of our Transformer model against adversarial\nattacks, we attempt to learn smoother decision boundaries. To this end, we\naugment the shape information of an image in the high-level feature space by\npermuting the feature mean and variance within mini-batches. With less\ncomputational complexity, our proposed hybrid model demonstrates its high\nrobustness and generalization ability compared to the state-of-the-art studies\non a large-scale collection of standardized MedMNIST-2D datasets.",
    "published": "2023-02-19T02:55:45Z",
    "updated": "2023-02-19T02:55:45Z",
    "authors": [
      "Omid Nejati Manzari",
      "Hamid Ahmadabadi",
      "Hossein Kashiani",
      "Shahriar B. Shokouhi",
      "Ahmad Ayatollahi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.13991v1",
    "title": "Vision Conformer: Incorporating Convolutions into Vision Transformer\n  Layers",
    "summary": "Transformers are popular neural network models that use layers of\nself-attention and fully-connected nodes with embedded tokens. Vision\nTransformers (ViT) adapt transformers for image recognition tasks. In order to\ndo this, the images are split into patches and used as tokens. One issue with\nViT is the lack of inductive bias toward image structures. Because ViT was\nadapted for image data from language modeling, the network does not explicitly\nhandle issues such as local translations, pixel information, and information\nloss in the structures and features shared by multiple patches. Conversely,\nConvolutional Neural Networks (CNN) incorporate this information. Thus, in this\npaper, we propose the use of convolutional layers within ViT. Specifically, we\npropose a model called a Vision Conformer (ViC) which replaces the Multi-Layer\nPerceptron (MLP) in a ViT layer with a CNN. In addition, to use the CNN, we\nproposed to reconstruct the image data after the self-attention in a reverse\nembedding layer. Through the evaluation, we demonstrate that the proposed\nconvolutions help improve the classification ability of ViT.",
    "published": "2023-04-27T07:27:44Z",
    "updated": "2023-04-27T07:27:44Z",
    "authors": [
      "Brian Kenji Iwana",
      "Akihiro Kusuda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.09828v1",
    "title": "Mimetic Initialization of Self-Attention Layers",
    "summary": "It is notoriously difficult to train Transformers on small datasets;\ntypically, large pre-trained models are instead used as the starting point. We\nexplore the weights of such pre-trained Transformers (particularly for vision)\nto attempt to find reasons for this discrepancy. Surprisingly, we find that\nsimply initializing the weights of self-attention layers so that they \"look\"\nmore like their pre-trained counterparts allows us to train vanilla\nTransformers faster and to higher final accuracies, particularly on vision\ntasks such as CIFAR-10 and ImageNet classification, where we see gains in\naccuracy of over 5% and 4%, respectively. Our initialization scheme is closed\nform, learning-free, and very simple: we set the product of the query and key\nweights to be approximately the identity, and the product of the value and\nprojection weights to approximately the negative identity. As this mimics the\npatterns we saw in pre-trained Transformers, we call the technique \"mimetic\ninitialization\".",
    "published": "2023-05-16T22:12:25Z",
    "updated": "2023-05-16T22:12:25Z",
    "authors": [
      "Asher Trockman",
      "J. Zico Kolter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.13417v2",
    "title": "VISIT: Visualizing and Interpreting the Semantic Information Flow of\n  Transformers",
    "summary": "Recent advances in interpretability suggest we can project weights and hidden\nstates of transformer-based language models (LMs) to their vocabulary, a\ntransformation that makes them more human interpretable. In this paper, we\ninvestigate LM attention heads and memory values, the vectors the models\ndynamically create and recall while processing a given input. By analyzing the\ntokens they represent through this projection, we identify patterns in the\ninformation flow inside the attention mechanism. Based on our discoveries, we\ncreate a tool to visualize a forward pass of Generative Pre-trained\nTransformers (GPTs) as an interactive flow graph, with nodes representing\nneurons or hidden states and edges representing the interactions between them.\nOur visualization simplifies huge amounts of data into easy-to-read plots that\ncan reflect the models' internal processing, uncovering the contribution of\neach component to the models' final prediction. Our visualization also unveils\nnew insights about the role of layer norms as semantic filters that influence\nthe models' output, and about neurons that are always activated during forward\npasses and act as regularization vectors.",
    "published": "2023-05-22T19:04:56Z",
    "updated": "2023-11-24T12:02:13Z",
    "authors": [
      "Shahar Katz",
      "Yonatan Belinkov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.17560v2",
    "title": "Scalable Transformer for PDE Surrogate Modeling",
    "summary": "Transformer has shown state-of-the-art performance on various applications\nand has recently emerged as a promising tool for surrogate modeling of partial\ndifferential equations (PDEs). Despite the introduction of linear-complexity\nattention, applying Transformer to problems with a large number of grid points\ncan be numerically unstable and computationally expensive. In this work, we\npropose Factorized Transformer (FactFormer), which is based on an axial\nfactorized kernel integral. Concretely, we introduce a learnable projection\noperator that decomposes the input function into multiple sub-functions with\none-dimensional domain. These sub-functions are then evaluated and used to\ncompute the instance-based kernel with an axial factorized scheme. We showcase\nthat the proposed model is able to simulate 2D Kolmogorov flow on a $256\\times\n256$ grid and 3D smoke buoyancy on a $64\\times64\\times64$ grid with good\naccuracy and efficiency. The proposed factorized scheme can serve as a\ncomputationally efficient low-rank surrogate for the full attention scheme when\ndealing with multi-dimensional problems.",
    "published": "2023-05-27T19:23:00Z",
    "updated": "2023-11-03T01:32:08Z",
    "authors": [
      "Zijie Li",
      "Dule Shu",
      "Amir Barati Farimani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.01381v1",
    "title": "Implicit Memory Transformer for Computationally Efficient Simultaneous\n  Speech Translation",
    "summary": "Simultaneous speech translation is an essential communication task difficult\nfor humans whereby a translation is generated concurrently with oncoming speech\ninputs. For such a streaming task, transformers using block processing to break\nan input sequence into segments have achieved state-of-the-art performance at a\nreduced cost. Current methods to allow information to propagate across\nsegments, including left context and memory banks, have faltered as they are\nboth insufficient representations and unnecessarily expensive to compute. In\nthis paper, we propose an Implicit Memory Transformer that implicitly retains\nmemory through a new left context method, removing the need to explicitly\nrepresent memory with memory banks. We generate the left context from the\nattention output of the previous segment and include it in the keys and values\nof the current segment's attention calculation. Experiments on the MuST-C\ndataset show that the Implicit Memory Transformer provides a substantial\nspeedup on the encoder forward pass with nearly identical translation quality\nwhen compared with the state-of-the-art approach that employs both left context\nand memory banks.",
    "published": "2023-07-03T22:20:21Z",
    "updated": "2023-07-03T22:20:21Z",
    "authors": [
      "Matthew Raffel",
      "Lizhong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.05032v1",
    "title": "Unified Contrastive Fusion Transformer for Multimodal Human Action\n  Recognition",
    "summary": "Various types of sensors have been considered to develop human action\nrecognition (HAR) models. Robust HAR performance can be achieved by fusing\nmultimodal data acquired by different sensors. In this paper, we introduce a\nnew multimodal fusion architecture, referred to as Unified Contrastive Fusion\nTransformer (UCFFormer) designed to integrate data with diverse distributions\nto enhance HAR performance. Based on the embedding features extracted from each\nmodality, UCFFormer employs the Unified Transformer to capture the\ninter-dependency among embeddings in both time and modality domains. We present\nthe Factorized Time-Modality Attention to perform self-attention efficiently\nfor the Unified Transformer. UCFFormer also incorporates contrastive learning\nto reduce the discrepancy in feature distributions across various modalities,\nthus generating semantically aligned features for information fusion.\nPerformance evaluation conducted on two popular datasets, UTD-MHAD and NTU\nRGB+D, demonstrates that UCFFormer achieves state-of-the-art performance,\noutperforming competing methods by considerable margins.",
    "published": "2023-09-10T14:10:56Z",
    "updated": "2023-09-10T14:10:56Z",
    "authors": [
      "Kyoung Ok Yang",
      "Junho Koh",
      "Jun Won Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.08220v1",
    "title": "UniST: Towards Unifying Saliency Transformer for Video Saliency\n  Prediction and Detection",
    "summary": "Video saliency prediction and detection are thriving research domains that\nenable computers to simulate the distribution of visual attention akin to how\nhumans perceiving dynamic scenes. While many approaches have crafted\ntask-specific training paradigms for either video saliency prediction or video\nsalient object detection tasks, few attention has been devoted to devising a\ngeneralized saliency modeling framework that seamlessly bridges both these\ndistinct tasks. In this study, we introduce the Unified Saliency Transformer\n(UniST) framework, which comprehensively utilizes the essential attributes of\nvideo saliency prediction and video salient object detection. In addition to\nextracting representations of frame sequences, a saliency-aware transformer is\ndesigned to learn the spatio-temporal representations at progressively\nincreased resolutions, while incorporating effective cross-scale saliency\ninformation to produce a robust representation. Furthermore, a task-specific\ndecoder is proposed to perform the final prediction for each task. To the best\nof our knowledge, this is the first work that explores designing a transformer\nstructure for both saliency modeling tasks. Convincible experiments demonstrate\nthat the proposed UniST achieves superior performance across seven challenging\nbenchmarks for two tasks, and significantly outperforms the other\nstate-of-the-art methods.",
    "published": "2023-09-15T07:39:53Z",
    "updated": "2023-09-15T07:39:53Z",
    "authors": [
      "Junwen Xiong",
      "Peng Zhang",
      "Chuanyue Li",
      "Wei Huang",
      "Yufei Zha",
      "Tao You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.06365v1",
    "title": "Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity\n  Alignment",
    "summary": "Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify\nequivalent entity pairs across multi-modal knowledge graphs (MMKGs). However,\nthis task faces challenges due to the presence of different types of\ninformation, including neighboring entities, multi-modal attributes, and entity\ntypes. Directly incorporating the above information (e.g., concatenation or\nattention) can lead to an unaligned information space. To address these\nchallenges, we propose a novel MMEA transformer, called MoAlign, that\nhierarchically introduces neighbor features, multi-modal attributes, and entity\ntypes to enhance the alignment task. Taking advantage of the transformer's\nability to better integrate multiple information, we design a hierarchical\nmodifiable self-attention block in a transformer encoder to preserve the unique\nsemantics of different information. Furthermore, we design two entity-type\nprefix injection methods to integrate entity-type information using type\nprefixes, which help to restrict the global information of entities not present\nin the MMKGs. Our extensive experiments on benchmark datasets demonstrate that\nour approach outperforms strong competitors and achieves excellent entity\nalignment performance.",
    "published": "2023-10-10T07:06:06Z",
    "updated": "2023-10-10T07:06:06Z",
    "authors": [
      "Qian Li",
      "Cheng Ji",
      "Shu Guo",
      "Zhaoji Liang",
      "Lihong Wang",
      "Jianxin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.16076v1",
    "title": "Practical Computational Power of Linear Transformers and Their Recurrent\n  and Self-Referential Extensions",
    "summary": "Recent studies of the computational power of recurrent neural networks (RNNs)\nreveal a hierarchy of RNN architectures, given real-time and finite-precision\nassumptions. Here we study auto-regressive Transformers with linearised\nattention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs).\nLTs are special in the sense that they are equivalent to RNN-like sequence\nprocessors with a fixed-size state, while they can also be expressed as the\nnow-popular self-attention networks. We show that many well-known results for\nthe standard Transformer directly transfer to LTs/FWPs. Our formal language\nrecognition experiments demonstrate how recently proposed FWP extensions such\nas recurrent FWPs and self-referential weight matrices successfully overcome\ncertain limitations of the LT, e.g., allowing for generalisation on the parity\nproblem. Our code is public.",
    "published": "2023-10-24T17:17:01Z",
    "updated": "2023-10-24T17:17:01Z",
    "authors": [
      "Kazuki Irie",
      "RÃ³bert CsordÃ¡s",
      "JÃ¼rgen Schmidhuber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.19588v1",
    "title": "DPATD: Dual-Phase Audio Transformer for Denoising",
    "summary": "Recent high-performance transformer-based speech enhancement models\ndemonstrate that time domain methods could achieve similar performance as\ntime-frequency domain methods. However, time-domain speech enhancement systems\ntypically receive input audio sequences consisting of a large number of time\nsteps, making it challenging to model extremely long sequences and train models\nto perform adequately. In this paper, we utilize smaller audio chunks as input\nto achieve efficient utilization of audio information to address the above\nchallenges. We propose a dual-phase audio transformer for denoising (DPATD), a\nnovel model to organize transformer layers in a deep structure to learn clean\naudio sequences for denoising. DPATD splits the audio input into smaller\nchunks, where the input length can be proportional to the square root of the\noriginal sequence length. Our memory-compressed explainable attention is\nefficient and converges faster compared to the frequently used self-attention\nmodule. Extensive experiments demonstrate that our model outperforms\nstate-of-the-art methods.",
    "published": "2023-10-30T14:44:59Z",
    "updated": "2023-10-30T14:44:59Z",
    "authors": [
      "Junhui Li",
      "Pu Wang",
      "Jialu Li",
      "Xinzhe Wang",
      "Youshan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.09590v2",
    "title": "MARformer: An Efficient Metal Artifact Reduction Transformer for Dental\n  CBCT Images",
    "summary": "Cone Beam Computed Tomography (CBCT) plays a key role in dental diagnosis and\nsurgery. However, the metal teeth implants could bring annoying metal artifacts\nduring the CBCT imaging process, interfering diagnosis and downstream\nprocessing such as tooth segmentation. In this paper, we develop an efficient\nTransformer to perform metal artifacts reduction (MAR) from dental CBCT images.\nThe proposed MAR Transformer (MARformer) reduces computation complexity in the\nmultihead self-attention by a new Dimension-Reduced Self-Attention (DRSA)\nmodule, based on that the CBCT images have globally similar structure. A\nPatch-wise Perceptive Feed Forward Network (P2FFN) is also proposed to perceive\nlocal image information for fine-grained restoration. Experimental results on\nCBCT images with synthetic and real-world metal artifacts show that our\nMARformer is efficient and outperforms previous MAR methods and two restoration\nTransformers.",
    "published": "2023-11-16T06:02:03Z",
    "updated": "2024-04-18T08:49:03Z",
    "authors": [
      "Yuxuan Shi",
      "Jun Xu",
      "Dinggang Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.09655v3",
    "title": "Multi-View Spectrogram Transformer for Respiratory Sound Classification",
    "summary": "Deep neural networks have been applied to audio spectrograms for respiratory\nsound classification. Existing models often treat the spectrogram as a\nsynthetic image while overlooking its physical characteristics. In this paper,\na Multi-View Spectrogram Transformer (MVST) is proposed to embed different\nviews of time-frequency characteristics into the vision transformer.\nSpecifically, the proposed MVST splits the mel-spectrogram into different sized\npatches, representing the multi-view acoustic elements of a respiratory sound.\nThese patches and positional embeddings are then fed into transformer encoders\nto extract the attentional information among patches through a self-attention\nmechanism. Finally, a gated fusion scheme is designed to automatically weigh\nthe multi-view features to highlight the best one in a specific scenario.\nExperimental results on the ICBHI dataset demonstrate that the proposed MVST\nsignificantly outperforms state-of-the-art methods for classifying respiratory\nsounds.",
    "published": "2023-11-16T08:17:02Z",
    "updated": "2024-05-30T05:42:03Z",
    "authors": [
      "Wentao He",
      "Yuchen Yan",
      "Jianfeng Ren",
      "Ruibin Bai",
      "Xudong Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.16826v4",
    "title": "A New Perspective on Speaker Verification: Joint Modeling with DFSMN and\n  Transformer",
    "summary": "Speaker verification is to judge the similarity between two unknown voices in\nan open set, where the ideal speaker embedding should be able to condense\ndiscriminant information into a compact utterance-level representation that has\nsmall intra-speaker distances and large inter-speaker distances. We propose\nVoice Transformer (VOT), a novel model for speaker verification, which\nintegrates parallel transformers at multiple scales. A deep feedforward\nsequential memory network (DFSMN) is incorporated into the attention part of\nthese transformers to increase feature granularity. The attentive statistics\npooling layer is added to focus on important frames and form utterance-level\nfeatures. We propose Additive Angular Margin Focal Loss (AAMF) to address the\nhard samples problem. We evaluate the proposed approach on the VoxCeleb1 and\nCN-Celeb2 datasets, demonstrating that VOT surpasses most mainstream models.\nThe code is available on\nGitHub\\footnote{\\url{https://github.com/luckyerr/Voice-Transformer_Speaker-Verification}}.",
    "published": "2023-12-28T04:55:59Z",
    "updated": "2024-09-08T07:13:26Z",
    "authors": [
      "Hongyu Wang",
      "Hui Li",
      "Bo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.10119v4",
    "title": "Towards Principled Graph Transformers",
    "summary": "Graph learning architectures based on the k-dimensional Weisfeiler-Leman\n(k-WL) hierarchy offer a theoretically well-understood expressive power.\nHowever, such architectures often fail to deliver solid predictive performance\non real-world tasks, limiting their practical impact. In contrast, global\nattention-based models such as graph transformers demonstrate strong\nperformance in practice, but comparing their expressive power with the k-WL\nhierarchy remains challenging, particularly since these architectures rely on\npositional or structural encodings for their expressivity and predictive\nperformance. To address this, we show that the recently proposed Edge\nTransformer, a global attention model operating on node pairs instead of nodes,\nhas at least 3-WL expressive power. Empirically, we demonstrate that the Edge\nTransformer surpasses other theoretically aligned architectures regarding\npredictive performance while not relying on positional or structural encodings.\nOur code is available at https://github.com/luis-mueller/towards-principled-gts",
    "published": "2024-01-18T16:50:55Z",
    "updated": "2024-11-08T10:06:06Z",
    "authors": [
      "Luis MÃ¼ller",
      "Daniel Kusuma",
      "Blai Bonet",
      "Christopher Morris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.11671v2",
    "title": "RTA-Former: Reverse Transformer Attention for Polyp Segmentation",
    "summary": "Polyp segmentation is a key aspect of colorectal cancer prevention, enabling\nearly detection and guiding subsequent treatments. Intelligent diagnostic\ntools, including deep learning solutions, are widely explored to streamline and\npotentially automate this process. However, even with many powerful network\narchitectures, there still comes the problem of producing accurate edge\nsegmentation. In this paper, we introduce a novel network, namely RTA-Former,\nthat employs a transformer model as the encoder backbone and innovatively\nadapts Reverse Attention (RA) with a transformer stage in the decoder for\nenhanced edge segmentation. The results of the experiments illustrate that\nRTA-Former achieves state-of-the-art (SOTA) performance in five polyp\nsegmentation datasets. The strong capability of RTA-Former holds promise in\nimproving the accuracy of Transformer-based polyp segmentation, potentially\nleading to better clinical decisions and patient outcomes. Our code is publicly\navailable on GitHub.",
    "published": "2024-01-22T03:09:00Z",
    "updated": "2024-04-28T22:21:56Z",
    "authors": [
      "Zhikai Li",
      "Murong Yi",
      "Ali Uneri",
      "Sihan Niu",
      "Craig Jones"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.00522v6",
    "title": "Understanding the Expressive Power and Mechanisms of Transformer for\n  Sequence Modeling",
    "summary": "We conduct a systematic study of the approximation properties of Transformer\nfor sequence modeling with long, sparse and complicated memory. We investigate\nthe mechanisms through which different components of Transformer, such as the\ndot-product self-attention, positional encoding and feed-forward layer, affect\nits expressive power, and we study their combined effects through establishing\nexplicit approximation rates. Our study reveals the roles of critical\nparameters in the Transformer, such as the number of layers and the number of\nattention heads. These theoretical insights are validated experimentally and\noffer natural suggestions for alternative architectures.",
    "published": "2024-02-01T11:43:13Z",
    "updated": "2024-10-30T08:47:36Z",
    "authors": [
      "Mingze Wang",
      "Weinan E"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.02634v1",
    "title": "Key-Graph Transformer for Image Restoration",
    "summary": "While it is crucial to capture global information for effective image\nrestoration (IR), integrating such cues into transformer-based methods becomes\ncomputationally expensive, especially with high input resolution. Furthermore,\nthe self-attention mechanism in transformers is prone to considering\nunnecessary global cues from unrelated objects or regions, introducing\ncomputational inefficiencies. In response to these challenges, we introduce the\nKey-Graph Transformer (KGT) in this paper. Specifically, KGT views patch\nfeatures as graph nodes. The proposed Key-Graph Constructor efficiently forms a\nsparse yet representative Key-Graph by selectively connecting essential nodes\ninstead of all the nodes. Then the proposed Key-Graph Attention is conducted\nunder the guidance of the Key-Graph only among selected nodes with linear\ncomputational complexity within each window. Extensive experiments across 6 IR\ntasks confirm the proposed KGT's state-of-the-art performance, showcasing\nadvancements both quantitatively and qualitatively.",
    "published": "2024-02-04T23:00:24Z",
    "updated": "2024-02-04T23:00:24Z",
    "authors": [
      "Bin Ren",
      "Yawei Li",
      "Jingyun Liang",
      "Rakesh Ranjan",
      "Mengyuan Liu",
      "Rita Cucchiara",
      "Luc Van Gool",
      "Nicu Sebe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.08450v2",
    "title": "Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph\n  Products",
    "summary": "In the realm of Graph Neural Networks (GNNs), two exciting research\ndirections have recently emerged: Subgraph GNNs and Graph Transformers. In this\npaper, we propose an architecture that integrates both approaches, dubbed\nSubgraphormer, which combines the enhanced expressive power, message-passing\nmechanisms, and aggregation schemes from Subgraph GNNs with attention and\npositional encodings, arguably the most important components in Graph\nTransformers. Our method is based on an intriguing new connection we reveal\nbetween Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be\nformulated as Message Passing Neural Networks (MPNNs) operating on a product of\nthe graph with itself. We use this formulation to design our architecture:\nfirst, we devise an attention mechanism based on the connectivity of the\nproduct graph. Following this, we propose a novel and efficient positional\nencoding scheme for Subgraph GNNs, which we derive as a positional encoding for\nthe product graph. Our experimental results demonstrate significant performance\nimprovements over both Subgraph GNNs and Graph Transformers on a wide range of\ndatasets.",
    "published": "2024-02-13T13:37:13Z",
    "updated": "2024-05-28T09:18:34Z",
    "authors": [
      "Guy Bar-Shalom",
      "Beatrice Bevilacqua",
      "Haggai Maron"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.04180v2",
    "title": "RATSF: Empowering Customer Service Volume Management through\n  Retrieval-Augmented Time-Series Forecasting",
    "summary": "An efficient customer service management system hinges on precise forecasting\nof service volume. In this scenario, where data non-stationarity is pronounced,\nsuccessful forecasting heavily relies on identifying and leveraging similar\nhistorical data rather than merely summarizing periodic patterns. Existing\nmodels based on RNN or Transformer architectures may struggle with this\nflexible and effective utilization. To tackle this challenge, we initially\ndeveloped the Time Series Knowledge Base (TSKB) with an advanced indexing\nsystem for efficient historical data retrieval. We also developed the Retrieval\nAugmented Cross-Attention (RACA) module, a variant of the cross-attention\nmechanism within Transformer's decoder layers, designed to be seamlessly\nintegrated into the vanilla Transformer architecture to assimilate key\nhistorical data segments. The synergy between TSKB and RACA forms the backbone\nof our Retrieval-Augmented Time Series Forecasting (RATSF) framework. Based on\nthe above two components, RATSF not only significantly enhances performance in\nthe context of Fliggy hotel service volume forecasting but also adapts flexibly\nto various scenarios and integrates with a multitude of Transformer variants\nfor time-series forecasting. Extensive experimentation has validated the\neffectiveness and generalizability of this system design across multiple\ndiverse contexts.",
    "published": "2024-03-07T03:23:13Z",
    "updated": "2024-06-16T15:59:13Z",
    "authors": [
      "Tianfeng Wang",
      "Gaojie Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.16860v1",
    "title": "CipherFormer: Efficient Transformer Private Inference with Low Round\n  Complexity",
    "summary": "There is a growing trend to outsource the inference task of large transformer\nmodels to cloud servers. However, this poses a severe threat to users' private\ndata as they are exposed to cloud servers after uploading. Although several\nworks attempted to provide private inference for transformer models, their\nhundreds of communication rounds limit the application scenarios. Motivated by\nthe desire to minimize round complexity, we propose CipherFormer, a novel\ntransformer private inference scheme using homomorphic encryption and garbled\ncircuits. We present a protocol for quickly computing homomorphic matrix\nmultiplications. We then modify the attention mechanism and design the\ncorresponding garbled circuits. Furthermore, we show how to use a lightweight\nattention mechanism and mixed-bitwidth to reduce the inference latency while\nmaintaining accuracy. In comparison with an advanced homomorphic encryption\nscheme on text classification tasks, our model improves accuracy by 3% to 11%\nwhile performing private inference with a 7.7x-11.9x speedup.",
    "published": "2024-03-25T15:24:57Z",
    "updated": "2024-03-25T15:24:57Z",
    "authors": [
      "Weize Wang",
      "Yi Kuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.06075v2",
    "title": "LIPT: Latency-aware Image Processing Transformer",
    "summary": "Transformer is leading a trend in the field of image processing. Despite the\ngreat success that existing lightweight image processing transformers have\nachieved, they are tailored to FLOPs or parameters reduction, rather than\npractical inference acceleration. In this paper, we present a latency-aware\nimage processing transformer, termed LIPT. We devise the low-latency proportion\nLIPT block that substitutes memory-intensive operators with the combination of\nself-attention and convolutions to achieve practical speedup. Specifically, we\npropose a novel non-volatile sparse masking self-attention (NVSM-SA) that\nutilizes a pre-computing sparse mask to capture contextual information from a\nlarger window with no extra computation overload. Besides, a high-frequency\nreparameterization module (HRM) is proposed to make LIPT block\nreparameterization friendly, which improves the model's detail reconstruction\ncapability. Extensive experiments on multiple image processing tasks (e.g.,\nimage super-resolution (SR), JPEG artifact reduction, and image denoising)\ndemonstrate the superiority of LIPT on both latency and PSNR. LIPT achieves\nreal-time GPU inference with state-of-the-art performance on multiple image SR\nbenchmarks.",
    "published": "2024-04-09T07:25:30Z",
    "updated": "2024-04-29T03:21:49Z",
    "authors": [
      "Junbo Qiao",
      "Wei Li",
      "Haizhen Xie",
      "Hanting Chen",
      "Yunshuai Zhou",
      "Zhijun Tu",
      "Jie Hu",
      "Shaohui Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.04009v1",
    "title": "Structured Click Control in Transformer-based Interactive Segmentation",
    "summary": "Click-point-based interactive segmentation has received widespread attention\ndue to its efficiency. However, it's hard for existing algorithms to obtain\nprecise and robust responses after multiple clicks. In this case, the\nsegmentation results tend to have little change or are even worse than before.\nTo improve the robustness of the response, we propose a structured click intent\nmodel based on graph neural networks, which adaptively obtains graph nodes via\nthe global similarity of user-clicked Transformer tokens. Then the graph nodes\nwill be aggregated to obtain structured interaction features. Finally, the dual\ncross-attention will be used to inject structured interaction features into\nvision Transformer features, thereby enhancing the control of clicks over\nsegmentation results. Extensive experiments demonstrated the proposed algorithm\ncan serve as a general structure in improving Transformer-based interactive\nsegmenta?tion performance. The code and data will be released at\nhttps://github.com/hahamyt/scc.",
    "published": "2024-05-07T04:57:25Z",
    "updated": "2024-05-07T04:57:25Z",
    "authors": [
      "Long Xu",
      "Yongquan Chen",
      "Rui Huang",
      "Feng Wu",
      "Shiwu Lai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.15780v1",
    "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on\n  Frontier",
    "summary": "Vision Transformers (ViTs) are pivotal for foundational models in scientific\nimagery, including Earth science applications, due to their capability to\nprocess large sequence lengths. While transformers for text has inspired\nscaling sequence lengths in ViTs, yet adapting these for ViTs introduces unique\nchallenges. We develop distributed sequence parallelism for ViTs, enabling them\nto handle up to 1M tokens. Our approach, leveraging DeepSpeed-Ulysses and\nLong-Sequence-Segmentation with model sharding, is the first to apply sequence\nparallelism in ViT training, achieving a 94% batch scaling efficiency on 2,048\nAMD-MI250X GPUs. Evaluating sequence parallelism in ViTs, particularly in\nmodels up to 10B parameters, highlighted substantial bottlenecks. We countered\nthese with hybrid sequence, pipeline, tensor parallelism, and flash attention\nstrategies, to scale beyond single GPU memory limits. Our method significantly\nenhances climate modeling accuracy by 20% in temperature predictions, marking\nthe first training of a transformer model on a full-attention matrix over 188K\nsequence length.",
    "published": "2024-04-17T19:57:07Z",
    "updated": "2024-04-17T19:57:07Z",
    "authors": [
      "Aristeidis Tsaris",
      "Chengming Zhang",
      "Xiao Wang",
      "Junqi Yin",
      "Siyan Liu",
      "Moetasim Ashfaq",
      "Ming Fan",
      "Jong Youl Choi",
      "Mohamed Wahib",
      "Dan Lu",
      "Prasanna Balaprakash",
      "Feiyi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.02847v2",
    "title": "Exact Conversion of In-Context Learning to Model Weights in\n  Linearized-Attention Transformers",
    "summary": "In-Context Learning (ICL) has been a powerful emergent property of large\nlanguage models that has attracted increasing attention in recent years. In\ncontrast to regular gradient-based learning, ICL is highly interpretable and\ndoes not require parameter updates. In this paper, we show that, for linearized\ntransformer networks, ICL can be made explicit and permanent through the\ninclusion of bias terms. We mathematically demonstrate the equivalence between\na model with ICL demonstration prompts and the same model with the additional\nbias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive\nmanner. Existing methods are not exact and require expensive parameter updates.\nWe demonstrate the efficacy of our approach through experiments that show the\nexact incorporation of ICL tokens into a linear transformer. We further suggest\nhow our method can be adapted to achieve cheap approximate conversion of ICL\ntokens, even in regular transformer networks that are not linearized. Our\nexperiments on GPT-2 show that, even though the conversion is only approximate,\nthe model still gains valuable context from the included bias terms.",
    "published": "2024-06-05T01:47:40Z",
    "updated": "2024-06-06T06:15:29Z",
    "authors": [
      "Brian K Chen",
      "Tianyang Hu",
      "Hui Jin",
      "Hwee Kuan Lee",
      "Kenji Kawaguchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.06069v1",
    "title": "PointABM:Integrating Bidirectional State Space Model with Multi-Head\n  Self-Attention for Point Cloud Analysis",
    "summary": "Mamba, based on state space model (SSM) with its linear complexity and great\nsuccess in classification provide its superiority in 3D point cloud analysis.\nPrior to that, Transformer has emerged as one of the most prominent and\nsuccessful architectures for point cloud analysis. We present PointABM, a\nhybrid model that integrates the Mamba and Transformer architectures for\nenhancing local feature to improve performance of 3D point cloud analysis. In\norder to enhance the extraction of global features, we introduce a\nbidirectional SSM (bi-SSM) framework, which comprises both a traditional token\nforward SSM and an innovative backward SSM. To enhance the bi-SSM's capability\nof capturing more comprehensive features without disrupting the sequence\nrelationships required by the bidirectional Mamba, we introduce Transformer,\nutilizing its self-attention mechanism to process point clouds. Extensive\nexperimental results demonstrate that integrating Mamba with Transformer\nsignificantly enhance the model's capability to analysis 3D point cloud.",
    "published": "2024-06-10T07:24:22Z",
    "updated": "2024-06-10T07:24:22Z",
    "authors": [
      "Jia-wei Chen",
      "Yu-jie Xiong",
      "Yong-bin Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.09297v3",
    "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
    "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
    "published": "2024-06-13T16:33:44Z",
    "updated": "2024-10-15T08:45:18Z",
    "authors": [
      "Zayd Muhammad Kawakibi Zuhri",
      "Muhammad Farid Adilazuarda",
      "Ayu Purwarianti",
      "Alham Fikri Aji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.04024v1",
    "title": "Adaptive Step-size Perception Unfolding Network with Non-local Hybrid\n  Attention for Hyperspectral Image Reconstruction",
    "summary": "Deep unfolding methods and transformer architecture have recently shown\npromising results in hyperspectral image (HSI) reconstruction. However, there\nstill exist two issues: (1) in the data subproblem, most methods represents the\nstepsize utilizing a learnable parameter. Nevertheless, for different spectral\nchannel, error between features and ground truth is unequal. (2) Transformer\nstruggles to balance receptive field size with pixel-wise detail information.\nTo overcome the aforementioned drawbacks, We proposed an adaptive step-size\nperception unfolding network (ASPUN), a deep unfolding network based on FISTA\nalgorithm, which uses an adaptive step-size perception module to estimate the\nupdate step-size of each spectral channel. In addition, we design a Non-local\nHybrid Attention Transformer(NHAT) module for fully leveraging the receptive\nfield advantage of transformer. By plugging the NLHA into the Non-local\nInformation Aggregation (NLIA) module, the unfolding network can achieve better\nreconstruction results. Experimental results show that our ASPUN is superior to\nthe existing SOTA algorithms and achieves the best performance.",
    "published": "2024-07-04T16:09:52Z",
    "updated": "2024-07-04T16:09:52Z",
    "authors": [
      "Yanan Yang",
      "Like Xin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.11542v3",
    "title": "Counting in Small Transformers: The Delicate Interplay between Attention\n  and Feed-Forward Layers",
    "summary": "Next to scaling considerations, architectural design choices profoundly shape\nthe solution space of transformers. In this work, we analyze the solutions\nsimple transformer blocks implement when tackling the histogram task: counting\nitems in sequences. Despite its simplicity, this task reveals a complex\ninterplay between predictive performance, vocabulary and embedding sizes,\ntoken-mixing mechanisms, and feed-forward layer capacity. We identify two\ntheoretical counting strategies transformers adopt, relation-based and\ninventory-based counting, each defining distinct learning regimes for the task.\nThese strategies dictate how functionality is distributed between attention and\nfeed-forward layers. We further show that adding softmax and\nbeginning-of-sequence tokens allow for more robustness when embedding\ndimensions are comparatively small. Empirical introspection of trained models\nclosely confirms both the learning regimes of the various architectures and the\nformation of these strategies during training. We demonstrate how a basic task\nthat requires only aggregation and selection is significantly impacted by minor\ndesign changes.",
    "published": "2024-07-16T09:48:10Z",
    "updated": "2025-05-20T11:40:37Z",
    "authors": [
      "Freya Behrens",
      "Luca Biggio",
      "Lenka ZdeborovÃ¡"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.12077v1",
    "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
    "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
    "published": "2024-07-16T18:00:00Z",
    "updated": "2024-07-16T18:00:00Z",
    "authors": [
      "Daniel Goldstein",
      "Fares Obeid",
      "Eric Alcaide",
      "Guangyu Song",
      "Eugene Cheah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.15425v2",
    "title": "Empirical Capacity Model for Self-Attention Neural Networks",
    "summary": "Large pretrained self-attention neural networks, or transformers, have been\nvery successful in various tasks recently. The performance of a model on a\ngiven task depends on its ability to memorize and generalize the training data.\nLarge transformer models, which may have billions of parameters, in theory have\na huge capacity to memorize content. However, the current algorithms for the\noptimization fall short of the theoretical capacity, and the capacity is also\nhighly dependent on the content. In this paper, we focus on the memory capacity\nof these models obtained using common training algorithms and synthetic\ntraining data. Based on the results, we derive an empirical capacity model\n(ECM) for a generic transformer. The ECM can be used to design task-specific\ntransformer models with an optimal number of parameters in cases where the\ntarget memorization capability of the task can be defined.",
    "published": "2024-07-22T07:02:15Z",
    "updated": "2024-07-31T10:27:37Z",
    "authors": [
      "Aki HÃ¤rmÃ¤",
      "Marcin Pietrasik",
      "Anna Wilbik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.03440v1",
    "title": "TF-Locoformer: Transformer with Local Modeling by Convolution for Speech\n  Separation and Enhancement",
    "summary": "Time-frequency (TF) domain dual-path models achieve high-fidelity speech\nseparation. While some previous state-of-the-art (SoTA) models rely on RNNs,\nthis reliance means they lack the parallelizability, scalability, and\nversatility of Transformer blocks. Given the wide-ranging success of pure\nTransformer-based architectures in other fields, in this work we focus on\nremoving the RNN from TF-domain dual-path models, while maintaining SoTA\nperformance. This work presents TF-Locoformer, a Transformer-based model with\nLOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with\nconvolution layers, instead of linear layers, to capture local information,\nletting the self-attention focus on capturing global patterns. We place two\nsuch FFNs before and after self-attention to enhance the local-modeling\ncapability. We also introduce a novel normalization for TF-domain dual-path\nmodels. Experiments on separation and enhancement datasets show that the\nproposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free\narchitecture.",
    "published": "2024-08-06T20:30:14Z",
    "updated": "2024-08-06T20:30:14Z",
    "authors": [
      "Kohei Saijo",
      "Gordon Wichern",
      "FranÃ§ois G. Germain",
      "Zexu Pan",
      "Jonathan Le Roux"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.13233v2",
    "title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear\n  Time",
    "summary": "The computational complexity of the self-attention mechanism in popular\ntransformer architectures poses significant challenges for training and\ninference, and becomes the bottleneck for long inputs. Is it possible to\nsignificantly reduce the quadratic time complexity of computing the gradients\nin multi-layer transformer models? This paper proves that a novel fast\napproximation method can calculate the gradients in almost linear time\n$n^{1+o(1)}$ where $n$ is the input sequence length, while it maintains a\npolynomially small approximation error $1 / \\mathrm{poly}(n)$ across the entire\nmodel. Our theory holds for general loss functions and when the multi-layer\ntransformer model contains many practical sub-modules, such as residual\nconnection, casual mask, and multi-head attention. By improving the efficiency\nof gradient computation, we hope that this work will facilitate more effective\ntraining and deployment of long-context language models based on our\ntheoretical results.",
    "published": "2024-08-23T17:16:43Z",
    "updated": "2024-10-15T04:11:19Z",
    "authors": [
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Yufa Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.13461v1",
    "title": "Probing the Robustness of Vision-Language Pretrained Models: A\n  Multimodal Adversarial Attack Approach",
    "summary": "Vision-language pretraining (VLP) with transformers has demonstrated\nexceptional performance across numerous multimodal tasks. However, the\nadversarial robustness of these models has not been thoroughly investigated.\nExisting multimodal attack methods have largely overlooked cross-modal\ninteractions between visual and textual modalities, particularly in the context\nof cross-attention mechanisms. In this paper, we study the adversarial\nvulnerability of recent VLP transformers and design a novel Joint Multimodal\nTransformer Feature Attack (JMTFA) that concurrently introduces adversarial\nperturbations in both visual and textual modalities under white-box settings.\nJMTFA strategically targets attention relevance scores to disrupt important\nfeatures within each modality, generating adversarial samples by fusing\nperturbations and leading to erroneous model predictions. Experimental results\nindicate that the proposed approach achieves high attack success rates on\nvision-language understanding and reasoning downstream tasks compared to\nexisting baselines. Notably, our findings reveal that the textual modality\nsignificantly influences the complex fusion processes within VLP transformers.\nMoreover, we observe no apparent relationship between model size and\nadversarial robustness under our proposed attacks. These insights emphasize a\nnew dimension of adversarial robustness and underscore potential risks in the\nreliable deployment of multimodal AI systems.",
    "published": "2024-08-24T04:31:37Z",
    "updated": "2024-08-24T04:31:37Z",
    "authors": [
      "Jiwei Guan",
      "Tianyu Ding",
      "Longbing Cao",
      "Lei Pan",
      "Chen Wang",
      "Xi Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.03223v1",
    "title": "Why mamba is effective? Exploit Linear Transformer-Mamba Network for\n  Multi-Modality Image Fusion",
    "summary": "Multi-modality image fusion aims to integrate the merits of images from\ndifferent sources and render high-quality fusion images. However, existing\nfeature extraction and fusion methods are either constrained by inherent local\nreduction bias and static parameters during inference (CNN) or limited by\nquadratic computational complexity (Transformers), and cannot effectively\nextract and fuse features. To solve this problem, we propose a dual-branch\nimage fusion network called Tmamba. It consists of linear Transformer and\nMamba, which has global modeling capabilities while maintaining linear\ncomplexity. Due to the difference between the Transformer and Mamba structures,\nthe features extracted by the two branches carry channel and position\ninformation respectively. T-M interaction structure is designed between the two\nbranches, using global learnable parameters and convolutional layers to\ntransfer position and channel information respectively. We further propose\ncross-modal interaction at the attention level to obtain cross-modal attention.\nExperiments show that our Tmamba achieves promising results in multiple fusion\ntasks, including infrared-visible image fusion and medical image fusion. Code\nwith checkpoints will be available after the peer-review process.",
    "published": "2024-09-05T03:42:11Z",
    "updated": "2024-09-05T03:42:11Z",
    "authors": [
      "Chenguang Zhu",
      "Shan Gao",
      "Huafeng Chen",
      "Guangqian Guo",
      "Chaowei Wang",
      "Yaoxing Wang",
      "Chen Shu Lei",
      "Quanjiang Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.16280v1",
    "title": "MonoFormer: One Transformer for Both Diffusion and Autoregression",
    "summary": "Most existing multimodality methods use separate backbones for\nautoregression-based discrete text generation and diffusion-based continuous\nvisual generation, or the same backbone by discretizing the visual data to use\nautoregression for both text and visual generation. In this paper, we propose\nto study a simple idea: share one transformer for both autoregression and\ndiffusion. The feasibility comes from two main aspects: (i) Transformer is\nsuccessfully applied to diffusion for visual generation, and (ii) transformer\ntraining for autoregression and diffusion is very similar, and the difference\nmerely lies in that diffusion uses bidirectional attention mask and\nautoregression uses causal attention mask. Experimental results show that our\napproach achieves comparable image generation performance to current\nstate-of-the-art methods as well as maintains the text generation capability.\nThe project is publicly available at https://monoformer.github.io/.",
    "published": "2024-09-24T17:51:04Z",
    "updated": "2024-09-24T17:51:04Z",
    "authors": [
      "Chuyang Zhao",
      "Yuxing Song",
      "Wenhao Wang",
      "Haocheng Feng",
      "Errui Ding",
      "Yifan Sun",
      "Xinyan Xiao",
      "Jingdong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.13846v2",
    "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
    "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
    "published": "2024-10-17T17:58:14Z",
    "updated": "2025-02-04T13:45:37Z",
    "authors": [
      "Xuan Zhang",
      "Fengzhuo Zhang",
      "Cunxiao Du",
      "Chao Du",
      "Tianyu Pang",
      "Wei Gao",
      "Min Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.14183v2",
    "title": "In-context Learning for Mixture of Linear Regressions: Existence,\n  Generalization and Training Dynamics",
    "summary": "We investigate the in-context learning capabilities of transformers for the\n$d$-dimensional mixture of linear regression model, providing theoretical\ninsights into their existence, generalization bounds, and training dynamics.\nSpecifically, we prove that there exists a transformer capable of achieving a\nprediction error of order $\\mathcal{O}(\\sqrt{d/n})$ with high probability,\nwhere $n$ represents the training prompt size in the high signal-to-noise ratio\n(SNR) regime. Moreover, we derive in-context excess risk bounds of order\n$\\mathcal{O}(L/\\sqrt{B})$ for the case of two mixtures, where $B$ denotes the\nnumber of training prompts, and $L$ represents the number of attention layers.\nThe dependence of $L$ on the SNR is explicitly characterized, differing between\nlow and high SNR settings. We further analyze the training dynamics of\ntransformers with single linear self-attention layers, demonstrating that, with\nappropriately initialized parameters, gradient flow optimization over the\npopulation mean square loss converges to a global optimum. Extensive\nsimulations suggest that transformers perform well on this task, potentially\noutperforming other baselines, such as the Expectation-Maximization algorithm.",
    "published": "2024-10-18T05:28:47Z",
    "updated": "2025-02-09T03:40:52Z",
    "authors": [
      "Yanhao Jin",
      "Krishnakumar Balasubramanian",
      "Lifeng Lai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.15899v2",
    "title": "On the Design and Performance of Machine Learning Based Error Correcting\n  Decoders",
    "summary": "This paper analyzes the design and competitiveness of four neural network\n(NN) architectures recently proposed as decoders for forward error correction\n(FEC) codes. We first consider the so-called single-label neural network (SLNN)\nand the multi-label neural network (MLNN) decoders which have been reported to\nachieve near maximum likelihood (ML) performance. Here, we show analytically\nthat SLNN and MLNN decoders can always achieve ML performance, regardless of\nthe code dimensions -- although at the cost of computational complexity -- and\nno training is in fact required. We then turn our attention to two\ntransformer-based decoders: the error correction code transformer (ECCT) and\nthe cross-attention message passing transformer (CrossMPT). We compare their\nperformance against traditional decoders, and show that ordered statistics\ndecoding outperforms these transformer-based decoders. The results in this\npaper cast serious doubts on the application of NN-based FEC decoders in the\nshort and medium block length regime.",
    "published": "2024-10-21T11:23:23Z",
    "updated": "2024-10-23T07:05:26Z",
    "authors": [
      "Yuncheng Yuan",
      "PÃ©ter Scheepers",
      "Lydia Tasiou",
      "Yunus Can GÃ¼ltekin",
      "Federico Corradi",
      "Alex Alvarado"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.19973v3",
    "title": "Multi-Class Abnormality Classification Task in Video Capsule Endoscopy",
    "summary": "In this work for Capsule Vision Challenge 2024, we addressed the challenge of\nmulticlass anomaly classification in video capsule Endoscopy (VCE)[1] with a\nvariety of deep learning models, ranging from custom CNNs to advanced\ntransformer architectures. The purpose is to correctly classify diverse\ngastrointestinal disorders, which is critical for increasing diagnostic\nefficiency in clinical settings. We started with a baseline CNN model and\nimproved performance with ResNet[2] for better feature extraction, followed by\nVision Transformer (ViT)[3] to capture global dependencies. We further improve\nthe results by using Multiscale Vision Transformer (MViT)[4] for improved\nhierarchical feature extraction, while Dual Attention Vision Transformer\n(DaViT) [5] delivered best results by combining spatial and channel attention\nmethods. Our best balanced accuracy on validation set [6] was 0.8592 and Mean\nAUC was 0.9932. This methodology enabled us to improve model accuracy across a\nwide range of criteria, greatly surpassing all other methods.Additionally, our\nteam capsule commandos achieved 7th place ranking with a test set[7]\nperformance of Mean AUC: 0.7314 and balanced accuracy: 0.3235",
    "published": "2024-10-25T21:22:52Z",
    "updated": "2024-12-03T14:54:39Z",
    "authors": [
      "Dev Rishi Verma",
      "Vibhor Saxena",
      "Dhruv Sharma",
      "Arpan Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.04861v1",
    "title": "High Entropy Alloy property predictions using Transformer-based language\n  model",
    "summary": "This study introduces a language transformer-based machine learning model to\npredict key mechanical properties of high-entropy alloys (HEAs), addressing the\nchallenges due to their complex, multi-principal element compositions and\nlimited experimental data. By pre-training the transformer on extensive\nsynthetic materials data and fine-tuning it with specific HEA datasets, the\nmodel effectively captures intricate elemental interactions through\nself-attention mechanisms. This approach mitigates data scarcity issues via\ntransfer learning, enhancing predictive accuracy for properties like elongation\n(%) and ultimate tensile strength (UTS) compared to traditional regression\nmodels such as Random Forests and Gaussian Processes. The model's\ninterpretability is enhanced by visualizing attention weights, revealing\nsignificant elemental relationships that align with known metallurgical\nprinciples. This work demonstrates the potential of transformer models to\naccelerate materials discovery and optimization, enabling accurate property\npredictions, thereby advancing the field of materials informatics.",
    "published": "2024-11-07T16:49:37Z",
    "updated": "2024-11-07T16:49:37Z",
    "authors": [
      "Spyros Kamnis",
      "Konstantinos Delibasis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.08701v2",
    "title": "TRACE: Transformer-based Risk Assessment for Clinical Evaluation",
    "summary": "We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation),\na novel method for clinical risk assessment based on clinical data, leveraging\nthe self-attention mechanism for enhanced feature interaction and result\ninterpretation. Our approach is able to handle different data modalities,\nincluding continuous, categorical and multiple-choice (checkbox) attributes.\nThe proposed architecture features a shared representation of the clinical data\nobtained by integrating specialized embeddings of each data modality, enabling\nthe detection of high-risk individuals using Transformer encoder layers. To\nassess the effectiveness of the proposed method, a strong baseline based on\nnon-negative multi-layer perceptrons (MLPs) is introduced. The proposed method\noutperforms various baselines widely used in the domain of clinical risk\nassessment, while effectively handling missing values. In terms of\nexplainability, our Transformer-based method offers easily interpretable\nresults via attention weights, further enhancing the clinicians'\ndecision-making process.",
    "published": "2024-11-13T15:42:28Z",
    "updated": "2025-05-22T15:19:34Z",
    "authors": [
      "Dionysis Christopoulos",
      "Sotiris Spanos",
      "Valsamis Ntouskos",
      "Konstantinos Karantzalos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.13520v1",
    "title": "Quantum Attention for Vision Transformers in High Energy Physics",
    "summary": "We present a novel hybrid quantum-classical vision transformer architecture\nincorporating quantum orthogonal neural networks (QONNs) to enhance performance\nand computational efficiency in high-energy physics applications. Building on\nadvancements in quantum vision transformers, our approach addresses limitations\nof prior models by leveraging the inherent advantages of QONNs, including\nstability and efficient parameterization in high-dimensional spaces. We\nevaluate the proposed architecture using multi-detector jet images from CMS\nOpen Data, focusing on the task of distinguishing quark-initiated from\ngluon-initiated jets. The results indicate that embedding quantum orthogonal\ntransformations within the attention mechanism can provide robust performance\nwhile offering promising scalability for machine learning challenges associated\nwith the upcoming High Luminosity Large Hadron Collider. This work highlights\nthe potential of quantum-enhanced models to address the computational demands\nof next-generation particle physics experiments.",
    "published": "2024-11-20T18:11:17Z",
    "updated": "2024-11-20T18:11:17Z",
    "authors": [
      "Alessandro Tesi",
      "Gopal Ramesh Dahale",
      "Sergei Gleyzer",
      "Kyoungchul Kong",
      "Tom Magorsch",
      "Konstantin T. Matchev",
      "Katia Matcheva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.19267v1",
    "title": "Transformer-Based Financial Fraud Detection with Cloud-Optimized\n  Real-Time Streaming",
    "summary": "As the financial industry becomes more interconnected and reliant on digital\nsystems, fraud detection systems must evolve to meet growing threats.\nCloud-enabled Transformer models present a transformative opportunity to\naddress these challenges. By leveraging the scalability, flexibility, and\nadvanced AI capabilities of cloud platforms, companies can deploy fraud\ndetection solutions that adapt to real-time data patterns and proactively\nrespond to evolving threats. Using the Graph self-attention Transformer neural\nnetwork module, we can directly excavate gang fraud features from the\ntransaction network without constructing complicated feature engineering.\nFinally, the fraud prediction network is combined to optimize the topological\npattern and the temporal transaction pattern to realize the high-precision\ndetection of fraudulent transactions. The results of antifraud experiments on\ncredit card transaction data show that the proposed model outperforms the 7\nbaseline models on all evaluation indicators: In the transaction fraud\ndetection task, the average accuracy (AP) increased by 20% and the area under\nthe ROC curve (AUC) increased by 2.7% on average compared with the benchmark\ngraph attention neural network (GAT), which verified the effectiveness of the\nproposed model in the detection of credit card fraud transactions.",
    "published": "2025-01-31T16:27:58Z",
    "updated": "2025-01-31T16:27:58Z",
    "authors": [
      "Tingting Deng",
      "Shuochen Bi",
      "Jue Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.02393v3",
    "title": "Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention\n  Transformers",
    "summary": "Chain-of-thought reasoning and scratchpads have emerged as critical tools for\nenhancing the computational capabilities of transformers. While theoretical\nresults show that polynomial-length scratchpads can extend transformers'\nexpressivity from $TC^0$ to $PTIME$, their required length remains poorly\nunderstood. Empirical evidence even suggests that transformers need scratchpads\neven for many problems in $TC^0$, such as Parity or Multiplication, challenging\noptimistic bounds derived from circuit complexity. In this work, we initiate\nthe study of systematic lower bounds for the number of chain-of-thought steps\nacross different algorithmic problems, in the hard-attention regime. We study a\nvariety of algorithmic problems, and provide bounds that are tight up to\nlogarithmic factors. Overall, these results contribute to emerging\nunderstanding of the power and limitations of chain-of-thought reasoning.",
    "published": "2025-02-04T15:14:01Z",
    "updated": "2025-07-12T18:49:06Z",
    "authors": [
      "Alireza Amiri",
      "Xinting Huang",
      "Mark Rofin",
      "Michael Hahn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08997v1",
    "title": "Hierarchical Vision Transformer with Prototypes for Interpretable\n  Medical Image Classification",
    "summary": "Explainability is a highly demanded requirement for applications in high-risk\nareas such as medicine. Vision Transformers have mainly been limited to\nattention extraction to provide insight into the model's reasoning. Our\napproach combines the high performance of Vision Transformers with the\nintroduction of new explainability capabilities. We present HierViT, a Vision\nTransformer that is inherently interpretable and adapts its reasoning to that\nof humans. A hierarchical structure is used to process domain-specific features\nfor prediction. It is interpretable by design, as it derives the target output\nwith human-defined features that are visualized by exemplary images\n(prototypes). By incorporating domain knowledge about these decisive features,\nthe reasoning is semantically similar to human reasoning and therefore\nintuitive. Moreover, attention heatmaps visualize the crucial regions for\nidentifying each feature, thereby providing HierViT with a versatile tool for\nvalidating predictions. Evaluated on two medical benchmark datasets, LIDC-IDRI\nfor lung nodule assessment and derm7pt for skin lesion classification, HierViT\nachieves superior and comparable prediction accuracy, respectively, while\noffering explanations that align with human reasoning.",
    "published": "2025-02-13T06:24:07Z",
    "updated": "2025-02-13T06:24:07Z",
    "authors": [
      "Luisa GallÃ©e",
      "Catharina Silvia Lisson",
      "Meinrad Beer",
      "Michael GÃ¶tz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.19597v1",
    "title": "Introduction to Sequence Modeling with Transformers",
    "summary": "Understanding the transformer architecture and its workings is essential for\nmachine learning (ML) engineers. However, truly understanding the transformer\narchitecture can be demanding, even if you have a solid background in machine\nlearning or deep learning. The main working horse is attention, which yields to\nthe transformer encoder-decoder structure. However, putting attention aside\nleaves several programming components that are easy to implement but whose role\nfor the whole is unclear. These components are 'tokenization', 'embedding'\n('un-embedding'), 'masking', 'positional encoding', and 'padding'. The focus of\nthis work is on understanding them. To keep things simple, the understanding is\nbuilt incrementally by adding components one by one, and after each step\ninvestigating what is doable and what is undoable with the current model.\nSimple sequences of zeros (0) and ones (1) are used to study the workings of\neach step.",
    "published": "2025-02-26T22:21:54Z",
    "updated": "2025-02-26T22:21:54Z",
    "authors": [
      "Joni-Kristian KÃ¤mÃ¤rÃ¤inen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.20681v2",
    "title": "Disentangling Feature Structure: A Mathematically Provable Two-Stage\n  Training Dynamics in Transformers",
    "summary": "Transformers may exhibit two-stage training dynamics during the real-world\ntraining process. For instance, when training GPT-2 on the Counterfact dataset,\nthe answers progress from syntactically incorrect to syntactically correct to\nsemantically correct. However, existing theoretical analyses hardly account for\nthis feature-level two-stage phenomenon, which originates from the disentangled\ntwo-type features like syntax and semantics. In this paper, we theoretically\ndemonstrate how the two-stage training dynamics potentially occur in\ntransformers. Specifically, we analyze the feature learning dynamics induced by\nthe aforementioned disentangled two-type feature structure, grounding our\nanalysis in a simplified yet illustrative setting that comprises a normalized\nReLU self-attention layer and structured data. Such disentanglement of feature\nstructure is general in practice, e.g., natural languages contain syntax and\nsemantics, and proteins contain primary and secondary structures. To our best\nknowledge, this is the first rigorous result regarding a feature-level\ntwo-stage optimization process in transformers. Additionally, a corollary\nindicates that such a two-stage process is closely related to the spectral\nproperties of the attention weights, which accords well with our empirical\nfindings.",
    "published": "2025-02-28T03:27:24Z",
    "updated": "2025-10-11T04:45:15Z",
    "authors": [
      "Zixuan Gong",
      "Shijia Li",
      "Yong Liu",
      "Jiaye Teng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18216v2",
    "title": "Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA\n  Adapters",
    "summary": "Large Language Models (LLMs) are computationally intensive, particularly\nduring inference. Neuron-adaptive techniques, which selectively activate\nneurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer\nfrom limitations in modern Transformers. These include reliance on sparse\nactivations, incompatibility with attention layers, and the use of costly\nneuron masking techniques. To address these issues, we propose the Adaptive\nRank Allocation framework and introduce the Rank and Neuron Allocator (RaNA)\nadapter. RaNA adapters leverage rank adapters, which operate on linear layers\nby applying both low-rank matrix decompositions and adaptive masking to\nefficiently allocate compute without depending on activation sparsity. This\nenables RaNA to be generally applied to MLPs and linear components of attention\nmodules, while eliminating the need for expensive maskers found in\nneuron-adaptive methods. Notably, when compared to neuron adapters, RaNA\nimproves perplexity by up to 7 points and increases accuracy by up to 8\npercentage-points when reducing FLOPs by $\\sim$44% in state-of-the-art\nTransformer architectures. These results position RaNA as a robust solution for\nimproving inference efficiency in modern Transformer architectures.",
    "published": "2025-03-23T21:38:19Z",
    "updated": "2025-05-06T21:45:53Z",
    "authors": [
      "Roberto Garcia",
      "Jerry Liu",
      "Daniel Sorvisto",
      "Sabri Eyuboglu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.02211v2",
    "title": "FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault\n  Tolerant Attention",
    "summary": "Transformer models rely on High-Performance Computing (HPC) resources for\ninference, where soft errors are inevitable in large-scale systems, making the\nreliability of the model particularly critical. Existing fault tolerance\nframeworks for Transformers are designed at the operation level without\narchitectural optimization, leading to significant computational and memory\noverhead, which in turn reduces protection efficiency and limits scalability to\nlarger models. In this paper, we implement module-level protection for\nTransformers by treating the operations within the attention module as a single\nkernel and applying end-to-end fault tolerance. This method provides unified\nprotection across multi-step computations, while achieving comprehensive\ncoverage of potential errors in the nonlinear computations. For linear modules,\nwe design a strided algorithm-based fault tolerance (ABFT) that avoids\ninter-thread communication. Experimental results show that our end-to-end fault\ntolerance achieves up to 7.56x speedup over traditional methods with an average\nfault tolerance overhead of 13.9%.",
    "published": "2025-04-03T02:05:08Z",
    "updated": "2025-08-13T02:23:25Z",
    "authors": [
      "Huangliang Dai",
      "Shixun Wu",
      "Jiajun Huang",
      "Zizhe Jian",
      "Yue Zhu",
      "Haiyang Hu",
      "Zizhong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.15054v1",
    "title": "Structure-guided Diffusion Transformer for Low-Light Image Enhancement",
    "summary": "While the diffusion transformer (DiT) has become a focal point of interest in\nrecent years, its application in low-light image enhancement remains a blank\narea for exploration. Current methods recover the details from low-light images\nwhile inevitably amplifying the noise in images, resulting in poor visual\nquality. In this paper, we firstly introduce DiT into the low-light enhancement\ntask and design a novel Structure-guided Diffusion Transformer based Low-light\nimage enhancement (SDTL) framework. We compress the feature through wavelet\ntransform to improve the inference efficiency of the model and capture the\nmulti-directional frequency band. Then we propose a Structure Enhancement\nModule (SEM) that uses structural prior to enhance the texture and leverages an\nadaptive fusion strategy to achieve more accurate enhancement effect. In\nAddition, we propose a Structure-guided Attention Block (SAB) to pay more\nattention to texture-riched tokens and avoid interference from noisy areas in\nnoise prediction. Extensive qualitative and quantitative experiments\ndemonstrate that our method achieves SOTA performance on several popular\ndatasets, validating the effectiveness of SDTL in improving image quality and\nthe potential of DiT in low-light enhancement tasks.",
    "published": "2025-04-21T12:30:01Z",
    "updated": "2025-04-21T12:30:01Z",
    "authors": [
      "Xiangchen Yin",
      "Zhenda Yu",
      "Longtao Jiang",
      "Xin Gao",
      "Xiao Sun",
      "Zhi Liu",
      "Xun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.01057v1",
    "title": "GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual\n  Feature Extraction in Image Segmentation",
    "summary": "This paper introduces GeloVec, a new CNN-based attention smoothing framework\nfor semantic segmentation that addresses critical limitations in conventional\napproaches. While existing attention-backed segmentation methods suffer from\nboundary instability and contextual discontinuities during feature mapping, our\nframework implements a higher-dimensional geometric smoothing method to\nestablish a robust manifold relationships between visually coherent regions.\nGeloVec combines modified Chebyshev distance metrics with multispatial\ntransformations to enhance segmentation accuracy through stabilized feature\nextraction. The core innovation lies in the adaptive sampling weights system\nthat calculates geometric distances in n-dimensional feature space, achieving\nsuperior edge preservation while maintaining intra-class homogeneity. The\nmultispatial transformation matrix incorporates tensorial projections with\northogonal basis vectors, creating more discriminative feature representations\nwithout sacrificing computational efficiency. Experimental validation across\nmultiple benchmark datasets demonstrates significant improvements in\nsegmentation performance, with mean Intersection over Union (mIoU) gains of\n2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets\nrespectively compared to state-of-the-art methods. GeloVec's mathematical\nfoundation in Riemannian geometry provides theoretical guarantees on\nsegmentation stability. Importantly, our framework maintains computational\nefficiency through parallelized implementation of geodesic transformations and\nexhibits strong generalization capabilities across disciplines due to the\nabsence of information loss during transformations.",
    "published": "2025-05-02T07:07:00Z",
    "updated": "2025-05-02T07:07:00Z",
    "authors": [
      "Boris Kriuk",
      "Matey Yordanov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11040v3",
    "title": "Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in\n  Transformers",
    "summary": "Recent advances in transformer architectures deeply enhanced long-context\nlanguage modeling. Among them, HyperAttention achieves competitive efficiency\nby combining a single-level LSH-based clustering with uniform residual\nsampling. However, HyperAttention fails to find all significant keys, which in\nturn raises the overall perplexity. We propose a pre-scoring mechanism that\nprioritizes significant keys before applying HyperAttention. We introduce three\nscoring methods: $k$-means and kernel $k$-means clustering, $k$-median\nclustering, and leverage score-based ranking (inspired by LevAttention) to\nfilter keys effectively. We further replace HyperAttention's original uniform\nresidual sampling, relying exclusively on our pre-scoring mechanism.\nExperiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,\nwhich outperforms standard HyperAttention. Moreover, when running on the\nVision-Transformer (ViT), our method shows that it can guarantee similar\naccuracy compared with LevAttention, and will surpass LevAttention given\nspecific parameters. Although this method introduces some computational\noverhead, its combination with HyperAttention achieves up to 20 times faster\nthan FlashAttention, providing a balanced trade-off between speed and modeling\naccuracy. Our results highlight the effectiveness of integrating pre-scoring\ninto hierarchical attention mechanisms, significantly improving transformer\nefficiency.",
    "published": "2025-05-16T09:35:11Z",
    "updated": "2025-10-31T06:09:08Z",
    "authors": [
      "Zhexiang Li",
      "Haoyu Wang",
      "Yutong Bao",
      "David Woodruff"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11151v1",
    "title": "STEP: A Unified Spiking Transformer Evaluation Platform for Fair and\n  Reproducible Benchmarking",
    "summary": "Spiking Transformers have recently emerged as promising architectures for\ncombining the efficiency of spiking neural networks with the representational\npower of self-attention. However, the lack of standardized implementations,\nevaluation pipelines, and consistent design choices has hindered fair\ncomparison and principled analysis. In this paper, we introduce \\textbf{STEP},\na unified benchmark framework for Spiking Transformers that supports a wide\nrange of tasks, including classification, segmentation, and detection across\nstatic, event-based, and sequential datasets. STEP provides modular support for\ndiverse components such as spiking neurons, input encodings, surrogate\ngradients, and multiple backends (e.g., SpikingJelly, BrainCog). Using STEP, we\nreproduce and evaluate several representative models, and conduct systematic\nablation studies on attention design, neuron types, encoding schemes, and\ntemporal modeling capabilities. We also propose a unified analytical model for\nenergy estimation, accounting for spike sparsity, bitwidth, and memory access,\nand show that quantized ANNs may offer comparable or better energy efficiency.\nOur results suggest that current Spiking Transformers rely heavily on\nconvolutional frontends and lack strong temporal modeling, underscoring the\nneed for spike-native architectural innovations. The full code is available at:\nhttps://github.com/Fancyssc/STEP",
    "published": "2025-05-16T11:50:14Z",
    "updated": "2025-05-16T11:50:14Z",
    "authors": [
      "Sicheng Shen",
      "Dongcheng Zhao",
      "Linghao Feng",
      "Zeyang Yue",
      "Jindong Li",
      "Tenglong Li",
      "Guobin Shen",
      "Yi Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19813v1",
    "title": "GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot\n  View Synthesis",
    "summary": "Neural Radiance Fields (NeRF) have transformed novel view synthesis by\nmodeling scene-specific volumetric representations directly from images. While\ngeneralizable NeRF models can generate novel views across unknown scenes by\nlearning latent ray representations, their performance heavily depends on a\nlarge number of multi-view observations. However, with limited input views,\nthese methods experience significant degradation in rendering quality. To\naddress this limitation, we propose GoLF-NRT: a Global and Local feature\nFusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable\nneural rendering from few input views by leveraging a 3D transformer with\nefficient sparse attention to capture global scene context. In parallel, it\nintegrates local geometric features extracted along the epipolar line, enabling\nhigh-quality scene reconstruction from as few as 1 to 3 input views.\nFurthermore, we introduce an adaptive sampling strategy based on attention\nweights and kernel regression, improving the accuracy of transformer-based\nneural rendering. Extensive experiments on public datasets show that GoLF-NRT\nachieves state-of-the-art performance across varying numbers of input views,\nhighlighting the effectiveness and superiority of our approach. Code is\navailable at https://github.com/KLMAV-CUC/GoLF-NRT.",
    "published": "2025-05-26T10:50:25Z",
    "updated": "2025-05-26T10:50:25Z",
    "authors": [
      "You Wang",
      "Li Fang",
      "Hao Zhu",
      "Fei Hu",
      "Long Ye",
      "Zhan Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08596v1",
    "title": "Transformers Meet Hyperspectral Imaging: A Comprehensive Study of\n  Models, Challenges and Open Problems",
    "summary": "Transformers have become the architecture of choice for learning long-range\ndependencies, yet their adoption in hyperspectral imaging (HSI) is still\nemerging. We reviewed more than 300 papers published up to 2025 and present the\nfirst end-to-end survey dedicated to Transformer-based HSI classification. The\nstudy categorizes every stage of a typical pipeline-pre-processing, patch or\npixel tokenization, positional encoding, spatial-spectral feature extraction,\nmulti-head self-attention variants, skip connections, and loss design-and\ncontrasts alternative design choices with the unique spatial-spectral\nproperties of HSI. We map the field's progress against persistent obstacles:\nscarce labeled data, extreme spectral dimensionality, computational overhead,\nand limited model explainability. Finally, we outline a research agenda\nprioritizing valuable public data sets, lightweight on-edge models,\nillumination and sensor shifts robustness, and intrinsically interpretable\nattention mechanisms. Our goal is to guide researchers in selecting, combining,\nor extending Transformer components that are truly fit for purpose for\nnext-generation HSI applications.",
    "published": "2025-06-10T09:04:30Z",
    "updated": "2025-06-10T09:04:30Z",
    "authors": [
      "Guyang Zhang",
      "Waleed Abdulla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.17425v1",
    "title": "Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT\n  Reconstruction",
    "summary": "Cone-beam computed tomography (CBCT) using only a few X-ray projection views\nenables faster scans with lower radiation dose, but the resulting severe\nunder-sampling causes strong artifacts and poor spatial coverage. We address\nthese challenges in a unified framework. First, we replace conventional\nUNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.\nConvolutional layers capture local details, while self-attention layers enhance\nglobal context. We adapt TransUNet to CBCT by combining multi-scale features,\nquerying view-specific features per 3D point, and adding a lightweight\nattenuation-prediction head. This yields Trans-CBCT, which surpasses prior\nbaselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.\nSecond, we introduce a neighbor-aware Point Transformer to enforce volumetric\ncoherence. This module uses 3D positional encoding and attention over k-nearest\nneighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,\nprovides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on\nLUNA16 and ToothFairy show consistent gains from six to ten views, validating\nthe effectiveness of combining CNN-Transformer features with point-based\ngeometry reasoning for sparse-view CBCT reconstruction.",
    "published": "2025-06-20T18:45:12Z",
    "updated": "2025-06-20T18:45:12Z",
    "authors": [
      "Minmin Yang",
      "Huantao Ren",
      "Senem Velipasalar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07388v1",
    "title": "GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction",
    "summary": "Gaining a deeper understanding of the thickness and variability of internal\nice layers in Radar imagery is essential in monitoring the snow accumulation,\nbetter evaluating ice dynamics processes, and minimizing uncertainties in\nclimate models. Radar sensors, capable of penetrating ice, capture detailed\nradargram images of internal ice layers. In this work, we introduce GRIT, graph\ntransformer for ice layer thickness. GRIT integrates an inductive geometric\ngraph learning framework with an attention mechanism, designed to map the\nrelationships between shallow and deeper ice layers. Compared to baseline graph\nneural networks, GRIT demonstrates consistently lower prediction errors. These\nresults highlight the attention mechanism's effectiveness in capturing temporal\nchanges across ice layers, while the graph transformer combines the strengths\nof transformers for learning long-range dependencies with graph neural networks\nfor capturing spatial patterns, enabling robust modeling of complex\nspatiotemporal dynamics.",
    "published": "2025-07-10T02:59:21Z",
    "updated": "2025-07-10T02:59:21Z",
    "authors": [
      "Zesheng Liu",
      "Maryam Rahnemoonfar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.11539v1",
    "title": "Streaming 4D Visual Geometry Transformer",
    "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
    "published": "2025-07-15T17:59:57Z",
    "updated": "2025-07-15T17:59:57Z",
    "authors": [
      "Dong Zhuo",
      "Wenzhao Zheng",
      "Jiahe Guo",
      "Yuqi Wu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.19514v1",
    "title": "Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain\n  Without Neural Networks",
    "summary": "We introduce a fully spectral learning framework that eliminates traditional\nneural layers by operating entirely in the wavelet domain. The model applies\nlearnable nonlinear transformations, including soft-thresholding and gain-phase\nmodulation, directly to wavelet coefficients. It also includes a differentiable\nwavelet basis selection mechanism, enabling adaptive processing using families\nsuch as Haar, Daubechies, and Biorthogonal wavelets.\n  Implemented in PyTorch with full 3D support, the model maintains a spectral\npipeline without spatial convolutions or attention. On synthetic 3D denoising\nand natural language tasks from the GLUE benchmark, including SST-2 sentiment\nclassification, the model achieves 89.3 percent accuracy, close to a 4-layer\nTransformer baseline (90.1 percent), while using 72 percent fewer parameters\nand 58 percent less peak memory. Faster early convergence is observed due to\nspectral sparsity priors.\n  In contrast to the quadratic complexity of self-attention and large matrix\nmultiplications in Transformers, our approach uses linear-time wavelet\ntransforms and pointwise nonlinearities, significantly reducing inference cost.\nThis yields a compact, interpretable, and efficient alternative to neural\nmodels. Our results support the viability of principled spectral learning in\nboth vision and language tasks, offering new directions for model design\nwithout overparameterized architectures.",
    "published": "2025-07-18T01:28:17Z",
    "updated": "2025-07-18T01:28:17Z",
    "authors": [
      "Andrew Kiruluta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10824v2",
    "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
    "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
    "published": "2025-08-14T16:48:38Z",
    "updated": "2025-08-16T03:17:35Z",
    "authors": [
      "Parsa Omidi",
      "Xingshuai Huang",
      "Axel Laborieux",
      "Bahareh Nikpour",
      "Tianyu Shi",
      "Armaghan Eshaghi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18130v2",
    "title": "Frozen in Time: Parameter-Efficient Time Series Transformers via\n  Reservoir-Induced Feature Expansion and Fixed Random Dynamics",
    "summary": "Transformers are the de-facto choice for sequence modelling, yet their\nquadratic self-attention and weak temporal bias can make long-range forecasting\nboth expensive and brittle. We introduce FreezeTST, a lightweight hybrid that\ninterleaves frozen random-feature (reservoir) blocks with standard trainable\nTransformer layers. The frozen blocks endow the network with rich nonlinear\nmemory at no optimisation cost; the trainable layers learn to query this memory\nthrough self-attention. The design cuts trainable parameters and also lowers\nwall-clock training time, while leaving inference complexity unchanged. On\nseven standard long-term forecasting benchmarks, FreezeTST consistently matches\nor surpasses specialised variants such as Informer, Autoformer, and PatchTST;\nwith substantially lower compute. Our results show that embedding reservoir\nprinciples within Transformers offers a simple, principled route to efficient\nlong-term time-series prediction.",
    "published": "2025-08-25T15:38:23Z",
    "updated": "2025-10-19T14:43:36Z",
    "authors": [
      "Pradeep Singh",
      "Mehak Sharma",
      "Anupriya Dey",
      "Balasubramanian Raman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.19029v2",
    "title": "Revisiting associative recall in modern recurrent models",
    "summary": "Despite the advantageous subquadratic complexity of modern recurrent deep\nlearning models -- such as state-space models (SSMs) -- recent studies have\nhighlighted their potential shortcomings compared to transformers on reasoning\nand memorization tasks. In this paper, we dive deeper into one of such\nbenchmarks: associative recall (AR), which has been shown to correlate well\nwith language modeling performance, and inspect in detail the effects of\nscaling and optimization issues in recently proposed token mixing strategies.\nWe first demonstrate that, unlike standard transformers, the choice of learning\nrate plays a critical role in the performance of modern recurrent models: an\nissue that can severely affect reported performance in previous works and\nsuggests further research is needed to stabilize training. Next, we show that\nrecurrent and attention-based models exhibit contrasting benefits when scaling\nin width as opposed to depth, with attention being notably unable to solve AR\nwhen limited to a single layer. We then further inspect 1-layer transformers,\nrevealing that despite their poor performance, their training dynamics\nsurprisingly resemble the formation of induction heads, a phenomenon previously\nobserved only in their 2-layer counterparts. Finally, through architectural\nablations, we study how components affects Transformer and Mamba's performance\nand optimization stability.",
    "published": "2025-08-26T13:45:08Z",
    "updated": "2025-10-10T13:13:46Z",
    "authors": [
      "Destiny Okpekpe",
      "Antonio Orvieto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.19389v1",
    "title": "DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term\n  Traffic Forecasting",
    "summary": "Accurate long-term traffic forecasting remains a critical challenge in\nintelligent transportation systems, particularly when predicting high-frequency\ntraffic phenomena such as shock waves and congestion boundaries over extended\nrollout horizons. Neural operators have recently gained attention as promising\ntools for modeling traffic flow. While effective at learning function space\nmappings, they inherently produce smooth predictions that fail to reconstruct\nhigh-frequency features such as sharp density gradients which results in rapid\nerror accumulation during multi-step rollout predictions essential for\nreal-time traffic management. To address these fundamental limitations, we\nintroduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)\narchitecture. DETNO leverages a transformer neural operator with\ncross-attention mechanisms, providing model expressivity and super-resolution,\ncoupled with a diffusion-based refinement component that iteratively\nreconstructs high-frequency traffic details through progressive denoising. This\novercomes the inherent smoothing limitations and rollout instability of\nstandard neural operators. Through comprehensive evaluation on chaotic traffic\ndatasets, our method demonstrates superior performance in extended rollout\npredictions compared to traditional and transformer-based neural operators,\npreserving high-frequency components and improving stability over long\nprediction horizons.",
    "published": "2025-08-26T19:32:32Z",
    "updated": "2025-08-26T19:32:32Z",
    "authors": [
      "Owais Ahmad",
      "Milad Ramezankhani",
      "Anirudh Deodhar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04226v1",
    "title": "Rethinking the long-range dependency in Mamba/SSM and transformer models",
    "summary": "Long-range dependency is one of the most desired properties of recent\nsequence models such as state-space models (particularly Mamba) and transformer\nmodels. New model architectures are being actively developed and benchmarked\nfor prediction tasks requiring long-range dependency. However, the capability\nof modeling long-range dependencies of these models has not been investigated\nfrom a theoretical perspective, which hinders a systematic improvement on this\naspect. In this work, we mathematically define long-range dependency using the\nderivative of hidden states with respect to past inputs and compare the\ncapability of SSM and transformer models of modeling long-range dependency\nbased on this definition. We showed that the long-range dependency of SSM\ndecays exponentially with the sequence length, which aligns with the\nexponential decay of memory function in RNN. But the attention mechanism used\nin transformers is more flexible and is not constrained to exponential decay,\nwhich could in theory perform better at modeling long-range dependency with\nsufficient training data, computing resources, and proper training. To combine\nthe flexibility of long-range dependency of attention mechanism and computation\nefficiency of SSM, we propose a new formulation for hidden state update in SSM\nand prove its stability under a standard Gaussian distribution of the input\ndata.",
    "published": "2025-09-04T13:56:47Z",
    "updated": "2025-09-04T13:56:47Z",
    "authors": [
      "Cong Ma",
      "Kayvan Najarian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.14136v1",
    "title": "SV-Mixer: Replacing the Transformer Encoder with Lightweight MLPs for\n  Self-Supervised Model Compression in Speaker Verification",
    "summary": "Self-supervised learning (SSL) has pushed speaker verification accuracy close\nto state-of-the-art levels, but the Transformer backbones used in most SSL\nencoders hinder on-device and real-time deployment. Prior compression work\ntrims layer depth or width yet still inherits the quadratic cost of\nself-attention. We propose SV-Mixer, the first fully MLP-based student encoder\nfor SSL distillation. SV-Mixer replaces Transformer with three lightweight\nmodules: Multi-Scale Mixing for multi-resolution temporal features,\nLocal-Global Mixing for frame-to-utterance context, and Group Channel Mixing\nfor spectral subspaces. Distilled from WavLM, SV-Mixer outperforms a\nTransformer student by 14.6% while cutting parameters and GMACs by over half,\nand at 75% compression, it closely matches the teacher's performance. Our\nresults show that attention-free SSL students can deliver teacher-level\naccuracy with hardware-friendly footprints, opening the door to robust\non-device speaker verification.",
    "published": "2025-09-17T16:16:30Z",
    "updated": "2025-09-17T16:16:30Z",
    "authors": [
      "Jungwoo Heo",
      "Hyun-seo Shin",
      "Chan-yeong Lim",
      "Kyo-won Koo",
      "Seung-bin Kim",
      "Jisoo Son",
      "Ha-Jin Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17365v1",
    "title": "Pre-Trained CNN Architecture for Transformer-Based Image Caption\n  Generation Model",
    "summary": "Automatic image captioning, a multifaceted task bridging computer vision and\nnatural language processing, aims to generate descriptive textual content from\nvisual input. While Convolutional Neural Networks (CNNs) and Long Short-Term\nMemory (LSTM) networks have achieved significant advancements, they present\nlimitations. The inherent sequential nature of RNNs leads to sluggish training\nand inference times. LSTMs further struggle with retaining information from\nearlier sequence elements when dealing with very long sequences. This project\npresents a comprehensive guide to constructing and comprehending transformer\nmodels for image captioning. Transformers employ self-attention mechanisms,\ncapturing both short- and long-range dependencies within the data. This\nfacilitates efficient parallelization during both training and inference\nphases. We leverage the well-established Transformer architecture, recognized\nfor its effectiveness in managing sequential data, and present a meticulous\nmethodology. Utilizing the Flickr30k dataset, we conduct data pre-processing,\nconstruct a model architecture that integrates an EfficientNetB0 CNN for\nfeature extraction, and train the model with attention mechanisms incorporated.\nOur approach exemplifies the utilization of parallelization for efficient\ntraining and inference. You can find the project on GitHub.",
    "published": "2025-09-22T05:32:52Z",
    "updated": "2025-09-22T05:32:52Z",
    "authors": [
      "Amanuel Tafese Dufera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23806v1",
    "title": "Influence-Guided Concolic Testing of Transformer Robustness",
    "summary": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing.",
    "published": "2025-09-28T11:09:15Z",
    "updated": "2025-09-28T11:09:15Z",
    "authors": [
      "Chih-Duo Hong",
      "Yu Wang",
      "Yao-Chen Chang",
      "Fang Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03784v1",
    "title": "Allocation of Parameters in Transformers",
    "summary": "Transformers have achieved remarkable successes across a wide range of\napplications, yet the theoretical foundation of their model efficiency remains\nunderexplored. In this work, we investigate how the model parameters -- mainly\nattention heads and head dimensions -- should be allocated across layers to\nbalance expressivity and efficiency. We first provide mathematical analysis on\nthe role of early layers in information extraction from an approximation\nperspective, with a theoretical characterization on the trade-off between the\nnumber of heads and head dimension under a fixed parameter budget. In addition,\nwe uncover and prove the \\emph{saturation} behavior of softmax activations:\nContinuously increasing head dimensions can lead to diminishing returns in\nlearning errors, particularly for long sequences. Supported by both theory and\nexperiments, this saturation pattern suggests that later layers can operate\nmore efficiently with reduced parameters. Combining these insights, we propose\nprincipled strategies for allocating attention heads and dimensions across\nTransformers' layers, shedding light on theoretically-grounded model efficiency\nof Transformer-based architectures.",
    "published": "2025-10-04T11:22:16Z",
    "updated": "2025-10-04T11:22:16Z",
    "authors": [
      "Ruoxi Yu",
      "Haotian Jiang",
      "Jingpu Cheng",
      "Penghao Yu",
      "Qianxiao Li",
      "Zhong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03886v1",
    "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer",
    "summary": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion\nTransformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim\nfor exceptional visual fidelity. As these models advance, users continually\npush the boundary with imaginative or rare prompts, which advanced models still\nfalter in generating, since their concepts are often too scarce to leave a\nstrong imprint during pre-training. In this paper, we propose a simple yet\neffective intervention that surfaces rare semantics inside MM-DiTs without\nadditional training steps, data, denoising-time optimization, or reliance on\nexternal modules (e.g., large language models). In particular, the\njoint-attention mechanism intrinsic to MM-DiT sequentially updates text\nembeddings alongside image embeddings throughout transformer blocks. We find\nthat by mathematically expanding representational basins around text token\nembeddings via variance scale-up before the joint-attention blocks, rare\nsemantics clearly emerge in MM-DiT's outputs. Furthermore, our results\ngeneralize effectively across text-to-vision tasks, including text-to-image,\ntext-to-video, and text-driven image editing. Our work invites generative\nmodels to reveal the semantics that users intend, once hidden yet ready to\nsurface.",
    "published": "2025-10-04T17:41:24Z",
    "updated": "2025-10-04T17:41:24Z",
    "authors": [
      "Seil Kang",
      "Woojung Han",
      "Dayun Ju",
      "Seong Jae Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03989v1",
    "title": "A Mathematical Explanation of Transformers for Large Language Models and\n  GPTs",
    "summary": "The Transformer architecture has revolutionized the field of sequence\nmodeling and underpins the recent breakthroughs in large language models\n(LLMs). However, a comprehensive mathematical theory that explains its\nstructure and operations remains elusive. In this work, we propose a novel\ncontinuous framework that rigorously interprets the Transformer as a\ndiscretization of a structured integro-differential equation. Within this\nformulation, the self-attention mechanism emerges naturally as a non-local\nintegral operator, and layer normalization is characterized as a projection to\na time-dependent constraint. This operator-theoretic and variational\nperspective offers a unified and interpretable foundation for understanding the\narchitecture's core components, including attention, feedforward layers, and\nnormalization. Our approach extends beyond previous theoretical analyses by\nembedding the entire Transformer operation in continuous domains for both token\nindices and feature dimensions. This leads to a principled and flexible\nframework that not only deepens theoretical insight but also offers new\ndirections for architecture design, analysis, and control-based\ninterpretations. This new interpretation provides a step toward bridging the\ngap between deep learning architectures and continuous mathematical modeling,\nand contributes a foundational perspective to the ongoing development of\ninterpretable and theoretically grounded neural network models.",
    "published": "2025-10-05T01:16:08Z",
    "updated": "2025-10-05T01:16:08Z",
    "authors": [
      "Xue-Cheng Tai",
      "Hao Liu",
      "Lingfeng Li",
      "Raymond H. Chan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.09004v3",
    "title": "EcoFormer: Energy-Saving Attention with Linear Complexity",
    "summary": "Transformer is a transformative framework that models sequential data and has\nachieved remarkable performance on a wide range of tasks, but with high\ncomputational and energy cost. To improve its efficiency, a popular choice is\nto compress the models via binarization which constrains the floating-point\nvalues into binary ones to save resource consumption owing to cheap bitwise\noperations significantly. However, existing binarization methods only aim at\nminimizing the information loss for the input distribution statistically, while\nignoring the pairwise similarity modeling at the core of the attention. To this\nend, we propose a new binarization paradigm customized to high-dimensional\nsoftmax attention via kernelized hashing, called EcoFormer, to map the original\nqueries and keys into low-dimensional binary codes in Hamming space. The\nkernelized hash functions are learned to match the ground-truth similarity\nrelations extracted from the attention map in a self-supervised way. Based on\nthe equivalence between the inner product of binary codes and the Hamming\ndistance as well as the associative property of matrix multiplication, we can\napproximate the attention in linear complexity by expressing it as a\ndot-product of binary codes. Moreover, the compact binary representations of\nqueries and keys enable us to replace most of the expensive multiply-accumulate\noperations in attention with simple accumulations to save considerable on-chip\nenergy footprint on edge devices. Extensive experiments on both vision and\nlanguage tasks show that EcoFormer consistently achieves comparable performance\nwith standard attentions while consuming much fewer resources. For example,\nbased on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy\nfootprint reduction with only a 0.33% performance drop compared to the standard\nattention. Code is available at https://github.com/ziplab/EcoFormer.",
    "published": "2022-09-19T13:28:32Z",
    "updated": "2023-03-20T04:49:10Z",
    "authors": [
      "Jing Liu",
      "Zizheng Pan",
      "Haoyu He",
      "Jianfei Cai",
      "Bohan Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.09827v3",
    "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
    "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
    "published": "2024-06-14T08:32:45Z",
    "updated": "2025-01-23T07:25:28Z",
    "authors": [
      "Heejun Lee",
      "Geon Park",
      "Youngwan Lee",
      "Jaduk Suh",
      "Jina Kim",
      "Wonyoung Jeong",
      "Bumsik Kim",
      "Hyemin Lee",
      "Myeongjae Jeon",
      "Sung Ju Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.04262v1",
    "title": "SDLFormer: A Sparse and Dense Locality-enhanced Transformer for\n  Accelerated MR Image Reconstruction",
    "summary": "Transformers have emerged as viable alternatives to convolutional neural\nnetworks owing to their ability to learn non-local region relationships in the\nspatial domain. The self-attention mechanism of the transformer enables\ntransformers to capture long-range dependencies in the images, which might be\ndesirable for accelerated MRI image reconstruction as the effect of\nundersampling is non-local in the image domain. Despite its computational\nefficiency, the window-based transformers suffer from restricted receptive\nfields as the dependencies are limited to within the scope of the image\nwindows. We propose a window-based transformer network that integrates dilated\nattention mechanism and convolution for accelerated MRI image reconstruction.\nThe proposed network consists of dilated and dense neighborhood attention\ntransformers to enhance the distant neighborhood pixel relationship and\nintroduce depth-wise convolutions within the transformer module to learn\nlow-level translation invariant features for accelerated MRI image\nreconstruction. The proposed model is trained in a self-supervised manner. We\nperform extensive experiments for multi-coil MRI acceleration for coronal PD,\ncoronal PDFS and axial T2 contrasts with 4x and 5x under-sampling in\nself-supervised learning based on k-space splitting. We compare our method\nagainst other reconstruction architectures and the parallel domain\nself-supervised learning baseline. Results show that the proposed model\nexhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 in\nSSIM on average over other architectures (ii) around 1.44 dB in PSNR and around\n0.029 in SSIM over parallel domain self-supervised learning. The code is\navailable at https://github.com/rahul-gs-16/sdlformer.git",
    "published": "2023-08-08T13:59:16Z",
    "updated": "2023-08-08T13:59:16Z",
    "authors": [
      "Rahul G. S.",
      "Sriprabha Ramnarayanan",
      "Mohammad Al Fahim",
      "Keerthi Ram",
      "Preejith S. P",
      "Mohanasankar Sivaprakasam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.04161v2",
    "title": "Attention with Markov: A Framework for Principled Analysis of\n  Transformers via Markov Chains",
    "summary": "Attention-based transformers have achieved tremendous success across a\nvariety of disciplines including natural languages. To deepen our understanding\nof their sequential modeling capabilities, there is a growing interest in using\nMarkov input processes to study them. A key finding is that when trained on\nfirst-order Markov chains, transformers with two or more layers consistently\ndevelop an induction head mechanism to estimate the in-context bigram\nconditional distribution. In contrast, single-layer transformers, unable to\nform an induction head, directly learn the Markov kernel but often face a\nsurprising challenge: they become trapped in local minima representing the\nunigram distribution, whereas deeper models reliably converge to the\nground-truth bigram. While single-layer transformers can theoretically model\nfirst-order Markov chains, their empirical failure to learn this simple kernel\nin practice remains a curious phenomenon. To explain this contrasting behavior\nof single-layer models, in this paper we introduce a new framework for a\nprincipled analysis of transformers via Markov chains. Leveraging our\nframework, we theoretically characterize the loss landscape of single-layer\ntransformers and show the existence of global minima (bigram) and bad local\nminima (unigram) contingent on data properties and model architecture. We\nprecisely delineate the regimes under which these local optima occur. Backed by\nexperiments, we demonstrate that our theoretical findings are in congruence\nwith the empirical results. Finally, we outline several open problems in this\narena. Code is available at https://github.com/Bond1995/Markov .",
    "published": "2024-02-06T17:18:59Z",
    "updated": "2025-07-21T14:23:40Z",
    "authors": [
      "Ashok Vardhan Makkuva",
      "Marco Bondaschi",
      "Adway Girish",
      "Alliot Nagle",
      "Martin Jaggi",
      "Hyeji Kim",
      "Michael Gastpar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1908.01259v3",
    "title": "Attentive Normalization",
    "summary": "In state-of-the-art deep neural networks, both feature normalization and\nfeature attention have become ubiquitous. % with significant performance\nimprovement shown in a vast amount of tasks. They are usually studied as\nseparate modules, however. In this paper, we propose a light-weight integration\nbetween the two schema and present Attentive Normalization (AN). Instead of\nlearning a single affine transformation, AN learns a mixture of affine\ntransformations and utilizes their weighted-sum as the final affine\ntransformation applied to re-calibrate features in an instance-specific way.\nThe weights are learned by leveraging channel-wise feature attention. In\nexperiments, we test the proposed AN using four representative neural\narchitectures in the ImageNet-1000 classification benchmark and the MS-COCO\n2017 object detection and instance segmentation benchmark. AN obtains\nconsistent performance improvement for different neural architectures in both\nbenchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between\n0.5\\% and 2.7\\%, and absolute increase up to 1.8\\% and 2.2\\% for bounding box\nand mask AP in MS-COCO respectively. We observe that the proposed AN provides a\nstrong alternative to the widely used Squeeze-and-Excitation (SE) module. The\nsource codes are publicly available at https://github.com/iVMCL/AOGNet-v2 (the\nImageNet Classification Repo) and\nhttps://github.com/iVMCL/AttentiveNorm\\_Detection (the MS-COCO Detection and\nSegmentation Repo).",
    "published": "2019-08-04T02:17:34Z",
    "updated": "2021-03-25T17:16:13Z",
    "authors": [
      "Xilai Li",
      "Wei Sun",
      "Tianfu Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1908.05147v3",
    "title": "SG-Net: Syntax-Guided Machine Reading Comprehension",
    "summary": "For machine reading comprehension, the capacity of effectively modeling the\nlinguistic knowledge from the detail-riddled and lengthy passages and getting\nride of the noises is essential to improve its performance. Traditional\nattentive models attend to all words without explicit constraint, which results\nin inaccurate concentration on some dispensable words. In this work, we propose\nusing syntax to guide the text modeling by incorporating explicit syntactic\nconstraints into attention mechanism for better linguistically motivated word\nrepresentations. In detail, for self-attention network (SAN) sponsored\nTransformer-based encoder, we introduce syntactic dependency of interest (SDOI)\ndesign into the SAN to form an SDOI-SAN with syntax-guided self-attention.\nSyntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the\nSAN from the original Transformer encoder through a dual contextual\narchitecture for better linguistics inspired representation. To verify its\neffectiveness, the proposed SG-Net is applied to typical pre-trained language\nmodel BERT which is right based on a Transformer encoder. Extensive experiments\non popular benchmarks including SQuAD 2.0 and RACE show that the proposed\nSG-Net design helps achieve substantial performance improvement over strong\nbaselines.",
    "published": "2019-08-14T14:28:07Z",
    "updated": "2019-11-20T10:21:30Z",
    "authors": [
      "Zhuosheng Zhang",
      "Yuwei Wu",
      "Junru Zhou",
      "Sufeng Duan",
      "Hai Zhao",
      "Rui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2004.09800v2",
    "title": "Keyphrase Generation with Cross-Document Attention",
    "summary": "Keyphrase generation aims to produce a set of phrases summarizing the\nessentials of a given document. Conventional methods normally apply an\nencoder-decoder architecture to generate the output keyphrases for an input\ndocument, where they are designed to focus on each current document so they\ninevitably omit crucial corpus-level information carried by other similar\ndocuments, i.e., the cross-document dependency and latent topics. In this\npaper, we propose CDKGen, a Transformer-based keyphrase generator, which\nexpands the Transformer to global attention with cross-document attention\nnetworks to incorporate available documents as references so as to generate\nbetter keyphrases with the guidance of topic information. On top of the\nproposed Transformer + cross-document attention architecture, we also adopt a\ncopy mechanism to enhance our model via selecting appropriate words from\ndocuments to deal with out-of-vocabulary words in keyphrases. Experiment\nresults on five benchmark datasets illustrate the validity and effectiveness of\nour model, which achieves the state-of-the-art performance on all datasets.\nFurther analyses confirm that the proposed model is able to generate keyphrases\nconsistent with references while keeping sufficient diversity. The code of\nCDKGen is available at https://github.com/SVAIGBA/CDKGen.",
    "published": "2020-04-21T07:58:27Z",
    "updated": "2022-12-22T17:17:04Z",
    "authors": [
      "Shizhe Diao",
      "Yan Song",
      "Tong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.04908v1",
    "title": "Local Self-Attention over Long Text for Efficient Document Retrieval",
    "summary": "Neural networks, particularly Transformer-based architectures, have achieved\nsignificant performance improvements on several retrieval benchmarks. When the\nitems being retrieved are documents, the time and memory cost of employing\nTransformers over a full sequence of document terms can be prohibitive. A\npopular strategy involves considering only the first n terms of the document.\nThis can, however, result in a biased system that under retrieves longer\ndocuments. In this work, we propose a local self-attention which considers a\nmoving window over the document terms and for each term attends only to other\nterms in the same window. This local attention incurs a fraction of the compute\nand memory cost of attention over the whole document. The windowed approach\nalso leads to more compact packing of padded documents in minibatches resulting\nin additional savings. We also employ a learned saturation function and a\ntwo-staged pooling strategy to identify relevant regions of the document. The\nTransformer-Kernel pooling model with these changes can efficiently elicit\nrelevance information from documents with thousands of tokens. We benchmark our\nproposed modifications on the document ranking task from the TREC 2019 Deep\nLearning track and observe significant improvements in retrieval quality as\nwell as increased retrieval of longer documents at moderate increase in compute\nand memory costs.",
    "published": "2020-05-11T08:03:21Z",
    "updated": "2020-05-11T08:03:21Z",
    "authors": [
      "Sebastian HofstÃ¤tter",
      "Hamed Zamani",
      "Bhaskar Mitra",
      "Nick Craswell",
      "Allan Hanbury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.16854v3",
    "title": "Facial Expression Recognition with Visual Transformers and Attentional\n  Selective Fusion",
    "summary": "Facial Expression Recognition (FER) in the wild is extremely challenging due\nto occlusions, variant head poses, face deformation and motion blur under\nunconstrained conditions. Although substantial progresses have been made in\nautomatic FER in the past few decades, previous studies were mainly designed\nfor lab-controlled FER. Real-world occlusions, variant head poses and other\nissues definitely increase the difficulty of FER on account of these\ninformation-deficient regions and complex backgrounds. Different from previous\npure CNNs based methods, we argue that it is feasible and practical to\ntranslate facial images into sequences of visual words and perform expression\nrecognition from a global perspective. Therefore, we propose the Visual\nTransformers with Feature Fusion (VTFF) to tackle FER in the wild by two main\nsteps. First, we propose the attentional selective fusion (ASF) for leveraging\ntwo kinds of feature maps generated by two-branch CNNs. The ASF captures\ndiscriminative information by fusing multiple features with the global-local\nattention. The fused feature maps are then flattened and projected into\nsequences of visual words. Second, inspired by the success of Transformers in\nnatural language processing, we propose to model relationships between these\nvisual words with the global self-attention. The proposed method is evaluated\non three public in-the-wild facial expression datasets (RAF-DB, FERPlus and\nAffectNet). Under the same settings, extensive experiments demonstrate that our\nmethod shows superior performance over other methods, setting new state of the\nart on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. The\ncross-dataset evaluation on CK+ shows the promising generalization capability\nof the proposed method.",
    "published": "2021-03-31T07:07:56Z",
    "updated": "2022-02-22T01:51:36Z",
    "authors": [
      "Fuyan Ma",
      "Bin Sun",
      "Shutao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.11518v2",
    "title": "Hybrid Attention-Based Transformer Block Model for Distant Supervision\n  Relation Extraction",
    "summary": "With an exponential explosive growth of various digital text information, it\nis challenging to efficiently obtain specific knowledge from massive\nunstructured text information. As one basic task for natural language\nprocessing (NLP), relation extraction aims to extract the semantic relation\nbetween entity pairs based on the given text. To avoid manual labeling of\ndatasets, distant supervision relation extraction (DSRE) has been widely used,\naiming to utilize knowledge base to automatically annotate datasets.\nUnfortunately, this method heavily suffers from wrong labelling due to the\nunderlying strong assumptions. To address this issue, we propose a new\nframework using hybrid attention-based Transformer block with multi-instance\nlearning to perform the DSRE task. More specifically, the Transformer block is\nfirstly used as the sentence encoder to capture syntactic information of\nsentences, which mainly utilizes multi-head self-attention to extract features\nfrom word level. Then, a more concise sentence-level attention mechanism is\nadopted to constitute the bag representation, aiming to incorporate valid\ninformation of each sentence to effectively represent the bag. Experimental\nresults on the public dataset New York Times (NYT) demonstrate that the\nproposed approach can outperform the state-of-the-art algorithms on the\nevaluation dataset, which verifies the effectiveness of our model for the DSRE\ntask.",
    "published": "2020-03-10T13:05:52Z",
    "updated": "2020-03-26T09:04:45Z",
    "authors": [
      "Yan Xiao",
      "Yaochu Jin",
      "Ran Cheng",
      "Kuangrong Hao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.13840v4",
    "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
    "summary": "Very recently, a variety of vision transformer architectures for dense\nprediction tasks have been proposed and they show that the design of spatial\nattention is critical to their success in these tasks. In this work, we revisit\nthe design of the spatial attention and demonstrate that a carefully-devised\nyet simple spatial attention mechanism performs favourably against the\nstate-of-the-art schemes. As a result, we propose two vision transformer\narchitectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures\nare highly-efficient and easy to implement, only involving matrix\nmultiplications that are highly optimized in modern deep learning frameworks.\nMore importantly, the proposed architectures achieve excellent performance on a\nwide range of visual tasks, including image level classification as well as\ndense detection and segmentation. The simplicity and strong performance suggest\nthat our proposed architectures may serve as stronger backbones for many vision\ntasks. Our code is released at https://github.com/Meituan-AutoML/Twins .",
    "published": "2021-04-28T15:42:31Z",
    "updated": "2021-09-30T02:16:36Z",
    "authors": [
      "Xiangxiang Chu",
      "Zhi Tian",
      "Yuqing Wang",
      "Bo Zhang",
      "Haibing Ren",
      "Xiaolin Wei",
      "Huaxia Xia",
      "Chunhua Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.14124v2",
    "title": "Attention-guided Progressive Mapping for Profile Face Recognition",
    "summary": "The past few years have witnessed great progress in the domain of face\nrecognition thanks to advances in deep learning. However, cross pose face\nrecognition remains a significant challenge. It is difficult for many deep\nlearning algorithms to narrow the performance gap caused by pose variations;\nthe main reasons for this relate to the intra-class discrepancy between face\nimages in different poses and the pose imbalances of training datasets.\nLearning pose-robust features by traversing to the feature space of frontal\nfaces provides an effective and cheap way to alleviate this problem. In this\npaper, we present a method for progressively transforming profile face\nrepresentations to the canonical pose with an attentive pair-wise loss.\nFirstly, to reduce the difficulty of directly transforming the profile face\nfeatures into a frontal pose, we propose to learn the feature residual between\nthe source pose and its nearby pose in a block-byblock fashion, and thus\ntraversing to the feature space of a smaller pose by adding the learned\nresidual. Secondly, we propose an attentive pair-wise loss to guide the feature\ntransformation progressing in the most effective direction. Finally, our\nproposed progressive module and attentive pair-wise loss are light-weight and\neasy to implement, adding only about 7:5% extra parameters. Evaluations on the\nCFP and CPLFW datasets demonstrate the superiority of our proposed method. Code\nis available at https://github.com/hjy1312/AGPM.",
    "published": "2021-06-27T02:21:41Z",
    "updated": "2021-06-29T10:33:31Z",
    "authors": [
      "Junyang Huang",
      "Changxing Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.00205v1",
    "title": "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding",
    "summary": "Current one-stage methods for visual grounding encode the language query as\none holistic sentence embedding before fusion with visual feature. Such a\nformulation does not treat each word of a query sentence on par when modeling\nlanguage to visual attention, therefore prone to neglect words which are less\nimportant for sentence embedding but critical for visual grounding. In this\npaper we propose Word2Pix: a one-stage visual grounding network based on\nencoder-decoder transformer architecture that enables learning for textual to\nvisual feature correspondence via word to pixel attention. The embedding of\neach word from the query sentence is treated alike by attending to visual\npixels individually instead of single holistic sentence embedding. In this way,\neach word is given equivalent opportunity to adjust the language to vision\nattention towards the referent target through multiple stacks of transformer\ndecoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg\ndatasets and the proposed Word2Pix outperforms existing one-stage methods by a\nnotable margin. The results obtained also show that Word2Pix surpasses\ntwo-stage visual grounding models, while at the same time keeping the merits of\none-stage paradigm namely end-to-end training and real-time inference speed\nintact.",
    "published": "2021-07-31T10:20:15Z",
    "updated": "2021-07-31T10:20:15Z",
    "authors": [
      "Heng Zhao",
      "Joey Tianyi Zhou",
      "Yew-Soon Ong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.05565v1",
    "title": "Vision-Language Transformer and Query Generation for Referring\n  Segmentation",
    "summary": "In this work, we address the challenging task of referring segmentation. The\nquery expression in referring segmentation typically indicates the target\nobject by describing its relationship with others. Therefore, to find the\ntarget one among all instances in the image, the model must have a holistic\nunderstanding of the whole image. To achieve this, we reformulate referring\nsegmentation as a direct attention problem: finding the region in the image\nwhere the query language expression is most attended to. We introduce\ntransformer and multi-head attention to build a network with an encoder-decoder\nattention mechanism architecture that \"queries\" the given image with the\nlanguage expression. Furthermore, we propose a Query Generation Module, which\nproduces multiple sets of queries with different attention weights that\nrepresent the diversified comprehensions of the language expression from\ndifferent aspects. At the same time, to find the best way from these\ndiversified comprehensions based on visual clues, we further propose a Query\nBalance Module to adaptively select the output features of these queries for a\nbetter mask generation. Without bells and whistles, our approach is\nlight-weight and achieves new state-of-the-art performance consistently on\nthree referring segmentation datasets, RefCOCO, RefCOCO+, and G-Ref. Our code\nis available at https://github.com/henghuiding/Vision-Language-Transformer.",
    "published": "2021-08-12T07:24:35Z",
    "updated": "2021-08-12T07:24:35Z",
    "authors": [
      "Henghui Ding",
      "Chang Liu",
      "Suchen Wang",
      "Xudong Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.10674v1",
    "title": "SEA: Graph Shell Attention in Graph Neural Networks",
    "summary": "A common issue in Graph Neural Networks (GNNs) is known as over-smoothing. By\nincreasing the number of iterations within the message-passing of GNNs, the\nnodes' representations of the input graph align with each other and become\nindiscernible. Recently, it has been shown that increasing a model's complexity\nby integrating an attention mechanism yields more expressive architectures.\nThis is majorly contributed to steering the nodes' representations only towards\nnodes that are more informative than others. Transformer models in combination\nwith GNNs result in architectures including Graph Transformer Layers (GTL),\nwhere layers are entirely based on the attention operation. However, the\ncalculation of a node's representation is still restricted to the computational\nworking flow of a GNN. In our work, we relax the GNN architecture by means of\nimplementing a routing heuristic. Specifically, the nodes' representations are\nrouted to dedicated experts. Each expert calculates the representations\naccording to their respective GNN workflow. The definitions of distinguishable\nGNNs result from k-localized views starting from the central node. We call this\nprocedure Graph Shell Attention (SEA), where experts process different\nsubgraphs in a transformer-motivated fashion. Intuitively, by increasing the\nnumber of experts, the models gain in expressiveness such that a node's\nrepresentation is solely based on nodes that are located within the receptive\nfield of an expert. We evaluate our architecture on various benchmark datasets\nshowing competitive results compared to state-of-the-art models.",
    "published": "2021-10-20T17:32:08Z",
    "updated": "2021-10-20T17:32:08Z",
    "authors": [
      "Christian M. M. Frey",
      "Yunpu Ma",
      "Matthias Schubert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.11940v1",
    "title": "PAM: Pose Attention Module for Pose-Invariant Face Recognition",
    "summary": "Pose variation is one of the key challenges in face recognition. Conventional\ntechniques mainly focus on face frontalization or face augmentation in image\nspace. However, transforming face images in image space is not guaranteed to\npreserve the lossless identity features of the original image. Moreover, these\nmethods suffer from more computational costs and memory requirements due to the\nadditional models. We argue that it is more desirable to perform feature\ntransformation in hierarchical feature space rather than image space, which can\ntake advantage of different feature levels and benefit from joint learning with\nrepresentation learning. To this end, we propose a lightweight and\neasy-to-implement attention block, named Pose Attention Module (PAM), for\npose-invariant face recognition. Specifically, PAM performs frontal-profile\nfeature transformation in hierarchical feature space by learning residuals\nbetween pose variations with a soft gate mechanism. We validated the\neffectiveness of PAM block design through extensive ablation studies and\nverified the performance on several popular benchmarks, including LFW, CFP-FP,\nAgeDB-30, CPLFW, and CALFW. Experimental results show that our method not only\noutperforms state-of-the-art methods but also effectively reduces memory\nrequirements by more than 75 times. It is noteworthy that our method is not\nlimited to face recognition with large pose variations. By adjusting the soft\ngate mechanism of PAM to a specific coefficient, such semantic attention block\ncan easily extend to address other intra-class imbalance problems in face\nrecognition, including large variations in age, illumination, expression, etc.",
    "published": "2021-11-23T15:18:33Z",
    "updated": "2021-11-23T15:18:33Z",
    "authors": [
      "En-Jung Tsai",
      "Wei-Chang Yeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.10233v1",
    "title": "DirecFormer: A Directed Attention in Transformer Approach to Robust\n  Action Recognition",
    "summary": "Human action recognition has recently become one of the popular research\ntopics in the computer vision community. Various 3D-CNN based methods have been\npresented to tackle both the spatial and temporal dimensions in the task of\nvideo action recognition with competitive results. However, these methods have\nsuffered some fundamental limitations such as lack of robustness and\ngeneralization, e.g., how does the temporal ordering of video frames affect the\nrecognition results? This work presents a novel end-to-end Transformer-based\nDirected Attention (DirecFormer) framework for robust action recognition. The\nmethod takes a simple but novel perspective of Transformer-based approach to\nunderstand the right order of sequence actions. Therefore, the contributions of\nthis work are three-fold. Firstly, we introduce the problem of ordered temporal\nlearning issues to the action recognition problem. Secondly, a new Directed\nAttention mechanism is introduced to understand and provide attentions to human\nactions in the right order. Thirdly, we introduce the conditional dependency in\naction sequence modeling that includes orders and classes. The proposed\napproach consistently achieves the state-of-the-art (SOTA) results compared\nwith the recent action recognition methods, on three standard large-scale\nbenchmarks, i.e. Jester, Kinetics-400 and Something-Something-V2.",
    "published": "2022-03-19T03:41:48Z",
    "updated": "2022-03-19T03:41:48Z",
    "authors": [
      "Thanh-Dat Truong",
      "Quoc-Huy Bui",
      "Chi Nhan Duong",
      "Han-Seok Seo",
      "Son Lam Phung",
      "Xin Li",
      "Khoa Luu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.03341v3",
    "title": "Softmax-free Linear Transformers",
    "summary": "Vision transformers (ViTs) have pushed the state-of-the-art for visual\nperception tasks. The self-attention mechanism underpinning the strength of\nViTs has a quadratic complexity in both computation and memory usage. This\nmotivates the development of approximating the self-attention at linear\ncomplexity. However, an in-depth analysis in this work reveals that existing\nmethods are either theoretically flawed or empirically ineffective for visual\nrecognition. We identify that their limitations are rooted in the inheritance\nof softmax-based self-attention during approximations, that is, normalizing the\nscaled dot-product between token feature vectors using the softmax function. As\npreserving the softmax operation challenges any subsequent linearization\nefforts. By this insight, a family of Softmax-Free Transformers (SOFT) are\nproposed. Specifically, a Gaussian kernel function is adopted to replace the\ndot-product similarity, enabling a full self-attention matrix to be\napproximated under low-rank matrix decomposition. For computational robustness,\nwe estimate the Moore-Penrose inverse using an iterative Newton-Raphson method\nin the forward process only, while calculating its theoretical gradients only\nonce in the backward process. To further expand applicability (e.g., dense\nprediction tasks), an efficient symmetric normalization technique is\nintroduced. Extensive experiments on ImageNet, COCO, and ADE20K show that our\nSOFT significantly improves the computational efficiency of existing ViT\nvariants. With linear complexity, much longer token sequences are permitted by\nSOFT, resulting in superior trade-off between accuracy and complexity. Code and\nmodels are available at https://github.com/fudan-zvg/SOFT.",
    "published": "2022-07-05T03:08:27Z",
    "updated": "2024-03-15T00:52:40Z",
    "authors": [
      "Jiachen Lu",
      "Junge Zhang",
      "Xiatian Zhu",
      "Jianfeng Feng",
      "Tao Xiang",
      "Li Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.11938v2",
    "title": "Reference-based Image Super-Resolution with Deformable Attention\n  Transformer",
    "summary": "Reference-based image super-resolution (RefSR) aims to exploit auxiliary\nreference (Ref) images to super-resolve low-resolution (LR) images. Recently,\nRefSR has been attracting great attention as it provides an alternative way to\nsurpass single image SR. However, addressing the RefSR problem has two critical\nchallenges: (i) It is difficult to match the correspondence between LR and Ref\nimages when they are significantly different; (ii) How to transfer the relevant\ntexture from Ref images to compensate the details for LR images is very\nchallenging. To address these issues of RefSR, this paper proposes a deformable\nattention Transformer, namely DATSR, with multiple scales, each of which\nconsists of a texture feature encoder (TFE) module, a reference-based\ndeformable attention (RDA) module and a residual feature aggregation (RFA)\nmodule. Specifically, TFE first extracts image transformation (e.g.,\nbrightness) insensitive features for LR and Ref images, RDA then can exploit\nmultiple relevant textures to compensate more information for LR features, and\nRFA lastly aggregates LR features and relevant textures to get a more visually\npleasant result. Extensive experiments demonstrate that our DATSR achieves\nstate-of-the-art performance on benchmark datasets quantitatively and\nqualitatively.",
    "published": "2022-07-25T07:07:00Z",
    "updated": "2022-08-04T23:06:18Z",
    "authors": [
      "Jiezhang Cao",
      "Jingyun Liang",
      "Kai Zhang",
      "Yawei Li",
      "Yulun Zhang",
      "Wenguan Wang",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.13596v1",
    "title": "Semi-supervised multiscale dual-encoding method for faulty traffic data\n  detection",
    "summary": "Inspired by the recent success of deep learning in multiscale information\nencoding, we introduce a variational autoencoder (VAE) based semi-supervised\nmethod for detection of faulty traffic data, which is cast as a classification\nproblem. Continuous wavelet transform (CWT) is applied to the time series of\ntraffic volume data to obtain rich features embodied in time-frequency\nrepresentation, followed by a twin of VAE models to separately encode normal\ndata and faulty data. The resulting multiscale dual encodings are concatenated\nand fed to an attention-based classifier, consisting of a self-attention module\nand a multilayer perceptron. For comparison, the proposed architecture is\nevaluated against five different encoding schemes, including (1) VAE with only\nnormal data encoding, (2) VAE with only faulty data encoding, (3) VAE with both\nnormal and faulty data encodings, but without attention module in the\nclassifier, (4) siamese encoding, and (5) cross-vision transformer (CViT)\nencoding. The first four encoding schemes adopted the same convolutional neural\nnetwork (CNN) architecture while the fifth encoding scheme follows the\ntransformer architecture of CViT. Our experiments show that the proposed\narchitecture with the dual encoding scheme, coupled with attention module,\noutperforms other encoding schemes and results in classification accuracy of\n96.4%, precision of 95.5%, and recall of 97.7%.",
    "published": "2022-12-27T20:07:52Z",
    "updated": "2022-12-27T20:07:52Z",
    "authors": [
      "Yongcan Huang",
      "Jidong J. Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.06963v1",
    "title": "Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image\n  Classification Using Transformers",
    "summary": "Whole-Slide Imaging allows for the capturing and digitization of\nhigh-resolution images of histological specimen. An automated analysis of such\nimages using deep learning models is therefore of high demand. The transformer\narchitecture has been proposed as a possible candidate for effectively\nleveraging the high-resolution information. Here, the whole-slide image is\npartitioned into smaller image patches and feature tokens are extracted from\nthese image patches. However, while the conventional transformer allows for a\nsimultaneous processing of a large set of input tokens, the computational\ndemand scales quadratically with the number of input tokens and thus\nquadratically with the number of image patches. To address this problem we\npropose a novel cascaded cross-attention network (CCAN) based on the\ncross-attention mechanism that scales linearly with the number of extracted\npatches. Our experiments demonstrate that this architecture is at least on-par\nwith and even outperforms other attention-based state-of-the-art methods on two\npublic datasets: On the use-case of lung cancer (TCGA NSCLC) our model reaches\na mean area under the receiver operating characteristic (AUC) of 0.970 $\\pm$\n0.008 and on renal cancer (TCGA RCC) reaches a mean AUC of 0.985 $\\pm$ 0.004.\nFurthermore, we show that our proposed model is efficient in low-data regimes,\nmaking it a promising approach for analyzing whole-slide images in\nresource-limited settings. To foster research in this direction, we make our\ncode publicly available on GitHub: XXX.",
    "published": "2023-05-11T16:42:24Z",
    "updated": "2023-05-11T16:42:24Z",
    "authors": [
      "Firas Khader",
      "Jakob Nikolas Kather",
      "Tianyu Han",
      "Sven Nebelung",
      "Christiane Kuhl",
      "Johannes Stegmaier",
      "Daniel Truhn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.12424v1",
    "title": "DualToken-ViT: Position-aware Efficient Vision Transformer with Dual\n  Token Fusion",
    "summary": "Self-attention-based vision transformers (ViTs) have emerged as a highly\ncompetitive architecture in computer vision. Unlike convolutional neural\nnetworks (CNNs), ViTs are capable of global information sharing. With the\ndevelopment of various structures of ViTs, ViTs are increasingly advantageous\nfor many vision tasks. However, the quadratic complexity of self-attention\nrenders ViTs computationally intensive, and their lack of inductive biases of\nlocality and translation equivariance demands larger model sizes compared to\nCNNs to effectively learn visual features. In this paper, we propose a\nlight-weight and efficient vision transformer model called DualToken-ViT that\nleverages the advantages of CNNs and ViTs. DualToken-ViT effectively fuses the\ntoken with local information obtained by convolution-based structure and the\ntoken with global information obtained by self-attention-based structure to\nachieve an efficient attention structure. In addition, we use position-aware\nglobal tokens throughout all stages to enrich the global information, which\nfurther strengthening the effect of DualToken-ViT. Position-aware global tokens\nalso contain the position information of the image, which makes our model\nbetter for vision tasks. We conducted extensive experiments on image\nclassification, object detection and semantic segmentation tasks to demonstrate\nthe effectiveness of DualToken-ViT. On the ImageNet-1K dataset, our models of\ndifferent scales achieve accuracies of 75.4% and 79.4% with only 0.5G and 1.0G\nFLOPs, respectively, and our model with 1.0G FLOPs outperforms LightViT-T using\nglobal tokens by 0.7%.",
    "published": "2023-09-21T18:46:32Z",
    "updated": "2023-09-21T18:46:32Z",
    "authors": [
      "Zhenzhen Chu",
      "Jiayu Chen",
      "Cen Chen",
      "Chengyu Wang",
      "Ziheng Wu",
      "Jun Huang",
      "Weining Qian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.14005v3",
    "title": "Leveraging Complementary Attention maps in vision transformers for OCT\n  image analysis",
    "summary": "Optical Coherence Tomography (OCT) scan yields all possible cross-section\nimages of a retina for detecting biomarkers linked to optical defects. Due to\nthe high volume of data generated, an automated and reliable biomarker\ndetection pipeline is necessary as a primary screening stage.\n  We outline our new state-of-the-art pipeline for identifying biomarkers from\nOCT scans. In collaboration with trained ophthalmologists, we identify local\nand global structures in biomarkers. Through a comprehensive and systematic\nreview of existing vision architectures, we evaluate different convolution and\nattention mechanisms for biomarker detection. We find that MaxViT, a hybrid\nvision transformer combining convolution layers with strided attention, is\nbetter suited for local feature detection, while EVA-02, a standard vision\ntransformer leveraging pure attention and large-scale knowledge distillation,\nexcels at capturing global features. We ensemble the predictions of both models\nto achieve first place in the IEEE Video and Image Processing Cup 2023\ncompetition on OCT biomarker detection, achieving a patient-wise F1 score of\n0.8527 in the final phase of the competition, scoring 3.8\\% higher than the\nnext best solution. Finally, we used knowledge distillation to train a single\nMaxViT to outperform our ensemble at a fraction of the computation cost.",
    "published": "2023-10-21T13:27:07Z",
    "updated": "2025-05-31T01:46:46Z",
    "authors": [
      "Haz Sameen Shahgir",
      "Tanjeem Azwad Zaman",
      "Khondker Salman Sayeed",
      "Md. Asif Haider",
      "Sheikh Saifur Rahman Jony",
      "M. Sohel Rahman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.19603v4",
    "title": "Transformers Can Solve Non-Linear and Non-Markovian Filtering Problems\n  in Continuous Time For Conditionally Gaussian Signals",
    "summary": "The use of attention-based deep learning models in stochastic filtering, e.g.\ntransformers and deep Kalman filters, has recently come into focus; however,\nthe potential for these models to solve stochastic filtering problems remains\nlargely unknown. The paper provides an affirmative answer to this open problem\nin the theoretical foundations of machine learning by showing that a class of\ncontinuous-time transformer models, called \\textit{filterformers}, can\napproximately implement the conditional law of a broad class of non-Markovian\nand conditionally Gaussian signal processes given noisy continuous-time\n(possibly non-Gaussian) measurements. Our approximation guarantees hold\nuniformly over sufficiently regular compact subsets of continuous-time paths,\nwhere the worst-case 2-Wasserstein distance between the true optimal filter and\nour deep learning model quantifies the approximation error. Our construction\nrelies on two new customizations of the standard attention mechanism: The first\ncan losslessly adapt to the characteristics of a broad range of paths since we\nshow that the attention mechanism implements bi-Lipschitz embeddings of\nsufficiently regular sets of paths into low-dimensional Euclidean spaces; thus,\nit incurs no ``dimension reduction error''. The latter attention mechanism is\ntailored to the geometry of Gaussian measures in the $2$-Wasserstein space. Our\nanalysis relies on new stability estimates of robust optimal filters in the\nconditionally Gaussian setting.",
    "published": "2023-10-30T14:58:12Z",
    "updated": "2025-07-14T13:17:47Z",
    "authors": [
      "Blanka Horvath",
      "Anastasis Kratsios",
      "Yannick Limmer",
      "Xuwei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.08747v3",
    "title": "Improved Dense Nested Attention Network Based on Transformer for\n  Infrared Small Target Detection",
    "summary": "Infrared small target detection based on deep learning offers unique\nadvantages in separating small targets from complex and dynamic backgrounds.\nHowever, the features of infrared small targets gradually weaken as the depth\nof convolutional neural network (CNN) increases. To address this issue, we\npropose a novel method for detecting infrared small targets called improved\ndense nested attention network (IDNANet), which is based on the transformer\narchitecture. We preserve the dense nested structure of dense nested attention\nnetwork (DNANet) and introduce the Swin-transformer during feature extraction\nstage to enhance the continuity of features. Furthermore, we integrate the\nACmix attention structure into the dense nested structure to enhance the\nfeatures of intermediate layers. Additionally, we design a weighted dice binary\ncross-entropy (WD-BCE) loss function to mitigate the negative impact of\nforeground-background imbalance in the samples. Moreover, we develop a dataset\nspecifically for infrared small targets, called BIT-SIRST. The dataset\ncomprises a significant amount of real-world targets and manually annotated\nlabels, as well as synthetic data and corresponding labels. We have evaluated\nthe effectiveness of our method through experiments conducted on public\ndatasets. In comparison to other state-of-the-art methods, our approach\noutperforms in terms of probability of detection ($P_d$), false-alarm rate\n($F_a$), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89\\% on\nthe NUDT-SIRST dataset and 79.72\\% on the SIRST dataset. The BIT-SIRST dataset\nand codes are available openly at\n\\href{https://github.com/EdwardBao1006/bit\\_sirst}{\\color[HTML]{B22222}{https://github.com/EdwardBao1006/bit\\_sirst}}.",
    "published": "2023-11-15T07:29:24Z",
    "updated": "2024-01-17T07:47:29Z",
    "authors": [
      "Chun Bao",
      "Jie Cao",
      "Yaqian Ning",
      "Tianhua Zhao",
      "Zhijun Li",
      "Zechen Wang",
      "Li Zhang",
      "Qun Hao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.16456v2",
    "title": "SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design",
    "summary": "Recently, efficient Vision Transformers have shown great performance with low\nlatency on resource-constrained devices. Conventionally, they use 4x4 patch\nembeddings and a 4-stage structure at the macro level, while utilizing\nsophisticated attention with multi-head configuration at the micro level. This\npaper aims to address computational redundancy at all design levels in a\nmemory-efficient manner. We discover that using larger-stride patchify stem not\nonly reduces memory access costs but also achieves competitive performance by\nleveraging token representations with reduced spatial redundancy from the early\nstages. Furthermore, our preliminary analyses suggest that attention layers in\nthe early stages can be substituted with convolutions, and several attention\nheads in the latter stages are computationally redundant. To handle this, we\nintroduce a single-head attention module that inherently prevents head\nredundancy and simultaneously boosts accuracy by parallelly combining global\nand local information. Building upon our solutions, we introduce SHViT, a\nSingle-Head Vision Transformer that obtains the state-of-the-art speed-accuracy\ntradeoff. For example, on ImageNet-1k, our SHViT-S4 is 3.3x, 8.1x, and 2.4x\nfaster than MobileViTv2 x1.0 on GPU, CPU, and iPhone12 mobile device,\nrespectively, while being 1.3% more accurate. For object detection and instance\nsegmentation on MS COCO using Mask-RCNN head, our model achieves performance\ncomparable to FastViT-SA12 while exhibiting 3.8x and 2.0x lower backbone\nlatency on GPU and mobile device, respectively.",
    "published": "2024-01-29T09:12:23Z",
    "updated": "2024-03-27T04:14:59Z",
    "authors": [
      "Seokju Yun",
      "Youngmin Ro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.08793v1",
    "title": "BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image\n  Segmentation",
    "summary": "The accurate segmentation of medical images is critical for various\nhealthcare applications. Convolutional neural networks (CNNs), especially Fully\nConvolutional Networks (FCNs) like U-Net, have shown remarkable success in\nmedical image segmentation tasks. However, they have limitations in capturing\nglobal context and long-range relations, especially for objects with\nsignificant variations in shape, scale, and texture. While transformers have\nachieved state-of-the-art results in natural language processing and image\nrecognition, they face challenges in medical image segmentation due to image\nlocality and translational invariance issues. To address these challenges, this\npaper proposes an innovative U-shaped network called BEFUnet, which enhances\nthe fusion of body and edge information for precise medical image segmentation.\nThe BEFUnet comprises three main modules, including a novel Local\nCross-Attention Feature (LCAF) fusion module, a novel Double-Level Fusion (DLF)\nmodule, and dual-branch encoder. The dual-branch encoder consists of an edge\nencoder and a body encoder. The edge encoder employs PDC blocks for effective\nedge information extraction, while the body encoder uses the Swin Transformer\nto capture semantic information with global attention. The LCAF module\nefficiently fuses edge and body features by selectively performing local\ncross-attention on features that are spatially close between the two\nmodalities. This local approach significantly reduces computational complexity\ncompared to global cross-attention while ensuring accurate feature matching.\nBEFUnet demonstrates superior performance over existing methods across various\nevaluation metrics on medical image segmentation datasets.",
    "published": "2024-02-13T21:03:36Z",
    "updated": "2024-02-13T21:03:36Z",
    "authors": [
      "Omid Nejati Manzari",
      "Javad Mirzapour Kaleybar",
      "Hooman Saadat",
      "Shahin Maleki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.14185v1",
    "title": "HINT: High-quality INPainting Transformer with Mask-Aware Encoding and\n  Enhanced Attention",
    "summary": "Existing image inpainting methods leverage convolution-based downsampling\napproaches to reduce spatial dimensions. This may result in information loss\nfrom corrupted images where the available information is inherently sparse,\nespecially for the scenario of large missing regions. Recent advances in\nself-attention mechanisms within transformers have led to significant\nimprovements in many computer vision tasks including inpainting. However,\nlimited by the computational costs, existing methods cannot fully exploit the\nefficacy of long-range modelling capabilities of such models. In this paper, we\npropose an end-to-end High-quality INpainting Transformer, abbreviated as HINT,\nwhich consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to\npreserve the visible information extracted from the corrupted image while\nmaintaining the integrity of the information available for high-level\ninferences made within the model. Moreover, we propose a Spatially-activated\nChannel Attention Layer (SCAL), an efficient self-attention mechanism\ninterpreting spatial awareness to model the corrupted image at multiple scales.\nTo further enhance the effectiveness of SCAL, motivated by recent advanced in\nspeech recognition, we introduce a sandwich structure that places feed-forward\nnetworks before and after the SCAL module. We demonstrate the superior\nperformance of HINT compared to contemporary state-of-the-art models on four\ndatasets, CelebA, CelebA-HQ, Places2, and Dunhuang.",
    "published": "2024-02-22T00:14:26Z",
    "updated": "2024-02-22T00:14:26Z",
    "authors": [
      "Shuang Chen",
      "Amir Atapour-Abarghouei",
      "Hubert P. H. Shum"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.02411v6",
    "title": "NiNformer: A Network in Network Transformer with Token Mixing Generated\n  Gating Function",
    "summary": "The attention mechanism is the primary component of the transformer\narchitecture; it has led to significant advancements in deep learning spanning\nmany domains and covering multiple tasks. In computer vision, the attention\nmechanism was first incorporated in the Vision Transformer ViT, and then its\nusage has expanded into many tasks in the vision domain, such as\nclassification, segmentation, object detection, and image generation. While the\nattention mechanism is very expressive and capable, it comes with the\ndisadvantage of being computationally expensive and requiring datasets of\nconsiderable size for effective optimization. To address these shortcomings,\nmany designs have been proposed in the literature to reduce the computational\nburden and alleviate the data size requirements. Examples of such attempts in\nthe vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many\nmore attempts with different sets of advantages and disadvantages. This paper\nintroduces a new computational block as an alternative to the standard ViT\nblock. The newly proposed block reduces the computational requirements by\nreplacing the normal attention layers with a Network in Network structure,\ntherefore enhancing the static approach of the MLP-Mixer with a dynamic\nlearning of element-wise gating function generated by a token mixing process.\nExtensive experimentation shows that the proposed design provides better\nperformance than the baseline architectures on multiple datasets applied in the\nimage classification task of the vision domain.",
    "published": "2024-03-04T19:08:20Z",
    "updated": "2025-05-01T19:29:24Z",
    "authors": [
      "Abdullah Nazhat Abdullah",
      "Tarkan Aydin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.06961v1",
    "title": "Explainable Transformer Prototypes for Medical Diagnoses",
    "summary": "Deployments of artificial intelligence in medical diagnostics mandate not\njust accuracy and efficacy but also trust, emphasizing the need for\nexplainability in machine decisions. The recent trend in automated medical\nimage diagnostics leans towards the deployment of Transformer-based\narchitectures, credited to their impressive capabilities. Since the\nself-attention feature of transformers contributes towards identifying crucial\nregions during the classification process, they enhance the trustability of the\nmethods. However, the complex intricacies of these attention mechanisms may\nfall short of effectively pinpointing the regions of interest directly\ninfluencing AI decisions. Our research endeavors to innovate a unique attention\nblock that underscores the correlation between 'regions' rather than 'pixels'.\nTo address this challenge, we introduce an innovative system grounded in\nprototype learning, featuring an advanced self-attention mechanism that goes\nbeyond conventional ad-hoc visual explanation techniques by offering\ncomprehensible visual insights. A combined quantitative and qualitative\nmethodological approach was used to demonstrate the effectiveness of the\nproposed method on the large-scale NIH chest X-ray dataset. Experimental\nresults showed that our proposed method offers a promising direction for\nexplainability, which can lead to the development of more trustable systems,\nwhich can facilitate easier and rapid adoption of such technology into routine\nclinics. The code is available at www.github.com/NUBagcilab/r2r_proto.",
    "published": "2024-03-11T17:46:21Z",
    "updated": "2024-03-11T17:46:21Z",
    "authors": [
      "Ugur Demir",
      "Debesh Jha",
      "Zheyuan Zhang",
      "Elif Keles",
      "Bradley Allen",
      "Aggelos K. Katsaggelos",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.12202v1",
    "title": "DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions",
    "summary": "In this paper, we introduce a novel approach that harnesses both 2D and 3D\nattentions to enable highly accurate depth completion without requiring\niterative spatial propagations. Specifically, we first enhance a baseline\nconvolutional depth completion model by applying attention to 2D features in\nthe bottleneck and skip connections. This effectively improves the performance\nof this simple network and sets it on par with the latest, complex\ntransformer-based models. Leveraging the initial depths and features from this\nnetwork, we uplift the 2D features to form a 3D point cloud and construct a 3D\npoint transformer to process it, allowing the model to explicitly learn and\nexploit 3D geometric features. In addition, we propose normalization techniques\nto process the point cloud, which improves learning and leads to better\naccuracy than directly using point transformers off the shelf. Furthermore, we\nincorporate global attention on downsampled point cloud features, which enables\nlong-range context while still being computationally feasible. We evaluate our\nmethod, DeCoTR, on established depth completion benchmarks, including NYU Depth\nV2 and KITTI, showcasing that it sets new state-of-the-art performance. We\nfurther conduct zero-shot evaluations on ScanNet and DDAD benchmarks and\ndemonstrate that DeCoTR has superior generalizability compared to existing\napproaches.",
    "published": "2024-03-18T19:22:55Z",
    "updated": "2024-03-18T19:22:55Z",
    "authors": [
      "Yunxiao Shi",
      "Manish Kumar Singh",
      "Hong Cai",
      "Fatih Porikli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.14124v1",
    "title": "Soft Masked Transformer for Point Cloud Processing with Skip\n  Attention-Based Upsampling",
    "summary": "Point cloud processing methods leverage local and global point features %at\nthe feature level to cater to downstream tasks, yet they often overlook the\ntask-level context inherent in point clouds during the encoding stage. We argue\nthat integrating task-level information into the encoding stage significantly\nenhances performance. To that end, we propose SMTransformer which incorporates\ntask-level information into a vector-based transformer by utilizing a soft mask\ngenerated from task-level queries and keys to learn the attention weights.\nAdditionally, to facilitate effective communication between features from the\nencoding and decoding layers in high-level tasks such as segmentation, we\nintroduce a skip-attention-based up-sampling block. This block dynamically\nfuses features from various resolution points across the encoding and decoding\nlayers. To mitigate the increase in network parameters and training time\nresulting from the complexity of the aforementioned blocks, we propose a novel\nshared position encoding strategy. This strategy allows various transformer\nblocks to share the same position information over the same resolution points,\nthereby reducing network parameters and training time without compromising\naccuracy.Experimental comparisons with existing methods on multiple datasets\ndemonstrate the efficacy of SMTransformer and skip-attention-based up-sampling\nfor point cloud processing tasks, including semantic segmentation and\nclassification. In particular, we achieve state-of-the-art semantic\nsegmentation results of 73.4% mIoU on S3DIS Area 5 and 62.4% mIoU on SWAN\ndataset",
    "published": "2024-03-21T04:34:24Z",
    "updated": "2024-03-21T04:34:24Z",
    "authors": [
      "Yong He",
      "Hongshan Yu",
      "Muhammad Ibrahim",
      "Xiaoyan Liu",
      "Tongjia Chen",
      "Anwaar Ulhaq",
      "Ajmal Mian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.17909v1",
    "title": "ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing\n  Change Detection",
    "summary": "Deep learning has shown remarkable success in remote sensing change detection\n(CD), aiming to identify semantic change regions between co-registered\nsatellite image pairs acquired at distinct time stamps. However, existing\nconvolutional neural network and transformer-based frameworks often struggle to\naccurately segment semantic change regions. Moreover, transformers-based\nmethods with standard self-attention suffer from quadratic computational\ncomplexity with respect to the image resolution, making them less practical for\nCD tasks with limited training data. To address these issues, we propose an\nefficient change detection framework, ELGC-Net, which leverages rich contextual\ninformation to precisely estimate change regions while reducing the model size.\nOur ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The\nfocus of our design is the introduction of an Efficient Local-Global Context\nAggregator module within the encoder, capturing enhanced global context and\nlocal spatial information through a novel pooled-transpose (PT) attention and\ndepthwise convolution, respectively. The PT attention employs pooling\noperations for robust feature extraction and minimizes computational cost with\ntransposed attention. Extensive experiments on three challenging CD datasets\ndemonstrate that ELGC-Net outperforms existing methods. Compared to the recent\ntransformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in\nintersection over union metric on the LEVIR-CD dataset, while significantly\nreducing trainable parameters. Our proposed ELGC-Net sets a new\nstate-of-the-art performance in remote sensing change detection benchmarks.\nFinally, we also introduce ELGC-Net-LW, a lighter variant with significantly\nreduced computational complexity, suitable for resource-constrained settings,\nwhile achieving comparable performance. Project url\nhttps://github.com/techmn/elgcnet.",
    "published": "2024-03-26T17:46:25Z",
    "updated": "2024-03-26T17:46:25Z",
    "authors": [
      "Mubashir Noman",
      "Mustansar Fiaz",
      "Hisham Cholakkal",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.07217v2",
    "title": "Attention-aware Semantic Communications for Collaborative Inference",
    "summary": "We propose a communication-efficient collaborative inference framework in the\ndomain of edge inference, focusing on the efficient use of vision transformer\n(ViT) models. The partitioning strategy of conventional collaborative inference\nfails to reduce communication cost because of the inherent architecture of ViTs\nmaintaining consistent layer dimensions across the entire transformer encoder.\nTherefore, instead of employing the partitioning strategy, our framework\nutilizes a lightweight ViT model on the edge device, with the server deploying\na complicated ViT model. To enhance communication efficiency and achieve the\nclassification accuracy of the server model, we propose two strategies: 1)\nattention-aware patch selection and 2) entropy-aware image transmission.\nAttention-aware patch selection leverages the attention scores generated by the\nedge device's transformer encoder to identify and select the image patches\ncritical for classification. This strategy enables the edge device to transmit\nonly the essential patches to the server, significantly improving communication\nefficiency. Entropy-aware image transmission uses min-entropy as a metric to\naccurately determine whether to depend on the lightweight model on the edge\ndevice or to request the inference from the server model. In our framework, the\nlightweight ViT model on the edge device acts as a semantic encoder,\nefficiently identifying and selecting the crucial image information required\nfor the classification task. Our experiments demonstrate that the proposed\ncollaborative inference framework can reduce communication overhead by 68% with\nonly a minimal loss in accuracy compared to the server model on the ImageNet\ndataset.",
    "published": "2024-02-23T10:08:45Z",
    "updated": "2024-05-31T14:23:09Z",
    "authors": [
      "Jiwoong Im",
      "Nayoung Kwon",
      "Taewoo Park",
      "Jiheon Woo",
      "Jaeho Lee",
      "Yongjune Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.07519v1",
    "title": "LATTE: Low-Precision Approximate Attention with Head-wise Trainable\n  Threshold for Efficient Transformer",
    "summary": "With the rise of Transformer models in NLP and CV domain, Multi-Head\nAttention has been proven to be a game-changer. However, its expensive\ncomputation poses challenges to the model throughput and efficiency, especially\nfor the long sequence tasks. Exploiting the sparsity in attention has been\nproven to be an effective way to reduce computation. Nevertheless, prior works\ndo not consider the various distributions among different heads and lack a\nsystematic method to determine the threshold. To address these challenges, we\npropose Low-Precision Approximate Attention with Head-wise Trainable Threshold\nfor Efficient Transformer (LATTE). LATTE employs a headwise threshold-based\nfilter with the low-precision dot product and computation reuse mechanism to\nreduce the computation of MHA. Moreover, the trainable threshold is introduced\nto provide a systematic method for adjusting the thresholds and enable\nend-to-end optimization. Experimental results indicate LATTE can smoothly adapt\nto both NLP and CV tasks, offering significant computation savings with only a\nminor compromise in performance. Also, the trainable threshold is shown to be\nessential for the leverage between the performance and the computation. As a\nresult, LATTE filters up to 85.16% keys with only a 0.87% accuracy drop in the\nCV task and 89.91% keys with a 0.86 perplexity increase in the NLP task.",
    "published": "2024-04-11T07:23:19Z",
    "updated": "2024-04-11T07:23:19Z",
    "authors": [
      "Jiing-Ping Wang",
      "Ming-Guang Lin",
      " An-Yeu",
      " Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.09789v1",
    "title": "LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for\n  Remote Sensing Image Interpretation",
    "summary": "Due to spatial redundancy in remote sensing images, sparse tokens containing\nrich information are usually involved in self-attention (SA) to reduce the\noverall token numbers within the calculation, avoiding the high computational\ncost issue in Vision Transformers. However, such methods usually obtain sparse\ntokens by hand-crafted or parallel-unfriendly designs, posing a challenge to\nreach a better balance between efficiency and performance. Different from them,\nthis paper proposes to use learnable meta tokens to formulate sparse tokens,\nwhich effectively learn key information meanwhile improving the inference\nspeed. Technically, the meta tokens are first initialized from image tokens via\ncross-attention. Then, we propose Dual Cross-Attention (DCA) to promote\ninformation exchange between image tokens and meta tokens, where they serve as\nquery and key (value) tokens alternatively in a dual-branch structure,\nsignificantly reducing the computational complexity compared to self-attention.\nBy employing DCA in the early stages with dense visual tokens, we obtain the\nhierarchical architecture LeMeViT with various sizes. Experimental results in\nclassification and dense prediction tasks show that LeMeViT has a significant\n$1.7 \\times$ speedup, fewer parameters, and competitive performance compared to\nthe baseline models, and achieves a better trade-off between efficiency and\nperformance.",
    "published": "2024-05-16T03:26:06Z",
    "updated": "2024-05-16T03:26:06Z",
    "authors": [
      "Wentao Jiang",
      "Jing Zhang",
      "Di Wang",
      "Qiming Zhang",
      "Zengmao Wang",
      "Bo Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.10172v2",
    "title": "Restoring Images in Adverse Weather Conditions via Histogram Transformer",
    "summary": "Transformer-based image restoration methods in adverse weather have achieved\nsignificant progress. Most of them use self-attention along the channel\ndimension or within spatially fixed-range blocks to reduce computational load.\nHowever, such a compromise results in limitations in capturing long-range\nspatial features. Inspired by the observation that the weather-induced\ndegradation factors mainly cause similar occlusion and brightness, in this\nwork, we propose an efficient Histogram Transformer (Histoformer) for restoring\nimages affected by adverse weather. It is powered by a mechanism dubbed\nhistogram self-attention, which sorts and segments spatial features into\nintensity-based bins. Self-attention is then applied across bins or within each\nbin to selectively focus on spatial features of dynamic range and process\nsimilar degraded pixels of the long range together. To boost histogram\nself-attention, we present a dynamic-range convolution enabling conventional\nconvolution to conduct operation over similar pixels rather than neighbor\npixels. We also observe that the common pixel-wise losses neglect linear\nassociation and correlation between output and ground-truth. Thus, we propose\nto leverage the Pearson correlation coefficient as a loss function to enforce\nthe recovered pixels following the identical order as ground-truth. Extensive\nexperiments demonstrate the efficacy and superiority of our proposed method. We\nhave released the codes in Github.",
    "published": "2024-07-14T11:59:22Z",
    "updated": "2024-07-25T07:26:40Z",
    "authors": [
      "Shangquan Sun",
      "Wenqi Ren",
      "Xinwei Gao",
      "Rui Wang",
      "Xiaochun Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.03703v2",
    "title": "CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications",
    "summary": "Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we propose Convolutional Additive\nToken Mixer (CATM) employing underlying spatial and channel attention as novel\ninteraction forms. This module eliminates troublesome complex operations such\nas matrix multiplication and Softmax. We introduce Convolutional Additive\nSelf-attention(CAS) block hybrid architecture and utilize CATM for each block.\nAnd further, we build a family of lightweight networks, which can be easily\nextended to various downstream tasks. Finally, we evaluate CAS-ViT across a\nvariety of vision tasks, including image classification, object detection,\ninstance segmentation, and semantic segmentation. Our M and T model achieves\n83.0\\%/84.1\\% top-1 with only 12M/21M parameters on ImageNet-1K. Meanwhile,\nthroughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior\nresults compared to other state-of-the-art backbones. Extensive experiments\ndemonstrate that our approach achieves a better balance of performance,\nefficient inference and easy-to-deploy. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}",
    "published": "2024-08-07T11:33:46Z",
    "updated": "2024-12-13T03:19:24Z",
    "authors": [
      "Tianfang Zhang",
      "Lei Li",
      "Yang Zhou",
      "Wentao Liu",
      "Chen Qian",
      "Jenq-Neng Hwang",
      "Xiangyang Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.13201v1",
    "title": "EAViT: External Attention Vision Transformer for Audio Classification",
    "summary": "This paper presents the External Attention Vision Transformer (EAViT) model,\na novel approach designed to enhance audio classification accuracy. As digital\naudio resources proliferate, the demand for precise and efficient audio\nclassification systems has intensified, driven by the need for improved\nrecommendation systems and user personalization in various applications,\nincluding music streaming platforms and environmental sound recognition.\nAccurate audio classification is crucial for organizing vast audio libraries\ninto coherent categories, enabling users to find and interact with their\npreferred audio content more effectively. In this study, we utilize the GTZAN\ndataset, which comprises 1,000 music excerpts spanning ten diverse genres. Each\n30-second audio clip is segmented into 3-second excerpts to enhance dataset\nrobustness and mitigate overfitting risks, allowing for more granular feature\nanalysis. The EAViT model integrates multi-head external attention (MEA)\nmechanisms into the Vision Transformer (ViT) framework, effectively capturing\nlong-range dependencies and potential correlations between samples. This\nexternal attention (EA) mechanism employs learnable memory units that enhance\nthe network's capacity to process complex audio features efficiently. The study\ndemonstrates that EAViT achieves a remarkable overall accuracy of 93.99%,\nsurpassing state-of-the-art models.",
    "published": "2024-08-23T16:31:06Z",
    "updated": "2024-08-23T16:31:06Z",
    "authors": [
      "Aquib Iqbal",
      "Abid Hasan Zim",
      "Md Asaduzzaman Tonmoy",
      "Limengnan Zhou",
      "Asad Malik",
      "Minoru Kuribayashi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.04239v1",
    "title": "On Deep-Learning-Based Closures for Algebraic Surrogate Models of\n  Turbulent Flows",
    "summary": "A deep-learning-based closure model to address energy loss in low-dimensional\nsurrogate models based on proper-orthogonal-decomposition (POD) modes is\nintroduced. Using a transformer-encoder block with easy-attention mechanism,\nthe model predicts the spatial probability density function of fluctuations not\ncaptured by the truncated POD modes. The methodology is demonstrated on the\nwake of the Windsor body at yaw angles of [2.5,5,7.5,10,12.5], with 7.5 as a\ntest case. Key coherent modes are identified by clustering them based on\ndominant frequency dynamics using Hotelling T2 on the spectral properties of\ntemporal coefficients. These coherent modes account for nearly 60% of the total\nenergy while comprising less than 10% of all modes. A common POD basis is\ncreated by concatenating coherent modes from training angles and\northonormalizing the set, reducing the basis vectors from 142 to 90 without\nlosing information. Transformers with different size on the attention layer,\n(64, 128 and 256), are trained to model the missing fluctuations. Larger\nattention sizes always improve predictions for the training set, but the\ntransformer with an attention layer of size 256 overshoots the fluctuations\npredictions in the test set because they have lower intensity than in the\ntraining cases. Adding the predicted fluctuations closes the energy gap between\nthe reconstruction and the original flow field, improving predictions for\nenergy, root-mean-square velocity fluctuations, and instantaneous flow fields.\nThe deepest architecture reduces mean energy error from 37% to 12% and\ndecreases the Kullback--Leibler divergence of velocity distributions from\nKL=0.2 to below KL=0.026.",
    "published": "2024-12-05T15:21:10Z",
    "updated": "2024-12-05T15:21:10Z",
    "authors": [
      "Benet Eiximeno",
      "Marcial SanchÃ­s-Agudo",
      "Arnau MirÃ³",
      "Ivette RodrÃ­guez",
      "Ricardo Vinuesa",
      "Oriol Lehmkuhl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.07044v1",
    "title": "Protego: Detecting Adversarial Examples for Vision Transformers via\n  Intrinsic Capabilities",
    "summary": "Transformer models have excelled in natural language tasks, prompting the\nvision community to explore their implementation in computer vision problems.\nHowever, these models are still influenced by adversarial examples. In this\npaper, we investigate the attack capabilities of six common adversarial attacks\non three pretrained ViT models to reveal the vulnerability of ViT models. To\nunderstand and analyse the bias in neural network decisions when the input is\nadversarial, we use two visualisation techniques that are attention rollout and\ngrad attention rollout. To prevent ViT models from adversarial attack, we\npropose Protego, a detection framework that leverages the transformer intrinsic\ncapabilities to detection adversarial examples of ViT models. Nonetheless, this\nis challenging due to a diversity of attack strategies that may be adopted by\nadversaries. Inspired by the attention mechanism, we know that the token of\nprediction contains all the information from the input sample. Additionally,\nthe attention region for adversarial examples differs from that of normal\nexamples. Given these points, we can train a detector that achieves superior\nperformance than existing detection methods to identify adversarial examples.\nOur experiments have demonstrated the high effectiveness of our detection\nmethod. For these six adversarial attack methods, our detector's AUC scores all\nexceed 0.95. Protego may advance investigations in metaverse security.",
    "published": "2025-01-13T03:54:19Z",
    "updated": "2025-01-13T03:54:19Z",
    "authors": [
      "Jialin Wu",
      "Kaikai Pan",
      "Yanjiao Chen",
      "Jiangyi Deng",
      "Shengyuan Pang",
      "Wenyuan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.17617v1",
    "title": "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment",
    "summary": "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications.",
    "published": "2025-01-29T12:46:42Z",
    "updated": "2025-01-29T12:46:42Z",
    "authors": [
      "Jonathan Teel",
      "Jocasta Cumberbatch",
      "Raphael Benington",
      "Quentin Baskerville"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.02004v1",
    "title": "Wavelet-based Positional Representation for Long Context",
    "summary": "In the realm of large-scale language models, a significant challenge arises\nwhen extrapolating sequences beyond the maximum allowable length. This is\nbecause the model's position embedding mechanisms are limited to positions\nencountered during training, thus preventing effective representation of\npositions in longer sequences. We analyzed conventional position encoding\nmethods for long contexts and found the following characteristics. (1) When the\nrepresentation dimension is regarded as the time axis, Rotary Position\nEmbedding (RoPE) can be interpreted as a restricted wavelet transform using\nHaar-like wavelets. However, because it uses only a fixed scale parameter, it\ndoes not fully exploit the advantages of wavelet transforms, which capture the\nfine movements of non-stationary signals using multiple scales (window sizes).\nThis limitation could explain why RoPE performs poorly in extrapolation. (2)\nPrevious research as well as our own analysis indicates that Attention with\nLinear Biases (ALiBi) functions similarly to windowed attention, using windows\nof varying sizes. However, it has limitations in capturing deep dependencies\nbecause it restricts the receptive field of the model. From these insights, we\npropose a new position representation method that captures multiple scales\n(i.e., window sizes) by leveraging wavelet transforms without limiting the\nmodel's attention field. Experimental results show that this new method\nimproves the performance of the model in both short and long contexts. In\nparticular, our method allows extrapolation of position information without\nlimiting the model's attention field.",
    "published": "2025-02-04T04:44:53Z",
    "updated": "2025-02-04T04:44:53Z",
    "authors": [
      "Yui Oka",
      "Taku Hasegawa",
      "Kyosuke Nishida",
      "Kuniko Saito"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.12677v1",
    "title": "Spiking Vision Transformer with Saccadic Attention",
    "summary": "The combination of Spiking Neural Networks (SNNs) and Vision Transformers\n(ViTs) holds potential for achieving both energy efficiency and high\nperformance, particularly suitable for edge vision applications. However, a\nsignificant performance gap still exists between SNN-based ViTs and their ANN\ncounterparts. Here, we first analyze why SNN-based ViTs suffer from limited\nperformance and identify a mismatch between the vanilla self-attention\nmechanism and spatio-temporal spike trains. This mismatch results in degraded\nspatial relevance and limited temporal interactions. To address these issues,\nwe draw inspiration from biological saccadic attention mechanisms and introduce\nan innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the\nspatial domain, SSSA employs a novel spike distribution-based method to\neffectively assess the relevance between Query and Key pairs in SNN-based ViTs.\nTemporally, SSSA employs a saccadic interaction module that dynamically focuses\non selected visual areas at each timestep and significantly enhances whole\nscene understanding through temporal interactions. Building on the SSSA\nmechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive\nexperiments across various visual tasks demonstrate that SNN-ViT achieves\nstate-of-the-art performance with linear computational complexity. The\neffectiveness and efficiency of the SNN-ViT highlight its potential for\npower-critical edge vision applications.",
    "published": "2025-02-18T09:32:29Z",
    "updated": "2025-02-18T09:32:29Z",
    "authors": [
      "Shuai Wang",
      "Malu Zhang",
      "Dehao Zhang",
      "Ammar Belatreche",
      "Yichen Xiao",
      "Yu Liang",
      "Yimeng Shan",
      "Qian Sun",
      "Enqi Zhang",
      "Yang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.12853v1",
    "title": "Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D\n  Segmentation",
    "summary": "This study proposes a 3D semantic segmentation method for the spine based on\nthe improved SwinUNETR to improve segmentation accuracy and robustness. Aiming\nat the complex anatomical structure of spinal images, this paper introduces a\nmulti-scale fusion mechanism to enhance the feature extraction capability by\nusing information of different scales, thereby improving the recognition\naccuracy of the model for the target area. In addition, the introduction of the\nadaptive attention mechanism enables the model to dynamically adjust the\nattention to the key area, thereby optimizing the boundary segmentation effect.\nThe experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net\n+ Transformer, the model of this study has achieved significant improvements in\nmIoU, mDice, and mAcc indicators, and has better segmentation performance. The\nablation experiment further verifies the effectiveness of the proposed improved\nmethod, proving that multi-scale fusion and adaptive attention mechanism have a\npositive effect on the segmentation task. Through the visualization analysis of\nthe inference results, the model can better restore the real anatomical\nstructure of the spinal image. Future research can further optimize the\nTransformer structure and expand the data scale to improve the generalization\nability of the model. This study provides an efficient solution for the task of\nmedical image segmentation, which is of great significance to intelligent\nmedical image analysis.",
    "published": "2025-03-17T06:27:43Z",
    "updated": "2025-03-17T06:27:43Z",
    "authors": [
      "Yanlin Xiang",
      "Qingyuan He",
      "Ting Xu",
      "Ran Hao",
      "Jiacheng Hu",
      "Hanchao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.01979v1",
    "title": "Correlation-Attention Masked Temporal Transformer for User Identity\n  Linkage Using Heterogeneous Mobility Data",
    "summary": "With the rise of social media and Location-Based Social Networks (LBSN),\ncheck-in data across platforms has become crucial for User Identity Linkage\n(UIL). These data not only reveal users' spatio-temporal information but also\nprovide insights into their behavior patterns and interests. However,\ncross-platform identity linkage faces challenges like poor data quality, high\nsparsity, and noise interference, which hinder existing methods from extracting\ncross-platform user information. To address these issues, we propose a\nCorrelation-Attention Masked Transformer for User Identity Linkage Network\n(MT-Link), a transformer-based framework to enhance model performance by\nlearning spatio-temporal co-occurrence patterns of cross-platform users. Our\nmodel effectively captures spatio-temporal co-occurrence in cross-platform user\ncheck-in sequences. It employs a correlation attention mechanism to detect the\nspatio-temporal co-occurrence between user check-in sequences. Guided by\nattention weight maps, the model focuses on co-occurrence points while\nfiltering out noise, ultimately improving classification performance.\nExperimental results show that our model significantly outperforms\nstate-of-the-art baselines by 12.92%~17.76% and 5.80%~8.38% improvements in\nterms of Macro-F1 and Area Under Curve (AUC).",
    "published": "2025-03-28T02:18:16Z",
    "updated": "2025-03-28T02:18:16Z",
    "authors": [
      "Ziang Yan",
      "Xingyu Zhao",
      "Hanqing Ma",
      "Wei Chen",
      "Jianpeng Qi",
      "Yanwei Yu",
      "Junyu Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16761v1",
    "title": "Tri-FusionNet: Enhancing Image Description Generation with\n  Transformer-based Fusion Network and Dual Attention Mechanism",
    "summary": "Image description generation is essential for accessibility and AI\nunderstanding of visual content. Recent advancements in deep learning have\nsignificantly improved natural language processing and computer vision. In this\nwork, we propose Tri-FusionNet, a novel image description generation model that\nintegrates transformer modules: a Vision Transformer (ViT) encoder module with\ndual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder\nmodule, and a Contrastive Language-Image Pre-Training (CLIP) integrating\nmodule. The ViT encoder, enhanced with dual attention, focuses on relevant\nspatial regions and linguistic context, improving image feature extraction. The\nRoBERTa decoder is employed to generate precise textual descriptions. CLIP's\nintegrating module aligns visual and textual data through contrastive learning,\nensuring effective combination of both modalities. This fusion of ViT, RoBERTa,\nand CLIP, along with dual attention, enables the model to produce more\naccurate, contextually rich, and flexible descriptions. The proposed framework\ndemonstrated competitive performance on the Flickr30k and Flickr8k datasets,\nwith BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores\nof 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of\n0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores\nof 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results\ndemonstrate the effectiveness of Tri-FusionNet in generating high-quality image\ndescriptions.",
    "published": "2025-04-23T14:33:29Z",
    "updated": "2025-04-23T14:33:29Z",
    "authors": [
      "Lakshita Agarwal",
      "Bindu Verma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.18574v2",
    "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of\n  the Gather-and-Aggregate Mechanism",
    "summary": "State-space models (SSMs) offer efficient alternatives to Transformers for\nlong sequences, but their fixed-size recurrent state limits capability on\nalgorithmic tasks, such as retrieving past context. In this work, we examine\nhow in-context retrieval operates in Transformer- and SSM-based language models\nand find that both rely on a similar Gather-and-Aggregate (G&A) mechanism: a\nGather Head extracts relevant information pieces from context, which an\nAggregate Head integrates into a single representation. In both architectures,\nG&A concentrates in a few heads, forming critical bottlenecks even for simple\nretrieval. For example, we show that disabling a single Gather or Aggregate\nHead in a pruned Llama-3.1-8B impairs retrieving the correct answer letter in\nMMLU, reducing its accuracy from 66% to 25% (random guessing). Moreover, this\nretrieval bottleneck can obscure limited knowledge demands of tasks as the\npruned model succeeds on MMLU with functioning G&A heads yet fails on other\nknowledge benchmarks. The bottleneck similarly extends to tasks where SSMs\ntypically underperform, such as GSM8K, BBH, and dialogue comprehension. We show\nthat SSMs' retrieval challenges manifest in these heads, creating smoother\nattention patterns instead of the sharp token transitions effective G&A\nrequires. Thus, the Transformer-SSM retrieval gap exists in just a few heads,\nrather than the entire language model. This suggests a unified explanation for\nTransformer vs. SSM performance gap while showing how to merge their strengths.\nWe find that pretrained hybrid models, where SSMs are combined with a few\nattention layers, delegate the role of Aggregate Heads to attention. Similarly,\nreplacing a single G&A head in a pretrained SSM with an attention variant\nboosts retrieval and benchmark scores.",
    "published": "2025-04-22T16:15:19Z",
    "updated": "2025-06-11T11:06:08Z",
    "authors": [
      "Aviv Bick",
      "Eric Xing",
      "Albert Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.17540v1",
    "title": "MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image\n  Colorization",
    "summary": "Thermal infrared (TIR) images, acquired through thermal radiation imaging,\nare unaffected by variations in lighting conditions and atmospheric haze.\nHowever, TIR images inherently lack color and texture information, limiting\ndownstream tasks and potentially causing visual fatigue. Existing colorization\nmethods primarily rely on single-band images with limited spectral information\nand insufficient feature extraction capabilities, which often result in image\ndistortion and semantic ambiguity. In contrast, multiband infrared imagery\nprovides richer spectral data, facilitating the preservation of finer details\nand enhancing semantic accuracy. In this paper, we propose a generative\nadversarial network (GAN)-based framework designed to integrate spectral\ninformation to enhance the colorization of infrared images. The framework\nemploys a multi-stage spectral self-attention Transformer network (MTSIC) as\nthe generator. Each spectral feature is treated as a token for self-attention\ncomputation, and a multi-head self-attention mechanism forms a spatial-spectral\nattention residual block (SARB), achieving multi-band feature mapping and\nreducing semantic confusion. Multiple SARB units are integrated into a\nTransformer-based single-stage network (STformer), which uses a U-shaped\narchitecture to extract contextual information, combined with multi-scale\nwavelet blocks (MSWB) to align semantic information in the spatial-frequency\ndual domain. Multiple STformer modules are cascaded to form MTSIC,\nprogressively optimizing the reconstruction quality. Experimental results\ndemonstrate that the proposed method significantly outperforms traditional\ntechniques and effectively enhances the visual quality of infrared images.",
    "published": "2025-06-21T01:42:25Z",
    "updated": "2025-06-21T01:42:25Z",
    "authors": [
      "Tingting Liu",
      "Yuan Liu",
      "Jinhui Tang",
      "Liyin Yuan",
      "Chengyu Liu",
      "Chunlai Li",
      "Xiubao Sui",
      "Qian Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09025v3",
    "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
    "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall.",
    "published": "2025-07-11T21:19:18Z",
    "updated": "2025-10-09T20:37:43Z",
    "authors": [
      "Chien Van Nguyen",
      "Ruiyi Zhang",
      "Hanieh Deilamsalehy",
      "Puneet Mathur",
      "Viet Dac Lai",
      "Haoliang Wang",
      "Jayakumar Subramanian",
      "Ryan A. Rossi",
      "Trung Bui",
      "Nikos Vlassis",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16663v1",
    "title": "The Loupe: A Plug-and-Play Attention Module for Amplifying\n  Discriminative Features in Vision Transformers",
    "summary": "Fine-Grained Visual Classification (FGVC) is a critical and challenging area\nwithin computer vision, demanding the identification of highly subtle,\nlocalized visual cues. The importance of FGVC extends to critical applications\nsuch as biodiversity monitoring and medical diagnostics, where precision is\nparamount. While large-scale Vision Transformers have achieved state-of-the-art\nperformance, their decision-making processes often lack the interpretability\nrequired for trust and verification in such domains. In this paper, we\nintroduce The Loupe, a novel, lightweight, and plug-and-play attention module\ndesigned to be inserted into pre-trained backbones like the Swin Transformer.\nThe Loupe is trained end-to-end with a composite loss function that implicitly\nguides the model to focus on the most discriminative object parts without\nrequiring explicit part-level annotations. Our unique contribution lies in\ndemonstrating that a simple, intrinsic attention mechanism can act as a\npowerful regularizer, significantly boosting performance while simultaneously\nproviding clear visual explanations. Our experimental evaluation on the\nchallenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of\na Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%.\nCrucially, our qualitative analysis of the learned attention maps reveals that\nThe Loupe effectively localizes semantically meaningful features, providing a\nvaluable tool for understanding and trusting the model's decision-making\nprocess.",
    "published": "2025-08-20T19:07:21Z",
    "updated": "2025-08-20T19:07:21Z",
    "authors": [
      "Naren Sengodan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18096v1",
    "title": "Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image\n  Diffusion Transformers",
    "summary": "Text-to-image diffusion models excel at translating language prompts into\nphotorealistic images by implicitly grounding textual concepts through their\ncross-modal attention mechanisms. Recent multi-modal diffusion transformers\nextend this by introducing joint self-attention over concatenated image and\ntext tokens, enabling richer and more scalable cross-modal alignment. However,\na detailed understanding of how and where these attention maps contribute to\nimage generation remains limited. In this paper, we introduce Seg4Diff\n(Segmentation for Diffusion), a systematic framework for analyzing the\nattention structures of MM-DiT, with a focus on how specific layers propagate\nsemantic information from text to image. Through comprehensive analysis, we\nidentify a semantic grounding expert layer, a specific MM-DiT block that\nconsistently aligns text tokens with spatially coherent image regions,\nnaturally producing high-quality semantic segmentation masks. We further\ndemonstrate that applying a lightweight fine-tuning scheme with mask-annotated\nimage data enhances the semantic grouping capabilities of these layers and\nthereby improves both segmentation performance and generated image fidelity.\nOur findings demonstrate that semantic grouping is an emergent property of\ndiffusion transformers and can be selectively amplified to advance both\nsegmentation and generation performance, paving the way for unified models that\nbridge visual perception and generation.",
    "published": "2025-09-22T17:59:54Z",
    "updated": "2025-09-22T17:59:54Z",
    "authors": [
      "Chaehyun Kim",
      "Heeseong Shin",
      "Eunbeen Hong",
      "Heeji Yoon",
      "Anurag Arnab",
      "Paul Hongsuck Seo",
      "Sunghwan Hong",
      "Seungryong Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06840v1",
    "title": "CNN-TFT explained by SHAP with multi-head attention weights for time\n  series forecasting",
    "summary": "Convolutional neural networks (CNNs) and transformer architectures offer\nstrengths for modeling temporal data: CNNs excel at capturing local patterns\nand translational invariances, while transformers effectively model long-range\ndependencies via self-attention. This paper proposes a hybrid architecture\nintegrating convolutional feature extraction with a temporal fusion transformer\n(TFT) backbone to enhance multivariate time series forecasting. The CNN module\nfirst applies a hierarchy of one-dimensional convolutional layers to distill\nsalient local patterns from raw input sequences, reducing noise and\ndimensionality. The resulting feature maps are then fed into the TFT, which\napplies multi-head attention to capture both short- and long-term dependencies\nand to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a\nhydroelectric natural flow time series dataset. Experimental results\ndemonstrate that CNN-TFT outperforms well-established deep learning models,\nwith a mean absolute percentage error of up to 2.2%. The explainability of the\nmodel is obtained by a proposed Shapley additive explanations with multi-head\nattention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,\nis promising for applications requiring high-fidelity, multivariate time series\nforecasts, being available for future analysis at\nhttps://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .",
    "published": "2025-10-08T10:08:28Z",
    "updated": "2025-10-08T10:08:28Z",
    "authors": [
      "Stefano F. Stefenon",
      "JoÃ£o P. Matos-Carvalho",
      "Valderi R. Q. Leithardt",
      "Kin-Choong Yow"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.04307v2",
    "title": "KSAT: Knowledge-infused Self Attention Transformer -- Integrating\n  Multiple Domain-Specific Contexts",
    "summary": "Domain-specific language understanding requires integrating multiple pieces\nof relevant contextual information. For example, we see both suicide and\ndepression-related behavior (multiple contexts) in the text ``I have a gun and\nfeel pretty bad about my life, and it wouldn't be the worst thing if I didn't\nwake up tomorrow''. Domain specificity in self-attention architectures is\nhandled by fine-tuning on excerpts from relevant domain specific resources\n(datasets and external knowledge - medical textbook chapters on mental health\ndiagnosis related to suicide and depression). We propose a modified\nself-attention architecture Knowledge-infused Self Attention Transformer (KSAT)\nthat achieves the integration of multiple domain-specific contexts through the\nuse of external knowledge sources. KSAT introduces knowledge-guided biases in\ndedicated self-attention layers for each knowledge source to accomplish this.\nIn addition, KSAT provides mechanics for controlling the trade-off between\nlearning from data and learning from knowledge. Our quantitative and\nqualitative evaluations show that (1) the KSAT architecture provides novel\nhuman-understandable ways to precisely measure and visualize the contributions\nof the infused domain contexts, and (2) KSAT performs competitively with other\nknowledge-infused baselines and significantly outperforms baselines that use\nfine-tuning for domain-specific tasks.",
    "published": "2022-10-09T17:23:49Z",
    "updated": "2023-06-24T04:33:03Z",
    "authors": [
      "Kaushik Roy",
      "Yuxin Zi",
      "Vignesh Narayanan",
      "Manas Gaur",
      "Amit Sheth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.16898v1",
    "title": "Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin\n  Lesion Segmentation",
    "summary": "Melanoma is caused by the abnormal growth of melanocytes in human skin. Like\nother cancers, this life-threatening skin cancer can be treated with early\ndiagnosis. To support a diagnosis by automatic skin lesion segmentation,\nseveral Fully Convolutional Network (FCN) approaches, specifically the U-Net\narchitecture, have been proposed. The U-Net model with a symmetrical\narchitecture has exhibited superior performance in the segmentation task.\nHowever, the locality restriction of the convolutional operation incorporated\nin the U-Net architecture limits its performance in capturing long-range\ndependency, which is crucial for the segmentation task in medical images. To\naddress this limitation, recently a Transformer based U-Net architecture that\nreplaces the CNN blocks with the Swin Transformer module has been proposed to\ncapture both local and global representation. In this paper, we propose\nAtt-SwinU-Net, an attention-based Swin U-Net extension, for medical image\nsegmentation. In our design, we seek to enhance the feature re-usability of the\nnetwork by carefully designing the skip connection path. We argue that the\nclassical concatenation operation utilized in the skip connection path can be\nfurther improved by incorporating an attention mechanism. By performing a\ncomprehensive ablation study on several skin lesion segmentation datasets, we\ndemonstrate the effectiveness of our proposed attention mechanism.",
    "published": "2022-10-30T17:41:35Z",
    "updated": "2022-10-30T17:41:35Z",
    "authors": [
      "Ehsan Khodapanah Aghdam",
      "Reza Azad",
      "Maral Zarvani",
      "Dorit Merhof"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1904.03375v1",
    "title": "Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling",
    "summary": "Geometric deep learning is increasingly important thanks to the popularity of\n3D sensors. Inspired by the recent advances in NLP domain, the self-attention\ntransformer is introduced to consume the point clouds. We develop Point\nAttention Transformers (PATs), using a parameter-efficient Group Shuffle\nAttention (GSA) to replace the costly Multi-Head Attention. We demonstrate its\nability to process size-varying inputs, and prove its permutation equivariance.\nBesides, prior work uses heuristics dependence on the input data (e.g.,\nFurthest Point Sampling) to hierarchically select subsets of input points.\nThereby, we for the first time propose an end-to-end learnable and\ntask-agnostic sampling operation, named Gumbel Subset Sampling (GSS), to select\na representative subset of input points. Equipped with Gumbel-Softmax, it\nproduces a \"soft\" continuous subset in training phase, and a \"hard\" discrete\nsubset in test phase. By selecting representative subsets in a hierarchical\nfashion, the networks learn a stronger representation of the input sets with\nlower computation cost. Experiments on classification and segmentation\nbenchmarks show the effectiveness and efficiency of our methods. Furthermore,\nwe propose a novel application, to process event camera stream as point clouds,\nand achieve a state-of-the-art performance on DVS128 Gesture Dataset.",
    "published": "2019-04-06T06:25:41Z",
    "updated": "2019-04-06T06:25:41Z",
    "authors": [
      "Jiancheng Yang",
      "Qiang Zhang",
      "Bingbing Ni",
      "Linguo Li",
      "Jinxian Liu",
      "Mengdie Zhou",
      "Qi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.03217v2",
    "title": "Local Information Assisted Attention-free Decoder for Audio Captioning",
    "summary": "Automated audio captioning aims to describe audio data with captions using\nnatural language. Existing methods often employ an encoder-decoder structure,\nwhere the attention-based decoder (e.g., Transformer decoder) is widely used\nand achieves state-of-the-art performance. Although this method effectively\ncaptures global information within audio data via the self-attention mechanism,\nit may ignore the event with short time duration, due to its limitation in\ncapturing local information in an audio signal, leading to inaccurate\nprediction of captions. To address this issue, we propose a method using the\npretrained audio neural networks (PANNs) as the encoder and local information\nassisted attention-free Transformer (LocalAFT) as the decoder. The novelty of\nour method is in the proposal of the LocalAFT decoder, which allows local\ninformation within an audio signal to be captured while retaining the global\ninformation. This enables the events of different duration, including short\nduration, to be captured for more precise caption generation. Experiments show\nthat our method outperforms the state-of-the-art methods in Task 6 of the DCASE\n2021 Challenge with the standard attention-based decoder for caption\ngeneration.",
    "published": "2022-01-10T08:55:52Z",
    "updated": "2022-07-03T04:26:41Z",
    "authors": [
      "Feiyang Xiao",
      "Jian Guan",
      "Haiyan Lan",
      "Qiaoxi Zhu",
      "Wenwu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.04590v1",
    "title": "GASP: Gated Attention For Saliency Prediction",
    "summary": "Saliency prediction refers to the computational task of modeling overt\nattention. Social cues greatly influence our attention, consequently altering\nour eye movements and behavior. To emphasize the efficacy of such features, we\npresent a neural model for integrating social cues and weighting their\ninfluences. Our model consists of two stages. During the first stage, we detect\ntwo social cues by following gaze, estimating gaze direction, and recognizing\naffect. These features are then transformed into spatiotemporal maps through\nimage processing operations. The transformed representations are propagated to\nthe second stage (GASP) where we explore various techniques of late fusion for\nintegrating social cues and introduce two sub-networks for directing attention\nto relevant stimuli. Our experiments indicate that fusion approaches achieve\nbetter results for static integration methods, whereas non-fusion approaches\nfor which the influence of each modality is unknown, result in better outcomes\nwhen coupled with recurrent models for dynamic saliency prediction. We show\nthat gaze direction and affective representations contribute a prediction to\nground-truth correspondence improvement of at least 5% compared to dynamic\nsaliency models without social cues. Furthermore, affective representations\nimprove GASP, supporting the necessity of considering affect-biased attention\nin predicting saliency.",
    "published": "2022-06-09T16:14:09Z",
    "updated": "2022-06-09T16:14:09Z",
    "authors": [
      "Fares Abawi",
      "Tom Weber",
      "Stefan Wermter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.01871v1",
    "title": "Attention-based Saliency Maps Improve Interpretability of Pneumothorax\n  Classification",
    "summary": "Purpose: To investigate chest radiograph (CXR) classification performance of\nvision transformers (ViT) and interpretability of attention-based saliency\nusing the example of pneumothorax classification.\n  Materials and Methods: In this retrospective study, ViTs were fine-tuned for\nlung disease classification using four public data sets: CheXpert, Chest X-Ray\n14, MIMIC CXR, and VinBigData. Saliency maps were generated using transformer\nmultimodal explainability and gradient-weighted class activation mapping\n(GradCAM). Classification performance was evaluated on the Chest X-Ray 14,\nVinBigData, and SIIM-ACR data sets using the area under the receiver operating\ncharacteristic curve analysis (AUC) and compared with convolutional neural\nnetworks (CNNs). The explainability methods were evaluated with\npositive/negative perturbation, sensitivity-n, effective heat ratio,\nintra-architecture repeatability and interarchitecture reproducibility. In the\nuser study, three radiologists classified 160 CXRs with/without saliency maps\nfor pneumothorax and rated their usefulness.\n  Results: ViTs had comparable CXR classification AUCs compared with\nstate-of-the-art CNNs 0.95 (95% CI: 0.943, 0.950) versus 0.83 (95%, CI 0.826,\n0.842) on Chest X-Ray 14, 0.84 (95% CI: 0.769, 0.912) versus 0.83 (95% CI:\n0.760, 0.895) on VinBigData, and 0.85 (95% CI: 0.847, 0.861) versus 0.87 (95%\nCI: 0.868, 0.882) on SIIM ACR. Both saliency map methods unveiled a strong bias\ntoward pneumothorax tubes in the models. Radiologists found 47% of the\nattention-based saliency maps useful and 39% of GradCAM. The attention-based\nmethods outperformed GradCAM on all metrics.\n  Conclusion: ViTs performed similarly to CNNs in CXR classification, and their\nattention-based saliency maps were more useful to radiologists and outperformed\nGradCAM.",
    "published": "2023-03-03T12:05:41Z",
    "updated": "2023-03-03T12:05:41Z",
    "authors": [
      "Alessandro Wollek",
      "Robert Graf",
      "SaÅ¡a ÄeÄatka",
      "Nicola Fink",
      "Theresa Willem",
      "Bastian O. Sabel",
      "Tobias Lasser"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.08415v2",
    "title": "GMAN: A Graph Multi-Attention Network for Traffic Prediction",
    "summary": "Long-term traffic prediction is highly challenging due to the complexity of\ntraffic systems and the constantly changing nature of many impacting factors.\nIn this paper, we focus on the spatio-temporal factors, and propose a graph\nmulti-attention network (GMAN) to predict traffic conditions for time steps\nahead at different locations on a road network graph. GMAN adapts an\nencoder-decoder architecture, where both the encoder and the decoder consist of\nmultiple spatio-temporal attention blocks to model the impact of the\nspatio-temporal factors on traffic conditions. The encoder encodes the input\ntraffic features and the decoder predicts the output sequence. Between the\nencoder and the decoder, a transform attention layer is applied to convert the\nencoded traffic features to generate the sequence representations of future\ntime steps as the input of the decoder. The transform attention mechanism\nmodels the direct relationships between historical and future time steps that\nhelps to alleviate the error propagation problem among prediction time steps.\nExperimental results on two real-world traffic prediction tasks (i.e., traffic\nvolume prediction and traffic speed prediction) demonstrate the superiority of\nGMAN. In particular, in the 1 hour ahead prediction, GMAN outperforms\nstate-of-the-art methods by up to 4% improvement in MAE measure. The source\ncode is available at https://github.com/zhengchuanpan/GMAN.",
    "published": "2019-11-11T07:48:43Z",
    "updated": "2019-11-26T03:10:26Z",
    "authors": [
      "Chuanpan Zheng",
      "Xiaoliang Fan",
      "Cheng Wang",
      "Jianzhong Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2007.14062v2",
    "title": "Big Bird: Transformers for Longer Sequences",
    "summary": "Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having $O(1)$ global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.",
    "published": "2020-07-28T08:34:04Z",
    "updated": "2021-01-08T07:41:50Z",
    "authors": [
      "Manzil Zaheer",
      "Guru Guruganesh",
      "Avinava Dubey",
      "Joshua Ainslie",
      "Chris Alberti",
      "Santiago Ontanon",
      "Philip Pham",
      "Anirudh Ravula",
      "Qifan Wang",
      "Li Yang",
      "Amr Ahmed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2008.01077v1",
    "title": "Self-attention encoding and pooling for speaker recognition",
    "summary": "The computing power of mobile devices limits the end-user applications in\nterms of storage size, processing, memory and energy consumption. These\nlimitations motivate researchers for the design of more efficient deep models.\nOn the other hand, self-attention networks based on Transformer architecture\nhave attracted remarkable interests due to their high parallelization\ncapabilities and strong performance on a variety of Natural Language Processing\n(NLP) applications. Inspired by the Transformer, we propose a tandem\nSelf-Attention Encoding and Pooling (SAEP) mechanism to obtain a discriminative\nspeaker embedding given non-fixed length speech utterances. SAEP is a stack of\nidentical blocks solely relied on self-attention and position-wise feed-forward\nnetworks to create vector representation of speakers. This approach encodes\nshort-term speaker spectral features into speaker embeddings to be used in\ntext-independent speaker verification. We have evaluated this approach on both\nVoxCeleb1 & 2 datasets. The proposed architecture is able to outperform the\nbaseline x-vector, and shows competitive performance to some other benchmarks\nbased on convolutions, with a significant reduction in model size. It employs\n94%, 95%, and 73% less parameters compared to ResNet-34, ResNet-50, and\nx-vector, respectively. This indicates that the proposed fully attention based\narchitecture is more efficient in extracting time-invariant features from\nspeaker utterances.",
    "published": "2020-08-03T09:31:27Z",
    "updated": "2020-08-03T09:31:27Z",
    "authors": [
      "Pooyan Safari",
      "Miquel India",
      "Javier Hernando"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.07625v2",
    "title": "A Fine-Grained Visual Attention Approach for Fingerspelling Recognition\n  in the Wild",
    "summary": "Fingerspelling in sign language has been the means of communicating technical\nterms and proper nouns when they do not have dedicated sign language gestures.\nAutomatic recognition of fingerspelling can help resolve communication barriers\nwhen interacting with deaf people. The main challenges prevalent in\nfingerspelling recognition are the ambiguity in the gestures and strong\narticulation of the hands. The automatic recognition model should address high\ninter-class visual similarity and high intra-class variation in the gestures.\nMost of the existing research in fingerspelling recognition has focused on the\ndataset collected in a controlled environment. The recent collection of a\nlarge-scale annotated fingerspelling dataset in the wild, from social media and\nonline platforms, captures the challenges in a real-world scenario. In this\nwork, we propose a fine-grained visual attention mechanism using the\nTransformer model for the sequence-to-sequence prediction task in the wild\ndataset. The fine-grained attention is achieved by utilizing the change in\nmotion of the video frames (optical flow) in sequential context-based attention\nalong with a Transformer encoder model. The unsegmented continuous video\ndataset is jointly trained by balancing the Connectionist Temporal\nClassification (CTC) loss and the maximum-entropy loss. The proposed approach\ncan capture better fine-grained attention in a single iteration. Experiment\nevaluations show that it outperforms the state-of-the-art approaches.",
    "published": "2021-05-17T06:15:35Z",
    "updated": "2021-08-22T05:31:45Z",
    "authors": [
      "Kamala Gajurel",
      "Cuncong Zhong",
      "Guanghui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.02253v2",
    "title": "X-volution: On the unification of convolution and self-attention",
    "summary": "Convolution and self-attention are acting as two fundamental building blocks\nin deep neural networks, where the former extracts local image features in a\nlinear way while the latter non-locally encodes high-order contextual\nrelationships. Though essentially complementary to each other, i.e.,\nfirst-/high-order, stat-of-the-art architectures, i.e., CNNs or transformers\nlack a principled way to simultaneously apply both operations in a single\ncomputational module, due to their heterogeneous computing pattern and\nexcessive burden of global dot-product for visual tasks. In this work, we\ntheoretically derive a global self-attention approximation scheme, which\napproximates a self-attention via the convolution operation on transformed\nfeatures. Based on the approximated scheme, we establish a multi-branch\nelementary module composed of both convolution and self-attention operation,\ncapable of unifying both local and non-local feature interaction. Importantly,\nonce trained, this multi-branch module could be conditionally converted into a\nsingle standard convolution operation via structural re-parameterization,\nrendering a pure convolution styled operator named X-volution, ready to be\nplugged into any modern networks as an atomic operation. Extensive experiments\ndemonstrate that the proposed X-volution, achieves highly competitive visual\nunderstanding improvements (+1.2% top-1 accuracy on ImageNet classification,\n+1.7 box AP and +1.5 mask AP on COCO detection and segmentation).",
    "published": "2021-06-04T04:32:02Z",
    "updated": "2021-06-07T09:03:46Z",
    "authors": [
      "Xuanhong Chen",
      "Hang Wang",
      "Bingbing Ni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.00606v6",
    "title": "Action Transformer: A Self-Attention Model for Short-Time Pose-Based\n  Human Action Recognition",
    "summary": "Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time,\nshort-time HAR. The proposed methodology was extensively tested on MPOSE2021\nand compared to several state-of-the-art architectures, proving the\neffectiveness of the AcT model and laying the foundations for future work on\nHAR.",
    "published": "2021-07-01T16:53:16Z",
    "updated": "2022-01-10T08:42:16Z",
    "authors": [
      "Vittorio Mazzia",
      "Simone Angarano",
      "Francesco Salvetti",
      "Federico Angelini",
      "Marcello Chiaberge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.12001v1",
    "title": "DA-FDFtNet: Dual Attention Fake Detection Fine-tuning Network to Detect\n  Various AI-Generated Fake Images",
    "summary": "Due to the advancement of Generative Adversarial Networks (GAN),\nAutoencoders, and other AI technologies, it has been much easier to create fake\nimages such as \"Deepfakes\". More recent research has introduced few-shot\nlearning, which uses a small amount of training data to produce fake images and\nvideos more effectively. Therefore, the ease of generating manipulated images\nand the difficulty of distinguishing those images can cause a serious threat to\nour society, such as propagating fake information. However, detecting realistic\nfake images generated by the latest AI technology is challenging due to the\nreasons mentioned above. In this work, we propose Dual Attention Fake Detection\nFine-tuning Network (DA-FDFtNet) to detect the manipulated fake face images\nfrom the real face data. Our DA-FDFtNet integrates the pre-trained model with\nFine-Tune Transformer, MBblockV3, and a channel attention module to improve the\nperformance and robustness across different types of fake images. In\nparticular, Fine-Tune Transformer consists of multiple numbers of an\nimage-based self-attention module and a down-sampling layer. The channel\nattention module is also connected with the pre-trained model to capture the\nfake images feature space. We experiment with our DA-FDFtNet with the\nFaceForensics++ dataset and various GAN-generated datasets, and we show that\nour approach outperforms the previous baseline models.",
    "published": "2021-12-22T16:25:24Z",
    "updated": "2021-12-22T16:25:24Z",
    "authors": [
      "Young Oh Bang",
      "Simon S. Woo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.05962v1",
    "title": "Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain\n  Analysis: From Theory to Practice",
    "summary": "Vision Transformer (ViT) has recently demonstrated promise in computer vision\nproblems. However, unlike Convolutional Neural Networks (CNN), it is known that\nthe performance of ViT saturates quickly with depth increasing, due to the\nobserved attention collapse or patch uniformity. Despite a couple of empirical\nsolutions, a rigorous framework studying on this scalability issue remains\nelusive. In this paper, we first establish a rigorous theory framework to\nanalyze ViT features from the Fourier spectrum domain. We show that the\nself-attention mechanism inherently amounts to a low-pass filter, which\nindicates when ViT scales up its depth, excessive low-pass filtering will cause\nfeature maps to only preserve their Direct-Current (DC) component. We then\npropose two straightforward yet effective techniques to mitigate the\nundesirable low-pass limitation. The first technique, termed AttnScale,\ndecomposes a self-attention block into low-pass and high-pass components, then\nrescales and combines these two filters to produce an all-pass self-attention\nmatrix. The second technique, termed FeatScale, re-weights feature maps on\nseparate frequency bands to amplify the high-frequency signals. Both techniques\nare efficient and hyperparameter-free, while effectively overcoming relevant\nViT training artifacts such as attention collapse and patch uniformity. By\nseamlessly plugging in our techniques to multiple ViT variants, we demonstrate\nthat they consistently help ViTs benefit from deeper architectures, bringing up\nto 1.1% performance gains \"for free\" (e.g., with little parameter overhead). We\npublicly release our codes and pre-trained models at\nhttps://github.com/VITA-Group/ViT-Anti-Oversmoothing.",
    "published": "2022-03-09T23:55:24Z",
    "updated": "2022-03-09T23:55:24Z",
    "authors": [
      "Peihao Wang",
      "Wenqing Zheng",
      "Tianlong Chen",
      "Zhangyang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.01765v2",
    "title": "Continuous Decomposition of Granularity for Neural Paraphrase Generation",
    "summary": "While Transformers have had significant success in paragraph generation, they\ntreat sentences as linear sequences of tokens and often neglect their\nhierarchical information. Prior work has shown that decomposing the levels of\ngranularity~(e.g., word, phrase, or sentence) for input tokens has produced\nsubstantial improvements, suggesting the possibility of enhancing Transformers\nvia more fine-grained modeling of granularity. In this work, we propose a\ncontinuous decomposition of granularity for neural paraphrase generation\n(C-DNPG). In order to efficiently incorporate granularity into sentence\nencoding, C-DNPG introduces a granularity-aware attention (GA-Attention)\nmechanism which extends the multi-head self-attention with: 1) a granularity\nhead that automatically infers the hierarchical structure of a sentence by\nneurally estimating the granularity level of each input token; and 2) two novel\nattention masks, namely, granularity resonance and granularity scope, to\nefficiently encode granularity into attention. Experiments on two benchmarks,\nincluding Quora question pairs and Twitter URLs have shown that C-DNPG\noutperforms baseline models by a remarkable margin and achieves\nstate-of-the-art results in terms of many metrics. Qualitative analysis reveals\nthat C-DNPG indeed captures fine-grained levels of granularity with\neffectiveness.",
    "published": "2022-09-05T05:02:42Z",
    "updated": "2022-09-16T08:43:27Z",
    "authors": [
      "Xiaodong Gu",
      "Zhaowei Zhang",
      "Sang-Woo Lee",
      "Kang Min Yoo",
      "Jung-Woo Ha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.01438v2",
    "title": "Variable Attention Masking for Configurable Transformer Transducer\n  Speech Recognition",
    "summary": "This work studies the use of attention masking in transformer transducer\nbased speech recognition for building a single configurable model for different\ndeployment scenarios. We present a comprehensive set of experiments comparing\nfixed masking, where the same attention mask is applied at every frame, with\nchunked masking, where the attention mask for each frame is determined by chunk\nboundaries, in terms of recognition accuracy and latency. We then explore the\nuse of variable masking, where the attention masks are sampled from a target\ndistribution at training time, to build models that can work in different\nconfigurations. Finally, we investigate how a single configurable model can be\nused to perform both first pass streaming recognition and second pass acoustic\nrescoring. Experiments show that chunked masking achieves a better accuracy vs\nlatency trade-off compared to fixed masking, both with and without FastEmit. We\nalso show that variable masking improves the accuracy by up to 8% relative in\nthe acoustic re-scoring scenario.",
    "published": "2022-11-02T19:14:02Z",
    "updated": "2023-04-18T09:59:52Z",
    "authors": [
      "Pawel Swietojanski",
      "Stefan Braun",
      "Dogan Can",
      "Thiago Fraga da Silva",
      "Arnab Ghoshal",
      "Takaaki Hori",
      "Roger Hsiao",
      "Henry Mason",
      "Erik McDermott",
      "Honza Silovsky",
      "Ruchir Travadi",
      "Xiaodan Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.13955v3",
    "title": "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision\n  Transformer with Heterogeneous Attention",
    "summary": "Secure multi-party computation (MPC) enables computation directly on\nencrypted data and protects both data and model privacy in deep learning\ninference. However, existing neural network architectures, including Vision\nTransformers (ViTs), are not designed or optimized for MPC and incur\nsignificant latency overhead. We observe Softmax accounts for the major latency\nbottleneck due to a high communication complexity, but can be selectively\nreplaced or linearized without compromising the model accuracy. Hence, in this\npaper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet\nefficient ViT inference in MPC. Based on a systematic latency and accuracy\nevaluation of the Softmax attention and other attention variants, we propose a\nheterogeneous attention optimization space. We also develop a simple yet\neffective MPC-aware neural architecture search algorithm for fast Pareto\noptimization. To further boost the inference efficiency, we propose MPCViT+, to\njointly optimize the Softmax attention and other network components, including\nGeLU, matrix multiplication, etc. With extensive experiments, we demonstrate\nthat MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2x, 2.9x and\n1.9x latency reduction compared with baseline ViT, MPCFormer and THE-X on the\nTiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto\nfront compared with MPCViT. The code and models for evaluation are available at\nhttps://github.com/PKU-SEC-Lab/mpcvit.",
    "published": "2022-11-25T08:37:17Z",
    "updated": "2023-08-19T08:00:55Z",
    "authors": [
      "Wenxuan Zeng",
      "Meng Li",
      "Wenjie Xiong",
      "Tong Tong",
      "Wen-jie Lu",
      "Jin Tan",
      "Runsheng Wang",
      "Ru Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.05694v1",
    "title": "Multi-scale Geometry-aware Transformer for 3D Point Cloud Classification",
    "summary": "Self-attention modules have demonstrated remarkable capabilities in capturing\nlong-range relationships and improving the performance of point cloud tasks.\nHowever, point cloud objects are typically characterized by complex,\ndisordered, and non-Euclidean spatial structures with multiple scales, and\ntheir behavior is often dynamic and unpredictable. The current self-attention\nmodules mostly rely on dot product multiplication and dimension alignment among\nquery-key-value features, which cannot adequately capture the multi-scale\nnon-Euclidean structures of point cloud objects. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants,\nMulti-scale Geometry-aware Transformer (MGT). MGT processes point cloud data\nwith multi-scale local and global geometric information in the following three\naspects. At first, the MGT divides point cloud data into patches with multiple\nscales. Secondly, a local feature extractor based on sphere mapping is proposed\nto explore the geometry inner each patch and generate a fixed-length\nrepresentation for each patch. Thirdly, the fixed-length representations are\nfed into a novel geodesic-based self-attention to capture the global\nnon-Euclidean geometry between patches. Finally, all the modules are integrated\ninto the framework of MGT with an end-to-end training scheme. Experimental\nresults demonstrate that the MGT vastly increases the capability of capturing\nmulti-scale geometry using the self-attention mechanism and achieves strong\ncompetitive performance on mainstream point cloud benchmarks.",
    "published": "2023-04-12T08:34:56Z",
    "updated": "2023-04-12T08:34:56Z",
    "authors": [
      "Xian Wei",
      "Muyu Wang",
      "Shing-Ho Jonathan Lin",
      "Zhengyu Li",
      "Jian Yang",
      "Arafat Al-Jawari",
      "Xuan Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.05322v1",
    "title": "TPS++: Attention-Enhanced Thin-Plate Spline for Scene Text Recognition",
    "summary": "Text irregularities pose significant challenges to scene text recognizers.\nThin-Plate Spline (TPS)-based rectification is widely regarded as an effective\nmeans to deal with them. Currently, the calculation of TPS transformation\nparameters purely depends on the quality of regressed text borders. It ignores\nthe text content and often leads to unsatisfactory rectified results for\nseverely distorted text. In this work, we introduce TPS++, an\nattention-enhanced TPS transformation that incorporates the attention mechanism\nto text rectification for the first time. TPS++ formulates the parameter\ncalculation as a joint process of foreground control point regression and\ncontent-based attention score estimation, which is computed by a dedicated\ndesigned gated-attention block. TPS++ builds a more flexible content-aware\nrectifier, generating a natural text correction that is easier to read by the\nsubsequent recognizer. Moreover, TPS++ shares the feature backbone with the\nrecognizer in part and implements the rectification at feature-level rather\nthan image-level, incurring only a small overhead in terms of parameters and\ninference time. Experiments on public benchmarks show that TPS++ consistently\nimproves the recognition and achieves state-of-the-art accuracy. Meanwhile, it\ngeneralizes well on different backbones and recognizers. Code is at\nhttps://github.com/simplify23/TPS_PP.",
    "published": "2023-05-09T10:16:43Z",
    "updated": "2023-05-09T10:16:43Z",
    "authors": [
      "Tianlun Zheng",
      "Zhineng Chen",
      "Jinfeng Bai",
      "Hongtao Xie",
      "Yu-Gang Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.12972v1",
    "title": "DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting",
    "summary": "In this paper, we propose a new operator, called 3D DeFormable Attention\n(DFA3D), for 2D-to-3D feature lifting, which transforms multi-view 2D image\nfeatures into a unified 3D space for 3D object detection. Existing feature\nlifting approaches, such as Lift-Splat-based and 2D attention-based, either use\nestimated depth to get pseudo LiDAR features and then splat them to a 3D space,\nwhich is a one-pass operation without feature refinement, or ignore depth and\nlift features by 2D attention mechanisms, which achieve finer semantics while\nsuffering from a depth ambiguity problem. In contrast, our DFA3D-based method\nfirst leverages the estimated depth to expand each view's 2D feature map to 3D\nand then utilizes DFA3D to aggregate features from the expanded 3D feature\nmaps. With the help of DFA3D, the depth ambiguity problem can be effectively\nalleviated from the root, and the lifted features can be progressively refined\nlayer by layer, thanks to the Transformer-like architecture. In addition, we\npropose a mathematically equivalent implementation of DFA3D which can\nsignificantly improve its memory efficiency and computational speed. We\nintegrate DFA3D into several methods that use 2D attention-based feature\nlifting with only a few modifications in code and evaluate on the nuScenes\ndataset. The experiment results show a consistent improvement of +1.41\\% mAP on\naverage, and up to +15.1\\% mAP improvement when high-quality depth information\nis available, demonstrating the superiority, applicability, and huge potential\nof DFA3D. The code is available at\nhttps://github.com/IDEA-Research/3D-deformable-attention.git.",
    "published": "2023-07-24T17:49:11Z",
    "updated": "2023-07-24T17:49:11Z",
    "authors": [
      "Hongyang Li",
      "Hao Zhang",
      "Zhaoyang Zeng",
      "Shilong Liu",
      "Feng Li",
      "Tianhe Ren",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.14010v1",
    "title": "ESSAformer: Efficient Transformer for Hyperspectral Image\n  Super-resolution",
    "summary": "Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a\nhigh-resolution hyperspectral image from a low-resolution observation. However,\nthe prevailing CNN-based approaches have shown limitations in building\nlong-range dependencies and capturing interaction information between spectral\nfeatures. This results in inadequate utilization of spectral information and\nartifacts after upsampling. To address this issue, we propose ESSAformer, an\nESSA attention-embedded Transformer network for single-HSI-SR with an iterative\nrefining structure. Specifically, we first introduce a robust and\nspectral-friendly similarity metric, \\ie, the spectral correlation coefficient\nof the spectrum (SCC), to replace the original attention matrix and\nincorporates inductive biases into the model to facilitate training. Built upon\nit, we further utilize the kernelizable attention technique with theoretical\nsupport to form a novel efficient SCC-kernel-based self-attention (ESSA) and\nreduce attention computation to linear complexity. ESSA enlarges the receptive\nfield for features after upsampling without bringing much computation and\nallows the model to effectively utilize spatial-spectral information from\ndifferent scales, resulting in the generation of more natural high-resolution\nimages. Without the need for pretraining on large-scale datasets, our\nexperiments demonstrate ESSA's effectiveness in both visual quality and\nquantitative results.",
    "published": "2023-07-26T07:45:14Z",
    "updated": "2023-07-26T07:45:14Z",
    "authors": [
      "Mingjin Zhang",
      "Chi Zhang",
      "Qiming Zhang",
      "Jie Guo",
      "Xinbo Gao",
      "Jing Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.03048v1",
    "title": "Multi-scale Alternated Attention Transformer for Generalized Stereo\n  Matching",
    "summary": "Recent stereo matching networks achieves dramatic performance by introducing\nepipolar line constraint to limit the matching range of dual-view. However, in\ncomplicated real-world scenarios, the feature information based on\nintra-epipolar line alone is too weak to facilitate stereo matching. In this\npaper, we present a simple but highly effective network called Alternated\nAttention U-shaped Transformer (AAUformer) to balance the impact of epipolar\nline in dual and single view respectively for excellent generalization\nperformance. Compared to other models, our model has several main designs: 1)\nto better liberate the local semantic features of the single-view at pixel\nlevel, we introduce window self-attention to break the limits of intra-row\nself-attention and completely replace the convolutional network for denser\nfeatures before cross-matching; 2) the multi-scale alternated attention\nbackbone network was designed to extract invariant features in order to\nachieves the coarse-to-fine matching process for hard-to-discriminate regions.\nWe performed a series of both comparative studies and ablation studies on\nseveral mainstream stereo matching datasets. The results demonstrate that our\nmodel achieves state-of-the-art on the Scene Flow dataset, and the fine-tuning\nperformance is competitive on the KITTI 2015 dataset. In addition, for cross\ngeneralization experiments on synthetic and real-world datasets, our model\noutperforms several state-of-the-art works.",
    "published": "2023-08-06T08:22:39Z",
    "updated": "2023-08-06T08:22:39Z",
    "authors": [
      "Wei Miao",
      "Hong Zhao",
      "Tongjia Chen",
      "Wei Huang",
      "Changyan Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.11523v6",
    "title": "RMT: Retentive Networks Meet Vision Transformers",
    "summary": "Vision Transformer (ViT) has gained increasing attention in the computer\nvision community in recent years. However, the core component of ViT,\nSelf-Attention, lacks explicit spatial priors and bears a quadratic\ncomputational complexity, thereby constraining the applicability of ViT. To\nalleviate these issues, we draw inspiration from the recent Retentive Network\n(RetNet) in the field of NLP, and propose RMT, a strong vision backbone with\nexplicit spatial prior for general purposes. Specifically, we extend the\nRetNet's temporal decay mechanism to the spatial domain, and propose a spatial\ndecay matrix based on the Manhattan distance to introduce the explicit spatial\nprior to Self-Attention. Additionally, an attention decomposition form that\nadeptly adapts to explicit spatial prior is proposed, aiming to reduce the\ncomputational burden of modeling global information without disrupting the\nspatial decay matrix. Based on the spatial decay matrix and the attention\ndecomposition form, we can flexibly integrate explicit spatial prior into the\nvision backbone with linear complexity. Extensive experiments demonstrate that\nRMT exhibits exceptional performance across various vision tasks. Specifically,\nwithout extra training data, RMT achieves **84.8%** and **86.1%** top-1 acc on\nImageNet-1k with **27M/4.5GFLOPs** and **96M/18.2GFLOPs**. For downstream\ntasks, RMT achieves **54.5** box AP and **47.2** mask AP on the COCO detection\ntask, and **52.8** mIoU on the ADE20K semantic segmentation task. Code is\navailable at https://github.com/qhfan/RMT",
    "published": "2023-09-20T00:57:48Z",
    "updated": "2025-02-27T03:14:35Z",
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Mingrui Chen",
      "Hongmin Liu",
      "Ran He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.04466v1",
    "title": "HartleyMHA: Self-Attention in Frequency Domain for Resolution-Robust and\n  Parameter-Efficient 3D Image Segmentation",
    "summary": "With the introduction of Transformers, different attention-based models have\nbeen proposed for image segmentation with promising results. Although\nself-attention allows capturing of long-range dependencies, it suffers from a\nquadratic complexity in the image size especially in 3D. To avoid the\nout-of-memory error during training, input size reduction is usually required\nfor 3D segmentation, but the accuracy can be suboptimal when the trained models\nare applied on the original image size. To address this limitation, inspired by\nthe Fourier neural operator (FNO), we introduce the HartleyMHA model which is\nrobust to training image resolution with efficient self-attention. FNO is a\ndeep learning framework for learning mappings between functions in partial\ndifferential equations, which has the appealing properties of zero-shot\nsuper-resolution and global receptive field. We modify the FNO by using the\nHartley transform with shared parameters to reduce the model size by orders of\nmagnitude, and this allows us to further apply self-attention in the frequency\ndomain for more expressive high-order feature combination with improved\nefficiency. When tested on the BraTS'19 dataset, it achieved superior\nrobustness to training image resolution than other tested models with less than\n1% of their model parameters.",
    "published": "2023-10-05T18:44:41Z",
    "updated": "2023-10-05T18:44:41Z",
    "authors": [
      "Ken C. L. Wong",
      "Hongzhi Wang",
      "Tanveer Syeda-Mahmood"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.17629v4",
    "title": "RQFormer: Rotated Query Transformer for End-to-End Oriented Object\n  Detection",
    "summary": "Oriented object detection presents a challenging task due to the presence of\nobject instances with multiple orientations, varying scales, and dense\ndistributions. Recently, end-to-end detectors have made significant strides by\nemploying attention mechanisms and refining a fixed number of queries through\nconsecutive decoder layers. However, existing end-to-end oriented object\ndetectors still face two primary challenges: 1) misalignment between positional\nqueries and keys, leading to inconsistency between classification and\nlocalization; and 2) the presence of a large number of similar queries, which\ncomplicates one-to-one label assignments and optimization. To address these\nlimitations, we propose an end-to-end oriented detector called the Rotated\nQuery Transformer, which integrates two key technologies: Rotated RoI Attention\n(RRoI Attention) and Selective Distinct Queries (SDQ). First, RRoI Attention\naligns positional queries and keys from oriented regions of interest through\ncross-attention. Second, SDQ collects queries from intermediate decoder layers\nand filters out similar ones to generate distinct queries, thereby facilitating\nthe optimization of one-to-one label assignments. Finally, extensive\nexperiments conducted on four remote sensing datasets and one scene text\ndataset demonstrate the effectiveness of our method. To further validate its\ngeneralization capability, we also extend our approach to horizontal object\ndetection The code is available at\n\\url{https://github.com/wokaikaixinxin/RQFormer}.",
    "published": "2023-11-29T13:43:17Z",
    "updated": "2024-12-16T14:56:52Z",
    "authors": [
      "Jiaqi Zhao",
      "Zeyu Ding",
      "Yong Zhou",
      "Hancheng Zhu",
      "Wenliang Du",
      "Rui Yao",
      "Abdulmotaleb El Saddik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.09529v1",
    "title": "Can Physician Judgment Enhance Model Trustworthiness? A Case Study on\n  Predicting Pathological Lymph Nodes in Rectal Cancer",
    "summary": "Explainability is key to enhancing artificial intelligence's trustworthiness\nin medicine. However, several issues remain concerning the actual benefit of\nexplainable models for clinical decision-making. Firstly, there is a lack of\nconsensus on an evaluation framework for quantitatively assessing the practical\nbenefits that effective explainability should provide to practitioners.\nSecondly, physician-centered evaluations of explainability are limited.\nThirdly, the utility of built-in attention mechanisms in transformer-based\nmodels as an explainability technique is unclear. We hypothesize that superior\nattention maps should align with the information that physicians focus on,\npotentially reducing prediction uncertainty and increasing model reliability.\nWe employed a multimodal transformer to predict lymph node metastasis in rectal\ncancer using clinical data and magnetic resonance imaging, exploring how well\nattention maps, visualized through a state-of-the-art technique, can achieve\nagreement with physician understanding. We estimated the model's uncertainty\nusing meta-level information like prediction probability variance and\nquantified agreement. Our assessment of whether this agreement reduces\nuncertainty found no significant effect. In conclusion, this case study did not\nconfirm the anticipated benefit of attention maps in enhancing model\nreliability. Superficial explanations could do more harm than good by\nmisleading physicians into relying on uncertain predictions, suggesting that\nthe current state of attention mechanisms in explainability should not be\noverestimated. Identifying explainability mechanisms truly beneficial for\nclinical decision-making remains essential.",
    "published": "2023-12-15T04:36:13Z",
    "updated": "2023-12-15T04:36:13Z",
    "authors": [
      "Kazuma Kobayashi",
      "Yasuyuki Takamizawa",
      "Mototaka Miyake",
      "Sono Ito",
      "Lin Gu",
      "Tatsuya Nakatsuka",
      "Yu Akagi",
      "Tatsuya Harada",
      "Yukihide Kanemitsu",
      "Ryuji Hamamoto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.00912v2",
    "title": "ScatterFormer: Efficient Voxel Transformer with Scattered Linear\n  Attention",
    "summary": "Window-based transformers excel in large-scale point cloud understanding by\ncapturing context-aware representations with affordable attention computation\nin a more localized manner. However, the sparse nature of point clouds leads to\na significant variance in the number of voxels per window. Existing methods\ngroup the voxels in each window into fixed-length sequences through extensive\nsorting and padding operations, resulting in a non-negligible computational and\nmemory overhead. In this paper, we introduce ScatterFormer, which to the best\nof our knowledge, is the first to directly apply attention to voxels across\ndifferent windows as a single sequence. The key of ScatterFormer is a Scattered\nLinear Attention (SLA) module, which leverages the pre-computation of key-value\npairs in linear attention to enable parallel computation on the variable-length\nvoxel sequences divided by windows. Leveraging the hierarchical structure of\nGPUs and shared memory, we propose a chunk-wise algorithm that reduces the SLA\nmodule's latency to less than 1 millisecond on moderate GPUs. Furthermore, we\ndevelop a cross-window interaction module that improves the locality and\nconnectivity of voxel features across different windows, eliminating the need\nfor extensive window shifting. Our proposed ScatterFormer demonstrates 73.8 mAP\n(L2) on the Waymo Open Dataset and 72.4 NDS on the NuScenes dataset, running at\nan outstanding detection rate of 23 FPS.The code is available at\n\\href{https://github.com/skyhehe123/ScatterFormer}{https://github.com/skyhehe123/ScatterFormer}.",
    "published": "2024-01-01T02:29:59Z",
    "updated": "2024-07-18T06:02:45Z",
    "authors": [
      "Chenhang He",
      "Ruihuang Li",
      "Guowen Zhang",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.01368v3",
    "title": "LIR: A Lightweight Baseline for Image Restoration",
    "summary": "Recently, there have been significant advancements in Image Restoration based\non CNN and transformer. However, the inherent characteristics of the Image\nRestoration task are often overlooked in many works. They, instead, tend to\nfocus on the basic block design and stack numerous such blocks to the model,\nleading to parameters redundant and computations unnecessary. Thus, the\nefficiency of the image restoration is hindered. In this paper, we propose a\nLightweight Baseline network for Image Restoration called LIR to efficiently\nrestore the image and remove degradations. First of all, through an ingenious\nstructural design, LIR removes the degradations existing in the local and\nglobal residual connections that are ignored by modern networks. Then, a\nLightweight Adaptive Attention (LAA) Block is introduced which is mainly\ncomposed of proposed Adaptive Filters and Attention Blocks. The proposed\nAdaptive Filter is used to adaptively extract high-frequency information and\nenhance object contours in various IR tasks, and Attention Block involves a\nnovel Patch Attention module to approximate the self-attention part of the\ntransformer. On the deraining task, our LIR achieves the state-of-the-art\nStructure Similarity Index Measure (SSIM) and comparable performance to\nstate-of-the-art models on Peak Signal-to-Noise Ratio (PSNR). For denoising,\ndehazing, and deblurring tasks, LIR also achieves a comparable performance to\nstate-of-the-art models with a parameter size of about 30\\%. In addition, it is\nworth noting that our LIR produces better visual results that are more in line\nwith the human aesthetic.",
    "published": "2024-02-02T12:39:47Z",
    "updated": "2024-06-24T06:27:44Z",
    "authors": [
      "Dongqi Fan",
      "Ting Yue",
      "Xin Zhao",
      "Renjing Xu",
      "Liang Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.02377v1",
    "title": "NOAH: Learning Pairwise Object Category Attentions for Image\n  Classification",
    "summary": "A modern deep neural network (DNN) for image classification tasks typically\nconsists of two parts: a backbone for feature extraction, and a head for\nfeature encoding and class predication. We observe that the head structures of\nmainstream DNNs adopt a similar feature encoding pipeline, exploiting global\nfeature dependencies while disregarding local ones. In this paper, we revisit\nthe feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that\nrelies on a new form of dot-product attention called pairwise object category\nattention (POCA), efficiently exploiting spatially dense category-specific\nattentions to augment classification performance. NOAH introduces a neat\ncombination of feature split, transform and merge operations to learn POCAs at\nlocal to global scales. As a drop-in design, NOAH can be easily used to replace\nexisting heads of various types of DNNs, improving classification performance\nwhile maintaining similar model efficiency. We validate the effectiveness of\nNOAH on ImageNet classification benchmark with 25 DNN architectures spanning\nconvolutional neural networks, vision transformers and multi-layer perceptrons.\nIn general, NOAH is able to significantly improve the performance of\nlightweight DNNs, e.g., showing 3.14\\%|5.3\\%|1.9\\% top-1 accuracy improvement\nto MobileNetV2 (0.5x)|Deit-Tiny (0.5x)|gMLP-Tiny (0.5x). NOAH also generalizes\nwell when applied to medium-size and large-size DNNs. We further show that NOAH\nretains its efficacy on other popular multi-class and multi-label image\nclassification benchmarks as well as in different training regimes, e.g.,\nshowing 3.6\\%|1.1\\% mAP improvement to large ResNet101|ViT-Large on MS-COCO\ndataset. Project page: https://github.com/OSVAI/NOAH.",
    "published": "2024-02-04T07:19:40Z",
    "updated": "2024-02-04T07:19:40Z",
    "authors": [
      "Chao Li",
      "Aojun Zhou",
      "Anbang Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.04084v1",
    "title": "Provably learning a multi-head attention layer",
    "summary": "The multi-head attention layer is one of the key components of the\ntransformer architecture that sets it apart from traditional feed-forward\nmodels. Given a sequence length $k$, attention matrices\n$\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and\nprojection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times\nd}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to\n\\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional\ntokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq\n\\sum^m_{i=1}\n\\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$.\nIn this work, we initiate the study of provably learning a multi-head attention\nlayer from random examples and give the first nontrivial upper and lower bounds\nfor this problem:\n  - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain\nnon-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns\n$F$ to small error given random labeled examples drawn uniformly from $\\{\\pm\n1\\}^{k\\times d}$.\n  - We prove computational lower bounds showing that in the worst case,\nexponential dependence on $m$ is unavoidable.\n  We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in\nlarge language models, though our techniques naturally extend to standard\ncontinuous settings, e.g. Gaussian. Our algorithm, which is centered around\nusing examples to sculpt a convex body containing the unknown parameters, is a\nsignificant departure from existing provable algorithms for learning\nfeedforward networks, which predominantly exploit algebraic and rotation\ninvariance properties of the Gaussian distribution. In contrast, our analysis\nis more flexible as it primarily relies on various upper and lower tail bounds\nfor the input distribution and \"slices\" thereof.",
    "published": "2024-02-06T15:39:09Z",
    "updated": "2024-02-06T15:39:09Z",
    "authors": [
      "Sitan Chen",
      "Yuanzhi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.16092v2",
    "title": "StochCA: A Novel Approach for Exploiting Pretrained Models with\n  Cross-Attention",
    "summary": "Utilizing large-scale pretrained models is a well-known strategy to enhance\nperformance on various target tasks. It is typically achieved through\nfine-tuning pretrained models on target tasks. However, na\\\"{\\i}ve fine-tuning\nmay not fully leverage knowledge embedded in pretrained models. In this study,\nwe introduce a novel fine-tuning method, called stochastic cross-attention\n(StochCA), specific to Transformer architectures. This method modifies the\nTransformer's self-attention mechanism to selectively utilize knowledge from\npretrained models during fine-tuning. Specifically, in each block, instead of\nself-attention, cross-attention is performed stochastically according to the\npredefined probability, where keys and values are extracted from the\ncorresponding block of a pretrained model. By doing so, queries and\nchannel-mixing multi-layer perceptron layers of a target model are fine-tuned\nto target tasks to learn how to effectively exploit rich representations of\npretrained models. To verify the effectiveness of StochCA, extensive\nexperiments are conducted on benchmarks in the areas of transfer learning and\ndomain generalization, where the exploitation of pretrained models is critical.\nOur experimental results show the superiority of StochCA over state-of-the-art\napproaches in both areas. Furthermore, we demonstrate that StochCA is\ncomplementary to existing approaches, i.e., it can be combined with them to\nfurther improve performance. Our code is available at\nhttps://github.com/daintlab/stochastic_cross_attention",
    "published": "2024-02-25T13:53:49Z",
    "updated": "2024-09-29T03:57:40Z",
    "authors": [
      "Seungwon Seo",
      "Suho Lee",
      "Sangheum Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.10413v2",
    "title": "Real-Time Image Segmentation via Hybrid Convolutional-Transformer\n  Architecture Search",
    "summary": "Image segmentation is one of the most fundamental problems in computer vision\nand has drawn a lot of attention due to its vast applications in image\nunderstanding and autonomous driving. However, designing effective and\nefficient segmentation neural architectures is a labor-intensive process that\nmay require numerous trials by human experts. In this paper, we address the\nchallenge of integrating multi-head self-attention into high-resolution\nrepresentation CNNs efficiently by leveraging architecture search. Manually\nreplacing convolution layers with multi-head self-attention is non-trivial due\nto the costly overhead in memory to maintain high resolution. By contrast, we\ndevelop a multi-target multi-branch supernet method, which not only fully\nutilizes the advantages of high-resolution features but also finds the proper\nlocation for placing the multi-head self-attention module. Our search algorithm\nis optimized towards multiple objectives (e.g., latency and mIoU) and is\ncapable of finding architectures on the Pareto frontier with an arbitrary\nnumber of branches in a single search. We further present a series of models\nvia the Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method\nthat searches for the best hybrid combination of light-weight convolution\nlayers and memory-efficient self-attention layers between branches from\ndifferent resolutions and fuses them to high resolution for both efficiency and\neffectiveness. Extensive experiments demonstrate that HyCTAS outperforms\nprevious methods in both semantic segmentation and panoptic segmentation tasks.\nCode and models are available at https://github.com/MarvinYu1995/HyCTAS.",
    "published": "2024-03-15T15:47:54Z",
    "updated": "2025-04-27T20:40:33Z",
    "authors": [
      "Hongyuan Yu",
      "Cheng Wan",
      "Xiyang Dai",
      "Mengchen Liu",
      "Dongdong Chen",
      "Bin Xiao",
      "Yan Huang",
      "Yuan Lu",
      "Liang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.06135v3",
    "title": "Efficient Concertormer for Image Deblurring and Beyond",
    "summary": "The Transformer architecture has achieved remarkable success in natural\nlanguage processing and high-level vision tasks over the past few years.\nHowever, the inherent complexity of self-attention is quadratic to the size of\nthe image, leading to unaffordable computational costs for high-resolution\nvision tasks. In this paper, we introduce Concertormer, featuring a novel\nConcerto Self-Attention (CSA) mechanism designed for image deblurring. The\nproposed CSA divides self-attention into two distinct components: one\nemphasizes generally global and another concentrates on specifically local\ncorrespondence. By retaining partial information in additional dimensions\nindependent from the self-attention calculations, our method effectively\ncaptures global contextual representations with complexity linear to the image\nsize. To effectively leverage the additional dimensions, we present a\nCross-Dimensional Communication module, which linearly combines attention maps\nand thus enhances expressiveness. Moreover, we amalgamate the two-staged\nTransformer design into a single stage using the proposed gated-dconv MLP\narchitecture. While our primary objective is single-image motion deblurring,\nextensive quantitative and qualitative evaluations demonstrate that our\napproach performs favorably against the state-of-the-art methods in other\ntasks, such as deraining and deblurring with JPEG artifacts. The source codes\nand trained models will be made available to the public.",
    "published": "2024-04-09T09:02:21Z",
    "updated": "2024-12-04T02:48:02Z",
    "authors": [
      "Pin-Hung Kuo",
      "Jinshan Pan",
      "Shao-Yi Chien",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.09707v1",
    "title": "Adaptive Patching for High-resolution Image Segmentation with\n  Transformers",
    "summary": "Attention-based models are proliferating in the space of image analytics,\nincluding segmentation. The standard method of feeding images to transformer\nencoders is to divide the images into patches and then feed the patches to the\nmodel as a linear sequence of tokens. For high-resolution images, e.g.\nmicroscopic pathology images, the quadratic compute and memory cost prohibits\nthe use of an attention-based model, if we are to use smaller patch sizes that\nare favorable in segmentation. The solution is to either use custom complex\nmulti-resolution models or approximate attention schemes. We take inspiration\nfrom Adapative Mesh Refinement (AMR) methods in HPC by adaptively patching the\nimages, as a pre-processing step, based on the image details to reduce the\nnumber of patches being fed to the model, by orders of magnitude. This method\nhas a negligible overhead, and works seamlessly with any attention-based model,\ni.e. it is a pre-processing step that can be adopted by any attention-based\nmodel without friction. We demonstrate superior segmentation quality over SoTA\nsegmentation models for real-world pathology datasets while gaining a geomean\nspeedup of $6.9\\times$ for resolutions up to $64K^2$, on up to $2,048$ GPUs.",
    "published": "2024-04-15T12:06:00Z",
    "updated": "2024-04-15T12:06:00Z",
    "authors": [
      "Enzhi Zhang",
      "Isaac Lyngaas",
      "Peng Chen",
      "Xiao Wang",
      "Jun Igarashi",
      "Yuankai Huo",
      "Mohamed Wahib",
      "Masaharu Munetomo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.11630v1",
    "title": "SNP: Structured Neuron-level Pruning to Preserve Attention Scores",
    "summary": "Multi-head self-attention (MSA) is a key component of Vision Transformers\n(ViTs), which have achieved great success in various vision tasks. However,\ntheir high computational cost and memory footprint hinder their deployment on\nresource-constrained devices. Conventional pruning approaches can only compress\nand accelerate the MSA module using head pruning, although the head is not an\natomic unit. To address this issue, we propose a novel graph-aware neuron-level\npruning method, Structured Neuron-level Pruning (SNP). SNP prunes neurons with\nless informative attention scores and eliminates redundancy among heads.\nSpecifically, it prunes graphically connected query and key layers having the\nleast informative attention scores while preserving the overall attention\nscores. Value layers, which can be pruned independently, are pruned to\neliminate inter-head redundancy. Our proposed method effectively compresses and\naccelerates Transformer-based models for both edge devices and server\nprocessors. For instance, the DeiT-Small with SNP runs 3.1$\\times$ faster than\nthe original model and achieves performance that is 21.94\\% faster and 1.12\\%\nhigher than the DeiT-Tiny. Additionally, SNP combine successfully with\nconventional head or block pruning approaches. SNP with head pruning could\ncompress the DeiT-Base by 80\\% of the parameters and computational costs and\nachieve 3.85$\\times$ faster inference speed on RTX3090 and 4.93$\\times$ on\nJetson Nano.",
    "published": "2024-04-18T03:21:28Z",
    "updated": "2024-04-18T03:21:28Z",
    "authors": [
      "Kyunghwan Shim",
      "Jaewoong Yun",
      "Shinkook Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.15729v1",
    "title": "Gradformer: Graph Transformer with Exponential Decay",
    "summary": "Graph Transformers (GTs) have demonstrated their advantages across a wide\nrange of tasks. However, the self-attention mechanism in GTs overlooks the\ngraph's inductive biases, particularly biases related to structure, which are\ncrucial for the graph tasks. Although some methods utilize positional encoding\nand attention bias to model inductive biases, their effectiveness is still\nsuboptimal analytically. Therefore, this paper presents Gradformer, a method\ninnovatively integrating GT with the intrinsic inductive bias by applying an\nexponential decay mask to the attention matrix. Specifically, the values in the\ndecay mask matrix diminish exponentially, correlating with the decreasing node\nproximities within the graph structure. This design enables Gradformer to\nretain its ability to capture information from distant nodes while focusing on\nthe graph's local details. Furthermore, Gradformer introduces a learnable\nconstraint into the decay mask, allowing different attention heads to learn\ndistinct decay masks. Such an design diversifies the attention heads, enabling\na more effective assimilation of diverse structural information within the\ngraph. Extensive experiments on various benchmarks demonstrate that Gradformer\nconsistently outperforms the Graph Neural Network and GT baseline models in\nvarious graph classification and regression tasks. Additionally, Gradformer has\nproven to be an effective method for training deep GT models, maintaining or\neven enhancing accuracy compared to shallow models as the network deepens, in\ncontrast to the significant accuracy drop observed in other GT models.Codes are\navailable at \\url{https://github.com/LiuChuang0059/Gradformer}.",
    "published": "2024-04-24T08:37:13Z",
    "updated": "2024-04-24T08:37:13Z",
    "authors": [
      "Chuang Liu",
      "Zelin Yao",
      "Yibing Zhan",
      "Xueqi Ma",
      "Shirui Pan",
      "Wenbin Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.17854v1",
    "title": "GLIMS: Attention-Guided Lightweight Multi-Scale Hybrid Network for\n  Volumetric Semantic Segmentation",
    "summary": "Convolutional Neural Networks (CNNs) have become widely adopted for medical\nimage segmentation tasks, demonstrating promising performance. However, the\ninherent inductive biases in convolutional architectures limit their ability to\nmodel long-range dependencies and spatial correlations. While recent\ntransformer-based architectures address these limitations by leveraging\nself-attention mechanisms to encode long-range dependencies and learn\nexpressive representations, they often struggle to extract low-level features\nand are highly dependent on data availability. This motivated us for the\ndevelopment of GLIMS, a data-efficient attention-guided hybrid volumetric\nsegmentation network. GLIMS utilizes Dilated Feature Aggregator Convolutional\nBlocks (DACB) to capture local-global feature correlations efficiently.\nFurthermore, the incorporated Swin Transformer-based bottleneck bridges the\nlocal and global features to improve the robustness of the model. Additionally,\nGLIMS employs an attention-guided segmentation approach through Channel and\nSpatial-Wise Attention Blocks (CSAB) to localize expressive features for\nfine-grained border segmentation. Quantitative and qualitative results on\nglioblastoma and multi-organ CT segmentation tasks demonstrate GLIMS'\neffectiveness in terms of complexity and accuracy. GLIMS demonstrated\noutstanding performance on BraTS2021 and BTCV datasets, surpassing the\nperformance of Swin UNETR. Notably, GLIMS achieved this high performance with a\nsignificantly reduced number of trainable parameters. Specifically, GLIMS has\n47.16M trainable parameters and 72.30G FLOPs, while Swin UNETR has 61.98M\ntrainable parameters and 394.84G FLOPs. The code is publicly available on\nhttps://github.com/yaziciz/GLIMS.",
    "published": "2024-04-27T10:18:55Z",
    "updated": "2024-04-27T10:18:55Z",
    "authors": [
      "Ziya Ata YazÄ±cÄ±",
      "Ä°lkay ÃksÃ¼z",
      "HazÄ±m Kemal Ekenel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.02422v2",
    "title": "Precision Enhancement in Sustained Visual Attention Training Platforms:\n  Offline EEG Signal Analysis for Classifier Fine-Tuning",
    "summary": "In this study, a novel open-source brain-computer interface (BCI) platform\nwas developed to decode scalp electroencephalography (EEG) signals associated\nwith sustained attention. The EEG signal collection was conducted using a\nwireless headset during a sustained visual attention task, where participants\nwere instructed to discriminate between composite images superimposed with\nscenes and faces, responding only to the relevant subcategory while ignoring\nthe irrelevant ones. Seven volunteers participated in this experiment. The data\ncollected were subjected to analyses through event-related potential (ERP),\nHilbert Transform, and Wavelet Transform to extract temporal and spectral\nfeatures. For each participant, utilizing its extracted features, personalized\nSupport Vector Machine (SVM) and Random Forest (RF) models with tuned\nhyperparameters were developed. The models aimed to decode the participant's\nattentional state towards the face and scene stimuli. The SVM models achieved a\nhigher average accuracy of 80\\% and an Area Under the Curve (AUC) of 0.86,\nwhile the RF models showed an average accuracy of 78\\% and AUC of 0.8. This\nwork suggests potential applications for the evaluation of visual attention and\nthe development of closed-loop brainwave regulation systems in the future.",
    "published": "2024-05-03T18:35:50Z",
    "updated": "2024-05-07T16:37:01Z",
    "authors": [
      "Maryam Norouzi",
      "Mohammad Zaeri Amirani",
      "Yalda Shahriari",
      "Reza Abiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.05418v1",
    "title": "EMBANet: A Flexible Efffcient Multi-branch Attention Network",
    "summary": "This work presents a novel module, namely multi-branch concat (MBC), to\nprocess the input tensor and obtain the multi-scale feature map. The proposed\nMBC module brings new degrees of freedom (DoF) for the design of attention\nnetworks by allowing the type of transformation operators and the number of\nbranches to be flexibly adjusted. Two important transformation operators,\nmultiplex and split, are considered in this work, both of which can represent\nmulti-scale features at a more granular level and increase the range of\nreceptive fields. By integrating the MBC and attention module, a multi-branch\nattention (MBA) module is consequently developed to capture the channel-wise\ninteraction of feature maps for establishing the long-range channel dependency.\nBy substituting the 3x3 convolutions in the bottleneck blocks of the ResNet\nwith the proposed MBA, a novel block namely efficient multi-branch attention\n(EMBA) is obtained, which can be easily plugged into the state-of-the-art\nbackbone CNN models. Furthermore, a new backbone network called EMBANet is\nestablished by stacking the EMBA blocks. The proposed EMBANet is extensively\nevaluated on representative computer vision tasks including: classification,\ndetection, and segmentation. And it demonstrates consistently superior\nperformance over the popular backbones.",
    "published": "2024-07-07T15:50:01Z",
    "updated": "2024-07-07T15:50:01Z",
    "authors": [
      "Keke Zu",
      "Hu Zhang",
      "Jian Lu",
      "Lei Zhang",
      "Chen Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.12255v1",
    "title": "Dual-Hybrid Attention Network for Specular Highlight Removal",
    "summary": "Specular highlight removal plays a pivotal role in multimedia applications,\nas it enhances the quality and interpretability of images and videos,\nultimately improving the performance of downstream tasks such as content-based\nretrieval, object recognition, and scene understanding. Despite significant\nadvances in deep learning-based methods, current state-of-the-art approaches\noften rely on additional priors or supervision, limiting their practicality and\ngeneralization capability. In this paper, we propose the Dual-Hybrid Attention\nNetwork for Specular Highlight Removal (DHAN-SHR), an end-to-end network that\nintroduces novel hybrid attention mechanisms to effectively capture and process\ninformation across different scales and domains without relying on additional\npriors or supervision. DHAN-SHR consists of two key components: the Adaptive\nLocal Hybrid-Domain Dual Attention Transformer (L-HD-DAT) and the Adaptive\nGlobal Dual Attention Transformer (G-DAT). The L-HD-DAT captures local\ninter-channel and inter-pixel dependencies while incorporating spectral domain\nfeatures, enabling the network to effectively model the complex interactions\nbetween specular highlights and the underlying surface properties. The G-DAT\nmodels global inter-channel relationships and long-distance pixel dependencies,\nallowing the network to propagate contextual information across the entire\nimage and generate more coherent and consistent highlight-free results. To\nevaluate the performance of DHAN-SHR and facilitate future research in this\narea, we compile a large-scale benchmark dataset comprising a diverse range of\nimages with varying levels of specular highlights. Through extensive\nexperiments, we demonstrate that DHAN-SHR outperforms 18 state-of-the-art\nmethods both quantitatively and qualitatively, setting a new standard for\nspecular highlight removal in multimedia applications.",
    "published": "2024-07-17T01:52:41Z",
    "updated": "2024-07-17T01:52:41Z",
    "authors": [
      "Xiaojiao Guo",
      "Xuhang Chen",
      "Shenghong Luo",
      "Shuqiang Wang",
      "Chi-Man Pun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.15714v1",
    "title": "Mamba meets crack segmentation",
    "summary": "Cracks pose safety risks to infrastructure and cannot be overlooked. The\nprevailing structures in existing crack segmentation networks predominantly\nconsist of CNNs or Transformers. However, CNNs exhibit a deficiency in global\nmodeling capability, hindering the representation to entire crack features.\nTransformers can capture long-range dependencies but suffer from high and\nquadratic complexity. Recently, Mamba has garnered extensive attention due to\nits linear spatial and computational complexity and its powerful global\nperception. This study explores the representation capabilities of Mamba to\ncrack features. Specifically, this paper uncovers the connection between Mamba\nand the attention mechanism, providing a profound insight, an attention\nperspective, into interpreting Mamba and devising a novel Mamba module\nfollowing the principles of attention blocks, namely CrackMamba. We compare\nCrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two\ndatasets comprising asphalt pavement and concrete pavement cracks, and steel\ncracks, respectively. The quantitative results show that CrackMamba stands out\nas the sole Mamba block consistently enhancing the baseline model's performance\nacross all evaluation measures, while reducing its parameters and computational\ncosts. Moreover, this paper substantiates that Mamba can achieve global\nreceptive fields through both theoretical analysis and visual interpretability.\nThe discoveries of this study offer a dual contribution. First, as a\nplug-and-play and simple yet effective Mamba module, CrackMamba exhibits\nimmense potential for integration into various crack segmentation models.\nSecond, the proposed innovative Mamba design concept, integrating Mamba with\nthe attention mechanism, holds significant reference value for all Mamba-based\ncomputer vision models, not limited to crack segmentation networks, as\ninvestigated in this study.",
    "published": "2024-07-22T15:21:35Z",
    "updated": "2024-07-22T15:21:35Z",
    "authors": [
      "Zhili He",
      "Yu-Hsing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.16291v2",
    "title": "TAPTRv2: Attention-based Position Update Improves Tracking Any Point",
    "summary": "In this paper, we present TAPTRv2, a Transformer-based approach built upon\nTAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from\nDEtection TRansformer (DETR) and formulates each tracking point as a point\nquery, making it possible to leverage well-studied operations in DETR-like\nalgorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its\nreliance on cost-volume,which contaminates the point query\\'s content feature\nand negatively impacts both visibility prediction and cost-volume computation.\nIn TAPTRv2, we propose a novel attention-based position update (APU) operation\nand use key-aware deformable attention to realize. For each query, this\noperation uses key-aware attention weights to combine their corresponding\ndeformable sampling positions to predict a new query position. This design is\nbased on the observation that local attention is essentially the same as\ncost-volume, both of which are computed by dot-production between a query and\nits surrounding features. By introducing this new operation, TAPTRv2 not only\nremoves the extra burden of cost-volume computation, but also leads to a\nsubstantial performance improvement. TAPTRv2 surpasses TAPTR and achieves\nstate-of-the-art performance on many challenging datasets, demonstrating the\nsuperiority",
    "published": "2024-07-23T08:46:14Z",
    "updated": "2025-05-09T08:02:26Z",
    "authors": [
      "Hongyang Li",
      "Hao Zhang",
      "Shilong Liu",
      "Zhaoyang Zeng",
      "Feng Li",
      "Tianhe Ren",
      "Bohan Li",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.04158v1",
    "title": "Efficient Single Image Super-Resolution with Entropy Attention and\n  Receptive Field Augmentation",
    "summary": "Transformer-based deep models for single image super-resolution (SISR) have\ngreatly improved the performance of lightweight SISR tasks in recent years.\nHowever, they often suffer from heavy computational burden and slow inference\ndue to the complex calculation of multi-head self-attention (MSA), seriously\nhindering their practical application and deployment. In this work, we present\nan efficient SR model to mitigate the dilemma between model efficiency and SR\nperformance, which is dubbed Entropy Attention and Receptive Field Augmentation\nnetwork (EARFA), and composed of a novel entropy attention (EA) and a shifting\nlarge kernel attention (SLKA). From the perspective of information theory, EA\nincreases the entropy of intermediate features conditioned on a Gaussian\ndistribution, providing more informative input for subsequent reasoning. On the\nother hand, SLKA extends the receptive field of SR models with the assistance\nof channel shifting, which also favors to boost the diversity of hierarchical\nfeatures. Since the implementation of EA and SLKA does not involve complex\ncomputations (such as extensive matrix multiplications), the proposed method\ncan achieve faster nonlinear inference than Transformer-based SR models while\nmaintaining better SR performance. Extensive experiments show that the proposed\nmodel can significantly reduce the delay of model inference while achieving the\nSR performance comparable with other advanced models.",
    "published": "2024-08-08T02:03:10Z",
    "updated": "2024-08-08T02:03:10Z",
    "authors": [
      "Xiaole Zhao",
      "Linze Li",
      "Chengxing Xie",
      "Xiaoming Zhang",
      "Ting Jiang",
      "Wenjie Lin",
      "Shuaicheng Liu",
      "Tianrui Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.07484v1",
    "title": "GRFormer: Grouped Residual Self-Attention for Lightweight Single Image\n  Super-Resolution",
    "summary": "Previous works have shown that reducing parameter overhead and computations\nfor transformer-based single image super-resolution (SISR) models (e.g.,\nSwinIR) usually leads to a reduction of performance. In this paper, we present\nGRFormer, an efficient and lightweight method, which not only reduces the\nparameter overhead and computations, but also greatly improves performance. The\ncore of GRFormer is Grouped Residual Self-Attention (GRSA), which is\nspecifically oriented towards two fundamental components. Firstly, it\nintroduces a novel grouped residual layer (GRL) to replace the Query, Key,\nValue (QKV) linear layer in self-attention, aimed at efficiently reducing\nparameter overhead, computations, and performance loss at the same time.\nSecondly, it integrates a compact Exponential-Space Relative Position Bias\n(ES-RPB) as a substitute for the original relative position bias to improve the\nability to represent position information while further minimizing the\nparameter count. Extensive experimental results demonstrate that GRFormer\noutperforms state-of-the-art transformer-based methods for $\\times$2, $\\times$3\nand $\\times$4 SISR tasks, notably outperforming SOTA by a maximum PSNR of\n0.23dB when trained on the DIV2K dataset, while reducing the number of\nparameter and MACs by about \\textbf{60\\%} and \\textbf{49\\% } in only\nself-attention module respectively. We hope that our simple and effective\nmethod that can easily applied to SR models based on window-division\nself-attention can serve as a useful tool for further research in image\nsuper-resolution. The code is available at\n\\url{https://github.com/sisrformer/GRFormer}.",
    "published": "2024-08-14T11:56:35Z",
    "updated": "2024-08-14T11:56:35Z",
    "authors": [
      "Yuzhen Li",
      "Zehang Deng",
      "Yuxin Cao",
      "Lihua Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.08454v2",
    "title": "Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention",
    "summary": "The Transformer architecture has revolutionized deep learning through its\nSelf-Attention mechanism, which effectively captures contextual information.\nHowever, the memory footprint of Self-Attention presents significant challenges\nfor long-sequence tasks. Grouped Query Attention (GQA) addresses this issue by\ngrouping queries and mean-pooling the corresponding key-value heads - reducing\nthe number of overall parameters and memory requirements in a flexible manner\nwithout adversely compromising model accuracy. In this work, we introduce\nenhancements to GQA, focusing on two novel approaches that deviate from the\nstatic nature of grouping: Key-Distributed GQA (KDGQA) and Dynamic\nKey-Distributed GQA (DGQA), which leverage information from the norms of the\nkey heads to inform query allocation. Specifically, KDGQA looks at the ratios\nof the norms of the key heads during each forward pass, while DGQA examines the\nratios of the norms as they evolve through training. Additionally, we present\nPerturbed GQA (PGQA) as a case-study, which introduces variability in (static)\ngroup formation via subtracting noise from the attention maps. Our experiments\nwith up-trained Vision Transformers, for Image Classification on datasets such\nas CIFAR-10, CIFAR-100, Food101, and Tiny ImageNet, demonstrate the promise of\nthese variants in improving upon the original GQA through more informed and\nadaptive grouping mechanisms: specifically ViT-L experiences accuracy gains of\nup to 8% when utilizing DGQA in comparison to GQA and other variants. We\nfurther analyze the impact of the number of Key-Value Heads on performance,\nunderscoring the importance of utilizing query-key affinities. Code is\navailable on GitHub.",
    "published": "2024-08-15T23:34:04Z",
    "updated": "2024-08-28T08:31:28Z",
    "authors": [
      "Zohaib Khan",
      "Muhammad Khaquan",
      "Omer Tafveez",
      "Burhanuddin Samiwala",
      "Agha Ali Raza"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.07793v1",
    "title": "Lagrange Duality and Compound Multi-Attention Transformer for\n  Semi-Supervised Medical Image Segmentation",
    "summary": "Medical image segmentation, a critical application of semantic segmentation\nin healthcare, has seen significant advancements through specialized computer\nvision techniques. While deep learning-based medical image segmentation is\nessential for assisting in medical diagnosis, the lack of diverse training data\ncauses the long-tail problem. Moreover, most previous hybrid CNN-ViT\narchitectures have limited ability to combine various attentions in different\nlayers of the Convolutional Neural Network. To address these issues, we propose\na Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware\nContrastive Loss, as the overall training objective for semi-supervised\nlearning to mitigate the long-tail problem. Additionally, we introduce\nCMAformer, a novel network that synergizes the strengths of ResUNet and\nTransformer. The cross-attention block in CMAformer effectively integrates\nspatial attention and channel attention for multi-scale feature fusion.\nOverall, our results indicate that CMAformer, combined with the feature fusion\nframework and the new consistency loss, demonstrates strong complementarity in\nsemi-supervised learning ensembles. We achieve state-of-the-art results on\nmultiple public medical image datasets. Example code are available at:\n\\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.",
    "published": "2024-09-12T06:52:46Z",
    "updated": "2024-09-12T06:52:46Z",
    "authors": [
      "Fuchen Zheng",
      "Quanjun Li",
      "Weixuan Li",
      "Xuhang Chen",
      "Yihang Dong",
      "Guoheng Huang",
      "Chi-Man Pun",
      "Shoujun Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.17661v2",
    "title": "A Fuzzy-based Approach to Predict Human Interaction by Functional\n  Near-Infrared Spectroscopy",
    "summary": "The paper introduces a Fuzzy-based Attention (Fuzzy Attention Layer)\nmechanism, a novel computational approach to enhance the interpretability and\nefficacy of neural models in psychological research. The proposed Fuzzy\nAttention Layer mechanism is integrated as a neural network layer within the\nTransformer Encoder model to facilitate the analysis of complex psychological\nphenomena through neural signals, such as those captured by functional\nNear-Infrared Spectroscopy (fNIRS). By leveraging fuzzy logic, the Fuzzy\nAttention Layer is capable of learning and identifying interpretable patterns\nof neural activity. This capability addresses a significant challenge when\nusing Transformer: the lack of transparency in determining which specific brain\nactivities most contribute to particular predictions. Our experimental results\ndemonstrated on fNIRS data from subjects engaged in social interactions\ninvolving handholding reveal that the Fuzzy Attention Layer not only learns\ninterpretable patterns of neural activity but also enhances model performance.\nAdditionally, the learned patterns provide deeper insights into the neural\ncorrelates of interpersonal touch and emotional exchange. The application of\nour model shows promising potential in deciphering the subtle complexities of\nhuman social behaviors, thereby contributing significantly to the fields of\nsocial neuroscience and psychological AI.",
    "published": "2024-09-26T09:20:12Z",
    "updated": "2025-01-23T23:18:13Z",
    "authors": [
      "Xiaowei Jiang",
      "Liang Ou",
      "Yanan Chen",
      "Na Ao",
      "Yu-Cheng Chang",
      "Thomas Do",
      "Chin-Teng Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.01003v1",
    "title": "Y-CA-Net: A Convolutional Attention Based Network for Volumetric Medical\n  Image Segmentation",
    "summary": "Recent attention-based volumetric segmentation (VS) methods have achieved\nremarkable performance in the medical domain which focuses on modeling\nlong-range dependencies. However, for voxel-wise prediction tasks,\ndiscriminative local features are key components for the performance of the VS\nmodels which is missing in attention-based VS methods. Aiming at resolving this\nissue, we deliberately incorporate the convolutional encoder branch with\ntransformer backbone to extract local and global features in a parallel manner\nand aggregate them in Cross Feature Mixer Module (CFMM) for better prediction\nof segmentation mask. Consequently, we observe that the derived model,\nY-CT-Net, achieves competitive performance on multiple medical segmentation\ntasks. For example, on multi-organ segmentation, Y-CT-Net achieves an 82.4%\ndice score, surpassing well-tuned VS Transformer/CNN-like baselines\nUNETR/ResNet-3D by 2.9%/1.4%. With the success of Y-CT-Net, we extend this\nconcept with hybrid attention models, that derived Y-CH-Net model, which brings\na 3% improvement in terms of HD95 score for same segmentation task. The\neffectiveness of both models Y-CT-Net and Y-CH-Net verifies our hypothesis and\nmotivates us to initiate the concept of Y-CA-Net, a versatile generic\narchitecture based upon any two encoders and a decoder backbones, to fully\nexploit the complementary strengths of both convolution and attention\nmechanisms. Based on experimental results, we argue Y-CA-Net is a key player in\nachieving superior results for volumetric segmentation.",
    "published": "2024-10-01T18:50:45Z",
    "updated": "2024-10-01T18:50:45Z",
    "authors": [
      "Muhammad Hamza Sharif",
      "Muzammal Naseer",
      "Mohammad Yaqub",
      "Min Xu",
      "Mohsen Guizani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.07799v3",
    "title": "Mind the Gap: a Spectral Analysis of Rank Collapse and Signal\n  Propagation in Attention Layers",
    "summary": "Attention layers are the core component of transformers, the current\nstate-of-the-art neural network architecture. Alternatives to softmax-based\nattention are being explored due to its tendency to hinder effective\ninformation flow. Even at initialisation, it remains poorly understood why the\npropagation of signals and gradients through these random networks can be\npathological, resulting in issues known as (i) vanishing/exploding gradients\nand (ii) rank collapse $\\textit{in depth}$, i.e. when all tokens converge to a\nsingle representation along layers. While rank collapse in depth naturally\narises from repeated matrix multiplications$\\unicode{x2013}$a common pattern\nacross various architectures$\\unicode{x2013}$we identify an additional and\npreviously unknown challenge unique to softmax attention layers: (iii) rank\ncollapse $\\textit{in width}$, which occurs as the context length increases.\nUsing Random Matrix Theory, we conduct a rigorous analysis that uncovers a\nspectral gap between the two largest singular values of the attention matrix as\nthe cause of (iii), which in turn exacerbates (i) and (ii). Building on this\ninsight, we propose a novel yet simple practical solution to mitigate rank\ncollapse in width by removing the outlier eigenvalue(s). Our theoretical\nframework offers a fresh perspective on recent practical studies, such as (Ye\net al., 2024; Ali et al., 2023), whose ad hoc solutions can now be interpreted\nas implicit efforts to address the spectral gap issue. This work provides\nvaluable theoretical support for ongoing large-scale empirical research,\nbringing theory and practice one step closer in the understanding of\ntransformers.",
    "published": "2024-10-10T10:34:18Z",
    "updated": "2025-06-16T12:30:24Z",
    "authors": [
      "Thiziri Nait Saada",
      "Alireza Naderi",
      "Jared Tanner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.13835v2",
    "title": "Active-Dormant Attention Heads: Mechanistically Demystifying\n  Extreme-Token Phenomena in LLMs",
    "summary": "Practitioners have consistently observed three puzzling phenomena in\ntransformer-based large language models (LLMs): attention sinks, value-state\ndrains, and residual-state peaks, collectively referred to as extreme-token\nphenomena. These phenomena are characterized by certain so-called \"sink tokens\"\nreceiving disproportionately high attention weights, exhibiting significantly\nsmaller value states, and having much larger residual-state norms than those of\nother tokens. These extreme tokens give rise to various challenges in LLM\ninference, quantization, and interpretability.\n  We elucidate the mechanisms behind extreme-token phenomena. First, we show\nthat these phenomena arise in very simple architectures -- transformers with\none to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.\nIn this setting, we identify an active-dormant mechanism, where attention heads\nbecome sinks for specific input domains while remaining non-sinks for others.\nOur theoretical analysis of the training dynamics reveals that these phenomena\nare driven by a mutual reinforcement mechanism. Building on these insights, we\npropose strategies to mitigate extreme-token phenomena during pretraining,\nincluding replacing softmax with ReLU and Adam with SGD. Next, we extend our\nanalysis to pretrained LLMs, including Llama and OLMo, showing that many\nattention heads exhibit a similar active-dormant mechanism as in the BB task,\nand that the mutual reinforcement mechanism also governs the emergence of\nextreme-token phenomena during LLM pretraining. Our results reveal that many of\nthe static and dynamic properties of extreme-token phenomena predicted by the\nBB task align with observations in pretrained LLMs.",
    "published": "2024-10-17T17:54:06Z",
    "updated": "2024-11-07T16:57:02Z",
    "authors": [
      "Tianyu Guo",
      "Druv Pai",
      "Yu Bai",
      "Jiantao Jiao",
      "Michael I. Jordan",
      "Song Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.21000v3",
    "title": "Efficient Bilinear Attention-based Fusion for Medical Visual Question\n  Answering",
    "summary": "Medical Visual Question Answering (MedVQA) has attracted growing interest at\nthe intersection of medical image understanding and natural language processing\nfor clinical applications. By interpreting medical images and providing precise\nanswers to relevant clinical inquiries, MedVQA has the potential to support\ndiagnostic decision-making and reduce workload across various fields like\nradiology. While recent approaches rely heavily on unified large pre-trained\nVisual-Language Models, research on more efficient fusion mechanisms remains\nrelatively limited in this domain. In this paper, we introduce a fusion model,\nOMniBAN, that integrates Orthogonality loss, Multi-head attention, and a\nBilinear Attention Network to achieve high computational efficiency as well as\nsolid performance. We conduct comprehensive experiments and demonstrate how\nbilinear attention fusion can approximate the performance of larger fusion\nmodels like cross-modal Transformer. Our results show that OMniBAN requires\nfewer parameters (approximately 2/3 of Transformer-based Co-Attention) and\nsubstantially lower FLOPs (approximately 1/4), while achieving comparable\noverall performance and even slight improvements on closed-ended questions on\ntwo key MedVQA benchmarks. This balance between efficiency and accuracy\nsuggests that OMniBAN could be a viable option for real-world medical image\nquestion answering, where computational resources are often constrained.",
    "published": "2024-10-28T13:24:12Z",
    "updated": "2025-05-11T14:45:34Z",
    "authors": [
      "Zhilin Zhang",
      "Jie Wang",
      "Zhanghao Qin",
      "Ruiqi Zhu",
      "Xiaoliang Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.21144v4",
    "title": "Enhancing Learned Image Compression via Cross Window-based Attention",
    "summary": "In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods. Our code is\npublicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .",
    "published": "2024-10-28T15:44:35Z",
    "updated": "2025-02-12T19:20:49Z",
    "authors": [
      "Priyanka Mudgal",
      "Feng Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.23039v1",
    "title": "Neural Attention Field: Emerging Point Relevance in 3D Scenes for\n  One-Shot Dexterous Grasping",
    "summary": "One-shot transfer of dexterous grasps to novel scenes with object and context\nvariations has been a challenging problem. While distilled feature fields from\nlarge vision models have enabled semantic correspondences across 3D scenes,\ntheir features are point-based and restricted to object surfaces, limiting\ntheir capability of modeling complex semantic feature distributions for\nhand-object interactions. In this work, we propose the \\textit{neural attention\nfield} for representing semantic-aware dense feature fields in the 3D space by\nmodeling inter-point relevance instead of individual point features. Core to it\nis a transformer decoder that computes the cross-attention between any 3D query\npoint with all the scene points, and provides the query point feature with an\nattention-based aggregation. We further propose a self-supervised framework for\ntraining the transformer decoder from only a few 3D pointclouds without hand\ndemonstrations. Post-training, the attention field can be applied to novel\nscenes for semantics-aware dexterous grasping from one-shot demonstration.\nExperiments show that our method provides better optimization landscapes by\nencouraging the end-effector to focus on task-relevant scene regions, resulting\nin significant improvements in success rates on real robots compared with the\nfeature-field-based methods.",
    "published": "2024-10-30T14:06:51Z",
    "updated": "2024-10-30T14:06:51Z",
    "authors": [
      "Qianxu Wang",
      "Congyue Deng",
      "Tyler Ga Wei Lum",
      "Yuanpei Chen",
      "Yaodong Yang",
      "Jeannette Bohg",
      "Yixin Zhu",
      "Leonidas Guibas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.12892v1",
    "title": "Selective Attention: Enhancing Transformer through Principled Context\n  Control",
    "summary": "The attention mechanism within the transformer architecture enables the model\nto weigh and combine tokens based on their relevance to the query. While\nself-attention has enjoyed major success, it notably treats all queries $q$ in\nthe same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$\nare the value and key embeddings respectively. In this work, we argue that this\nuniform treatment hinders the ability to control contextual sparsity and\nrelevance. As a solution, we introduce the $\\textit{Selective Self-Attention}$\n(SSA) layer that augments the softmax nonlinearity with a principled\ntemperature scaling strategy. By controlling temperature, SSA adapts the\ncontextual sparsity of the attention map to the query embedding and its\nposition in the context window. Through theory and experiments, we demonstrate\nthat this alleviates attention dilution, aids the optimization process, and\nenhances the model's ability to control softmax spikiness of individual\nqueries. We also incorporate temperature scaling for value embeddings and show\nthat it boosts the model's ability to suppress irrelevant/noisy tokens.\nNotably, SSA is a lightweight method which introduces less than 0.5% new\nparameters through a weight-sharing strategy and can be fine-tuned on existing\nLLMs. Extensive empirical evaluations demonstrate that SSA-equipped models\nachieve a noticeable and consistent accuracy improvement on language modeling\nbenchmarks.",
    "published": "2024-11-19T22:17:18Z",
    "updated": "2024-11-19T22:17:18Z",
    "authors": [
      "Xuechen Zhang",
      "Xiangyu Chang",
      "Mingchen Li",
      "Amit Roy-Chowdhury",
      "Jiasi Chen",
      "Samet Oymak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03814v2",
    "title": "Exploring Real&Synthetic Dataset and Linear Attention in Image\n  Restoration",
    "summary": "Image restoration (IR) aims to recover high-quality images from degraded\ninputs, with recent deep learning advancements significantly enhancing\nperformance. However, existing methods lack a unified training benchmark for\niterations and configurations. We also identify a bias in image complexity\ndistributions between commonly used IR training and testing datasets, resulting\nin suboptimal restoration outcomes. To address this, we introduce a large-scale\nIR dataset called ReSyn, which employs a novel image filtering method based on\nimage complexity to ensure a balanced distribution and includes both real and\nAIGC synthetic images. We establish a unified training standard that specifies\niterations and configurations for image restoration models, focusing on\nmeasuring model convergence and restoration capability. Additionally, we\nenhance transformer-based image restoration models using linear attention\nmechanisms by proposing RWKV-IR, which integrates linear complexity RWKV into\nthe transformer structure, allowing for both global and local receptive fields.\nInstead of directly using Vision-RWKV, we replace the original Q-Shift in RWKV\nwith a Depth-wise Convolution shift to better model local dependencies,\ncombined with Bi-directional attention for comprehensive linear attention. We\nalso introduce a Cross-Bi-WKV module that merges two Bi-WKV modules with\ndifferent scanning orders for balanced horizontal and vertical attention.\nExtensive experiments validate the effectiveness of our RWKV-IR model.",
    "published": "2024-12-05T02:11:51Z",
    "updated": "2024-12-11T07:50:40Z",
    "authors": [
      "Yuzhen Du",
      "Teng Hu",
      "Jiangning Zhang",
      "Ran Yi Chengming Xu",
      "Xiaobin Hu",
      "Kai Wu",
      "Donghao Luo",
      "Yabiao Wang",
      "Lizhuang Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.12465v3",
    "title": "Core Context Aware Transformers for Long Context Language Modeling",
    "summary": "Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in extensive tasks primarily attributed to self-attention mechanism,\nwhich requires a token to consider all preceding tokens as its context to\ncompute attention. However, when the context length L becomes very large (e.g.,\n128K), the amount of potentially redundant information in the context tends to\nincrease. The redundant context not only hampers the modeling representation\nperformance but also incurs unnecessary computational and storage overhead. In\nthis paper, we propose a plug-and-play Core Context Aware (CCA) Attention for\nefficient long-context modeling, comprising two complementary modules: 1)\nGlobality-aware pooling module groups input tokens and dynamically compresses\neach group into one core token based on their significance. In this way, our\nmethod automatically focuses and strengthens core context while diminishing\nredundancy during the learning process, leading to effective long-term\ndependency modeling. 2) Locality-preserving module incorporates neighboring\ntokens to preserve local context for detailed representation. Notably, our\nCCA-Attention is able to replace the self-attention module in existing LLMs\nwith minimal fine-tuning cost. Extensive experimental results show the\nsuperiority of our method in both long-context modeling and computational\nefficiency over state-of-the-art methods.",
    "published": "2024-12-17T01:54:08Z",
    "updated": "2025-08-04T03:37:34Z",
    "authors": [
      "Yaofo Chen",
      "Zeng You",
      "Shuhai Zhang",
      "Haokun Li",
      "Yirui Li",
      "Yaowei Wang",
      "Mingkui Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.17241v2",
    "title": "QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level\n  Feature Distribution for Medical Image Segmentation",
    "summary": "Medical image segmentation plays a crucial role in assisting healthcare\nprofessionals with accurate diagnoses and enabling automated diagnostic\nprocesses. Traditional convolutional neural networks (CNNs) often struggle with\ncapturing long-range dependencies, while transformer-based architectures,\ndespite their effectiveness, come with increased computational complexity.\nRecent efforts have focused on combining CNNs and transformers to balance\nperformance and efficiency, but existing approaches still face challenges in\nachieving high segmentation accuracy while maintaining low computational costs.\nFurthermore, many methods underutilize the CNN encoder's capability to capture\nlocal spatial information, concentrating primarily on mitigating long-range\ndependency issues. To address these limitations, we propose QTSeg, a novel\narchitecture for medical image segmentation that effectively integrates local\nand global information. QTSeg features a dual-mix attention decoder designed to\nenhance segmentation performance through: (1) a cross-attention mechanism for\nimproved feature alignment, (2) a spatial attention module to capture\nlong-range dependencies, and (3) a channel attention block to learn\ninter-channel relationships. Additionally, we introduce a multi-level feature\ndistribution module, which adaptively balances feature propagation between the\nencoder and decoder, further boosting performance. Extensive experiments on\nfive publicly available datasets covering diverse segmentation tasks, including\nlesion, polyp, breast cancer, cell, and retinal vessel segmentation,\ndemonstrate that QTSeg outperforms state-of-the-art methods across multiple\nevaluation metrics while maintaining lower computational costs. Our\nimplementation can be found at: https://github.com/tpnam0901/QTSeg (v1.0.0)",
    "published": "2024-12-23T03:22:44Z",
    "updated": "2025-02-14T04:03:10Z",
    "authors": [
      "Phuong-Nam Tran",
      "Nhat Truong Pham",
      "Duc Ngoc Minh Dang",
      "Eui-Nam Huh",
      "Choong Seon Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.18597v2",
    "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion\n  Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "summary": "Sora-like video generation models have achieved remarkable progress with a\nMulti-Modal Diffusion Transformer MM-DiT architecture. However, the current\nvideo generation models predominantly focus on single-prompt, struggling to\ngenerate coherent scenes with multiple sequential prompts that better reflect\nreal-world dynamic scenarios. While some pioneering works have explored\nmulti-prompt video generation, they face significant challenges including\nstrict training data requirements, weak prompt following, and unnatural\ntransitions. To address these problems, we propose DiTCtrl, a training-free\nmulti-prompt video generation method under MM-DiT architectures for the first\ntime. Our key idea is to take the multi-prompt video generation task as\ntemporal video editing with smooth transitions. To achieve this goal, we first\nanalyze MM-DiT's attention mechanism, finding that the 3D full attention\nbehaves similarly to that of the cross/self-attention blocks in the UNet-like\ndiffusion models, enabling mask-guided precise semantic control across\ndifferent prompts with attention sharing for multi-prompt video generation.\nBased on our careful design, the video generated by DiTCtrl achieves smooth\ntransitions and consistent object motion given multiple sequential prompts\nwithout additional training. Besides, we also present MPVBench, a new benchmark\nspecially designed for multi-prompt video generation to evaluate the\nperformance of multi-prompt generation. Extensive experiments demonstrate that\nour method achieves state-of-the-art performance without additional training.",
    "published": "2024-12-24T18:51:19Z",
    "updated": "2025-03-26T09:05:41Z",
    "authors": [
      "Minghong Cai",
      "Xiaodong Cun",
      "Xiaoyu Li",
      "Wenze Liu",
      "Zhaoyang Zhang",
      "Yong Zhang",
      "Ying Shan",
      "Xiangyu Yue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01461v1",
    "title": "Docking-Aware Attention: Dynamic Protein Representations through\n  Molecular Context Integration",
    "summary": "Computational prediction of enzymatic reactions represents a crucial\nchallenge in sustainable chemical synthesis across various scientific domains,\nranging from drug discovery to materials science and green chemistry. These\nsyntheses rely on proteins that selectively catalyze complex molecular\ntransformations. These protein catalysts exhibit remarkable substrate\nadaptability, with the same protein often catalyzing different chemical\ntransformations depending on its molecular partners. Current approaches to\nprotein representation in reaction prediction either ignore protein structure\nentirely or rely on static embeddings, failing to capture how proteins\ndynamically adapt their behavior to different substrates. We present\nDocking-Aware Attention (DAA), a novel architecture that generates dynamic,\ncontext-dependent protein representations by incorporating molecular docking\ninformation into the attention mechanism. DAA combines physical interaction\nscores from docking predictions with learned attention patterns to focus on\nprotein regions most relevant to specific molecular interactions. We evaluate\nour method on enzymatic reaction prediction, where it outperforms previous\nstate-of-the-art methods, achieving 62.2\\% accuracy versus 56.79\\% on complex\nmolecules and 55.54\\% versus 49.45\\% on innovative reactions. Through detailed\nablation studies and visualizations, we demonstrate how DAA generates\ninterpretable attention patterns that adapt to different molecular contexts.\nOur approach represents a general framework for context-aware protein\nrepresentation in biocatalysis prediction, with potential applications across\nenzymatic synthesis planning. We open-source our implementation and pre-trained\nmodels to facilitate further research.",
    "published": "2025-02-03T15:52:38Z",
    "updated": "2025-02-03T15:52:38Z",
    "authors": [
      "Amitay Sicherman",
      "Kira Radinsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09931v1",
    "title": "TransGUNet: Transformer Meets Graph-based Skip Connection for Medical\n  Image Segmentation",
    "summary": "Skip connection engineering is primarily employed to address the semantic gap\nbetween the encoder and decoder, while also integrating global dependencies to\nunderstand the relationships among complex anatomical structures in medical\nimage segmentation. Although several models have proposed transformer-based\napproaches to incorporate global dependencies within skip connections, they\noften face limitations in capturing detailed local features with high\ncomputational complexity. In contrast, graph neural networks (GNNs) exploit\ngraph structures to effectively capture local and global features. Leveraging\nthese properties, we introduce an attentional cross-scale graph neural network\n(ACS-GNN), which enhances the skip connection framework by converting\ncross-scale feature maps into a graph structure and capturing complex\nanatomical structures through node attention. Additionally, we observed that\ndeep learning models often produce uninformative feature maps, which degrades\nthe quality of spatial attention maps. To address this problem, we integrated\nentropy-driven feature selection (EFS) with spatial attention, calculating an\nentropy score for each channel and filtering out high-entropy feature maps. Our\ninnovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial\nattentio} to effectively enhance domain generalizability across various\nmodalities by leveraging GNNs alongside a reliable spatial attention map,\nensuring more robust features within the skip connection. Through comprehensive\nexperiments and analysis, TransGUNet achieved superior segmentation performance\non six seen and eight unseen datasets, demonstrating significantly higher\nefficiency compared to previous methods.",
    "published": "2025-02-14T05:54:13Z",
    "updated": "2025-02-14T05:54:13Z",
    "authors": [
      "Ju-Hyeon Nam",
      "Nur Suriza Syazwany",
      "Sang-Chul Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.11178v2",
    "title": "DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage\n  Object Detection",
    "summary": "Recent 2D CNN-based domain adaptation approaches struggle with long-range\ndependencies due to limited receptive fields, making it difficult to adapt to\ntarget domains with significant spatial distribution changes. While\ntransformer-based domain adaptation methods better capture distant\nrelationships through self-attention mechanisms that facilitate more effective\ncross-domain feature alignment, their quadratic computational complexity makes\npractical deployment challenging for object detection tasks across diverse\ndomains. Inspired by the global modeling and linear computation complexity of\nthe Mamba architecture, we present the first domain-adaptive Mamba-based\none-stage object detection model, termed DA-Mamba. Specifically, we combine\nMamba's efficient state-space modeling with attention mechanisms to address\ndomain-specific spatial and channel-wise variations. Our design leverages\ndomain-adaptive spatial and channel-wise scanning within the Mamba block to\nextract highly transferable representations for efficient sequential\nprocessing, while cross-attention modules generate long-range, mixed-domain\nspatial features to enable robust soft alignment across domains. Besides,\nmotivated by the observation that hybrid architectures introduce feature noise\nin domain adaptation tasks, we propose an entropy-based knowledge distillation\nframework with margin ReLU, which adaptively refines multi-level\nrepresentations by suppressing irrelevant activations and aligning uncertainty\nacross source and target domains. Finally, to prevent overfitting caused by the\nmixed-up features generated through cross-attention mechanisms, we propose\nentropy-driven gating attention with random perturbations that simultaneously\nrefine target features and enhance model generalization.",
    "published": "2025-02-16T15:58:54Z",
    "updated": "2025-05-07T10:18:31Z",
    "authors": [
      "A. Enes Doruk",
      "Hasan F. Ates"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.15331v2",
    "title": "Lightweight yet Efficient: An External Attentive Graph Convolutional\n  Network with Positional Prompts for Sequential Recommendation",
    "summary": "Graph-based Sequential Recommender systems (GSRs) have gained significant\nresearch attention due to their ability to simultaneously handle user-item\ninteractions and sequential relationships between items. Current GSRs often\nutilize composite or in-depth structures for graph encoding (e.g., the Graph\nTransformer). Nevertheless, they have high computational complexity, hindering\nthe deployment on resource-constrained edge devices. Moreover, the relative\nposition encoding in Graph Transformer has difficulty in considering the\ncomplicated positional dependencies within sequence. To this end, we propose an\nExternal Attentive Graph convolutional network with Positional prompts for\nSequential recommendation, namely EA-GPS. Specifically, we first introduce an\nexternal attentive graph convolutional network that linearly measures the\nglobal associations among nodes via two external memory units. Then, we present\na positional prompt-based decoder that explicitly treats the absolute item\npositions as external prompts. By introducing length-adaptive sequential\nmasking and a soft attention network, such a decoder facilitates the model to\ncapture the long-term positional dependencies and contextual relationships\nwithin sequences. Extensive experimental results on five real-world datasets\ndemonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.\nRemarkably, it achieves the superior performance while maintaining a smaller\nparameter size and lower training overhead. The implementation of this work is\npublicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.",
    "published": "2025-02-21T09:34:31Z",
    "updated": "2025-03-04T02:18:36Z",
    "authors": [
      "Jinyu Zhang",
      "Chao Li",
      "Zhongying Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09852v1",
    "title": "GFT: Gradient Focal Transformer",
    "summary": "Fine-Grained Image Classification (FGIC) remains a complex task in computer\nvision, as it requires models to distinguish between categories with subtle\nlocalized visual differences. Well-studied CNN-based models, while strong in\nlocal feature extraction, often fail to capture the global context required for\nfine-grained recognition, while more recent ViT-backboned models address FGIC\nwith attention-driven mechanisms but lack the ability to adaptively focus on\ntruly discriminative regions. TransFG and other ViT-based extensions introduced\npart-aware token selection to enhance attention localization, yet they still\nstruggle with computational efficiency, attention region selection flexibility,\nand detail-focus narrative in complex environments. This paper introduces GFT\n(Gradient Focal Transformer), a new ViT-derived framework created for FGIC\ntasks. GFT integrates the Gradient Attention Learning Alignment (GALA)\nmechanism to dynamically prioritize class-discriminative features by analyzing\nattention gradient flow. Coupled with a Progressive Patch Selection (PPS)\nstrategy, the model progressively filters out less informative regions,\nreducing computational overhead while enhancing sensitivity to fine details.\nGFT achieves SOTA accuracy on FGVC Aircraft, Food-101, and COCO datasets with\n93M parameters, outperforming ViT-based advanced FGIC models in efficiency. By\nbridging global context and localized detail extraction, GFT sets a new\nbenchmark in fine-grained recognition, offering interpretable solutions for\nreal-world deployment scenarios.",
    "published": "2025-04-14T03:49:06Z",
    "updated": "2025-04-14T03:49:06Z",
    "authors": [
      "Boris Kriuk",
      "Simranjit Kaur Gill",
      "Shoaib Aslam",
      "Amir Fakhrutdinov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16097v1",
    "title": "A CNN-based Local-Global Self-Attention via Averaged Window Embeddings\n  for Hierarchical ECG Analysis",
    "summary": "Cardiovascular diseases remain the leading cause of global mortality,\nemphasizing the critical need for efficient diagnostic tools such as\nelectrocardiograms (ECGs). Recent advancements in deep learning, particularly\ntransformers, have revolutionized ECG analysis by capturing detailed waveform\nfeatures as well as global rhythm patterns. However, traditional transformers\nstruggle to effectively capture local morphological features that are critical\nfor accurate ECG interpretation. We propose a novel Local-Global Attention ECG\nmodel (LGA-ECG) to address this limitation, integrating convolutional inductive\nbiases with global self-attention mechanisms. Our approach extracts queries by\naveraging embeddings obtained from overlapping convolutional windows, enabling\nfine-grained morphological analysis, while simultaneously modeling global\ncontext through attention to keys and values derived from the entire sequence.\nExperiments conducted on the CODE-15 dataset demonstrate that LGA-ECG\noutperforms state-of-the-art models and ablation studies validate the\neffectiveness of the local-global attention strategy. By capturing the\nhierarchical temporal dependencies and morphological patterns in ECG signals,\nthis new design showcases its potential for clinical deployment with robust\nautomated ECG classification.",
    "published": "2025-04-13T01:21:18Z",
    "updated": "2025-04-13T01:21:18Z",
    "authors": [
      "Arthur Buzelin",
      "Pedro Robles Dutenhefner",
      "Turi Rezende",
      "Luisa G. Porfirio",
      "Pedro Bento",
      "Yan Aquino",
      "Jose Fernandes",
      "Caio Santana",
      "Gabriela Miana",
      "Gisele L. Pappa",
      "Antonio Ribeiro",
      "Wagner Meira Jr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.02533v1",
    "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
    "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
    "published": "2025-05-05T10:16:16Z",
    "updated": "2025-05-05T10:16:16Z",
    "authors": [
      "Dimitrios Kafetzis",
      "Ramin Khalili",
      "Iordanis Koutsopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07986v3",
    "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
    "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}",
    "published": "2025-06-09T17:54:04Z",
    "updated": "2025-07-23T03:45:11Z",
    "authors": [
      "Zhengyao Lv",
      "Tianlin Pan",
      "Chenyang Si",
      "Zhaoxi Chen",
      "Wangmeng Zuo",
      "Ziwei Liu",
      "Kwan-Yee K. Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21711v1",
    "title": "CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake\n  detection",
    "summary": "Deepfakes have emerged as a significant threat to digital media authenticity,\nincreasing the need for advanced detection techniques that can identify subtle\nand time-dependent manipulations. CNNs are effective at capturing spatial\nartifacts, and Transformers excel at modeling temporal inconsistencies.\nHowever, many existing CNN-Transformer models process spatial and temporal\nfeatures independently. In particular, attention-based methods often use\nseparate attention mechanisms for spatial and temporal features and combine\nthem using naive approaches like averaging, addition, or concatenation, which\nlimits the depth of spatio-temporal interaction. To address this challenge, we\npropose a unified CAST model that leverages cross-attention to effectively fuse\nspatial and temporal features in a more integrated manner. Our approach allows\ntemporal features to dynamically attend to relevant spatial regions, enhancing\nthe model's ability to detect fine-grained, time-evolving artifacts such as\nflickering eyes or warped lips. This design enables more precise localization\nand deeper contextual understanding, leading to improved performance across\ndiverse and challenging scenarios. We evaluate the performance of our model\nusing the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both\nintra- and cross-dataset settings to affirm the superiority of our approach.\nOur model achieves strong performance with an AUC of 99.49 percent and an\naccuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset\ntesting, it demonstrates impressive generalization by achieving a 93.31 percent\nAUC on the unseen DeepfakeDetection dataset. These results highlight the\neffectiveness of cross-attention-based feature fusion in enhancing the\nrobustness of deepfake video detection.",
    "published": "2025-06-26T18:51:17Z",
    "updated": "2025-06-26T18:51:17Z",
    "authors": [
      "Aryan Thakre",
      "Omkar Nagwekar",
      "Vedang Talekar",
      "Aparna Santra Biswas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.12006v4",
    "title": "Frequency-Dynamic Attention Modulation for Dense Prediction",
    "summary": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\nhttps://github.com/Linwei-Chen/FDAM.",
    "published": "2025-07-16T07:59:54Z",
    "updated": "2025-10-23T14:06:56Z",
    "authors": [
      "Linwei Chen",
      "Lin Gu",
      "Ying Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.14560v2",
    "title": "The Origin of Self-Attention: Pairwise Affinity Matrices in Feature\n  Selection and the Emergence of Self-Attention",
    "summary": "The self-attention mechanism, now central to deep learning architectures such\nas Transformers, is a modern instance of a more general computational\nprinciple: learning and using pairwise affinity matrices to control how\ninformation flows through a model. This paper traces the conceptual origins of\nself-attention across multiple domains, including computer vision, natural\nlanguage processing, and graph learning, through their shared reliance on an\naffinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)\nas a foundational approach that generalizes the idea of affinity-based\nweighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS\ndefines A either through domain knowledge or by learning, and computes feature\nrelevance through multi-hop propagation over the affinity graph. From this\nperspective, self-attention can be seen as a special case of Inf-FS: it uses a\nsingle-hop affinity computation where A is dynamically built from token\nsimilarities. We argue that the underlying structure, reasoning over pairwise\nrelationships, is preserved across both approaches, and the key differences lie\nin how the affinity matrix is defined and applied. By situating self-attention\nwithin the broader paradigm of affinity-based computation, we unify several\nstrands of machine learning research and highlight a common mathematical\nfoundation that underpins diverse models and tasks.",
    "published": "2025-07-19T09:51:03Z",
    "updated": "2025-07-26T15:30:58Z",
    "authors": [
      "Giorgio Roffo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.15465v2",
    "title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and\n  Mixture-of-Experts",
    "summary": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models.",
    "published": "2025-07-21T10:18:33Z",
    "updated": "2025-07-23T20:55:41Z",
    "authors": [
      "Sungmin Yun",
      "Seonyong Park",
      "Hwayong Nam",
      "Younjoo Lee",
      "Gunjun Lee",
      "Kwanhee Kyung",
      "Sangpyo Kim",
      "Nam Sung Kim",
      "Jongmin Kim",
      "Hyungyo Kim",
      "Juhwan Cho",
      "Seungmin Baek",
      "Jung Ho Ahn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.22498v2",
    "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping",
    "summary": "Adverse weather conditions cause diverse and complex degradation patterns,\ndriving the development of All-in-One (AiO) models. However, recent AiO\nsolutions still struggle to capture diverse degradations, since global\nfiltering methods like direct operations on the frequency domain fail to handle\nhighly variable and localized distortions. To address these issue, we propose\nSpectral-based Spatial Grouping Transformer (SSGformer), a novel approach that\nleverages spectral decomposition and group-wise attention for multi-weather\nimage restoration. SSGformer decomposes images into high-frequency edge\nfeatures using conventional edge detection and low-frequency information via\nSingular Value Decomposition. We utilize multi-head linear attention to\neffectively model the relationship between these features. The fused features\nare integrated with the input to generate a grouping-mask that clusters regions\nbased on the spatial similarity and image texture. To fully leverage this mask,\nwe introduce a group-wise attention mechanism, enabling robust adverse weather\nremoval and ensuring consistent performance across diverse weather conditions.\nWe also propose a Spatial Grouping Transformer Block that uses both channel\nattention and spatial attention, effectively balancing feature-wise\nrelationships and spatial dependencies. Extensive experiments show the\nsuperiority of our approach, validating its effectiveness in handling the\nvaried and intricate adverse weather degradations.",
    "published": "2025-07-30T09:08:34Z",
    "updated": "2025-07-31T10:38:29Z",
    "authors": [
      "Yuhwan Jeong",
      "Yunseo Yang",
      "Youngho Yoon",
      "Kuk-Jin Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.07987v1",
    "title": "Automated Trading System for Straddle-Option Based on Deep Q-Learning",
    "summary": "Straddle Option is a financial trading tool that explores volatility premiums\nin high-volatility markets without predicting price direction. Although deep\nreinforcement learning has emerged as a powerful approach to trading automation\nin financial markets, existing work mostly focused on predicting price trends\nand making trading decisions by combining multi-dimensional datasets like blogs\nand videos, which led to high computational costs and unstable performance in\nhigh-volatility markets. To tackle this challenge, we develop automated\nstraddle option trading based on reinforcement learning and attention\nmechanisms to handle unpredictability in high-volatility markets. Firstly, we\nleverage the attention mechanisms in Transformer-DDQN through both\nself-attention with time series data and channel attention with multi-cycle\ninformation. Secondly, a novel reward function considering excess earnings is\ndesigned to focus on long-term profits and neglect short-term losses over a\nstop line. Thirdly, we identify the resistance levels to provide reference\ninformation when great uncertainty in price movements occurs with intensified\nbattle between the buyers and sellers. Through extensive experiments on the\nChinese stock, Brent crude oil, and Bitcoin markets, our attention-based\nTransformer-DDQN model exhibits the lowest maximum drawdown across all markets,\nand outperforms other models by 92.5\\% in terms of the average return excluding\nthe crude oil market due to relatively low fluctuation.",
    "published": "2025-08-01T16:20:19Z",
    "updated": "2025-08-01T16:20:19Z",
    "authors": [
      "Yiran Wan",
      "Xinyu Ying",
      "Shengzhen Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07019v2",
    "title": "Native Hybrid Attention for Efficient Sequence Modeling",
    "summary": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\n\\texttt{softmax attention} operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.",
    "published": "2025-10-08T13:44:57Z",
    "updated": "2025-10-11T09:31:02Z",
    "authors": [
      "Jusen Du",
      "Jiaxi Hu",
      "Tao Zhang",
      "Weigao Sun",
      "Yu Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24709v1",
    "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?",
    "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.",
    "published": "2025-10-28T17:57:05Z",
    "updated": "2025-10-28T17:57:05Z",
    "authors": [
      "Yihao Li",
      "Saeed Salehi",
      "Lyle Ungar",
      "Konrad P. Kording"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.07976v5",
    "title": "TransFG: A Transformer Architecture for Fine-grained Recognition",
    "summary": "Fine-grained visual classification (FGVC) which aims at recognizing objects\nfrom subcategories is a very challenging task due to the inherently subtle\ninter-class differences. Most existing works mainly tackle this problem by\nreusing the backbone network to extract features of detected discriminative\nregions. However, this strategy inevitably complicates the pipeline and pushes\nthe proposed regions to contain most parts of the objects thus fails to locate\nthe really important parts. Recently, vision transformer (ViT) shows its strong\nperformance in the traditional classification task. The self-attention\nmechanism of the transformer links every patch token to the classification\ntoken. In this work, we first evaluate the effectiveness of the ViT framework\nin the fine-grained recognition setting. Then motivated by the strength of the\nattention link can be intuitively considered as an indicator of the importance\nof tokens, we further propose a novel Part Selection Module that can be applied\nto most of the transformer architectures where we integrate all raw attention\nweights of the transformer into an attention map for guiding the network to\neffectively and accurately select discriminative image patches and compute\ntheir relations. A contrastive loss is applied to enlarge the distance between\nfeature representations of confusing classes. We name the augmented\ntransformer-based model TransFG and demonstrate the value of it by conducting\nexperiments on five popular fine-grained benchmarks where we achieve\nstate-of-the-art performance. Qualitative results are presented for better\nunderstanding of our model.",
    "published": "2021-03-14T17:03:53Z",
    "updated": "2021-12-01T22:24:53Z",
    "authors": [
      "Ju He",
      "Jie-Neng Chen",
      "Shuai Liu",
      "Adam Kortylewski",
      "Cheng Yang",
      "Yutong Bai",
      "Changhu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.11956v4",
    "title": "On the Connection Between MPNN and Graph Transformer",
    "summary": "Graph Transformer (GT) recently has emerged as a new paradigm of graph\nlearning algorithms, outperforming the previously popular Message Passing\nNeural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022)\nshows that with proper position embedding, GT can approximate MPNN arbitrarily\nwell, implying that GT is at least as powerful as MPNN. In this paper, we study\nthe inverse connection and show that MPNN with virtual node (VN), a commonly\nused heuristic with little theoretical understanding, is powerful enough to\narbitrarily approximate the self-attention layer of GT.\n  In particular, we first show that if we consider one type of linear\ntransformer, the so-called Performer/Linear Transformer (Choromanski et al.,\n2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1)\nwidth can approximate a self-attention layer in Performer/Linear Transformer.\nNext, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN\nwith O(n^d) width and O(1) depth can approximate the self-attention layer\narbitrarily well, where d is the input feature dimension. Lastly, under some\nassumptions, we provide an explicit construction of MPNN + VN with O(1) width\nand O(n) depth approximating the self-attention layer in GT arbitrarily well.\nOn the empirical side, we demonstrate that 1) MPNN + VN is a surprisingly\nstrong baseline, outperforming GT on the recently proposed Long Range Graph\nBenchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation\non a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Transformer\nand MPNN on the climate modeling task.",
    "published": "2023-01-27T19:15:31Z",
    "updated": "2023-06-21T03:52:06Z",
    "authors": [
      "Chen Cai",
      "Truong Son Hy",
      "Rose Yu",
      "Yusu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.04086v3",
    "title": "TEC-Net: Vision Transformer Embrace Convolutional Neural Networks for\n  Medical Image Segmentation",
    "summary": "The hybrid architecture of convolution neural networks (CNN) and Transformer\nhas been the most popular method for medical image segmentation. However, the\nexisting networks based on the hybrid architecture suffer from two problems.\nFirst, although the CNN branch can capture image local features by using\nconvolution operation, the vanilla convolution is unable to achieve adaptive\nextraction of image features. Second, although the Transformer branch can model\nthe global information of images, the conventional self-attention only focuses\non the spatial self-attention of images and ignores the channel and\ncross-dimensional self-attention leading to low segmentation accuracy for\nmedical images with complex backgrounds. To solve these problems, we propose\nvision Transformer embrace convolutional neural networks for medical image\nsegmentation (TEC-Net). Our network has two advantages. First, dynamic\ndeformable convolution (DDConv) is designed in the CNN branch, which not only\novercomes the difficulty of adaptive feature extraction using fixed-size\nconvolution kernels, but also solves the defect that different inputs share the\nsame convolution kernel parameters, effectively improving the feature\nexpression ability of CNN branch. Second, in the Transformer branch, a\n(shifted)-window adaptive complementary attention module ((S)W-ACAM) and\ncompact convolutional projection are designed to enable the network to fully\nlearn the cross-dimensional long-range dependency of medical images with few\nparameters and calculations. Experimental results show that the proposed\nTEC-Net provides better medical image segmentation results than SOTA methods\nincluding CNN and Transformer networks. In addition, our TEC-Net requires fewer\nparameters and computational costs and does not rely on pre-training. The code\nis publicly available at https://github.com/SR0920/TEC-Net.",
    "published": "2023-06-07T01:14:16Z",
    "updated": "2023-12-20T02:34:49Z",
    "authors": [
      "Rui Sun",
      "Tao Lei",
      "Weichuan Zhang",
      "Yong Wan",
      "Yong Xia",
      "Asoke K. Nandi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.09998v2",
    "title": "FBPT: A Fully Binary Point Transformer",
    "summary": "This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) model\nwhich has the potential to be widely applied and expanded in the fields of\nrobotics and mobile devices. By compressing the weights and activations of a\n32-bit full-precision network to 1-bit binary values, the proposed binary point\ncloud Transformer network significantly reduces the storage footprint and\ncomputational resource requirements of neural network models for point cloud\nprocessing tasks, compared to full-precision point cloud networks. However,\nachieving a fully binary point cloud Transformer network, where all parts\nexcept the modules specific to the task are binary, poses challenges and\nbottlenecks in quantizing the activations of Q, K, V and self-attention in the\nattention module, as they do not adhere to simple probability distributions and\ncan vary with input data. Furthermore, in our network, the binary attention\nmodule undergoes a degradation of the self-attention module due to the uniform\ndistribution that occurs after the softmax operation. The primary focus of this\npaper is on addressing the performance degradation issue caused by the use of\nbinary point cloud Transformer modules. We propose a novel binarization\nmechanism called dynamic-static hybridization. Specifically, our approach\ncombines static binarization of the overall network model with fine granularity\ndynamic binarization of data-sensitive components. Furthermore, we make use of\na novel hierarchical training scheme to obtain the optimal model and\nbinarization parameters. These above improvements allow the proposed\nbinarization method to outperform binarization methods applied to convolution\nneural networks when used in point cloud Transformer structures. To demonstrate\nthe superiority of our algorithm, we conducted experiments on two different\ntasks: point cloud classification and place recognition.",
    "published": "2024-03-15T03:45:10Z",
    "updated": "2024-05-09T06:35:38Z",
    "authors": [
      "Zhixing Hou",
      "Yuzhang Shang",
      "Yan Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.16855v1",
    "title": "Stack Transformer Based Spatial-Temporal Attention Model for Dynamic\n  Multi-Culture Sign Language Recognition",
    "summary": "Hand gesture-based Sign Language Recognition (SLR) serves as a crucial\ncommunication bridge between deaf and non-deaf individuals. Existing SLR\nsystems perform well for their cultural SL but may struggle with multi-cultural\nsign languages (McSL). To address these challenges, this paper proposes a Stack\nSpatial-Temporal Transformer Network that leverages multi-head attention\nmechanisms to capture both spatial and temporal dependencies with hierarchical\nfeatures using the Stack Transfer concept. In the proceed, firstly, we applied\na fully connected layer to make a embedding vector which has high expressive\npower from the original dataset, then fed them a stack newly proposed\ntransformer to achieve hierarchical features with short-range and long-range\ndependency. The network architecture is composed of several stages that process\nspatial and temporal relationships sequentially, ensuring effective feature\nextraction. After making the fully connected layer, the embedding vector is\nprocessed by the Spatial Multi-Head Attention Transformer, which captures\nspatial dependencies between joints. In the next stage, the Temporal Multi-Head\nAttention Transformer captures long-range temporal dependencies, and again, the\nfeatures are concatenated with the output using another skip connection. The\nprocessed features are then passed to the Feed-Forward Network (FFN), which\nrefines the feature representations further. After the FFN, additional skip\nconnections are applied to combine the output with earlier layers, followed by\na final normalization layer to produce the final output feature tensor. This\nprocess is repeated for 10 transformer blocks. The extensive experiment shows\nthat the JSL, KSL and ASL datasets achieved good performance accuracy. Our\napproach demonstrates improved performance in McSL, and it will be consider as\na novel work in this domain.",
    "published": "2025-03-21T04:57:18Z",
    "updated": "2025-03-21T04:57:18Z",
    "authors": [
      "Koki Hirooka",
      "Abu Saleh Musa Miah",
      "Tatsuya Murakami",
      "Yuto Akiba",
      "Yong Seok Hwang",
      "Jungpil Shin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/hep-th/0403021v2",
    "title": "Particles and energy fluxes from a CFT perspective",
    "summary": "We analyze the creation of particles in two dimensions under the action of\nconformal transformations. We focus our attention on Mobius transformations and\ncompare the usual approach, based on the Bogolubov coefficients, with an\nalternative but equivalent viewpoint based on correlation functions. In the\nlatter approach the absence of particle production under full Mobius\ntransformations is manifest. Moreover, we give examples, using the\nmoving-mirror analogy, to illustrate the close relation between the production\nof quanta and energy.",
    "published": "2004-03-01T16:23:33Z",
    "updated": "2004-07-28T15:37:46Z",
    "authors": [
      "A. Fabbri",
      "J. Navarro-Salas",
      "G. J. Olmo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/0704.0531v3",
    "title": "Gravitational Duality Transformations on (A)dS4",
    "summary": "We discuss the implementation of electric-magnetic duality transformations in\nfour-dimensional gravity linearized around Minkowski or (A)dS4 backgrounds. In\nthe presence of a cosmological constant duality generically modifies the\nHamiltonian, nevertheless the bulk dynamics is unchanged. We pay particular\nattention to the boundary terms generated by the duality transformations and\ndiscuss their implications for holography.",
    "published": "2007-04-04T09:34:35Z",
    "updated": "2007-08-14T16:01:10Z",
    "authors": [
      "Robert G. Leigh",
      "Anastasios C. Petkou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/0902.2483v1",
    "title": "On the local Borel transform of Perturbation Theory",
    "summary": "We prove existence of the local Borel transform for the perturbative series\nof massive $\\vp_4^4$-theory. As compared to previous proofs in the literature,\nthe present bounds are much sharper as regards the dependence on external\nmomenta, they are explicit in the number of external legs, and they are\nobtained quite simply through a judiciously chosen induction hypothesis applied\nto the Wegner-Wilson-Polchinski flow equations. We pay attention not to\ngenerate an astronomically large numerical constant for the inverse radius of\nconvergence of the Borel transform.",
    "published": "2009-02-14T18:31:15Z",
    "updated": "2009-02-14T18:31:15Z",
    "authors": [
      "Christoph Kopper"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1107.2320v1",
    "title": "Photon Scattering with the Lorentz Integral Transform Method",
    "summary": "The application of the Lorentz integral transform (LIT) method to photon\nscattering off nuclei is presented in general. As an example, elastic photon\nscattering off the deuteron in the unretarded dipole approximation is\nconsidered using the LIT method. The inversion of the integral transform is\ndiscussed in detail paying particular attention to the high-energy\ncontributions in the resonance term. The obtained E1-polarizabilities are\ncompared to results from the literature. The corresponding theoretical cross\nsection is confronted with experimental results confirming, as already known\nfrom previous studies, that the E1-contribution is the most important one at\nlower energies.",
    "published": "2011-07-12T15:17:51Z",
    "updated": "2011-07-12T15:17:51Z",
    "authors": [
      "Giulia Bampa",
      "Winfried Leidemann",
      "H. ArenhÃ¶vel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1603.03247v1",
    "title": "Bose-condensed atomic systems with nonlocal interaction potentials",
    "summary": "The general approach for describing systems with Bose-Einstein condensate,\nwhere atoms interact through nonlocal pair potentials, is presented. A special\nattention is paid to nonintegrable potentials, such as the dipolar interaction\npotential. The potentials that are not absolutely integrable can have not well\ndefined Fourier transforms. Using formally such not defined Fourier transforms\nleads to unphysical conclusions. For making the Fourier transform well defined,\nthe interaction potential has to be regularized. This is illustrated by the\nexample of dipolar interactions.",
    "published": "2016-03-10T13:01:59Z",
    "updated": "2016-03-10T13:01:59Z",
    "authors": [
      "V. I. Yukalov",
      "E. P. Yukalova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1704.08773v2",
    "title": "Large gauge transformation and little group for soft photons",
    "summary": "Recently, large gauge transformation (LGT), the residual gauge symmetry after\ngauge fixing that survives at null infinity, has drawn much attention\nconcerning soft theorems and the memory effect. We point out that LGT charges\nin quantum electrodynamics are in fact one of non-compact generators of the two\ndimensional Euclidean group. Moreover, by comparing two equivalent descriptions\nof gauge transformation, we suggest that LGT is simply another way of\ndescribing the gauged little group for massless soft photons.",
    "published": "2017-04-27T23:36:26Z",
    "updated": "2017-11-15T15:51:47Z",
    "authors": [
      "Yuta Hamada",
      "Min-Seok Seo",
      "Gary Shiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.11424v1",
    "title": "Transformer-based Context-aware Sarcasm Detection in Conversation\n  Threads from Social Media",
    "summary": "We present a transformer-based sarcasm detection model that accounts for the\ncontext from the entire conversation thread for more robust predictions. Our\nmodel uses deep transformer layers to perform multi-head attentions among the\ntarget utterance and the relevant context in the thread. The context-aware\nmodels are evaluated on two datasets from social media, Twitter and Reddit, and\nshow 3.1% and 7.0% improvements over their baselines. Our best models give the\nF1-scores of 79.0% and 75.0% for the Twitter and Reddit datasets respectively,\nbecoming one of the highest performing systems among 36 participants in this\nshared task.",
    "published": "2020-05-22T23:41:35Z",
    "updated": "2020-05-22T23:41:35Z",
    "authors": [
      "Xiangjue Dong",
      "Changmao Li",
      "Jinho D. Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.02138v1",
    "title": "A Transformer-based Model to Detect Phishing URLs",
    "summary": "Phishing attacks are among emerging security issues that recently draws\nsignificant attention in the cyber security community. There are numerous\nexisting approaches for phishing URL detection. However, malicious URL\ndetection is still a research hotspot because attackers can bypass newly\nintroduced detection mechanisms by changing their tactics. This paper will\nintroduce a transformer-based malicious URL detection model, which has\nsignificant accuracy and outperforms current detection methods. We conduct\nexperiments and compare them with six existing classical detection models.\nExperiments demonstrate that our transformer-based model is the best performing\nmodel from all perspectives among the seven models and achieves 97.3 % of\ndetection accuracy.",
    "published": "2021-09-05T18:26:10Z",
    "updated": "2021-09-05T18:26:10Z",
    "authors": [
      "Pingfan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.00334v1",
    "title": "Cyberbullying detection across social media platforms via platform-aware\n  adversarial encoding",
    "summary": "Despite the increasing interest in cyberbullying detection, existing efforts\nhave largely been limited to experiments on a single platform and their\ngeneralisability across different social media platforms have received less\nattention. We propose XP-CB, a novel cross-platform framework based on\nTransformers and adversarial learning. XP-CB can enhance a Transformer\nleveraging unlabelled data from the source and target platforms to come up with\na common representation while preventing platform-specific training. To\nvalidate our proposed framework, we experiment on cyberbullying datasets from\nthree different platforms through six cross-platform configurations, showing\nits effectiveness with both BERT and RoBERTa as the underlying Transformer\nmodels.",
    "published": "2022-04-01T10:25:46Z",
    "updated": "2022-04-01T10:25:46Z",
    "authors": [
      "Peiling Yi",
      "Arkaitz Zubiaga"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.00367v1",
    "title": "A Comparison of Transformer, Convolutional, and Recurrent Neural\n  Networks on Phoneme Recognition",
    "summary": "Phoneme recognition is a very important part of speech recognition that\nrequires the ability to extract phonetic features from multiple frames. In this\npaper, we compare and analyze CNN, RNN, Transformer, and Conformer models using\nphoneme recognition. For CNN, the ContextNet model is used for the experiments.\nFirst, we compare the accuracy of various architectures under different\nconstraints, such as the receptive field length, parameter size, and layer\ndepth. Second, we interpret the performance difference of these models,\nespecially when the observable sequence length varies. Our analyses show that\nTransformer and Conformer models benefit from the long-range accessibility of\nself-attention through input frames.",
    "published": "2022-10-01T20:47:25Z",
    "updated": "2022-10-01T20:47:25Z",
    "authors": [
      "Kyuhong Shim",
      "Wonyong Sung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.09220v1",
    "title": "A Saccaded Visual Transformer for General Object Spotting",
    "summary": "This paper presents the novel combination of a visual transformer style patch\nclassifier with saccaded local attention. A novel optimisation paradigm for\ntraining object models is also presented, rather than the optimisation function\nminimising class membership probability error the network is trained to\nestimate the normalised distance to the centroid of labelled objects. This\napproach builds a degree of transnational invariance directly into the model\nand allows fast saccaded search with gradient ascent to find object centroids.\nThe resulting saccaded visual transformer is demonstrated on human faces.",
    "published": "2022-10-17T16:17:02Z",
    "updated": "2022-10-17T16:17:02Z",
    "authors": [
      "Willem. T. Pye",
      "David. A. Sinclair"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.09145v1",
    "title": "glassoformer: a query-sparse transformer for post-fault power grid\n  voltage prediction",
    "summary": "We propose GLassoformer, a novel and efficient transformer architecture\nleveraging group Lasso regularization to reduce the number of queries of the\nstandard self-attention mechanism. Due to the sparsified queries, GLassoformer\nis more computationally efficient than the standard transformers. On the power\ngrid post-fault voltage prediction task, GLassoformer shows remarkably better\nprediction than many existing benchmark algorithms in terms of accuracy and\nstability.",
    "published": "2022-01-22T23:29:54Z",
    "updated": "2022-01-22T23:29:54Z",
    "authors": [
      "Yunling Zheng",
      "Carson Hu",
      "Guang Lin",
      "Meng Yue",
      "Bao Wang",
      "Jack Xin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.09293v2",
    "title": "A transformer-based approach to video frame-level prediction in\n  Affective Behaviour Analysis In-the-wild",
    "summary": "In recent years, transformer architecture has been a dominating paradigm in\nmany applications, including affective computing. In this report, we propose\nour transformer-based model to handle Emotion Classification Task in the 5th\nAffective Behavior Analysis In-the-wild Competition. By leveraging the\nattentive model and the synthetic dataset, we attain a score of 0.4775 on the\nvalidation set of Aff-Wild2, the dataset provided by the organizer.",
    "published": "2023-03-16T13:13:13Z",
    "updated": "2023-03-19T05:27:26Z",
    "authors": [
      "Dang-Khanh Nguyen",
      "Ngoc-Huynh Ho",
      "Sudarshan Pant",
      "Hyung-Jeong Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1708.09760v1",
    "title": "Split Weyl transformations in quantum gravity",
    "summary": "We discuss various realizations of the Weyl group in the background field\nexpansion of quantum gravity, in the presence of a cutoff, as required in\napplications of the functional renormalization group. In order to study the\nbackground--dependence, special attention is given to split gauge\ntransformations, which act on the background field and fluctuation keeping the\ntotal metric unchanged. The results generalize previous works on global and\nlocal scale transformations.",
    "published": "2017-08-31T14:54:04Z",
    "updated": "2017-08-31T14:54:04Z",
    "authors": [
      "Carlos M. Nieto",
      "Roberto Percacci",
      "Vedran Skrinjar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1710.05665v1",
    "title": "Some combinatorial properties of the Hurwitz series ring",
    "summary": "We study some properties and perspectives of the Hurwitz series ring\n$H_R[[t]]$, for a commutative ring with identity $R$. Specifically, we provide\na closed form for the invertible elements by means of the complete ordinary\nBell polynomials, we highlight some connections with well--known transforms of\nsequences, and we see that the Stirling transforms are automorphisms of\n$H_R[[t]]$. Moreover, we focus the attention on some special subgroups studying\ntheir properties. Finally, we introduce a new transform of sequences that\nallows to see one of this subgroup as an ultrametric dynamic space.",
    "published": "2017-10-16T12:55:39Z",
    "updated": "2017-10-16T12:55:39Z",
    "authors": [
      "Stefano Barbero",
      "Umberto Cerruti",
      "Nadir Murru"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.00698v1",
    "title": "Kwak Transform and Inertial Manifolds revisited",
    "summary": "The paper gives sharp spectral gap conditions for existence of inertial\nmanifolds for abstract semilinear parabolic equations with non-self-adjoint\nleading part. Main attention is paid to the case where this leading part have\nJordan cells which appear after applying the so-called Kwak transform to\nvarious important equations such as 2D Navier-Stokes equations,\nreaction-diffusion-advection systems, etc. The different forms of Kwak\ntransforms and relations between them are also discussed.",
    "published": "2019-11-02T11:28:03Z",
    "updated": "2019-11-02T11:28:03Z",
    "authors": [
      "Anna Kostianko",
      "Sergey Zelik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1911.05507v1",
    "title": "Compressive Transformers for Long-Range Sequence Modelling",
    "summary": "We present the Compressive Transformer, an attentive sequence model which\ncompresses past memories for long-range sequence learning. We find the\nCompressive Transformer obtains state-of-the-art language modelling results in\nthe WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc\nrespectively. We also find it can model high-frequency speech effectively and\ncan be used as a memory mechanism for RL, demonstrated on an object matching\ntask. To promote the domain of long-range sequence learning, we propose a new\nopen-vocabulary language modelling benchmark derived from books, PG-19.",
    "published": "2019-11-13T14:36:01Z",
    "updated": "2019-11-13T14:36:01Z",
    "authors": [
      "Jack W. Rae",
      "Anna Potapenko",
      "Siddhant M. Jayakumar",
      "Timothy P. Lillicrap"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2009.13270v2",
    "title": "Dissecting Lottery Ticket Transformers: Structural and Behavioral Study\n  of Sparse Neural Machine Translation",
    "summary": "Recent work on the lottery ticket hypothesis has produced highly sparse\nTransformers for NMT while maintaining BLEU. However, it is unclear how such\npruning techniques affect a model's learned representations. By probing\nTransformers with more and more low-magnitude weights pruned away, we find that\ncomplex semantic information is first to be degraded. Analysis of internal\nactivations reveals that higher layers diverge most over the course of pruning,\ngradually becoming less complex than their dense counterparts. Meanwhile, early\nlayers of sparse models begin to perform more encoding. Attention mechanisms\nremain remarkably consistent as sparsity increases.",
    "published": "2020-09-17T02:08:45Z",
    "updated": "2020-10-12T18:55:22Z",
    "authors": [
      "Rajiv Movva",
      "Jason Y. Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.00400v1",
    "title": "Transformer-Encoder-GRU (T-E-GRU) for Chinese Sentiment Analysis on\n  Chinese Comment Text",
    "summary": "Chinese sentiment analysis (CSA) has always been one of the challenges in\nnatural language processing due to its complexity and uncertainty. Transformer\nhas succeeded in capturing semantic features, but it uses position encoding to\ncapture sequence features, which has great shortcomings compared with the\nrecurrent model. In this paper, we propose T-E-GRU for Chinese sentiment\nanalysis, which combine transformer encoder and GRU. We conducted experiments\non three Chinese comment datasets. In view of the confusion of punctuation\nmarks in Chinese comment texts, we selectively retain some punctuation marks\nwith sentence segmentation ability. The experimental results show that T-E-GRU\noutperforms classic recurrent model and recurrent model with attention.",
    "published": "2021-08-01T08:42:26Z",
    "updated": "2021-08-01T08:42:26Z",
    "authors": [
      "Binlong Zhang",
      "Wei Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.02034v1",
    "title": "Effective Theory of Transformers at Initialization",
    "summary": "We perform an effective-theory analysis of forward-backward signal\npropagation in wide and deep Transformers, i.e., residual neural networks with\nmulti-head self-attention blocks and multilayer perceptron blocks. This\nanalysis suggests particular width scalings of initialization and training\nhyperparameters for these models. We then take up such suggestions, training\nVision and Language Transformers in practical setups.",
    "published": "2023-04-04T18:00:01Z",
    "updated": "2023-04-04T18:00:01Z",
    "authors": [
      "Emily Dinan",
      "Sho Yaida",
      "Susan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.15097v1",
    "title": "Cascaded Cross-Modal Transformer for Request and Complaint Detection",
    "summary": "We propose a novel cascaded cross-modal transformer (CCMT) that combines\nspeech and text transcripts to detect customer requests and complaints in phone\nconversations. Our approach leverages a multimodal paradigm by transcribing the\nspeech using automatic speech recognition (ASR) models and translating the\ntranscripts into different languages. Subsequently, we combine\nlanguage-specific BERT-based models with Wav2Vec2.0 audio features in a novel\ncascaded cross-attention transformer model. We apply our system to the Requests\nSub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics\nChallenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for\nthe complaint and request classes, respectively.",
    "published": "2023-07-27T13:45:42Z",
    "updated": "2023-07-27T13:45:42Z",
    "authors": [
      "Nicolae-Catalin Ristea",
      "Radu Tudor Ionescu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.04118v1",
    "title": "Convolutional Transformer-Based Image Compression",
    "summary": "In this paper, we present a novel transformer-based architecture for\nend-to-end image compression. Our architecture incorporates blocks that\neffectively capture local dependencies between tokens, eliminating the need for\npositional encoding by integrating convolutional operations within the\nmulti-head attention mechanism. We demonstrate through experiments that our\nproposed framework surpasses state-of-the-art CNN-based architectures in terms\nof the trade-off between bit-rate and distortion and achieves comparable\nresults to transformer-based methods while maintaining lower computational\ncomplexity.",
    "published": "2024-09-06T08:36:35Z",
    "updated": "2024-09-06T08:36:35Z",
    "authors": [
      "Bouzid Arezki",
      "Fangchen Feng",
      "Anissa Mokraoui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.03503v2",
    "title": "Analyzing limits for in-context learning",
    "summary": "We examine limits of in-context learning (ICL) in transformer models trained\nfrom scratch, focusing on function approximation tasks as a controlled setting\nto uncover fundamental behaviors. While we show empirically that transformer\nmodels can generalize, approximating unseen classes of polynomial (non linear)\nfunctions, they cannot generalize beyond certain values. We provide both\nempirical and mathematical arguments explaining that these limitations stem\nfrom architectural components, namely layer normalization and the attention\nscoring function, softmax. Together, our findings reveal structural constraints\non ICL that are often masked in more complex NLP tasks but that need to be\nunderstood to improve robustness and interpretability in transformer-based\nmodels.",
    "published": "2025-02-05T11:03:36Z",
    "updated": "2025-05-30T13:55:14Z",
    "authors": [
      "Omar Naim",
      "Nicholas Asher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.02215v1",
    "title": "Interpretable Emergent Language Using Inter-Agent Transformers",
    "summary": "This paper explores the emergence of language in multi-agent reinforcement\nlearning (MARL) using transformers. Existing methods such as RIAL, DIAL, and\nCommNet enable agent communication but lack interpretability. We propose\nDifferentiable Inter-Agent Transformers (DIAT), which leverage self-attention\nto learn symbolic, human-understandable communication protocols. Through\nexperiments, DIAT demonstrates the ability to encode observations into\ninterpretable vocabularies and meaningful embeddings, effectively solving\ncooperative tasks. These results highlight the potential of DIAT for\ninterpretable communication in complex multi-agent environments.",
    "published": "2025-05-04T18:57:57Z",
    "updated": "2025-05-04T18:57:57Z",
    "authors": [
      "Mannan Bhardwaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07649v1",
    "title": "Constructing Bayes Minimax Estimators through Integral Transformations",
    "summary": "The problem of Bayes minimax estimation for the mean of a multivariate normal\ndistribution under quadratic loss has attracted significant attention recently.\nThese estimators have the advantageous property of being admissible, similar to\nBayes procedures, while also providing the conservative risk guarantees typical\nof frequentist methods. This paper demonstrates that Bayes minimax estimators\ncan be derived using integral transformation techniques, specifically through\nthe \\( I \\)-transform and the Laplace transform, as long as appropriate\nspherical priors are selected. Several illustrative examples are included to\nhighlight the effectiveness of the proposed approach.",
    "published": "2025-05-12T15:21:13Z",
    "updated": "2025-05-12T15:21:13Z",
    "authors": [
      "Dominique Fourdrinier",
      "William E. Strawderman",
      "Martin T. Wells"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21042v1",
    "title": "What exactly did the Transformer learn from our physics data?",
    "summary": "Transformer networks excel in scientific applications. We explore two\nscenarios in ultra-high-energy cosmic ray simulations to examine what these\nnetwork architectures learn. First, we investigate the trained positional\nencodings in air showers which are azimuthally symmetric. Second, we visualize\nthe attention values assigned to cosmic particles originating from a galaxy\ncatalog. In both cases, the Transformers learn plausible, physically meaningful\nfeatures.",
    "published": "2025-05-27T11:24:17Z",
    "updated": "2025-05-27T11:24:17Z",
    "authors": [
      "Martin Erdmann",
      "Niklas Langner",
      "Josina Schulte",
      "Dominik Wirtz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.08456v1",
    "title": "Space filling positionality and the Spiroformer",
    "summary": "Transformers excel when dealing with sequential data. Generalizing\ntransformer models to geometric domains, such as manifolds, we encounter the\nproblem of not having a well-defined global order. We propose a solution with\nattention heads following a space-filling curve. As a first experimental\nexample, we present the Spiroformer, a transformer that follows a polar spiral\non the $2$-sphere.",
    "published": "2025-07-11T09:56:15Z",
    "updated": "2025-07-11T09:56:15Z",
    "authors": [
      "M. Maurin",
      "M. Ã. Evangelista-Alvarado",
      "P. SuÃ¡rez-Serrato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.11040v1",
    "title": "Combining Transformers and CNNs for Efficient Object Detection in\n  High-Resolution Satellite Imagery",
    "summary": "We present GLOD, a transformer-first architecture for object detection in\nhigh-resolution satellite imagery. GLOD replaces CNN backbones with a Swin\nTransformer for end-to-end feature extraction, combined with novel UpConvMixer\nblocks for robust upsampling and Fusion Blocks for multi-scale feature\nintegration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods\nby 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a\nmulti-path head design capturing objects across scales. The architecture is\noptimized for satellite imagery challenges, leveraging spatial priors while\nmaintaining computational efficiency.",
    "published": "2025-07-15T07:10:34Z",
    "updated": "2025-07-15T07:10:34Z",
    "authors": [
      "Nicolas Drapier",
      "Aladine Chetouani",
      "AurÃ©lien Chateigner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04507v1",
    "title": "From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM\n  Approach",
    "summary": "Silent Speech Interfaces (SSIs) have gained attention for their ability to\ngenerate intelligible speech from non-acoustic signals. While significant\nprogress has been made in advancing speech generation pipelines, limited work\nhas addressed the recognition and downstream processing of synthesized speech,\nwhich often suffers from phonetic ambiguity and noise. To overcome these\nchallenges, we propose an enhanced automatic speech recognition framework that\ncombines a transformer-based acoustic model with a large language model (LLM)\nfor post-processing. The transformer captures full utterance context, while the\nLLM ensures linguistic consistency. Experimental results show a 16% relative\nand 6% absolute reduction in word error rate (WER) over a 36% baseline,\ndemonstrating substantial improvements in intelligibility for silent speech\ninterfaces.",
    "published": "2025-09-02T16:13:29Z",
    "updated": "2025-09-02T16:13:29Z",
    "authors": [
      "Nithyashree Sivasubramaniam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23178v1",
    "title": "Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information\n  Propagation Rules Based on Transformers",
    "summary": "Transformers are able to perform reasoning tasks, however the intrinsic\nmechanism remains widely open. In this paper we propose a set of information\npropagation rules based on Transformers and utilize symbolic reasoning tasks to\ntheoretically analyze the limit reasoning steps. We show that the limit number\nof reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with\n$L$ attention layers in a single-pass.",
    "published": "2025-09-27T08:13:39Z",
    "updated": "2025-09-27T08:13:39Z",
    "authors": [
      "Tian Qin",
      "Yuhan Chen",
      "Zhiwei Wang",
      "Zhi-Qin John Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24178v1",
    "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
    "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
    "published": "2025-09-29T01:52:10Z",
    "updated": "2025-09-29T01:52:10Z",
    "authors": [
      "Chengwei Zhou",
      "Steve Majerus",
      "Gourav Datta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.02671v7",
    "title": "A Logic for Expressing Log-Precision Transformers",
    "summary": "One way to interpret the reasoning power of transformer-based language models\nis to describe the types of logical rules they can resolve over some input\ntext. Recently, Chiang et al. (2023) showed that finite-precision transformers\ncan be equivalently expressed in a generalization of first-order logic.\nHowever, finite-precision transformers are a weak transformer variant because,\nas we show, a single head can only attend to a constant number of tokens and,\nin particular, cannot represent uniform attention. Since attending broadly is a\ncore capability for transformers, we ask whether a minimally more expressive\nmodel that can attend universally can also be characterized in logic. To this\nend, we analyze transformers whose forward pass is computed in $\\log n$\nprecision on contexts of length $n$. We prove that any log-precision\ntransformer can be equivalently expressed as a first-order logic sentence that,\nin addition to standard universal and existential quantifiers, may also contain\nmajority-vote quantifiers. This is the tightest known upper bound and first\nlogical characterization of log-precision transformers.",
    "published": "2022-10-06T04:18:09Z",
    "updated": "2025-09-10T19:48:04Z",
    "authors": [
      "William Merrill",
      "Ashish Sabharwal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.04006v1",
    "title": "Long Range Arena: A Benchmark for Efficient Transformers",
    "summary": "Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.",
    "published": "2020-11-08T15:53:56Z",
    "updated": "2020-11-08T15:53:56Z",
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Samira Abnar",
      "Yikang Shen",
      "Dara Bahri",
      "Philip Pham",
      "Jinfeng Rao",
      "Liu Yang",
      "Sebastian Ruder",
      "Donald Metzler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.12753v3",
    "title": "Vision Transformers with Patch Diversification",
    "summary": "Vision transformer has demonstrated promising performance on challenging\ncomputer vision tasks. However, directly training the vision transformers may\nyield unstable and sub-optimal results. Recent works propose to improve the\nperformance of the vision transformers by modifying the transformer structures,\ne.g., incorporating convolution layers. In contrast, we investigate an\northogonal approach to stabilize the vision transformer training without\nmodifying the networks. We observe the instability of the training can be\nattributed to the significant similarity across the extracted patch\nrepresentations. More specifically, for deep vision transformers, the\nself-attention blocks tend to map different patches into similar latent\nrepresentations, yielding information loss and performance degradation. To\nalleviate this problem, in this work, we introduce novel loss functions in\nvision transformer training to explicitly encourage diversity across patch\nrepresentations for more discriminative feature extraction. We empirically show\nthat our proposed techniques stabilize the training and allow us to train wider\nand deeper vision transformers. We further show the diversified features\nsignificantly benefit the downstream tasks in transfer learning. For semantic\nsegmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and\nADE20k. Our code is available at\nhttps://github.com/ChengyueGongR/PatchVisionTransformer.",
    "published": "2021-04-26T17:43:04Z",
    "updated": "2021-06-11T01:35:08Z",
    "authors": [
      "Chengyue Gong",
      "Dilin Wang",
      "Meng Li",
      "Vikas Chandra",
      "Qiang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.05305v2",
    "title": "ConvNets vs. Transformers: Whose Visual Representations are More\n  Transferable?",
    "summary": "Vision transformers have attracted much attention from computer vision\nresearchers as they are not restricted to the spatial inductive bias of\nConvNets. However, although Transformer-based backbones have achieved much\nprogress on ImageNet classification, it is still unclear whether the learned\nrepresentations are as transferable as or even more transferable than ConvNets'\nfeatures. To address this point, we systematically investigate the transfer\nlearning ability of ConvNets and vision transformers in 15 single-task and\nmulti-task performance evaluations. Given the strong correlation between the\nperformance of pre-trained models and transfer learning, we include 2 residual\nConvNets (i.e., R-101x3 and R-152x4) and 3 Transformer-based visual backbones\n(i.e., ViT-B, ViT-L and Swin-B), which have close error rates on ImageNet, that\nindicate similar transfer learning performance on downstream datasets.\n  We observe consistent advantages of Transformer-based backbones on 13\ndownstream tasks (out of 15), including but not limited to fine-grained\nclassification, scene recognition (classification, segmentation and depth\nestimation), open-domain classification, face recognition, etc. More\nspecifically, we find that two ViT models heavily rely on whole network\nfine-tuning to achieve performance gains while Swin Transformer does not have\nsuch a requirement. Moreover, vision transformers behave more robustly in\nmulti-task learning, i.e., bringing more improvements when managing mutually\nbeneficial tasks and reducing performance losses when tackling irrelevant\ntasks. We hope our discoveries can facilitate the exploration and exploitation\nof vision transformers in the future.",
    "published": "2021-08-11T16:20:38Z",
    "updated": "2021-08-17T04:53:18Z",
    "authors": [
      "Hong-Yu Zhou",
      "Chixiang Lu",
      "Sibei Yang",
      "Yizhou Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.06747v2",
    "title": "SOTR: Segmenting Objects with Transformers",
    "summary": "Most recent transformer-based models show impressive performance on vision\ntasks, even better than Convolution Neural Networks (CNN). In this work, we\npresent a novel, flexible, and effective transformer-based model for\nhigh-quality instance segmentation. The proposed method, Segmenting Objects\nwith TRansformers (SOTR), simplifies the segmentation pipeline, building on an\nalternative CNN backbone appended with two parallel subtasks: (1) predicting\nper-instance category via transformer and (2) dynamically generating\nsegmentation mask with the multi-level upsampling module. SOTR can effectively\nextract lower-level feature representations and capture long-range context\ndependencies by Feature Pyramid Network (FPN) and twin transformer,\nrespectively. Meanwhile, compared with the original transformer, the proposed\ntwin transformer is time- and resource-efficient since only a row and a column\nattention are involved to encode pixels. Moreover, SOTR is easy to be\nincorporated with various CNN backbones and transformer model variants to make\nconsiderable improvements for the segmentation accuracy and training\nconvergence. Extensive experiments show that our SOTR performs well on the MS\nCOCO dataset and surpasses state-of-the-art instance segmentation approaches.\nWe hope our simple but strong framework could serve as a preferment baseline\nfor instance-level recognition. Our code is available at\nhttps://github.com/easton-cau/SOTR.",
    "published": "2021-08-15T14:10:11Z",
    "updated": "2021-08-17T04:15:21Z",
    "authors": [
      "Ruohao Guo",
      "Dantong Niu",
      "Liao Qu",
      "Zhenbo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.06091v4",
    "title": "A Survey of Visual Transformers",
    "summary": "Transformer, an attention-based encoder-decoder model, has already\nrevolutionized the field of natural language processing (NLP). Inspired by such\nsignificant achievements, some pioneering works have recently been done on\nemploying Transformer-liked architectures in the computer vision (CV) field,\nwhich have demonstrated their effectiveness on three fundamental CV tasks\n(classification, detection, and segmentation) as well as multiple sensory data\nstream (images, point clouds, and vision-language data). Because of their\ncompetitive modeling capabilities, the visual Transformers have achieved\nimpressive performance improvements over multiple benchmarks as compared with\nmodern Convolution Neural Networks (CNNs). In this survey, we have reviewed\nover one hundred of different visual Transformers comprehensively according to\nthree fundamental CV tasks and different data stream types, where a taxonomy is\nproposed to organize the representative methods according to their motivations,\nstructures, and application scenarios. Because of their differences on training\nsettings and dedicated vision tasks, we have also evaluated and compared all\nthese existing visual Transformers under different configurations. Furthermore,\nwe have revealed a series of essential but unexploited aspects that may empower\nsuch visual Transformers to stand out from numerous architectures, e.g., slack\nhigh-level semantic embeddings to bridge the gap between the visual\nTransformers and the sequential ones. Finally, three promising research\ndirections are suggested for future investment. We will continue to update the\nlatest articles and their released source codes at\nhttps://github.com/liuyang-ict/awesome-visual-transformers.",
    "published": "2021-11-11T07:56:04Z",
    "updated": "2022-12-06T16:26:56Z",
    "authors": [
      "Yang Liu",
      "Yao Zhang",
      "Yixin Wang",
      "Feng Hou",
      "Jin Yuan",
      "Jiang Tian",
      "Yang Zhang",
      "Zhongchao Shi",
      "Jianping Fan",
      "Zhiqiang He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.09300v1",
    "title": "Towards End-to-End Image Compression and Analysis with Transformers",
    "summary": "We propose an end-to-end image compression and analysis model with\nTransformers, targeting to the cloud-based image classification application.\nInstead of placing an existing Transformer-based image classification model\ndirectly after an image codec, we aim to redesign the Vision Transformer (ViT)\nmodel to perform image classification from the compressed features and\nfacilitate image compression with the long-term information from the\nTransformer. Specifically, we first replace the patchify stem (i.e., image\nsplitting and embedding) of the ViT model with a lightweight image encoder\nmodelled by a convolutional neural network. The compressed features generated\nby the image encoder are injected convolutional inductive bias and are fed to\nthe Transformer for image classification bypassing image reconstruction.\nMeanwhile, we propose a feature aggregation module to fuse the compressed\nfeatures with the selected intermediate features of the Transformer, and feed\nthe aggregated features to a deconvolutional neural network for image\nreconstruction. The aggregated features can obtain the long-term information\nfrom the self-attention mechanism of the Transformer and improve the\ncompression performance. The rate-distortion-accuracy optimization problem is\nfinally solved by a two-step training strategy. Experimental results\ndemonstrate the effectiveness of the proposed model in both the image\ncompression and the classification tasks.",
    "published": "2021-12-17T03:28:14Z",
    "updated": "2021-12-17T03:28:14Z",
    "authors": [
      "Yuanchao Bai",
      "Xu Yang",
      "Xianming Liu",
      "Junjun Jiang",
      "Yaowei Wang",
      "Xiangyang Ji",
      "Wen Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.14538v2",
    "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
    "summary": "Designing better deep networks and better reinforcement learning (RL)\nalgorithms are both important for deep RL. This work focuses on the former.\nPrevious methods build the network with several modules like CNN, LSTM and\nAttention. Recent methods combine the Transformer with these modules for better\nperformance. However, it requires tedious optimization skills to train a\nnetwork composed of mixed modules, making these methods inconvenient to be used\nin practice. In this paper, we propose to design \\emph{pure Transformer-based\nnetworks} for deep RL, aiming at providing off-the-shelf backbones for both the\nonline and offline settings. Specifically, the Transformer in Transformer (TIT)\nbackbone is proposed, which cascades two Transformers in a very natural way:\nthe inner one is used to process a single observation, while the outer one is\nresponsible for processing the observation history; combining both is expected\nto extract spatial-temporal representations for good decision-making.\nExperiments show that TIT can achieve satisfactory performance in different\nsettings consistently.",
    "published": "2022-12-30T03:50:38Z",
    "updated": "2023-01-03T06:51:22Z",
    "authors": [
      "Hangyu Mao",
      "Rui Zhao",
      "Hao Chen",
      "Jianye Hao",
      "Yiqun Chen",
      "Dong Li",
      "Junge Zhang",
      "Zhen Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.17589v1",
    "title": "Graph Inductive Biases in Transformers without Message Passing",
    "summary": "Transformers for graph data are increasingly widely studied and successful in\nnumerous learning tasks. Graph inductive biases are crucial for Graph\nTransformers, and previous works incorporate them using message-passing modules\nand/or positional encodings. However, Graph Transformers that use\nmessage-passing inherit known issues of message-passing, and differ\nsignificantly from Transformers used in other domains, thus making transfer of\nresearch advances more difficult. On the other hand, Graph Transformers without\nmessage-passing often perform poorly on smaller datasets, where inductive\nbiases are more crucial. To bridge this gap, we propose the Graph Inductive\nbias Transformer (GRIT) -- a new Graph Transformer that incorporates graph\ninductive biases without using message passing. GRIT is based on several\narchitectural changes that are each theoretically and empirically justified,\nincluding: learned relative positional encodings initialized with random walk\nprobabilities, a flexible attention mechanism that updates node and node-pair\nrepresentations, and injection of degree information in each layer. We prove\nthat GRIT is expressive -- it can express shortest path distances and various\ngraph propagation matrices. GRIT achieves state-of-the-art empirical\nperformance across a variety of graph datasets, thus showing the power that\nGraph Transformers without message-passing can deliver.",
    "published": "2023-05-27T22:26:27Z",
    "updated": "2023-05-27T22:26:27Z",
    "authors": [
      "Liheng Ma",
      "Chen Lin",
      "Derek Lim",
      "Adriana Romero-Soriano",
      "Puneet K. Dokania",
      "Mark Coates",
      "Philip Torr",
      "Ser-Nam Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.07303v1",
    "title": "A Comprehensive Survey on Applications of Transformers for Deep Learning\n  Tasks",
    "summary": "Transformer is a deep neural network that employs a self-attention mechanism\nto comprehend the contextual relationships within sequential data. Unlike\nconventional neural networks or updated versions of Recurrent Neural Networks\n(RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in\nhandling long dependencies between input sequence elements and enable parallel\nprocessing. As a result, transformer-based models have attracted substantial\ninterest among researchers in the field of artificial intelligence. This can be\nattributed to their immense potential and remarkable achievements, not only in\nNatural Language Processing (NLP) tasks but also in a wide range of domains,\nincluding computer vision, audio and speech processing, healthcare, and the\nInternet of Things (IoT). Although several survey papers have been published\nhighlighting the transformer's contributions in specific fields, architectural\ndifferences, or performance evaluations, there is still a significant absence\nof a comprehensive survey paper encompassing its major applications across\nvarious domains. Therefore, we undertook the task of filling this gap by\nconducting an extensive survey of proposed transformer models from 2017 to\n2022. Our survey encompasses the identification of the top five application\ndomains for transformer-based models, namely: NLP, Computer Vision,\nMulti-Modality, Audio and Speech Processing, and Signal Processing. We analyze\nthe impact of highly influential transformer-based models in these domains and\nsubsequently classify them based on their respective tasks using a proposed\ntaxonomy. Our aim is to shed light on the existing potential and future\npossibilities of transformers for enthusiastic researchers, thus contributing\nto the broader understanding of this groundbreaking technology.",
    "published": "2023-06-11T23:13:51Z",
    "updated": "2023-06-11T23:13:51Z",
    "authors": [
      "Saidul Islam",
      "Hanae Elmekki",
      "Ahmed Elsebai",
      "Jamal Bentahar",
      "Najat Drawel",
      "Gaith Rjoub",
      "Witold Pedrycz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.10898v2",
    "title": "B-cos Alignment for Inherently Interpretable CNNs and Vision\n  Transformers",
    "summary": "We present a new direction for increasing the interpretability of deep neural\nnetworks (DNNs) by promoting weight-input alignment during training. For this,\nwe propose to replace the linear transformations in DNNs by our novel B-cos\ntransformation. As we show, a sequence (network) of such transformations\ninduces a single linear transformation that faithfully summarises the full\nmodel computations. Moreover, the B-cos transformation is designed such that\nthe weights align with relevant signals during optimisation. As a result, those\ninduced linear transformations become highly interpretable and highlight\ntask-relevant features. Importantly, the B-cos transformation is designed to be\ncompatible with existing architectures and we show that it can easily be\nintegrated into virtually all of the latest state of the art models for\ncomputer vision - e.g. ResNets, DenseNets, ConvNext models, as well as Vision\nTransformers - by combining the B-cos-based explanations with normalisation and\nattention layers, all whilst maintaining similar accuracy on ImageNet. Finally,\nwe show that the resulting explanations are of high visual quality and perform\nwell under quantitative interpretability metrics.",
    "published": "2023-06-19T12:54:28Z",
    "updated": "2024-01-15T09:13:05Z",
    "authors": [
      "Moritz BÃ¶hle",
      "Navdeeppal Singh",
      "Mario Fritz",
      "Bernt Schiele"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.06693v1",
    "title": "Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation",
    "summary": "Recent leading zero-shot video object segmentation (ZVOS) works devote to\nintegrating appearance and motion information by elaborately designing feature\nfusion modules and identically applying them in multiple feature stages. Our\npreliminary experiments show that with the strong long-range dependency\nmodeling capacity of Transformer, simply concatenating the two modality\nfeatures and feeding them to vanilla Transformers for feature fusion can\ndistinctly benefit the performance but at a cost of heavy computation. Through\nfurther empirical analysis, we find that attention dependencies learned in\nTransformer in different stages exhibit completely different properties: global\nquery-independent dependency in the low-level stages and semantic-specific\ndependency in the high-level stages. Motivated by the observations, we propose\ntwo Transformer variants: i) Context-Sharing Transformer (CST) that learns the\nglobal-shared contextual information within image frames with a lightweight\ncomputation. ii) Semantic Gathering-Scattering Transformer (SGST) that models\nthe semantic correlation separately for the foreground and background and\nreduces the computation cost with a soft token merging mechanism. We apply CST\nand SGST for low-level and high-level feature fusions, respectively,\nformulating a level-isomerous Transformer framework for ZVOS task. Compared\nwith the baseline that uses vanilla Transformers for multi-stage fusion, ours\nsignificantly increase the speed by 13 times and achieves new state-of-the-art\nZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer.",
    "published": "2023-08-13T06:12:00Z",
    "updated": "2023-08-13T06:12:00Z",
    "authors": [
      "Yichen Yuan",
      "Yifan Wang",
      "Lijun Wang",
      "Xiaoqi Zhao",
      "Huchuan Lu",
      "Yu Wang",
      "Weibo Su",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.12296v1",
    "title": "Understanding Video Transformers for Segmentation: A Survey of\n  Application and Interpretability",
    "summary": "Video segmentation encompasses a wide range of categories of problem\nformulation, e.g., object, scene, actor-action and multimodal video\nsegmentation, for delineating task-specific scene components with pixel-level\nmasks. Recently, approaches in this research area shifted from concentrating on\nConvNet-based to transformer-based models. In addition, various\ninterpretability approaches have appeared for transformer models and video\ntemporal dynamics, motivated by the growing interest in basic scientific\nunderstanding, model diagnostics and societal implications of real-world\ndeployment. Previous surveys mainly focused on ConvNet models on a subset of\nvideo segmentation tasks or transformers for classification tasks. Moreover,\ncomponent-wise discussion of transformer-based video segmentation models has\nnot yet received due focus. In addition, previous reviews of interpretability\nmethods focused on transformers for classification, while analysis of video\ntemporal dynamics modelling capabilities of video models received less\nattention. In this survey, we address the above with a thorough discussion of\nvarious categories of video segmentation, a component-wise discussion of the\nstate-of-the-art transformer-based models, and a review of related\ninterpretability methods. We first present an introduction to the different\nvideo segmentation task categories, their objectives, specific challenges and\nbenchmark datasets. Next, we provide a component-wise review of recent\ntransformer-based models and document the state of the art on different video\nsegmentation tasks. Subsequently, we discuss post-hoc and ante-hoc\ninterpretability methods for transformer models and interpretability methods\nfor understanding the role of the temporal dimension in video models. Finally,\nwe conclude our discussion with future research directions.",
    "published": "2023-10-18T19:58:25Z",
    "updated": "2023-10-18T19:58:25Z",
    "authors": [
      "Rezaul Karim",
      "Richard P. Wildes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.02586v1",
    "title": "ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars\n  for Write Noise Mitigation",
    "summary": "Transformers have revolutionized various real-world applications from natural\nlanguage processing to computer vision. However, traditional von-Neumann\ncomputing paradigm faces memory and bandwidth limitations in accelerating\ntransformers owing to their massive model sizes. To this end, In-memory\nComputing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their\nability to perform highly parallelized Matrix-Vector-Multiplications (MVMs)\nwith high energy-efficiencies, have emerged as a promising solution for\naccelerating transformers. However, analog MVM operations in crossbars\nintroduce non-idealities, such as stochastic read & write noise, which affect\nthe inference accuracy of the deployed transformers. Specifically, we find\npre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the\nimpact of write noise on the dynamically-generated Key (K) and Value (V)\nmatrices in the attention layers, an effect not accounted for in prior studies.\nWe, thus, propose ClipFormer, a transformation on the K and V matrices during\ninference, to boost the non-ideal accuracies of pre-trained ViT models.\nClipFormer requires no additional hardware and training overhead and is\namenable to transformers deployed on any memristive crossbar platform. Our\nexperiments on Imagenet-1k dataset using pre-trained DeiT-S transformers,\nsubjected to standard training and variation-aware-training, show >10-40%\nhigher non-ideal accuracies at the high write noise regime by applying\nClipFormer.",
    "published": "2024-02-04T19:04:37Z",
    "updated": "2024-02-04T19:04:37Z",
    "authors": [
      "Abhiroop Bhattacharjee",
      "Abhishek Moitra",
      "Priyadarshini Panda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.14202v4",
    "title": "Comparing Graph Transformers via Positional Encodings",
    "summary": "The distinguishing power of graph transformers is closely tied to the choice\nof positional encoding: features used to augment the base transformer with\ninformation about the graph. There are two primary types of positional\nencoding: absolute positional encodings (APEs) and relative positional\nencodings (RPEs). APEs assign features to each node and are given as input to\nthe transformer. RPEs instead assign a feature to each pair of nodes, e.g.,\ngraph distance, and are used to augment the attention block. A priori, it is\nunclear which method is better for maximizing the power of the resulting graph\ntransformer. In this paper, we aim to understand the relationship between these\ndifferent types of positional encodings. Interestingly, we show that graph\ntransformers using APEs and RPEs are equivalent in terms of distinguishing\npower. In particular, we demonstrate how to interchange APEs and RPEs while\nmaintaining their distinguishing power in terms of graph transformers. Based on\nour theoretical results, we provide a study on several APEs and RPEs (including\nthe resistance distance and the recently introduced stable and expressive\npositional encoding (SPE)) and compare their distinguishing power in terms of\ntransformers. We believe our work will help navigate the huge number of choices\nof positional encoding and will provide guidance on the future design of\npositional encodings for graph transformers.",
    "published": "2024-02-22T01:07:48Z",
    "updated": "2024-08-22T23:22:33Z",
    "authors": [
      "Mitchell Black",
      "Zhengchao Wan",
      "Gal Mishne",
      "Amir Nayyeri",
      "Yusu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.01765v1",
    "title": "Reproducibility Study on Adversarial Attacks Against Robust Transformer\n  Trackers",
    "summary": "New transformer networks have been integrated into object tracking pipelines\nand have demonstrated strong performance on the latest benchmarks. This paper\nfocuses on understanding how transformer trackers behave under adversarial\nattacks and how different attacks perform on tracking datasets as their\nparameters change. We conducted a series of experiments to evaluate the\neffectiveness of existing adversarial attacks on object trackers with\ntransformer and non-transformer backbones. We experimented on 7 different\ntrackers, including 3 that are transformer-based, and 4 which leverage other\narchitectures. These trackers are tested against 4 recent attack methods to\nassess their performance and robustness on VOT2022ST, UAV123 and GOT10k\ndatasets. Our empirical study focuses on evaluating adversarial robustness of\nobject trackers based on bounding box versus binary mask predictions, and\nattack methods at different levels of perturbations. Interestingly, our study\nfound that altering the perturbation level may not significantly affect the\noverall object tracking results after the attack. Similarly, the sparsity and\nimperceptibility of the attack perturbations may remain stable against\nperturbation level shifts. By applying a specific attack on all transformer\ntrackers, we show that new transformer trackers having a stronger\ncross-attention modeling achieve a greater adversarial robustness on tracking\ndatasets, such as VOT2022ST and GOT10k. Our results also indicate the necessity\nfor new attack methods to effectively tackle the latest types of transformer\ntrackers. The codes necessary to reproduce this study are available at\nhttps://github.com/fatemehN/ReproducibilityStudy.",
    "published": "2024-06-03T20:13:38Z",
    "updated": "2024-06-03T20:13:38Z",
    "authors": [
      "Fatemeh Nourilenjan Nokabadi",
      "Jean-FranÃ§ois Lalonde",
      "Christian GagnÃ©"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.05221v1",
    "title": "Early-Stage Requirements Transformation Approaches: A Systematic Review",
    "summary": "Transformation approaches for automatically constructing analysis models from\ntextual requirements are critical to software development, as they can bring\nforward the use of precise formal languages from the coding phase to the\nrequirement analysis phase in the software development life-cycle. Over the\ndecades, numerous transformation approaches have been developed in an attempt\nto fully or partially automate this initial phase. This systematic review\nexamines transformation approaches in the early stages of software development,\nexamining 25 studies on early-stage requirements transformation documented\nbetween 2000 and 2014. The review highlights the widespread use of natural\nlanguage processing techniques, with tools like the Stanford parser and WordNet\nbeing essential. Intermediate models are often used in the transformation\nprocess to bridge the gap between textual requirements and analysis models.\nSignificant advancements have been made in early-stage requirements\ntransformation approaches; however, several areas require attention to enhance\ntheir effectiveness and reliability. A challenge identified is the lack of\nrobust evaluation methods, with most approaches using simple case studies and\nrunning examples for evaluation. This makes it difficult to compare and\nevaluate the performance these approaches. Although most approaches can\ngenerate structural models from textual requirements, many generate incomplete\nmodels with missing elements. Furthermore, requirements traceability is largely\nneglected, with only two approaches addressing it and lacking explicit detail\non how traceability links are maintained during the transformation process.\nThis review emphasize the need for formalized evaluation techniques and greater\ntransparency and accessibility of approaches used in the early-stage\nrequirements transformation.",
    "published": "2024-07-25T18:13:29Z",
    "updated": "2024-07-25T18:13:29Z",
    "authors": [
      "Keletso J. Letsholo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.07602v2",
    "title": "Circuit Complexity Bounds for RoPE-based Transformer Architecture",
    "summary": "Characterizing the express power of the Transformer architecture is critical\nto understanding its capacity limits and scaling law. Recent works provide the\ncircuit complexity bounds to Transformer-like architecture. On the other hand,\nRotary Position Embedding ($\\mathsf{RoPE}$) has emerged as a crucial technique\nin modern large language models, offering superior performance in capturing\npositional information compared to traditional position embeddings, which shows\ngreat potential in application prospects, particularly for the long context\nscenario. Empirical evidence also suggests that $\\mathsf{RoPE}$-based\nTransformer architectures demonstrate greater generalization capabilities\ncompared to conventional Transformer models. In this work, we establish a\ncircuit complexity bound for Transformers with $\\mathsf{RoPE}$ attention. Our\nkey contribution is that we show that unless $\\mathsf{TC}^0 = \\mathsf{NC}^1$, a\n$\\mathsf{RoPE}$-based Transformer with $\\mathrm{poly}(n)$-precision, $O(1)$\nlayers, hidden dimension $d \\leq O(n)$ cannot solve the Arithmetic formula\nevaluation problem or the Boolean formula value problem. This result\nsignificantly demonstrates the fundamental limitation of the expressivity of\nthe $\\mathsf{RoPE}$-based Transformer architecture, although it achieves giant\nempirical success. Our theoretical result not only establishes the complexity\nbound but also may instruct further work on the $\\mathsf{RoPE}$-based\nTransformer.",
    "published": "2024-11-12T07:24:41Z",
    "updated": "2024-12-01T06:39:41Z",
    "authors": [
      "Bo Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Jiangxuan Long",
      "Zhenmei Shi",
      "Zhao Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.10573v1",
    "title": "An Innovative Next Activity Prediction Approach Using Process Entropy\n  and DAW-Transformer",
    "summary": "Purpose - In Business Process Management (BPM), accurate prediction of the\nnext activities is vital for operational efficiency and decision-making.\nCurrent Artificial Intelligence (AI)/Machine Learning (ML) models struggle with\nthe complexity and evolving nature of business process event logs, balancing\naccuracy and interpretability. This paper proposes an entropy-driven model\nselection approach and DAW-Transformer, which stands for Dynamic\nAttribute-Aware Transformer, to integrate all attributes with a dynamic window\nfor better accuracy.\n  Design/methodology/approach - This paper introduces a novel next-activity\nprediction approach that uses process entropy to assess the complexity of event\nlogs and dynamically select the most suitable ML model. A new transformer-based\narchitecture with multi-head attention and dynamic windowing mechanism,\nDAW-Transformer, is proposed to capture long-range dependencies and utilize all\nrelevant event log attributes. Experiments were conducted on six public\ndatasets, and the performance was evaluated with process entropy.\n  Finding - The results demonstrate the effectiveness of the approach across\nthese publicly available datasets. DAW-Transformer achieved superior\nperformance, especially on high-entropy datasets such as Sepsis exceeding\nLimited window Multi-Transformers by 4.69% and a benchmark CNN-LSTM-SAtt model\nby 3.07%. For low-entropy datasets like Road Traffic Fine, simpler, more\ninterpretable algorithms like Random Forest performed nearly as well as the\nmore complex DAW-Transformer and offered better handling of imbalanced data and\nimproved explainability.\n  Originality/ value - This work's novelty lies in the proposed\nDAW-Transformer, with a dynamic window and considering all relevant attributes.\nAlso, entropy-driven selection methods offer a robust, accurate, and\ninterpretable solution for next-activity prediction.",
    "published": "2025-02-14T22:02:00Z",
    "updated": "2025-02-14T22:02:00Z",
    "authors": [
      "Hadi Zare",
      "Mostafa Abbasi",
      "Maryam Ahang",
      "Homayoun Najjaran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.13558v1",
    "title": "Transformers Can Overcome the Curse of Dimensionality: A Theoretical\n  Study from an Approximation Perspective",
    "summary": "The Transformer model is widely used in various application areas of machine\nlearning, such as natural language processing. This paper investigates the\napproximation of the H\\\"older continuous function class\n$\\mathcal{H}_{Q}^{\\beta}\\left([0,1]^{d\\times n},\\mathbb{R}^{d\\times n}\\right)$\nby Transformers and constructs several Transformers that can overcome the curse\nof dimensionality. These Transformers consist of one self-attention layer with\none head and the softmax function as the activation function, along with\nseveral feedforward layers. For example, to achieve an approximation accuracy\nof $\\epsilon$, if the activation functions of the feedforward layers in the\nTransformer are ReLU and floor, only\n$\\mathcal{O}\\left(\\log\\frac{1}{\\epsilon}\\right)$ layers of feedforward layers\nare needed, with widths of these layers not exceeding\n$\\mathcal{O}\\left(\\frac{1}{\\epsilon^{2/\\beta}}\\log\\frac{1}{\\epsilon}\\right)$.\nIf other activation functions are allowed in the feedforward layers, the width\nof the feedforward layers can be further reduced to a constant. These results\ndemonstrate that Transformers have a strong expressive capability. The\nconstruction in this paper is based on the Kolmogorov-Arnold Representation\nTheorem and does not require the concept of contextual mapping, hence our proof\nis more intuitively clear compared to previous Transformer approximation works.\nAdditionally, the translation technique proposed in this paper helps to apply\nthe previous approximation results of feedforward neural networks to\nTransformer research.",
    "published": "2025-04-18T08:56:53Z",
    "updated": "2025-04-18T08:56:53Z",
    "authors": [
      "Yuling Jiao",
      "Yanming Lai",
      "Yang Wang",
      "Bokai Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.01575v2",
    "title": "Asset Pricing in Pre-trained Transformer",
    "summary": "This paper proposes an innovative Transformer model, Single-directional\nrepresentative from Transformer (SERT), for US large capital stock pricing. It\nalso innovatively applies the pre-trained Transformer models under the stock\npricing and factor investment context. They are compared with standard\nTransformer models and encoder-only Transformer models in three periods\ncovering the entire COVID-19 pandemic to examine the model adaptivity and\nsuitability during the extreme market fluctuations. Namely, pre-COVID-19 period\n(mild up-trend), COVID-19 period (sharp up-trend with deep down shock) and\n1-year post-COVID-19 (high fluctuation sideways movement). The best proposed\nSERT model achieves the highest out-of-sample R2, 11.2% and 10.91%\nrespectively, when extreme market fluctuation takes place followed by\npre-trained Transformer models (10.38% and 9.15%). Their Trend-following-based\nstrategy wise performance also proves their excellent capability for hedging\ndownside risks during market shocks. The proposed SERT model achieves a Sortino\nratio 47% higher than the buy-and-hold benchmark in the equal-weighted\nportfolio and 28% higher in the value-weighted portfolio when the pandemic\nperiod is attended. It proves that Transformer models have a great capability\nto capture patterns of temporal sparsity data in the asset pricing factor\nmodel, especially with considerable volatilities. We also find the softmax\nsignal filter as the common configuration of Transformer models in alternative\ncontexts, which only eliminates differences between models, but does not\nimprove strategy-wise performance, while increasing attention heads improve the\nmodel performance insignificantly and applying the 'layer norm first' method do\nnot boost the model performance in our case.",
    "published": "2025-05-02T20:38:59Z",
    "updated": "2025-05-06T10:14:15Z",
    "authors": [
      "Shanyan Lai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.09903v1",
    "title": "Transformer-based ASR Incorporating Time-reduction Layer and Fine-tuning\n  with Self-Knowledge Distillation",
    "summary": "End-to-end automatic speech recognition (ASR), unlike conventional ASR, does\nnot have modules to learn the semantic representation from speech encoder.\nMoreover, the higher frame-rate of speech representation prevents the model to\nlearn the semantic representation properly. Therefore, the models that are\nconstructed by the lower frame-rate of speech encoder lead to better\nperformance. For Transformer-based ASR, the lower frame-rate is not only\nimportant for learning better semantic representation but also for reducing the\ncomputational complexity due to the self-attention mechanism which has O(n^2)\norder of complexity in both training and inference. In this paper, we propose a\nTransformer-based ASR model with the time reduction layer, in which we\nincorporate time reduction layer inside transformer encoder layers in addition\nto traditional sub-sampling methods to input features that further reduce the\nframe-rate. This can help in reducing the computational cost of the\nself-attention process for training and inference with performance improvement.\nMoreover, we introduce a fine-tuning approach for pre-trained ASR models using\nself-knowledge distillation (S-KD) which further improves the performance of\nour ASR model. Experiments on LibriSpeech datasets show that our proposed\nmethods outperform all other Transformer-based ASR systems. Furthermore, with\nlanguage model (LM) fusion, we achieve new state-of-the-art word error rate\n(WER) results for Transformer-based ASR models with just 30 million parameters\ntrained without any external data.",
    "published": "2021-03-17T21:02:36Z",
    "updated": "2021-03-17T21:02:36Z",
    "authors": [
      "Md Akmal Haidar",
      "Chao Xing",
      "Mehdi Rezagholizadeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2010.09697v5",
    "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive\n  Bias from Gradient Descent",
    "summary": "The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically a variant of gradient descent\n(GD). To better understand this bias, we study the tendency for transformer\nparameters to grow in magnitude ($\\ell_2$ norm) during training, and its\nimplications for the emergent representations within self attention layers.\nEmpirically, we document norm growth in the training of transformer language\nmodels, including T5 during its pretraining. As the parameters grow in\nmagnitude, we prove that the network approximates a discretized network with\nsaturated activation functions. Such \"saturated\" networks are known to have a\nreduced capacity compared to the full network family that can be described in\nterms of formal languages and automata. Our results suggest saturation is a new\ncharacterization of an inductive bias implicit in GD of particular interest for\nNLP. We leverage the emergent discrete structure in a saturated transformer to\nanalyze the role of different attention heads, finding that some focus locally\non a small number of positions, while other heads compute global averages,\nallowing counting. We believe understanding the interplay between these two\ncapabilities may shed further light on the structure of computation within\nlarge transformers.",
    "published": "2020-10-19T17:40:38Z",
    "updated": "2023-03-07T23:09:55Z",
    "authors": [
      "William Merrill",
      "Vivek Ramanujan",
      "Yoav Goldberg",
      "Roy Schwartz",
      "Noah Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.00462v2",
    "title": "D-Former: A U-shaped Dilated Transformer for 3D Medical Image\n  Segmentation",
    "summary": "Computer-aided medical image segmentation has been applied widely in\ndiagnosis and treatment to obtain clinically useful information of shapes and\nvolumes of target organs and tissues. In the past several years, convolutional\nneural network (CNN) based methods (e.g., U-Net) have dominated this area, but\nstill suffered from inadequate long-range information capturing. Hence, recent\nwork presented computer vision Transformer variants for medical image\nsegmentation tasks and obtained promising performances. Such Transformers model\nlong-range dependency by computing pair-wise patch relations. However, they\nincur prohibitive computational costs, especially on 3D medical images (e.g.,\nCT and MRI). In this paper, we propose a new method called Dilated Transformer,\nwhich conducts self-attention for pair-wise patch relations captured\nalternately in local and global scopes. Inspired by dilated convolution\nkernels, we conduct the global self-attention in a dilated manner, enlarging\nreceptive fields without increasing the patches involved and thus reducing\ncomputational costs. Based on this design of Dilated Transformer, we construct\na U-shaped encoder-decoder hierarchical architecture called D-Former for 3D\nmedical image segmentation. Experiments on the Synapse and ACDC datasets show\nthat our D-Former model, trained from scratch, outperforms various competitive\nCNN-based or Transformer-based segmentation models at a low computational cost\nwithout time-consuming per-training process.",
    "published": "2022-01-03T03:20:35Z",
    "updated": "2022-01-10T02:57:28Z",
    "authors": [
      "Yixuan Wu",
      "Kuanlun Liao",
      "Jintai Chen",
      "Jinhong Wang",
      "Danny Z. Chen",
      "Honghao Gao",
      "Jian Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.11838v3",
    "title": "Clinical-Longformer and Clinical-BigBird: Transformers for long clinical\n  sequences",
    "summary": "Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nour source code available at\n[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models\navailable for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].",
    "published": "2022-01-27T22:51:58Z",
    "updated": "2022-04-15T05:46:23Z",
    "authors": [
      "Yikuan Li",
      "Ramsey M. Wehbe",
      "Faraz S. Ahmad",
      "Hanyin Wang",
      "Yuan Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.09853v1",
    "title": "DisCoVQA: Temporal Distortion-Content Transformers for Video Quality\n  Assessment",
    "summary": "The temporal relationships between frames and their influences on video\nquality assessment (VQA) are still under-studied in existing works. These\nrelationships lead to two important types of effects for video quality.\nFirstly, some temporal variations (such as shaking, flicker, and abrupt scene\ntransitions) are causing temporal distortions and lead to extra quality\ndegradations, while other variations (e.g. those related to meaningful\nhappenings) do not. Secondly, the human visual system often has different\nattention to frames with different contents, resulting in their different\nimportance to the overall video quality. Based on prominent time-series\nmodeling ability of transformers, we propose a novel and effective\ntransformer-based VQA method to tackle these two issues. To better\ndifferentiate temporal variations and thus capture the temporal distortions, we\ndesign a transformer-based Spatial-Temporal Distortion Extraction (STDE)\nmodule. To tackle with temporal quality attention, we propose the\nencoder-decoder-like temporal content transformer (TCT). We also introduce the\ntemporal sampling on features to reduce the input length for the TCT, so as to\nimprove the learning effectiveness and efficiency of this module. Consisting of\nthe STDE and the TCT, the proposed Temporal Distortion-Content Transformers for\nVideo Quality Assessment (DisCoVQA) reaches state-of-the-art performance on\nseveral VQA benchmarks without any extra pre-training datasets and up to 10%\nbetter generalization ability than existing methods. We also conduct extensive\nablation experiments to prove the effectiveness of each part in our proposed\nmodel, and provide visualizations to prove that the proposed modules achieve\nour intention on modeling these temporal issues. We will publish our codes and\npretrained weights later.",
    "published": "2022-06-20T15:31:27Z",
    "updated": "2022-06-20T15:31:27Z",
    "authors": [
      "Haoning Wu",
      "Chaofeng Chen",
      "Liang Liao",
      "Jingwen Hou",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.12398v1",
    "title": "Few-Shot Learning Meets Transformer: Unified Query-Support Transformers\n  for Few-Shot Classification",
    "summary": "Few-shot classification which aims to recognize unseen classes using very\nlimited samples has attracted more and more attention. Usually, it is\nformulated as a metric learning problem. The core issue of few-shot\nclassification is how to learn (1) consistent representations for images in\nboth support and query sets and (2) effective metric learning for images\nbetween support and query sets. In this paper, we show that the two challenges\ncan be well modeled simultaneously via a unified Query-Support TransFormer\n(QSFormer) model. To be specific,the proposed QSFormer involves global\nquery-support sample Transformer (sampleFormer) branch and local patch\nTransformer (patchFormer) learning branch. sampleFormer aims to capture the\ndependence of samples in support and query sets for image representation. It\nadopts the Encoder, Decoder and Cross-Attention to respectively model the\nSupport, Query (image) representation and Metric learning for few-shot\nclassification task. Also, as a complementary to global learning branch, we\nadopt a local patch Transformer to extract structural representation for each\nimage sample by capturing the long-range dependence of local image patches. In\naddition, a novel Cross-scale Interactive Feature Extractor (CIFE) is proposed\nto extract and fuse multi-scale CNN features as an effective backbone module\nfor the proposed few-shot learning method. All modules are integrated into a\nunified framework and trained in an end-to-end manner. Extensive experiments on\nfour popular datasets demonstrate the effectiveness and superiority of the\nproposed QSFormer.",
    "published": "2022-08-26T01:53:23Z",
    "updated": "2022-08-26T01:53:23Z",
    "authors": [
      "Xixi Wang",
      "Xiao Wang",
      "Bo Jiang",
      "Bin Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.07470v1",
    "title": "X-Former: In-Memory Acceleration of Transformers",
    "summary": "Transformers have achieved great success in a wide variety of natural\nlanguage processing (NLP) tasks due to the attention mechanism, which assigns\nan importance score for every word relative to other words in a sequence.\nHowever, these models are very large, often reaching hundreds of billions of\nparameters, and therefore require a large number of DRAM accesses. Hence,\ntraditional deep neural network (DNN) accelerators such as GPUs and TPUs face\nlimitations in processing Transformers efficiently. In-memory accelerators\nbased on non-volatile memory promise to be an effective solution to this\nchallenge, since they provide high storage density while performing massively\nparallel matrix vector multiplications within memory arrays. However, attention\nscore computations, which are frequently used in Transformers (unlike CNNs and\nRNNs), require matrix vector multiplications (MVM) where both operands change\ndynamically for each input. As a result, conventional NVM-based accelerators\nincur high write latency and write energy when used for Transformers, and\nfurther suffer from the low endurance of most NVM technologies. To address\nthese challenges, we present X-Former, a hybrid in-memory hardware accelerator\nthat consists of both NVM and CMOS processing elements to execute transformer\nworkloads efficiently. To improve the hardware utilization of X-Former, we also\npropose a sequence blocking dataflow, which overlaps the computations of the\ntwo processing elements and reduces execution time. Across several benchmarks,\nwe show that X-Former achieves upto 85x and 7.5x improvements in latency and\nenergy over a NVIDIA GeForce GTX 1060 GPU and upto 10.7x and 4.6x improvements\nin latency and energy over a state-of-the-art in-memory NVM accelerator.",
    "published": "2023-03-13T21:11:54Z",
    "updated": "2023-03-13T21:11:54Z",
    "authors": [
      "Shrihari Sridharan",
      "Jacob R. Stevens",
      "Kaushik Roy",
      "Anand Raghunathan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.12194v2",
    "title": "LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR\n  Perception",
    "summary": "There is a recent trend in the LiDAR perception field towards unifying\nmultiple tasks in a single strong network with improved performance, as opposed\nto using separate networks for each task. In this paper, we introduce a new\nLiDAR multi-task learning paradigm based on the transformer. The proposed\nLiDARFormer utilizes cross-space global contextual feature information and\nexploits cross-task synergy to boost the performance of LiDAR perception tasks\nacross multiple large-scale datasets and benchmarks. Our novel\ntransformer-based framework includes a cross-space transformer module that\nlearns attentive features between the 2D dense Bird's Eye View (BEV) and 3D\nsparse voxel feature maps. Additionally, we propose a transformer decoder for\nthe segmentation task to dynamically adjust the learned features by leveraging\nthe categorical feature representations. Furthermore, we combine the\nsegmentation and detection features in a shared transformer decoder with\ncross-task attention layers to enhance and integrate the object-level and\nclass-level features. LiDARFormer is evaluated on the large-scale nuScenes and\nthe Waymo Open datasets for both 3D detection and semantic segmentation tasks,\nand it outperforms all previously published methods on both tasks. Notably,\nLiDARFormer achieves the state-of-the-art performance of 76.4% L2 mAPH and\n74.3% NDS on the challenging Waymo and nuScenes detection benchmarks for a\nsingle model LiDAR-only method.",
    "published": "2023-03-21T20:52:02Z",
    "updated": "2024-03-02T22:18:12Z",
    "authors": [
      "Zixiang Zhou",
      "Dongqiangzi Ye",
      "Weijia Chen",
      "Yufei Xie",
      "Yu Wang",
      "Panqu Wang",
      "Hassan Foroosh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.11527v2",
    "title": "Memory Transformer",
    "summary": "Transformer-based models have achieved state-of-the-art results in many\nnatural language processing tasks. The self-attention architecture allows\ntransformer to combine information from all elements of a sequence into\ncontext-aware representations. However, information about the context is stored\nmostly in the same element-wise representations. This might limit the\nprocessing of properties related to the sequence as a whole more difficult.\nAdding trainable memory to selectively store local as well as global\nrepresentations of a sequence is a promising direction to improve the\nTransformer model. Memory-augmented neural networks (MANNs) extend traditional\nneural architectures with general-purpose memory for representations. MANNs\nhave demonstrated the capability to learn simple algorithms like Copy or\nReverse and can be successfully trained via backpropagation on diverse tasks\nfrom question answering to language modeling outperforming RNNs and LSTMs of\ncomparable complexity. In this work, we propose and study few extensions of the\nTransformer baseline (1) by adding memory tokens to store non-local\nrepresentations, (2) creating memory bottleneck for the global information, (3)\ncontrolling memory update with dedicated layer. We evaluate these memory\naugmented Transformers and demonstrate that presence of memory positively\ncorrelates with the model performance for machine translation and language\nmodelling tasks. Augmentation of pre-trained masked language model with memory\ntokens shows mixed results for tasks from GLUE benchmark. Visualization of\nattention patterns over the memory suggest that it improves the model's ability\nto process a global context.",
    "published": "2020-06-20T09:06:27Z",
    "updated": "2021-02-16T08:06:47Z",
    "authors": [
      "Mikhail S. Burtsev",
      "Yuri Kuratov",
      "Anton Peganov",
      "Grigory V. Sapunov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2008.04659v2",
    "title": "S-vectors and TESA: Speaker Embeddings and a Speaker Authenticator Based\n  on Transformer Encoder",
    "summary": "One of the most popular speaker embeddings is x-vectors, which are obtained\nfrom an architecture that gradually builds a larger temporal context with\nlayers. In this paper, we propose to derive speaker embeddings from\nTransformer's encoder trained for speaker classification. Self-attention, on\nwhich Transformer's encoder is built, attends to all the features over the\nentire utterance and might be more suitable in capturing the speaker\ncharacteristics in an utterance. We refer to the speaker embeddings obtained\nfrom the proposed speaker classification model as s-vectors to emphasize that\nthey are obtained from an architecture that heavily relies on self-attention.\nThrough experiments, we demonstrate that s-vectors perform better than\nx-vectors. In addition to the s-vectors, we also propose a new architecture\nbased on Transformer's encoder for speaker verification as a replacement for\nspeaker verification based on conventional probabilistic linear discriminant\nanalysis (PLDA). This architecture is inspired by the next sentence prediction\ntask of bidirectional encoder representations from Transformers (BERT), and we\nfeed the s-vectors of two utterances to verify whether they belong to the same\nspeaker. We name this architecture the Transformer encoder speaker\nauthenticator (TESA). Our experiments show that the performance of s-vectors\nwith TESA is better than s-vectors with conventional PLDA-based speaker\nverification.",
    "published": "2020-08-11T12:23:21Z",
    "updated": "2021-12-12T09:08:01Z",
    "authors": [
      "N J Metilda Sagaya Mary",
      "S Umesh",
      "Sandesh V Katta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.06637v1",
    "title": "Decoupled Spatial-Temporal Transformer for Video Inpainting",
    "summary": "Video inpainting aims to fill the given spatiotemporal holes with realistic\nappearance but is still a challenging task even with prosperous deep learning\napproaches. Recent works introduce the promising Transformer architecture into\ndeep video inpainting and achieve better performance. However, it still suffers\nfrom synthesizing blurry texture as well as huge computational cost. Towards\nthis end, we propose a novel Decoupled Spatial-Temporal Transformer (DSTT) for\nimproving video inpainting with exceptional efficiency. Our proposed DSTT\ndisentangles the task of learning spatial-temporal attention into 2 sub-tasks:\none is for attending temporal object movements on different frames at same\nspatial locations, which is achieved by temporally-decoupled Transformer block,\nand the other is for attending similar background textures on same frame of all\nspatial positions, which is achieved by spatially-decoupled Transformer block.\nThe interweaving stack of such two blocks makes our proposed model attend\nbackground textures and moving objects more precisely, and thus the attended\nplausible and temporally-coherent appearance can be propagated to fill the\nholes. In addition, a hierarchical encoder is adopted before the stack of\nTransformer blocks, for learning robust and hierarchical features that maintain\nmulti-level local spatial structure, resulting in the more representative token\nvectors. Seamless combination of these two novel designs forms a better\nspatial-temporal attention scheme and our proposed model achieves better\nperformance than state-of-the-art video inpainting approaches with significant\nboosted efficiency.",
    "published": "2021-04-14T05:47:46Z",
    "updated": "2021-04-14T05:47:46Z",
    "authors": [
      "Rui Liu",
      "Hanming Deng",
      "Yangyi Huang",
      "Xiaoyu Shi",
      "Lewei Lu",
      "Wenxiu Sun",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2104.09340v2",
    "title": "Code Structure Guided Transformer for Source Code Summarization",
    "summary": "Code summaries help developers comprehend programs and reduce their time to\ninfer the program functionalities during software maintenance. Recent efforts\nresort to deep learning techniques such as sequence-to-sequence models for\ngenerating accurate code summaries, among which Transformer-based approaches\nhave achieved promising performance. However, effectively integrating the code\nstructure information into the Transformer is under-explored in this task\ndomain. In this paper, we propose a novel approach named SG-Trans to\nincorporate code structural properties into Transformer. Specifically, we\ninject the local symbolic information (e.g., code tokens and statements) and\nglobal syntactic structure (e.g., data flow graph) into the self-attention\nmodule of Transformer as inductive bias. To further capture the hierarchical\ncharacteristics of code, the local information and global structure are\ndesigned to distribute in the attention heads of lower layers and high layers\nof Transformer. Extensive evaluation shows the superior performance of SG-Trans\nover the state-of-the-art approaches. Compared with the best-performing\nbaseline, SG-Trans still improves 1.4% and 2.0% in terms of METEOR score, a\nmetric widely used for measuring generation quality, respectively on two\nbenchmark datasets.",
    "published": "2021-04-19T14:26:56Z",
    "updated": "2022-07-22T14:20:02Z",
    "authors": [
      "Shuzheng Gao",
      "Cuiyun Gao",
      "Yulan He",
      "Jichuan Zeng",
      "Lun Yiu Nie",
      "Xin Xia",
      "Michael R. Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.04888v1",
    "title": "Hierarchical RNNs-Based Transformers MADDPG for Mixed\n  Cooperative-Competitive Environments",
    "summary": "At present, attention mechanism has been widely applied to the fields of deep\nlearning models. Structural models that based on attention mechanism can not\nonly record the relationships between features position, but also can measure\nthe importance of different features based on their weights. By establishing\ndynamically weighted parameters for choosing relevant and irrelevant features,\nthe key information can be strengthened, and the irrelevant information can be\nweakened. Therefore, the efficiency of deep learning algorithms can be\nsignificantly elevated and improved. Although transformers have been performed\nvery well in many fields including reinforcement learning, there are still many\nproblems and applications can be solved and made with transformers within this\narea. MARL (known as Multi-Agent Reinforcement Learning) can be recognized as a\nset of independent agents trying to adapt and learn through their way to reach\nthe goal. In order to emphasize the relationship between each MDP decision in a\ncertain time period, we applied the hierarchical coding method and validated\nthe effectiveness of this method. This paper proposed a hierarchical\ntransformers MADDPG based on RNN which we call it Hierarchical RNNs-Based\nTransformers MADDPG(HRTMADDPG). It consists of a lower level encoder based on\nRNNs that encodes multiple step sizes in each time sequence, and it also\nconsists of an upper sequence level encoder based on transformer for learning\nthe correlations between multiple sequences so that we can capture the causal\nrelationship between sub-time sequences and make HRTMADDPG more efficient.",
    "published": "2021-05-11T09:22:52Z",
    "updated": "2021-05-11T09:22:52Z",
    "authors": [
      "Xiaolong Wei",
      "LiFang Yang",
      "Xianglin Huang",
      "Gang Cao",
      "Tao Zhulin",
      "Zhengyang Du",
      "Jing An"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.03348v4",
    "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
    "summary": "Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.",
    "published": "2021-06-07T05:31:06Z",
    "updated": "2021-12-24T02:57:08Z",
    "authors": [
      "Yufei Xu",
      "Qiming Zhang",
      "Jing Zhang",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.03864v1",
    "title": "Boundary-aware Transformers for Skin Lesion Segmentation",
    "summary": "Skin lesion segmentation from dermoscopy images is of great importance for\nimproving the quantitative analysis of skin cancer. However, the automatic\nsegmentation of melanoma is a very challenging task owing to the large\nvariation of melanoma and ambiguous boundaries of lesion areas. While\nconvolutional neutral networks (CNNs) have achieved remarkable progress in this\ntask, most of existing solutions are still incapable of effectively capturing\nglobal dependencies to counteract the inductive bias caused by limited\nreceptive fields. Recently, transformers have been proposed as a promising tool\nfor global context modeling by employing a powerful global attention mechanism,\nbut one of their main shortcomings when applied to segmentation tasks is that\nthey cannot effectively extract sufficient local details to tackle ambiguous\nboundaries. We propose a novel boundary-aware transformer (BAT) to\ncomprehensively address the challenges of automatic skin lesion segmentation.\nSpecifically, we integrate a new boundary-wise attention gate (BAG) into\ntransformers to enable the whole network to not only effectively model global\nlong-range dependencies via transformers but also, simultaneously, capture more\nlocal details by making full use of boundary-wise prior knowledge.\nParticularly, the auxiliary supervision of BAG is capable of assisting\ntransformers to learn position embedding as it provides much spatial\ninformation. We conducted extensive experiments to evaluate the proposed BAT\nand experiments corroborate its effectiveness, consistently outperforming\nstate-of-the-art methods in two famous datasets.",
    "published": "2021-10-08T02:43:34Z",
    "updated": "2021-10-08T02:43:34Z",
    "authors": [
      "Jiacheng Wang",
      "Lan Wei",
      "Liansheng Wang",
      "Qichao Zhou",
      "Lei Zhu",
      "Jing Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.13156v1",
    "title": "Global Interaction Modelling in Vision Transformer via Super Tokens",
    "summary": "With the popularity of Transformer architectures in computer vision, the\nresearch focus has shifted towards developing computationally efficient\ndesigns. Window-based local attention is one of the major techniques being\nadopted in recent works. These methods begin with very small patch size and\nsmall embedding dimensions and then perform strided convolution (patch merging)\nin order to reduce the feature map size and increase embedding dimensions,\nhence, forming a pyramidal Convolutional Neural Network (CNN) like design. In\nthis work, we investigate local and global information modelling in\ntransformers by presenting a novel isotropic architecture that adopts local\nwindows and special tokens, called Super tokens, for self-attention.\nSpecifically, a single Super token is assigned to each image window which\ncaptures the rich local details for that window. These tokens are then employed\nfor cross-window communication and global representation learning. Hence, most\nof the learning is independent of the image patches $(N)$ in the higher layers,\nand the class embedding is learned solely based on the Super tokens $(N/M^2)$\nwhere $M^2$ is the window size. In standard image classification on\nImagenet-1K, the proposed Super tokens based transformer (STT-S25) achieves\n83.5\\% accuracy which is equivalent to Swin transformer (Swin-B) with circa\nhalf the number of parameters (49M) and double the inference time throughput.\nThe proposed Super token transformer offers a lightweight and promising\nbackbone for visual recognition tasks.",
    "published": "2021-11-25T16:22:57Z",
    "updated": "2021-11-25T16:22:57Z",
    "authors": [
      "Ammarah Farooq",
      "Muhammad Awais",
      "Sara Ahmed",
      "Josef Kittler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.13824v4",
    "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision\n  Transformer",
    "summary": "Network quantization significantly reduces model inference complexity and has\nbeen widely used in real-world deployments. However, most existing quantization\nmethods have been developed mainly on Convolutional Neural Networks (CNNs), and\nsuffer severe degradation when applied to fully quantized vision transformers.\nIn this work, we demonstrate that many of these difficulties arise because of\nserious inter-channel variation in LayerNorm inputs, and present, Power-of-Two\nFactor (PTF), a systematic method to reduce the performance degradation and\ninference complexity of fully quantized vision transformers. In addition,\nobserving an extreme non-uniform distribution in attention maps, we propose\nLog-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit\nquantization and the BitShift operator. Comprehensive experiments on various\ntransformer-based architectures and benchmarks show that our Fully Quantized\nVision Transformer (FQ-ViT) outperforms previous works while even using lower\nbit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with\nViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our\nknowledge, we are the first to achieve lossless accuracy degradation (~1%) on\nfully quantized vision transformers. The code is available at\nhttps://github.com/megvii-research/FQ-ViT.",
    "published": "2021-11-27T06:20:53Z",
    "updated": "2023-02-17T13:17:52Z",
    "authors": [
      "Yang Lin",
      "Tianyu Zhang",
      "Peiqin Sun",
      "Zheng Li",
      "Shuchang Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.15299v1",
    "title": "CSformer: Bridging Convolution and Transformer for Compressive Sensing",
    "summary": "Convolution neural networks (CNNs) have succeeded in compressive image\nsensing. However, due to the inductive bias of locality and weight sharing, the\nconvolution operations demonstrate the intrinsic limitations in modeling the\nlong-range dependency. Transformer, designed initially as a\nsequence-to-sequence model, excels at capturing global contexts due to the\nself-attention-based architectures even though it may be equipped with limited\nlocalization abilities. This paper proposes CSformer, a hybrid framework that\nintegrates the advantages of leveraging both detailed spatial information from\nCNN and the global context provided by transformer for enhanced representation\nlearning. The proposed approach is an end-to-end compressive image sensing\nmethod, composed of adaptive sampling and recovery. In the sampling module,\nimages are measured block-by-block by the learned sampling matrix. In the\nreconstruction stage, the measurement is projected into dual stems. One is the\nCNN stem for modeling the neighborhood relationships by convolution, and the\nother is the transformer stem for adopting global self-attention mechanism. The\ndual branches structure is concurrent, and the local features and global\nrepresentations are fused under different resolutions to maximize the\ncomplementary of features. Furthermore, we explore a progressive strategy and\nwindow-based transformer block to reduce the parameter and computational\ncomplexity. The experimental results demonstrate the effectiveness of the\ndedicated transformer-based architecture for compressive sensing, which\nachieves superior performance compared to state-of-the-art methods on different\ndatasets.",
    "published": "2021-12-31T04:37:11Z",
    "updated": "2021-12-31T04:37:11Z",
    "authors": [
      "Dongjie Ye",
      "Zhangkai Ni",
      "Hanli Wang",
      "Jian Zhang",
      "Shiqi Wang",
      "Sam Kwong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.07852v3",
    "title": "Block-Recurrent Transformers",
    "summary": "We introduce the Block-Recurrent Transformer, which applies a transformer\nlayer in a recurrent fashion along a sequence, and has linear complexity with\nrespect to sequence length. Our recurrent cell operates on blocks of tokens\nrather than single tokens during training, and leverages parallel computation\nwithin a block in order to make efficient use of accelerator hardware. The cell\nitself is strikingly simple. It is merely a transformer layer: it uses\nself-attention and cross-attention to efficiently compute a recurrent function\nover a large set of state vectors and tokens. Our design was inspired in part\nby LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM\ncell up by several orders of magnitude. Our implementation of recurrence has\nthe same cost in both computation time and parameter count as a conventional\ntransformer layer, but offers dramatically improved perplexity in language\nmodeling tasks over very long sequences. Our model out-performs a long-range\nTransformer XL baseline by a wide margin, while running twice as fast. We\ndemonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source\ncode. Our code has been released as open source.",
    "published": "2022-03-11T23:44:33Z",
    "updated": "2022-11-02T00:35:56Z",
    "authors": [
      "DeLesley Hutchins",
      "Imanol Schlag",
      "Yuhuai Wu",
      "Ethan Dyer",
      "Behnam Neyshabur"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.10663v2",
    "title": "Transformer based Generative Adversarial Network for Liver Segmentation",
    "summary": "Automated liver segmentation from radiology scans (CT, MRI) can improve\nsurgery and therapy planning and follow-up assessment in addition to\nconventional use for diagnosis and prognosis. Although convolutional neural\nnetworks (CNNs) have become the standard image segmentation tasks, more\nrecently this has started to change towards Transformers based architectures\nbecause Transformers are taking advantage of capturing long range dependence\nmodeling capability in signals, so called attention mechanism. In this study,\nwe propose a new segmentation approach using a hybrid approach combining the\nTransformer(s) with the Generative Adversarial Network (GAN) approach. The\npremise behind this choice is that the self-attention mechanism of the\nTransformers allows the network to aggregate the high dimensional feature and\nprovide global information modeling. This mechanism provides better\nsegmentation performance compared with traditional methods. Furthermore, we\nencode this generator into the GAN based architecture so that the discriminator\nnetwork in the GAN can classify the credibility of the generated segmentation\nmasks compared with the real masks coming from human (expert) annotations. This\nallows us to extract the high dimensional topology information in the mask for\nbiomedical image segmentation and provide more reliable segmentation results.\nOur model achieved a high dice coefficient of 0.9433, recall of 0.9515, and\nprecision of 0.9376 and outperformed other Transformer based approaches.",
    "published": "2022-05-21T19:55:43Z",
    "updated": "2022-05-28T15:22:14Z",
    "authors": [
      "Ugur Demir",
      "Zheyuan Zhang",
      "Bin Wang",
      "Matthew Antalek",
      "Elif Keles",
      "Debesh Jha",
      "Amir Borhani",
      "Daniela Ladner",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.15836v1",
    "title": "Surface Analysis with Vision Transformers",
    "summary": "The extension of convolutional neural networks (CNNs) to non-Euclidean\ngeometries has led to multiple frameworks for studying manifolds. Many of those\nmethods have shown design limitations resulting in poor modelling of long-range\nassociations, as the generalisation of convolutions to irregular surfaces is\nnon-trivial. Recent state-of-the-art performance of Vision Transformers (ViTs)\ndemonstrates that a general-purpose architecture, which implements\nself-attention, could replace the local feature learning operations of CNNs.\nMotivated by the success of attention-modelling in computer vision, we extend\nViTs to surfaces by reformulating the task of surface learning as a\nsequence-to-sequence problem and propose a patching mechanism for surface\nmeshes. We validate the performance of the proposed Surface Vision Transformer\n(SiT) on two brain age prediction tasks in the developing Human Connectome\nProject (dHCP) dataset and investigate the impact of pre-training on model\nperformance. Experiments show that the SiT outperforms many surface CNNs, while\nindicating some evidence of general transformation invariance. Code available\nat https://github.com/metrics-lab/surface-vision-transformers",
    "published": "2022-05-31T14:41:01Z",
    "updated": "2022-05-31T14:41:01Z",
    "authors": [
      "Simon Dahan",
      "Logan Z. J. Williams",
      "Abdulah Fawaz",
      "Daniel Rueckert",
      "Emma C. Robinson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.05358v1",
    "title": "eX-ViT: A Novel eXplainable Vision Transformer for Weakly Supervised\n  Semantic Segmentation",
    "summary": "Recently vision transformer models have become prominent models for a range\nof vision tasks. These models, however, are usually opaque with weak feature\ninterpretability. Moreover, there is no method currently built for an\nintrinsically interpretable transformer, which is able to explain its reasoning\nprocess and provide a faithful explanation. To close these crucial gaps, we\npropose a novel vision transformer dubbed the eXplainable Vision Transformer\n(eX-ViT), an intrinsically interpretable transformer model that is able to\njointly discover robust interpretable features and perform the prediction.\nSpecifically, eX-ViT is composed of the Explainable Multi-Head Attention\n(E-MHA) module, the Attribute-guided Explainer (AttE) module and the\nself-supervised attribute-guided loss. The E-MHA tailors explainable attention\nweights that are able to learn semantically interpretable representations from\nlocal patches in terms of model decisions with noise robustness. Meanwhile,\nAttE is proposed to encode discriminative attribute features for the target\nobject through diverse attribute discovery, which constitutes faithful evidence\nfor the model's predictions. In addition, a self-supervised attribute-guided\nloss is developed for our eX-ViT, which aims at learning enhanced\nrepresentations through the attribute discriminability mechanism and attribute\ndiversity mechanism, to localize diverse and discriminative attributes and\ngenerate more robust explanations. As a result, we can uncover faithful and\nrobust interpretations with diverse attributes through the proposed eX-ViT.",
    "published": "2022-07-12T07:43:29Z",
    "updated": "2022-07-12T07:43:29Z",
    "authors": [
      "Lu Yu",
      "Wei Xiang",
      "Juan Fang",
      "Yi-Ping Phoebe Chen",
      "Lianhua Chi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.06881v2",
    "title": "Recurrent Memory Transformer",
    "summary": "Transformer-based models show their effectiveness across multiple domains and\ntasks. The self-attention allows to combine information from all sequence\nelements into context-aware representations. However, global and local\ninformation has to be stored mostly in the same element-wise representations.\nMoreover, the length of an input sequence is limited by quadratic computational\ncomplexity of self-attention.\n  In this work, we propose and study a memory-augmented segment-level recurrent\nTransformer (RMT). Memory allows to store and process local and global\ninformation as well as to pass information between segments of the long\nsequence with the help of recurrence.\n  We implement a memory mechanism with no changes to Transformer model by\nadding special memory tokens to the input or output sequence. Then the model is\ntrained to control both memory operations and sequence representations\nprocessing.\n  Results of experiments show that RMT performs on par with the Transformer-XL\non language modeling for smaller memory sizes and outperforms it for tasks that\nrequire longer sequence processing. We show that adding memory tokens to Tr-XL\nis able to improve its performance. This makes Recurrent Memory Transformer a\npromising architecture for applications that require learning of long-term\ndependencies and general purpose in memory processing, such as algorithmic\ntasks and reasoning.",
    "published": "2022-07-14T13:00:22Z",
    "updated": "2022-12-08T12:34:16Z",
    "authors": [
      "Aydar Bulatov",
      "Yuri Kuratov",
      "Mikhail S. Burtsev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.08194v1",
    "title": "PPT: token-Pruned Pose Transformer for monocular and multi-view human\n  pose estimation",
    "summary": "Recently, the vision transformer and its variants have played an increasingly\nimportant role in both monocular and multi-view human pose estimation.\nConsidering image patches as tokens, transformers can model the global\ndependencies within the entire image or across images from other views.\nHowever, global attention is computationally expensive. As a consequence, it is\ndifficult to scale up these transformer-based methods to high-resolution\nfeatures and many views.\n  In this paper, we propose the token-Pruned Pose Transformer (PPT) for 2D\nhuman pose estimation, which can locate a rough human mask and performs\nself-attention only within selected tokens. Furthermore, we extend our PPT to\nmulti-view human pose estimation. Built upon PPT, we propose a new cross-view\nfusion strategy, called human area fusion, which considers all human foreground\npixels as corresponding candidates. Experimental results on COCO and MPII\ndemonstrate that our PPT can match the accuracy of previous pose transformer\nmethods while reducing the computation. Moreover, experiments on Human 3.6M and\nSki-Pose demonstrate that our Multi-view PPT can efficiently fuse cues from\nmultiple views and achieve new state-of-the-art results.",
    "published": "2022-09-16T23:22:47Z",
    "updated": "2022-09-16T23:22:47Z",
    "authors": [
      "Haoyu Ma",
      "Zhe Wang",
      "Yifei Chen",
      "Deying Kong",
      "Liangjian Chen",
      "Xingwei Liu",
      "Xiangyi Yan",
      "Hao Tang",
      "Xiaohui Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.11963v1",
    "title": "A Deep Investigation of RNN and Self-attention for the\n  Cyrillic-Traditional Mongolian Bidirectional Conversion",
    "summary": "Cyrillic and Traditional Mongolian are the two main members of the Mongolian\nwriting system. The Cyrillic-Traditional Mongolian Bidirectional Conversion\n(CTMBC) task includes two conversion processes, including Cyrillic Mongolian to\nTraditional Mongolian (C2T) and Traditional Mongolian to Cyrillic Mongolian\nconversions (T2C). Previous researchers adopted the traditional joint sequence\nmodel, since the CTMBC task is a natural Sequence-to-Sequence (Seq2Seq)\nmodeling problem. Recent studies have shown that Recurrent Neural Network (RNN)\nand Self-attention (or Transformer) based encoder-decoder models have shown\nsignificant improvement in machine translation tasks between some major\nlanguages, such as Mandarin, English, French, etc. However, an open problem\nremains as to whether the CTMBC quality can be improved by utilizing the RNN\nand Transformer models. To answer this question, this paper investigates the\nutility of these two powerful techniques for CTMBC task combined with\nagglutinative characteristics of Mongolian language. We build the\nencoder-decoder based CTMBC model based on RNN and Transformer respectively and\ncompare the different network configurations deeply. The experimental results\nshow that both RNN and Transformer models outperform the traditional joint\nsequence model, where the Transformer achieves the best performance. Compared\nwith the joint sequence baseline, the word error rate (WER) of the Transformer\nfor C2T and T2C decreased by 5.72\\% and 5.06\\% respectively.",
    "published": "2022-09-24T08:55:22Z",
    "updated": "2022-09-24T08:55:22Z",
    "authors": [
      "Muhan Na",
      "Rui Liu",
      " Feilong",
      "Guanglai Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.08739v3",
    "title": "FlatFormer: Flattened Window Attention for Efficient Point Cloud\n  Transformer",
    "summary": "Transformer, as an alternative to CNN, has been proven effective in many\nmodalities (e.g., texts and images). For 3D point cloud transformers, existing\nefforts focus primarily on pushing their accuracy to the state-of-the-art\nlevel. However, their latency lags behind sparse convolution-based models (3x\nslower), hindering their usage in resource-constrained, latency-sensitive\napplications (such as autonomous driving). This inefficiency comes from point\nclouds' sparse and irregular nature, whereas transformers are designed for\ndense, regular workloads. This paper presents FlatFormer to close this latency\ngap by trading spatial proximity for better computational regularity. We first\nflatten the point cloud with window-based sorting and partition points into\ngroups of equal sizes rather than windows of equal shapes. This effectively\navoids expensive structuring and padding overheads. We then apply\nself-attention within groups to extract local features, alternate sorting axis\nto gather features from different directions, and shift windows to exchange\nfeatures across groups. FlatFormer delivers state-of-the-art accuracy on Waymo\nOpen Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup\nover (sparse convolutional) CenterPoint. This is the first point cloud\ntransformer that achieves real-time performance on edge GPUs and is faster than\nsparse convolutional methods while achieving on-par or even superior accuracy\non large-scale benchmarks.",
    "published": "2023-01-20T18:59:57Z",
    "updated": "2023-07-14T18:57:30Z",
    "authors": [
      "Zhijian Liu",
      "Xinyu Yang",
      "Haotian Tang",
      "Shang Yang",
      "Song Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.10847v1",
    "title": "Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale\n  Feature Fusion Approach",
    "summary": "While CNN-based methods have been the cornerstone of medical image\nsegmentation due to their promising performance and robustness, they suffer\nfrom limitations in capturing long-range dependencies. Transformer-based\napproaches are currently prevailing since they enlarge the reception field to\nmodel global contextual correlation. To further extract rich representations,\nsome extensions of the U-Net employ multi-scale feature extraction and fusion\nmodules and obtain improved performance. Inspired by this idea, we propose\nTransCeption for medical image segmentation, a pure transformer-based U-shape\nnetwork featured by incorporating the inception-like module into the encoder\nand adopting a contextual bridge for better feature fusion. The design proposed\nin this work is based on three core principles: (1) The patch merging module in\nthe encoder is redesigned with ResInception Patch Merging (RIPM). Multi-branch\ntransformer (MB transformer) adopts the same number of branches as the outputs\nof RIPM. Combining the two modules enables the model to capture a multi-scale\nrepresentation within a single stage. (2) We construct an Intra-stage Feature\nFusion (IFF) module following the MB transformer to enhance the aggregation of\nfeature maps from all the branches and particularly focus on the interaction\nbetween the different channels of all the scales. (3) In contrast to a bridge\nthat only contains token-wise self-attention, we propose a Dual Transformer\nBridge that also includes channel-wise self-attention to exploit correlations\nbetween scales at different stages from a dual perspective. Extensive\nexperiments on multi-organ and skin lesion segmentation tasks present the\nsuperior performance of TransCeption compared to previous work. The code is\npublicly available at \\url{https://github.com/mindflow-institue/TransCeption}.",
    "published": "2023-01-25T22:09:07Z",
    "updated": "2023-01-25T22:09:07Z",
    "authors": [
      "Reza Azad",
      "Yiwei Jia",
      "Ehsan Khodapanah Aghdam",
      "Julien Cohen-Adad",
      "Dorit Merhof"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.01925v2",
    "title": "Learning a Fourier Transform for Linear Relative Positional Encodings in\n  Transformers",
    "summary": "We propose a new class of linear Transformers called\nFourierLearner-Transformers (FLTs), which incorporate a wide range of relative\npositional encoding mechanisms (RPEs). These include regular RPE techniques\napplied for sequential data, as well as novel RPEs operating on geometric data\nembedded in higher-dimensional Euclidean spaces. FLTs construct the optimal RPE\nmechanism implicitly by learning its spectral representation. As opposed to\nother architectures combining efficient low-rank linear attention with RPEs,\nFLTs remain practical in terms of their memory usage and do not require\nadditional assumptions about the structure of the RPE mask. Besides, FLTs allow\nfor applying certain structural inductive bias techniques to specify masking\nstrategies, e.g. they provide a way to learn the so-called local RPEs\nintroduced in this paper and give accuracy gains as compared with several other\nlinear Transformers for language modeling. We also thoroughly test FLTs on\nother data modalities and tasks, such as image classification, 3D molecular\nmodeling, and learnable optimizers. To the best of our knowledge, for 3D\nmolecular data, FLTs are the first Transformer architectures providing linear\nattention and incorporating RPE masking.",
    "published": "2023-02-03T18:57:17Z",
    "updated": "2024-04-03T21:24:50Z",
    "authors": [
      "Krzysztof Marcin Choromanski",
      "Shanda Li",
      "Valerii Likhosherstov",
      "Kumar Avinava Dubey",
      "Shengjie Luo",
      "Di He",
      "Yiming Yang",
      "Tamas Sarlos",
      "Thomas Weingarten",
      "Adrian Weller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.03548v1",
    "title": "PhysFormer++: Facial Video-based Physiological Measurement with SlowFast\n  Temporal Difference Transformer",
    "summary": "Remote photoplethysmography (rPPG), which aims at measuring heart activities\nand physiological signals from facial video without any contact, has great\npotential in many applications (e.g., remote healthcare and affective\ncomputing). Recent deep learning approaches focus on mining subtle rPPG clues\nusing convolutional neural networks with limited spatio-temporal receptive\nfields, which neglect the long-range spatio-temporal perception and interaction\nfor rPPG modeling. In this paper, we propose two end-to-end video transformer\nbased architectures, namely PhysFormer and PhysFormer++, to adaptively\naggregate both local and global spatio-temporal features for rPPG\nrepresentation enhancement. As key modules in PhysFormer, the temporal\ndifference transformers first enhance the quasi-periodic rPPG features with\ntemporal difference guided global attention, and then refine the local\nspatio-temporal representation against interference. To better exploit the\ntemporal contextual and periodic rPPG clues, we also extend the PhysFormer to\nthe two-pathway SlowFast based PhysFormer++ with temporal difference periodic\nand cross-attention transformers. Furthermore, we propose the label\ndistribution learning and a curriculum learning inspired dynamic constraint in\nfrequency domain, which provide elaborate supervisions for PhysFormer and\nPhysFormer++ and alleviate overfitting. Comprehensive experiments are performed\non four benchmark datasets to show our superior performance on both intra- and\ncross-dataset testings. Unlike most transformer networks needed pretraining\nfrom large-scale datasets, the proposed PhysFormer family can be easily trained\nfrom scratch on rPPG datasets, which makes it promising as a novel transformer\nbaseline for the rPPG community.",
    "published": "2023-02-07T15:56:03Z",
    "updated": "2023-02-07T15:56:03Z",
    "authors": [
      "Zitong Yu",
      "Yuming Shen",
      "Jingang Shi",
      "Hengshuang Zhao",
      "Yawen Cui",
      "Jiehua Zhang",
      "Philip Torr",
      "Guoying Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.04416v4",
    "title": "High Dynamic Range Imaging with Context-aware Transformer",
    "summary": "Avoiding the introduction of ghosts when synthesising LDR images as high\ndynamic range (HDR) images is a challenging task. Convolutional neural networks\n(CNNs) are effective for HDR ghost removal in general, but are challenging to\ndeal with the LDR images if there are large movements or\noversaturation/undersaturation. Existing dual-branch methods combining CNN and\nTransformer omit part of the information from non-reference images, while the\nfeatures extracted by the CNN-based branch are bound to the kernel size with\nsmall receptive field, which are detrimental to the deblurring and the recovery\nof oversaturated/undersaturated regions. In this paper, we propose a novel\nhierarchical dual Transformer method for ghost-free HDR (HDT-HDR) images\ngeneration, which extracts global features and local features simultaneously.\nFirst, we use a CNN-based head with spatial attention mechanisms to extract\nfeatures from all the LDR images. Second, the LDR features are delivered to the\nHierarchical Dual Transformer (HDT). In each Dual Transformer (DT), the global\nfeatures are extracted by the window-based Transformer, while the local details\nare extracted using the channel attention mechanism with deformable CNNs.\nFinally, the ghost free HDR image is obtained by dimensional mapping on the HDT\noutput. Abundant experiments demonstrate that our HDT-HDR achieves the\nstate-of-the-art performance among existing HDR ghost removal methods.",
    "published": "2023-04-10T06:56:01Z",
    "updated": "2023-04-21T06:56:13Z",
    "authors": [
      "Fangfang Zhou",
      "Dan Zhang",
      "Zhenming Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.04614v1",
    "title": "HST-MRF: Heterogeneous Swin Transformer with Multi-Receptive Field for\n  Medical Image Segmentation",
    "summary": "The Transformer has been successfully used in medical image segmentation due\nto its excellent long-range modeling capabilities. However, patch segmentation\nis necessary when building a Transformer class model. This process may disrupt\nthe tissue structure in medical images, resulting in the loss of relevant\ninformation. In this study, we proposed a Heterogeneous Swin Transformer with\nMulti-Receptive Field (HST-MRF) model based on U-shaped networks for medical\nimage segmentation. The main purpose is to solve the problem of loss of\nstructural information caused by patch segmentation using transformer by fusing\npatch information under different receptive fields. The heterogeneous Swin\nTransformer (HST) is the core module, which achieves the interaction of\nmulti-receptive field patch information through heterogeneous attention and\npasses it to the next stage for progressive learning. We also designed a\ntwo-stage fusion module, multimodal bilinear pooling (MBP), to assist HST in\nfurther fusing multi-receptive field information and combining low-level and\nhigh-level semantic information for accurate localization of lesion regions. In\naddition, we developed adaptive patch embedding (APE) and soft channel\nattention (SCA) modules to retain more valuable information when acquiring\npatch embedding and filtering channel features, respectively, thereby improving\nmodel segmentation quality. We evaluated HST-MRF on multiple datasets for polyp\nand skin lesion segmentation tasks. Experimental results show that our proposed\nmethod outperforms state-of-the-art models and can achieve superior\nperformance. Furthermore, we verified the effectiveness of each module and the\nbenefits of multi-receptive field segmentation in reducing the loss of\nstructural information through ablation experiments.",
    "published": "2023-04-10T14:30:03Z",
    "updated": "2023-04-10T14:30:03Z",
    "authors": [
      "Xiaofei Huang",
      "Hongfang Gong",
      "Jin Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.13030v1",
    "title": "CompletionFormer: Depth Completion with Convolutions and Vision\n  Transformers",
    "summary": "Given sparse depths and the corresponding RGB images, depth completion aims\nat spatially propagating the sparse measurements throughout the whole image to\nget a dense depth prediction. Despite the tremendous progress of\ndeep-learning-based depth completion methods, the locality of the convolutional\nlayer or graph model makes it hard for the network to model the long-range\nrelationship between pixels. While recent fully Transformer-based architecture\nhas reported encouraging results with the global receptive field, the\nperformance and efficiency gaps to the well-developed CNN models still exist\nbecause of its deteriorative local feature details. This paper proposes a Joint\nConvolutional Attention and Transformer block (JCAT), which deeply couples the\nconvolutional attention layer and Vision Transformer into one block, as the\nbasic unit to construct our depth completion model in a pyramidal structure.\nThis hybrid architecture naturally benefits both the local connectivity of\nconvolutions and the global context of the Transformer in one single model. As\na result, our CompletionFormer outperforms state-of-the-art CNNs-based methods\non the outdoor KITTI Depth Completion benchmark and indoor NYUv2 dataset,\nachieving significantly higher efficiency (nearly 1/3 FLOPs) compared to pure\nTransformer-based methods. Code is available at\n\\url{https://github.com/youmi-zym/CompletionFormer}.",
    "published": "2023-04-25T17:59:47Z",
    "updated": "2023-04-25T17:59:47Z",
    "authors": [
      "Zhang Youmin",
      "Guo Xianda",
      "Poggi Matteo",
      "Zhu Zheng",
      "Huang Guan",
      "Mattoccia Stefano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.05177v1",
    "title": "Hybrid Transformer and CNN Attention Network for Stereo Image\n  Super-resolution",
    "summary": "Multi-stage strategies are frequently employed in image restoration tasks.\nWhile transformer-based methods have exhibited high efficiency in single-image\nsuper-resolution tasks, they have not yet shown significant advantages over\nCNN-based methods in stereo super-resolution tasks. This can be attributed to\ntwo key factors: first, current single-image super-resolution transformers are\nunable to leverage the complementary stereo information during the process;\nsecond, the performance of transformers is typically reliant on sufficient\ndata, which is absent in common stereo-image super-resolution algorithms. To\naddress these issues, we propose a Hybrid Transformer and CNN Attention Network\n(HTCAN), which utilizes a transformer-based network for single-image\nenhancement and a CNN-based network for stereo information fusion. Furthermore,\nwe employ a multi-patch training strategy and larger window sizes to activate\nmore input pixels for super-resolution. We also revisit other advanced\ntechniques, such as data augmentation, data ensemble, and model ensemble to\nreduce overfitting and data bias. Finally, our approach achieved a score of\n23.90dB and emerged as the winner in Track 1 of the NTIRE 2023 Stereo Image\nSuper-Resolution Challenge.",
    "published": "2023-05-09T05:19:16Z",
    "updated": "2023-05-09T05:19:16Z",
    "authors": [
      "Ming Cheng",
      "Haoyu Ma",
      "Qiufang Ma",
      "Xiaopeng Sun",
      "Weiqi Li",
      "Zhenyu Zhang",
      "Xuhan Sheng",
      "Shijie Zhao",
      "Junlin Li",
      "Li Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.01988v1",
    "title": "Lightweight Structure-aware Transformer Network for VHR Remote Sensing\n  Image Change Detection",
    "summary": "Popular Transformer networks have been successfully applied to remote sensing\n(RS) image change detection (CD) identifications and achieve better results\nthan most convolutional neural networks (CNNs), but they still suffer from two\nmain problems. First, the computational complexity of the Transformer grows\nquadratically with the increase of image spatial resolution, which is\nunfavorable to very high-resolution (VHR) RS images. Second, these popular\nTransformer networks tend to ignore the importance of fine-grained features,\nwhich results in poor edge integrity and internal tightness for largely changed\nobjects and leads to the loss of small changed objects. To address the above\nissues, this Letter proposes a Lightweight Structure-aware Transformer (LSAT)\nnetwork for RS image CD. The proposed LSAT has two advantages. First, a\nCross-dimension Interactive Self-attention (CISA) module with linear complexity\nis designed to replace the vanilla self-attention in visual Transformer, which\neffectively reduces the computational complexity while improving the feature\nrepresentation ability of the proposed LSAT. Second, a Structure-aware\nEnhancement Module (SAEM) is designed to enhance difference features and edge\ndetail information, which can achieve double enhancement by difference\nrefinement and detail aggregation so as to obtain fine-grained features of\nbi-temporal RS images. Experimental results show that the proposed LSAT\nachieves significant improvement in detection accuracy and offers a better\ntradeoff between accuracy and computational costs than most state-of-the-art CD\nmethods for VHR RS images.",
    "published": "2023-06-03T03:21:18Z",
    "updated": "2023-06-03T03:21:18Z",
    "authors": [
      "Tao Lei",
      "Yetong Xu",
      "Hailong Ning",
      "Zhiyong Lv",
      "Chongdan Min",
      "Yaochu Jin",
      "Asoke K. Nandi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.13245v1",
    "title": "RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias",
    "summary": "Recently, there has been a surge of interest and attention in\nTransformer-based structures, such as Vision Transformer (ViT) and Vision\nMultilayer Perceptron (VMLP). Compared with the previous convolution-based\nstructures, the Transformer-based structure under investigation showcases a\ncomparable or superior performance under its distinctive attention-based input\ntoken mixer strategy. Introducing adversarial examples as a robustness\nconsideration has had a profound and detrimental impact on the performance of\nwell-established convolution-based structures. This inherent vulnerability to\nadversarial attacks has also been demonstrated in Transformer-based structures.\nIn this paper, our emphasis lies on investigating the intrinsic robustness of\nthe structure rather than introducing novel defense measures against\nadversarial attacks. To address the susceptibility to robustness issues, we\nemploy a rational structure design approach to mitigate such vulnerabilities.\nSpecifically, we enhance the adversarial robustness of the structure by\nincreasing the proportion of high-frequency structural robust biases. As a\nresult, we introduce a novel structure called Robust Bias Transformer-based\nStructure (RBFormer) that shows robust superiority compared to several existing\nbaseline structures. Through a series of extensive experiments, RBFormer\noutperforms the original structures by a significant margin, achieving an\nimpressive improvement of +16.12% and +5.04% across different evaluation\ncriteria on CIFAR-10 and ImageNet-1k, respectively.",
    "published": "2023-09-23T03:55:51Z",
    "updated": "2023-09-23T03:55:51Z",
    "authors": [
      "Hao Cheng",
      "Jinhao Duan",
      "Hui Li",
      "Lyutianyang Zhang",
      "Jiahang Cao",
      "Ping Wang",
      "Jize Zhang",
      "Kaidi Xu",
      "Renjing Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.09998v3",
    "title": "SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical\n  Image Segmentation",
    "summary": "Automated medical image segmentation is becoming increasingly crucial to\nmodern clinical practice, driven by the growing demand for precise diagnosis,\nthe push towards personalized treatment plans, and the advancements in machine\nlearning algorithms, especially the incorporation of deep learning methods.\nWhile convolutional neural networks (CNN) have been prevalent among these\nmethods, the remarkable potential of Transformer-based models for computer\nvision tasks is gaining more acknowledgment. To harness the advantages of both\nCNN-based and Transformer-based models, we propose a simple yet effective\nUNet-Transformer (seUNet-Trans) model for medical image segmentation. In our\napproach, the UNet model is designed as a feature extractor to generate\nmultiple feature maps from the input images, then the maps are propagated into\na bridge layer, which is introduced to sequentially connect the UNet and the\nTransformer. In this stage, we approach the pixel-level embedding technique\nwithout position embedding vectors, aiming to make the model more efficient.\nMoreover, we apply spatial-reduction attention in the Transformer to reduce the\ncomputational/memory overhead. By leveraging the UNet architecture and the\nself-attention mechanism, our model not only retains the preservation of both\nlocal and global context information but also is capable of capturing\nlong-range dependencies between input elements. The proposed model is\nextensively experimented on seven medical image segmentation datasets including\npolyp segmentation to demonstrate its efficacy. Comparison with several\nstate-of-the-art segmentation models on these datasets shows the superior\nperformance of our proposed seUNet-Trans network.",
    "published": "2023-10-16T01:13:38Z",
    "updated": "2023-11-10T15:01:01Z",
    "authors": [
      "Tan-Hanh Pham",
      "Xianqi Li",
      "Kim-Doang Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.12570v2",
    "title": "DA-TransUNet: Integrating Spatial and Channel Dual Attention with\n  Transformer U-Net for Medical Image Segmentation",
    "summary": "Accurate medical image segmentation is critical for disease quantification\nand treatment evaluation. While traditional Unet architectures and their\ntransformer-integrated variants excel in automated segmentation tasks. However,\nthey lack the ability to harness the intrinsic position and channel features of\nimage. Existing models also struggle with parameter efficiency and\ncomputational complexity, often due to the extensive use of Transformers. To\naddress these issues, this study proposes a novel deep medical image\nsegmentation framework, called DA-TransUNet, aiming to integrate the\nTransformer and dual attention block(DA-Block) into the traditional U-shaped\narchitecture. Unlike earlier transformer-based U-net models, DA-TransUNet\nutilizes Transformers and DA-Block to integrate not only global and local\nfeatures, but also image-specific positional and channel features, improving\nthe performance of medical image segmentation. By incorporating a DA-Block at\nthe embedding layer and within each skip connection layer, we substantially\nenhance feature extraction capabilities and improve the efficiency of the\nencoder-decoder structure. DA-TransUNet demonstrates superior performance in\nmedical image segmentation tasks, consistently outperforming state-of-the-art\ntechniques across multiple datasets. In summary, DA-TransUNet offers a\nsignificant advancement in medical image segmentation, providing an effective\nand powerful alternative to existing techniques. Our architecture stands out\nfor its ability to improve segmentation accuracy, thereby advancing the field\nof automated medical image diagnostics. The codes and parameters of our model\nwill be publicly available at https://github.com/SUN-1024/DA-TransUnet.",
    "published": "2023-10-19T08:25:03Z",
    "updated": "2023-11-14T11:32:53Z",
    "authors": [
      "Guanqun Sun",
      "Yizhi Pan",
      "Weikun Kong",
      "Zichang Xu",
      "Jianhua Ma",
      "Teeradaj Racharak",
      "Le-Minh Nguyen",
      "Junyi Xin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.04434v1",
    "title": "A Hierarchical Spatial Transformer for Massive Point Samples in\n  Continuous Space",
    "summary": "Transformers are widely used deep learning architectures. Existing\ntransformers are mostly designed for sequences (texts or time series), images\nor videos, and graphs. This paper proposes a novel transformer model for\nmassive (up to a million) point samples in continuous space. Such data are\nubiquitous in environment sciences (e.g., sensor observations), numerical\nsimulations (e.g., particle-laden flow, astrophysics), and location-based\nservices (e.g., POIs and trajectories). However, designing a transformer for\nmassive spatial points is non-trivial due to several challenges, including\nimplicit long-range and multi-scale dependency on irregular points in\ncontinuous space, a non-uniform point distribution, the potential high\ncomputational costs of calculating all-pair attention across massive points,\nand the risks of over-confident predictions due to varying point density. To\naddress these challenges, we propose a new hierarchical spatial transformer\nmodel, which includes multi-resolution representation learning within a\nquad-tree hierarchy and efficient spatial attention via coarse approximation.\nWe also design an uncertainty quantification branch to estimate prediction\nconfidence related to input feature noise and point sparsity. We provide a\ntheoretical analysis of computational time complexity and memory costs.\nExtensive experiments on both real-world and synthetic datasets show that our\nmethod outperforms multiple baselines in prediction accuracy and our model can\nscale up to one million points on one NVIDIA A100 GPU. The code is available at\n\\url{https://github.com/spatialdatasciencegroup/HST}.",
    "published": "2023-11-08T02:54:19Z",
    "updated": "2023-11-08T02:54:19Z",
    "authors": [
      "Wenchong He",
      "Zhe Jiang",
      "Tingsong Xiao",
      "Zelin Xu",
      "Shigang Chen",
      "Ronald Fick",
      "Miles Medina",
      "Christine Angelini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.05733v1",
    "title": "LogShield: A Transformer-based APT Detection System Leveraging\n  Self-Attention",
    "summary": "Cyber attacks are often identified using system and network logs. There have\nbeen significant prior works that utilize provenance graphs and ML techniques\nto detect attacks, specifically advanced persistent threats, which are very\ndifficult to detect. Lately, there have been studies where transformer-based\nlanguage models are being used to detect various types of attacks from system\nlogs. However, no such attempts have been made in the case of APTs. In\naddition, existing state-of-the-art techniques that use system provenance\ngraphs, lack a data processing framework generalized across datasets for\noptimal performance. For mitigating this limitation as well as exploring the\neffectiveness of transformer-based language models, this paper proposes\nLogShield, a framework designed to detect APT attack patterns leveraging the\npower of self-attention in transformers. We incorporate customized embedding\nlayers to effectively capture the context of event sequences derived from\nprovenance graphs. While acknowledging the computational overhead associated\nwith training transformer networks, our framework surpasses existing LSTM and\nLanguage models regarding APT detection. We integrated the model parameters and\ntraining procedure from the RoBERTa model and conducted extensive experiments\non well-known APT datasets (DARPA OpTC and DARPA TC E3). Our framework achieved\nsuperior F1 scores of 98% and 95% on the two datasets respectively, surpassing\nthe F1 scores of 96% and 94% obtained by LSTM models. Our findings suggest that\nLogShield's performance benefits from larger datasets and demonstrates its\npotential for generalization across diverse domains. These findings contribute\nto the advancement of APT attack detection methods and underscore the\nsignificance of transformer-based architectures in addressing security\nchallenges in computer systems.",
    "published": "2023-11-09T20:43:15Z",
    "updated": "2023-11-09T20:43:15Z",
    "authors": [
      "Sihat Afnan",
      "Mushtari Sadia",
      "Shahrear Iqbal",
      "Anindya Iqbal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.06638v1",
    "title": "Transformers with Attentive Federated Aggregation for Time Series Stock\n  Forecasting",
    "summary": "Recent innovations in transformers have shown their superior performance in\nnatural language processing (NLP) and computer vision (CV). The ability to\ncapture long-range dependencies and interactions in sequential data has also\ntriggered a great interest in time series modeling, leading to the widespread\nuse of transformers in many time series applications. However, being the most\ncommon and crucial application, the adaptation of transformers to time series\nforecasting has remained limited, with both promising and inconsistent results.\nIn contrast to the challenges in NLP and CV, time series problems not only add\nthe complexity of order or temporal dependence among input sequences but also\nconsider trend, level, and seasonality information that much of this data is\nvaluable for decision making. The conventional training scheme has shown\ndeficiencies regarding model overfitting, data scarcity, and privacy issues\nwhen working with transformers for a forecasting task. In this work, we propose\nattentive federated transformers for time series stock forecasting with better\nperformance while preserving the privacy of participating enterprises.\nEmpirical results on various stock data from the Yahoo! Finance website\nindicate the superiority of our proposed scheme in dealing with the above\nchallenges and data heterogeneity in federated learning.",
    "published": "2024-01-22T07:33:28Z",
    "updated": "2024-01-22T07:33:28Z",
    "authors": [
      "Chu Myaet Thwal",
      "Ye Lin Tun",
      "Kitae Kim",
      "Seong-Bae Park",
      "Choong Seon Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.08211v1",
    "title": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When\n  Trained on Human Working Memory Tasks",
    "summary": "Models based on the Transformer neural network architecture have seen success\non a wide variety of tasks that appear to require complex \"cognitive branching\"\n-- or the ability to maintain pursuit of one goal while accomplishing others.\nIn cognitive neuroscience, success on such tasks is thought to rely on\nsophisticated frontostriatal mechanisms for selective \\textit{gating}, which\nenable role-addressable updating -- and later readout -- of information to and\nfrom distinct \"addresses\" of memory, in the form of clusters of neurons.\nHowever, Transformer models have no such mechanisms intentionally built-in. It\nis thus an open question how Transformers solve such tasks, and whether the\nmechanisms that emerge to help them to do so bear any resemblance to the gating\nmechanisms in the human brain. In this work, we analyze the mechanisms that\nemerge within a vanilla attention-only Transformer trained on a simple sequence\nmodeling task inspired by a task explicitly designed to study working memory\ngating in computational cognitive neuroscience. We find that, as a result of\ntraining, the self-attention mechanism within the Transformer specializes in a\nway that mirrors the input and output gating mechanisms which were explicitly\nincorporated into earlier, more biologically-inspired architectures. These\nresults suggest opportunities for future research on computational similarities\nbetween modern AI architectures and models of the human brain.",
    "published": "2024-02-13T04:28:43Z",
    "updated": "2024-02-13T04:28:43Z",
    "authors": [
      "Aaron Traylor",
      "Jack Merullo",
      "Michael J. Frank",
      "Ellie Pavlick"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.00358v3",
    "title": "Spread Your Wings: A Radial Strip Transformer for Image Deblurring",
    "summary": "Exploring motion information is important for the motion deblurring task.\nRecent the window-based transformer approaches have achieved decent performance\nin image deblurring. Note that the motion causing blurry results is usually\ncomposed of translation and rotation movements and the window-shift operation\nin the Cartesian coordinate system by the window-based transformer approaches\nonly directly explores translation motion in orthogonal directions. Thus, these\nmethods have the limitation of modeling the rotation part. To alleviate this\nproblem, we introduce the polar coordinate-based transformer, which has the\nangles and distance to explore rotation motion and translation information\ntogether. In this paper, we propose a Radial Strip Transformer (RST), which is\na transformer-based architecture that restores the blur images in a polar\ncoordinate system instead of a Cartesian one. RST contains a dynamic radial\nembedding module (DRE) to extract the shallow feature by a radial deformable\nconvolution. We design a polar mask layer to generate the offsets for the\ndeformable convolution, which can reshape the convolution kernel along the\nradius to better capture the rotation motion information. Furthermore, we\nproposed a radial strip attention solver (RSAS) as deep feature extraction,\nwhere the relationship of windows is organized by azimuth and radius. This\nattention module contains radial strip windows to reweight image features in\nthe polar coordinate, which preserves more useful information in rotation and\ntranslation motion together for better recovering the sharp images.\nExperimental results on six synthesis and real-world datasets prove that our\nmethod performs favorably against other SOTA methods for the image deblurring\ntask.",
    "published": "2024-03-30T13:20:04Z",
    "updated": "2024-05-22T02:50:58Z",
    "authors": [
      "Duosheng Chen",
      "Shihao Zhou",
      "Jinshan Pan",
      "Jinglei Shi",
      "Lishen Qu",
      "Jufeng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.04620v5",
    "title": "Folded Context Condensation in Path Integral Formalism for Infinite\n  Context Transformers",
    "summary": "In this work, we present a generalized formulation of the Transformer\nalgorithm by reinterpreting its core mechanisms within the framework of Path\nIntegral formalism. In this perspective, the attention mechanism is recast as a\nprocess that integrates all possible transition paths leading to future token\nstates, with temporal evolution governed by the Feed-Forward Network. By\nsystematically mapping each component of the Transformer to its counterpart in\nthe Path Integral formulation, we obtain a more compact and efficient\nrepresentation, in which the contextual information of a sequence is condensed\ninto memory-like segments. These segments are recurrently processed across\nTransformer layers, enabling more effective long-term information retention. We\nvalidate the effectiveness of this approach through the Passkey retrieval task\nand a summarization task, demonstrating that the proposed method preserves\nhistorical information while exhibiting memory usage that scales linearly with\nsequence length. This contrasts with the non-linear memory growth typically\nobserved in standard attention mechanisms. We expect that this quantum-inspired\ngeneralization of the Transformer architecture will open new avenues for\nenhancing both the efficiency and expressiveness of future Transformer models.",
    "published": "2024-05-07T19:05:26Z",
    "updated": "2025-05-01T04:45:29Z",
    "authors": [
      "Won-Gi Paeng",
      "Daesuk Kwon",
      "Kyungwon Jeong",
      "Honggyo Suh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.20519v1",
    "title": "DuA: Dual Attentive Transformer in Long-Term Continuous EEG Emotion\n  Analysis",
    "summary": "Affective brain-computer interfaces (aBCIs) are increasingly recognized for\ntheir potential in monitoring and interpreting emotional states through\nelectroencephalography (EEG) signals. Current EEG-based emotion recognition\nmethods perform well with short segments of EEG data. However, these methods\nencounter significant challenges in real-life scenarios where emotional states\nevolve over extended periods. To address this issue, we propose a Dual\nAttentive (DuA) transformer framework for long-term continuous EEG emotion\nanalysis. Unlike segment-based approaches, the DuA transformer processes an\nentire EEG trial as a whole, identifying emotions at the trial level, referred\nto as trial-based emotion analysis. This framework is designed to adapt to\nvarying signal lengths, providing a substantial advantage over traditional\nmethods. The DuA transformer incorporates three key modules: the\nspatial-spectral network module, the temporal network module, and the transfer\nlearning module. The spatial-spectral network module simultaneously captures\nspatial and spectral information from EEG signals, while the temporal network\nmodule detects temporal dependencies within long-term EEG data. The transfer\nlearning module enhances the model's adaptability across different subjects and\nconditions. We extensively evaluate the DuA transformer using a\nself-constructed long-term EEG emotion database, along with two benchmark EEG\nemotion databases. On the basis of the trial-based leave-one-subject-out\ncross-subject cross-validation protocol, our experimental results demonstrate\nthat the proposed DuA transformer significantly outperforms existing methods in\nlong-term continuous EEG emotion analysis, with an average enhancement of\n5.28%.",
    "published": "2024-07-30T03:31:03Z",
    "updated": "2024-07-30T03:31:03Z",
    "authors": [
      "Yue Pan",
      "Qile Liu",
      "Qing Liu",
      "Li Zhang",
      "Gan Huang",
      "Xin Chen",
      "Fali Li",
      "Peng Xu",
      "Zhen Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.01120v1",
    "title": "An Efficient and Effective Transformer Decoder-Based Framework for\n  Multi-Task Visual Grounding",
    "summary": "Most advanced visual grounding methods rely on Transformers for\nvisual-linguistic feature fusion. However, these Transformer-based approaches\nencounter a significant drawback: the computational costs escalate\nquadratically due to the self-attention mechanism in the Transformer Encoder,\nparticularly when dealing with high-resolution images or long context\nsentences. This quadratic increase in computational burden restricts the\napplicability of visual grounding to more intricate scenes, such as\nconversation-based reasoning segmentation, which involves lengthy language\nexpressions. In this paper, we propose an efficient and effective multi-task\nvisual grounding (EEVG) framework based on Transformer Decoder to address this\nissue, which reduces the cost in both language and visual aspects. In the\nlanguage aspect, we employ the Transformer Decoder to fuse visual and\nlinguistic features, where linguistic features are input as memory and visual\nfeatures as queries. This allows fusion to scale linearly with language\nexpression length. In the visual aspect, we introduce a parameter-free approach\nto reduce computation by eliminating background visual tokens based on\nattention scores. We then design a light mask head to directly predict\nsegmentation masks from the remaining sparse feature maps. Extensive results\nand ablation studies on benchmarks demonstrate the efficiency and effectiveness\nof our approach. Code is available in https://github.com/chenwei746/EEVG.",
    "published": "2024-08-02T09:01:05Z",
    "updated": "2024-08-02T09:01:05Z",
    "authors": [
      "Wei Chen",
      "Long Chen",
      "Yu Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.02056v1",
    "title": "F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and\n  Selective Frequency Transformer for Image Deblurring",
    "summary": "Recent progress in image deblurring techniques focuses mainly on operating in\nboth frequency and spatial domains using the Fourier transform (FT) properties.\nHowever, their performance is limited due to the dependency of FT on stationary\nsignals and its lack of capability to extract spatial-frequency properties. In\nthis paper, we propose a novel approach based on the Fractional Fourier\nTransform (FRFT), a unified spatial-frequency representation leveraging both\nspatial and frequency components simultaneously, making it ideal for processing\nnon-stationary signals like images. Specifically, we introduce a Fractional\nFourier Transformer (F2former), where we combine the classical fractional\nFourier based Wiener deconvolution (F2WD) as well as a multi-branch\nencoder-decoder transformer based on a new fractional frequency aware\ntransformer block (F2TB). We design F2TB consisting of a fractional frequency\naware self-attention (F2SA) to estimate element-wise product attention based on\nimportant frequency components and a novel feed-forward network based on\nfrequency division multiplexing (FM-FFN) to refine high and low frequency\nfeatures separately for efficient latent clear image restoration. Experimental\nresults for the cases of both motion deblurring as well as defocus deblurring\nshow that the performance of our proposed method is superior to other\nstate-of-the-art (SOTA) approaches.",
    "published": "2024-09-03T17:05:12Z",
    "updated": "2024-09-03T17:05:12Z",
    "authors": [
      "Subhajit Paul",
      "Sahil Kumawat",
      "Ashutosh Gupta",
      "Deepak Mishra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.03171v3",
    "title": "Dual Selective Fusion Transformer Network for Hyperspectral Image\n  Classification",
    "summary": "Transformer has achieved satisfactory results in the field of hyperspectral\nimage (HSI) classification. However, existing Transformer models face two key\nchallenges when dealing with HSI scenes characterized by diverse land cover\ntypes and rich spectral information: (1) A fixed receptive field overlooks the\neffective contextual scales required by various HSI objects; (2) invalid\nself-attention features in context fusion affect model performance. To address\nthese limitations, we propose a novel Dual Selective Fusion Transformer Network\n(DSFormer) for HSI classification. DSFormer achieves joint spatial and spectral\ncontextual modeling by flexibly selecting and fusing features across different\nreceptive fields, effectively reducing unnecessary information interference by\nfocusing on the most relevant spatial-spectral tokens. Specifically, we design\na Kernel Selective Fusion Transformer Block (KSFTB) to learn an optimal\nreceptive field by adaptively fusing spatial and spectral features across\ndifferent scales, enhancing the model's ability to accurately identify diverse\nHSI objects. Additionally, we introduce a Token Selective Fusion Transformer\nBlock (TSFTB), which strategically selects and combines essential tokens during\nthe spatial-spectral self-attention fusion process to capture the most crucial\ncontexts. Extensive experiments conducted on four benchmark HSI datasets\ndemonstrate that the proposed DSFormer significantly improves land cover\nclassification accuracy, outperforming existing state-of-the-art methods.\nSpecifically, DSFormer achieves overall accuracies of 96.59%, 97.66%, 95.17%,\nand 94.59% in the Pavia University, Houston, Indian Pines, and Whu-HongHu\ndatasets, respectively, reflecting improvements of 3.19%, 1.14%, 0.91%, and\n2.80% over the previous model. The code will be available online at\nhttps://github.com/YichuXu/DSFormer.",
    "published": "2024-10-04T06:05:26Z",
    "updated": "2025-02-26T07:43:03Z",
    "authors": [
      "Yichu Xu",
      "Di Wang",
      "Lefei Zhang",
      "Liangpei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16249v2",
    "title": "Linear Attention for Efficient Bidirectional Sequence Modeling",
    "summary": "Linear Transformers and State Space Models have emerged as efficient\nalternatives to softmax Transformers for causal sequence modeling, enabling\nparallel training via matrix multiplication and efficient RNN-style inference.\nHowever, despite their success in causal tasks, no unified framework exists for\napplying Linear Transformers to bidirectional sequence modeling. We introduce\nLION, the first framework to systematically extend Linear Transformers to the\nbidirectional setting. LION generalizes three core representations commonly\nused in the causal case - full Linear Attention , bidirectional RNN, and\nchunkwise parallel form - to the bidirectional setting. These forms are\ntheoretically equivalent and enable models to exploit the strengths of each\nduring training and inference. We prove that a broad class of Linear\nTransformers can be extended using LION and validate our framework via three\ncore examples based on the choice of decay type: LION-LIT, the bidirectional\nextension of arXiv:2006.16236; LION-D, based on arXiv:2307.08621; and LION-S, a\nvariant using selective decay arXiv:2103.02143, arXiv:2312.0075. Across\nstandard bidirectional tasks, LION enables models to match or exceed the\nperformance of softmax Transformers, while offering significantly faster\ntraining and more efficient inference than existing State Space Models.",
    "published": "2025-02-22T14:52:17Z",
    "updated": "2025-09-30T14:14:55Z",
    "authors": [
      "Arshia Afzal",
      "Elias Abad Rocamora",
      "Leyla Naz Candogan",
      "Pol Puigdemont",
      "Francesco Tonin",
      "Yongtao Wu",
      "Mahsa Shoaran",
      "Volkan Cevher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.08226v1",
    "title": "A Grey-box Text Attack Framework using Explainable AI",
    "summary": "Explainable AI is a strong strategy implemented to understand complex\nblack-box model predictions in a human interpretable language. It provides the\nevidence required to execute the use of trustworthy and reliable AI systems. On\nthe other hand, however, it also opens the door to locating possible\nvulnerabilities in an AI model. Traditional adversarial text attack uses word\nsubstitution, data augmentation techniques and gradient-based attacks on\npowerful pre-trained Bidirectional Encoder Representations from Transformers\n(BERT) variants to generate adversarial sentences. These attacks are generally\nwhitebox in nature and not practical as they can be easily detected by humans\nE.g. Changing the word from \"Poor\" to \"Rich\". We proposed a simple yet\neffective Grey-box cum Black-box approach that does not require the knowledge\nof the model while using a set of surrogate Transformer/BERT models to perform\nthe attack using Explainable AI techniques. As Transformers are the current\nstate-of-the-art models for almost all Natural Language Processing (NLP) tasks,\nan attack generated from BERT1 is transferable to BERT2. This transferability\nis made possible due to the attention mechanism in the transformer that allows\nthe model to capture long-range dependencies in a sequence. Using the power of\nBERT generalisation via attention, we attempt to exploit how transformers learn\nby attacking a few surrogate transformer variants which are all based on a\ndifferent architecture. We demonstrate that this approach is highly effective\nto generate semantically good sentences by changing as little as one word that\nis not detectable by humans while still fooling other BERT models.",
    "published": "2025-03-11T09:44:17Z",
    "updated": "2025-03-11T09:44:17Z",
    "authors": [
      "Esther Chiramal",
      "Kelvin Soh Boon Kai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.14150v1",
    "title": "Comparative and Interpretative Analysis of CNN and Transformer Models in\n  Predicting Wildfire Spread Using Remote Sensing Data",
    "summary": "Facing the escalating threat of global wildfires, numerous computer vision\ntechniques using remote sensing data have been applied in this area. However,\nthe selection of deep learning methods for wildfire prediction remains\nuncertain due to the lack of comparative analysis in a quantitative and\nexplainable manner, crucial for improving prevention measures and refining\nmodels. This study aims to thoroughly compare the performance, efficiency, and\nexplainability of four prevalent deep learning architectures: Autoencoder,\nResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset\nthat includes nearly a decade of remote sensing data from California, U.S.,\nthese models predict the spread of wildfires for the following day. Through\ndetailed quantitative comparison analysis, we discovered that Transformer-based\nSwin-UNet and UNet generally outperform Autoencoder and ResNet, particularly\ndue to the advanced attention mechanisms in Transformer-based Swin-UNet and the\nefficient use of skip connections in both UNet and Transformer-based Swin-UNet,\nwhich contribute to superior predictive accuracy and model interpretability.\nThen we applied XAI techniques on all four models, this not only enhances the\nclarity and trustworthiness of models but also promotes focused improvements in\nwildfire prediction capabilities. The XAI analysis reveals that UNet and\nTransformer-based Swin-UNet are able to focus on critical features such as\n'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the\nother two models, while also maintaining balanced attention to the remaining\nfeatures, leading to their superior performance. The insights from our thorough\ncomparative analysis offer substantial implications for future model design and\nalso provide guidance for model selection in different scenarios.",
    "published": "2025-03-18T11:16:48Z",
    "updated": "2025-03-18T11:16:48Z",
    "authors": [
      "Yihang Zhou",
      "Ruige Kong",
      "Zhengsen Xu",
      "Linlin Xu",
      "Sibo Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.00307v3",
    "title": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations",
    "summary": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer.",
    "published": "2025-05-01T04:59:05Z",
    "updated": "2025-07-03T07:13:52Z",
    "authors": [
      "Yu-Hsiang Lan",
      "Eric K. Oermann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.08550v2",
    "title": "OLinear: A Linear Model for Time Series Forecasting in Orthogonally\n  Transformed Domain",
    "summary": "This paper presents $\\mathbf{OLinear}$, a $\\mathbf{linear}$-based\nmultivariate time series forecasting model that operates in an\n$\\mathbf{o}$rthogonally transformed domain. Recent forecasting models typically\nadopt the temporal forecast (TF) paradigm, which directly encode and decode\ntime series in the time domain. However, the entangled step-wise dependencies\nin series data can hinder the performance of TF. To address this, some\nforecasters conduct encoding and decoding in the transformed domain using\nfixed, dataset-independent bases (e.g., sine and cosine signals in the Fourier\ntransform). In contrast, we utilize $\\mathbf{OrthoTrans}$, a data-adaptive\ntransformation based on an orthogonal matrix that diagonalizes the series'\ntemporal Pearson correlation matrix. This approach enables more effective\nencoding and decoding in the decorrelated feature domain and can serve as a\nplug-in module to enhance existing forecasters. To enhance the representation\nlearning for multivariate time series, we introduce a customized linear layer,\n$\\mathbf{NormLin}$, which employs a normalized weight matrix to capture\nmultivariate dependencies. Empirically, the NormLin module shows a surprising\nperformance advantage over multi-head self-attention, while requiring nearly\nhalf the FLOPs. Extensive experiments on 24 benchmarks and 140 forecasting\ntasks demonstrate that OLinear consistently achieves state-of-the-art\nperformance with high efficiency. Notably, as a plug-in replacement for\nself-attention, the NormLin module consistently enhances Transformer-based\nforecasters. The code and datasets are available at\nhttps://anonymous.4open.science/r/OLinear",
    "published": "2025-05-12T10:39:37Z",
    "updated": "2025-05-14T11:00:57Z",
    "authors": [
      "Wenzhen Yue",
      "Yong Liu",
      "Haoxuan Li",
      "Hao Wang",
      "Xianghua Ying",
      "Ruohao Guo",
      "Bowei Xing",
      "Ji Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08696v1",
    "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
    "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
    "published": "2025-09-10T15:41:15Z",
    "updated": "2025-09-10T15:41:15Z",
    "authors": [
      "Siratish Sakpiboonchit"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03511v2",
    "title": "Platonic Transformers: A Solid Choice For Equivariance",
    "summary": "While widespread, Transformers lack inductive biases for geometric symmetries\ncommon in science and computer vision. Existing equivariant methods often\nsacrifice the efficiency and flexibility that make Transformers so effective\nthrough complex, computationally intensive designs. We introduce the Platonic\nTransformer to resolve this trade-off. By defining attention relative to\nreference frames from the Platonic solid symmetry groups, our method induces a\nprincipled weight-sharing scheme. This enables combined equivariance to\ncontinuous translations and Platonic symmetries, while preserving the exact\narchitecture and computational cost of a standard Transformer. Furthermore, we\nshow that this attention is formally equivalent to a dynamic group convolution,\nwhich reveals that the model learns adaptive geometric filters and enables a\nhighly scalable, linear-time convolutional variant. Across diverse benchmarks\nin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular\nproperty prediction (QM9, OMol25), the Platonic Transformer achieves\ncompetitive performance by leveraging these geometric constraints at no\nadditional cost.",
    "published": "2025-10-03T20:51:25Z",
    "updated": "2025-10-08T00:09:15Z",
    "authors": [
      "Mohammad Mohaiminul Islam",
      "Rishabh Anand",
      "David R. Wessels",
      "Friso de Kruiff",
      "Thijs P. Kuipers",
      "Rex Ying",
      "Clara I. SÃ¡nchez",
      "Sharvaree Vadgama",
      "Georg BÃ¶kman",
      "Erik J. Bekkers"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24784v1",
    "title": "Sub-microsecond Transformers for Jet Tagging on FPGAs",
    "summary": "We present the first sub-microsecond transformer implementation on an FPGA\nachieving competitive performance for state-of-the-art high-energy physics\nbenchmarks. Transformers have shown exceptional performance on multiple tasks\nin modern machine learning applications, including jet tagging at the CERN\nLarge Hadron Collider (LHC). However, their computational complexity prohibits\nuse in real-time applications, such as the hardware trigger system of the\ncollider experiments up until now. In this work, we demonstrate the first\napplication of transformers for jet tagging on FPGAs, achieving\n$\\mathcal{O}(100)$ nanosecond latency with superior performance compared to\nalternative baseline models. We leverage high-granularity quantization and\ndistributed arithmetic optimization to fit the entire transformer model on a\nsingle FPGA, achieving the required throughput and latency. Furthermore, we add\nmulti-head attention and linear attention support to hls4ml, making our work\naccessible to the broader fast machine learning community. This work advances\nthe next-generation trigger systems for the High Luminosity LHC, enabling the\nuse of transformers for real-time applications in high-energy physics and\nbeyond.",
    "published": "2025-10-26T23:13:00Z",
    "updated": "2025-10-26T23:13:00Z",
    "authors": [
      "Lauri Laatu",
      "Chang Sun",
      "Arianna Cox",
      "Abhijith Gandrakota",
      "Benedikt Maier",
      "Jennifer Ngadiuba",
      "Zhiqiang Que",
      "Wayne Luk",
      "Maria Spiropulu",
      "Alexander Tapper"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.01109v1",
    "title": "Anatomically Constrained Transformers for Echocardiogram Analysis",
    "summary": "Video transformers have recently demonstrated strong potential for\nechocardiogram (echo) analysis, leveraging self-supervised pre-training and\nflexible adaptation across diverse tasks. However, like other models operating\non videos, they are prone to learning spurious correlations from non-diagnostic\nregions such as image backgrounds. To overcome this limitation, we propose the\nVideo Anatomically Constrained Transformer (ViACT), a novel framework that\nintegrates anatomical priors directly into the transformer architecture. ViACT\nrepresents a deforming anatomical structure as a point set and encodes both its\nspatial geometry and corresponding image patches into transformer tokens.\nDuring pre-training, ViACT follows a masked autoencoding strategy that masks\nand reconstructs only anatomical patches, enforcing that representation\nlearning is focused on the anatomical region. The pre-trained model can then be\nfine-tuned for tasks localized to this region. In this work we focus on the\nmyocardium, demonstrating the framework on echo analysis tasks such as left\nventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)\ndetection. The anatomical constraint focuses transformer attention within the\nmyocardium, yielding interpretable attention maps aligned with regions of known\nCA pathology. Moreover, ViACT generalizes to myocardium point tracking without\nrequiring task-specific components such as correlation volumes used in\nspecialized tracking networks.",
    "published": "2025-11-02T22:52:30Z",
    "updated": "2025-11-02T22:52:30Z",
    "authors": [
      "Alexander Thorley",
      "Agis Chartsias",
      "Jordan Strom",
      "Jeremy Slivnick",
      "Dipak Kotecha",
      "Alberto Gomez",
      "Jinming Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/hep-th/9406167v1",
    "title": "Canonical Transformations and Path Integral Measures",
    "summary": "This paper is a generalization of previous work on the use of classical\ncanonical transformations to evaluate Hamiltonian path integrals for quantum\nmechanical systems. Relevant aspects of the Hamiltonian path integral and its\nmeasure are discussed and used to show that the quantum mechanical version of\nthe classical transformation does not leave the measure of the path integral\ninvariant, instead inducing an anomaly. The relation to operator techniques and\nordering problems is discussed, and special attention is paid to incorporation\nof the initial and final states of the transition element into the boundary\nconditions of the problem. Classical canonical transformations are developed to\nrender an arbitrary power potential cyclic. The resulting Hamiltonian is\nanalyzed as a quantum system to show its relation to known quantum mechanical\nresults. A perturbative argument is used to suppress ordering related terms in\nthe transformed Hamiltonian in the event that the classical canonical\ntransformation leads to a nonquadratic cyclic Hamiltonian. The associated\nanomalies are analyzed to yield general methods to evaluate the path integral's\nprefactor for such systems. The methods are applied to several systems,\nincluding linear and quadratic potentials, the velocity-dependent potential,\nand the time-dependent harmonic oscillator.",
    "published": "1994-06-24T15:43:42Z",
    "updated": "1994-06-24T15:43:42Z",
    "authors": [
      "Mark S. Swanson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.02534v2",
    "title": "The Cascade Transformer: an Application for Efficient Answer Sentence\n  Selection",
    "summary": "Large transformer-based language models have been shown to be very effective\nin many classification tasks. However, their computational complexity prevents\ntheir use in applications requiring the classification of a large set of\ncandidates. While previous works have investigated approaches to reduce model\nsize, relatively little attention has been paid to techniques to improve batch\nthroughput during inference. In this paper, we introduce the Cascade\nTransformer, a simple yet effective technique to adapt transformer-based models\ninto a cascade of rankers. Each ranker is used to prune a subset of candidates\nin a batch, thus dramatically increasing throughput at inference time. Partial\nencodings from the transformer model are shared among rerankers, providing\nfurther speed-up. When compared to a state-of-the-art transformer model, our\napproach reduces computation by 37% with almost no impact on accuracy, as\nmeasured on two English Question Answering datasets.",
    "published": "2020-05-05T23:32:01Z",
    "updated": "2020-05-07T15:07:38Z",
    "authors": [
      "Luca Soldaini",
      "Alessandro Moschitti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2005.03825v1",
    "title": "Learned Multi-layer Residual Sparsifying Transform Model for Low-dose CT\n  Reconstruction",
    "summary": "Signal models based on sparse representation have received considerable\nattention in recent years. Compared to synthesis dictionary learning,\nsparsifying transform learning involves highly efficient sparse coding and\noperator update steps. In this work, we propose a Multi-layer Residual\nSparsifying Transform (MRST) learning model wherein the transform domain\nresiduals are jointly sparsified over layers. In particular, the transforms for\nthe deeper layers exploit the more intricate properties of the residual maps.\nWe investigate the application of the learned MRST model for low-dose CT\nreconstruction using Penalized Weighted Least Squares (PWLS) optimization.\nExperimental results on Mayo Clinic data show that the MRST model outperforms\nconventional methods such as FBP and PWLS methods based on edge-preserving (EP)\nregularizer and single-layer transform (ST) model, especially for maintaining\nsome subtle details.",
    "published": "2020-05-08T02:36:50Z",
    "updated": "2020-05-08T02:36:50Z",
    "authors": [
      "Xikai Yang",
      "Xuehang Zheng",
      "Yong Long",
      "Saiprasad Ravishankar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.05218v1",
    "title": "Bornon: Bengali Image Captioning with Transformer-based Deep learning\n  approach",
    "summary": "Image captioning using Encoder-Decoder based approach where CNN is used as\nthe Encoder and sequence generator like RNN as Decoder has proven to be very\neffective. However, this method has a drawback that is sequence needs to be\nprocessed in order. To overcome this drawback some researcher has utilized the\nTransformer model to generate captions from images using English datasets.\nHowever, none of them generated captions in Bengali using the transformer\nmodel. As a result, we utilized three different Bengali datasets to generate\nBengali captions from images using the Transformer model. Additionally, we\ncompared the performance of the transformer-based model with a visual\nattention-based Encoder-Decoder approach. Finally, we compared the result of\nthe transformer-based model with other models that employed different Bengali\nimage captioning datasets.",
    "published": "2021-09-11T08:29:26Z",
    "updated": "2021-09-11T08:29:26Z",
    "authors": [
      "Faisal Muhammad Shah",
      "Mayeesha Humaira",
      "Md Abidur Rahman Khan Jim",
      "Amit Saha Ami",
      "Shimul Paul"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.09796v2",
    "title": "Transforming Fake News: Robust Generalisable News Classification Using\n  Transformers",
    "summary": "As online news has become increasingly popular and fake news increasingly\nprevalent, the ability to audit the veracity of online news content has become\nmore important than ever. Such a task represents a binary classification\nchallenge, for which transformers have achieved state-of-the-art results. Using\nthe publicly available ISOT and Combined Corpus datasets, this study explores\ntransformers' abilities to identify fake news, with particular attention given\nto investigating generalisation to unseen datasets with varying styles, topics\nand class distributions. Moreover, we explore the idea that opinion-based news\narticles cannot be classified as real or fake due to their subjective nature\nand often sensationalised language, and propose a novel two-step classification\npipeline to remove such articles from both model training and the final\ndeployed inference system. Experiments over the ISOT and Combined Corpus\ndatasets show that transformers achieve an increase in F1 scores of up to 4.9%\nfor out of distribution generalisation compared to baseline approaches, with a\nfurther increase of 10.1% following the implementation of our two-step\nclassification pipeline. To the best of our knowledge, this study is the first\nto investigate generalisation of transformers in this context.",
    "published": "2021-09-20T19:03:16Z",
    "updated": "2021-12-03T15:08:36Z",
    "authors": [
      "Ciara Blackledge",
      "Amir Atapour-Abarghouei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.09920v1",
    "title": "Survey: Transformer based Video-Language Pre-training",
    "summary": "Inspired by the success of transformer-based pre-training methods on natural\nlanguage tasks and further computer vision tasks, researchers have begun to\napply transformer to video processing. This survey aims to give a comprehensive\noverview on transformer-based pre-training methods for Video-Language learning.\nWe first briefly introduce the transformer tructure as the background\nknowledge, including attention mechanism, position encoding etc. We then\ndescribe the typical paradigm of pre-training & fine-tuning on Video-Language\nprocessing in terms of proxy tasks, downstream tasks and commonly used video\ndatasets. Next, we categorize transformer models into Single-Stream and\nMulti-Stream structures, highlight their innovations and compare their\nperformances. Finally, we analyze and discuss the current challenges and\npossible future research directions for Video-Language pre-training.",
    "published": "2021-09-21T02:36:06Z",
    "updated": "2021-09-21T02:36:06Z",
    "authors": [
      "Ludan Ruan",
      "Qin Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.10559v1",
    "title": "Hierarchical Multimodal Transformer to Summarize Videos",
    "summary": "Although video summarization has achieved tremendous success benefiting from\nRecurrent Neural Networks (RNN), RNN-based methods neglect the global\ndependencies and multi-hop relationships among video frames, which limits the\nperformance. Transformer is an effective model to deal with this problem, and\nsurpasses RNN-based methods in several sequence modeling tasks, such as machine\ntranslation, video captioning, \\emph{etc}. Motivated by the great success of\ntransformer and the natural structure of video (frame-shot-video), a\nhierarchical transformer is developed for video summarization, which can\ncapture the dependencies among frame and shots, and summarize the video by\nexploiting the scene information formed by shots. Furthermore, we argue that\nboth the audio and visual information are essential for the video summarization\ntask. To integrate the two kinds of information, they are encoded in a\ntwo-stream scheme, and a multimodal fusion mechanism is developed based on the\nhierarchical transformer. In this paper, the proposed method is denoted as\nHierarchical Multimodal Transformer (HMT). Practically, extensive experiments\nshow that HMT surpasses most of the traditional, RNN-based and attention-based\nvideo summarization methods.",
    "published": "2021-09-22T07:38:59Z",
    "updated": "2021-09-22T07:38:59Z",
    "authors": [
      "Bin Zhao",
      "Maoguo Gong",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.11338v2",
    "title": "Orthogonal Graph Neural Networks",
    "summary": "Graph neural networks (GNNs) have received tremendous attention due to their\nsuperiority in learning node representations. These models rely on message\npassing and feature transformation functions to encode the structural and\nfeature information from neighbors. However, stacking more convolutional layers\nsignificantly decreases the performance of GNNs. Most recent studies attribute\nthis limitation to the over-smoothing issue, where node embeddings converge to\nindistinguishable vectors. Through a number of experimental observations, we\nargue that the main factor degrading the performance is the unstable forward\nnormalization and backward gradient resulted from the improper design of the\nfeature transformation, especially for shallow GNNs where the over-smoothing\nhas not happened. Therefore, we propose a novel orthogonal feature\ntransformation, named Ortho-GConv, which could generally augment the existing\nGNN backbones to stabilize the model training and improve the model's\ngeneralization performance. Specifically, we maintain the orthogonality of the\nfeature transformation comprehensively from three perspectives, namely hybrid\nweight initialization, orthogonal transformation, and orthogonal\nregularization. By equipping the existing GNNs (e.g. GCN, JKNet, GCNII) with\nOrtho-GConv, we demonstrate the generality of the orthogonal feature\ntransformation to enable stable training, and show its effectiveness for node\nand graph classification tasks.",
    "published": "2021-09-23T12:39:01Z",
    "updated": "2021-12-01T12:19:07Z",
    "authors": [
      "Kai Guo",
      "Kaixiong Zhou",
      "Xia Hu",
      "Yu Li",
      "Yi Chang",
      "Xin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2109.12948v1",
    "title": "Understanding and Overcoming the Challenges of Efficient Transformer\n  Quantization",
    "summary": "Transformer-based architectures have become the de-facto standard models for\na wide range of Natural Language Processing tasks. However, their memory\nfootprint and high latency are prohibitive for efficient deployment and\ninference on resource-limited devices. In this work, we explore quantization\nfor transformers. We show that transformers have unique quantization challenges\n-- namely, high dynamic activation ranges that are difficult to represent with\na low bit fixed-point format. We establish that these activations contain\nstructured outliers in the residual connections that encourage specific\nattention patterns, such as attending to the special separator token. To combat\nthese challenges, we present three solutions based on post-training\nquantization and quantization-aware training, each with a different set of\ncompromises for accuracy, model size, and ease of use. In particular, we\nintroduce a novel quantization scheme -- per-embedding-group quantization. We\ndemonstrate the effectiveness of our methods on the GLUE benchmark using BERT,\nestablishing state-of-the-art results for post-training quantization. Finally,\nwe show that transformer weights and embeddings can be quantized to ultra-low\nbit-widths, leading to significant memory savings with a minimum accuracy loss.\nOur source code is available\nat~\\url{https://github.com/qualcomm-ai-research/transformer-quantization}.",
    "published": "2021-09-27T10:57:18Z",
    "updated": "2021-09-27T10:57:18Z",
    "authors": [
      "Yelysei Bondarenko",
      "Markus Nagel",
      "Tijmen Blankevoort"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.00097v1",
    "title": "TransGeo: Transformer Is All You Need for Cross-view Image\n  Geo-localization",
    "summary": "The dominant CNN-based methods for cross-view image geo-localization rely on\npolar transform and fail to model global correlation. We propose a pure\ntransformer-based approach (TransGeo) to address these limitations from a\ndifferent perspective. TransGeo takes full advantage of the strengths of\ntransformer related to global information modeling and explicit position\ninformation encoding. We further leverage the flexibility of transformer input\nand propose an attention-guided non-uniform cropping method, so that\nuninformative image patches are removed with negligible drop on performance to\nreduce computation cost. The saved computation can be reallocated to increase\nresolution only for informative patches, resulting in performance improvement\nwith no additional computation cost. This \"attend and zoom-in\" strategy is\nhighly similar to human behavior when observing images. Remarkably, TransGeo\nachieves state-of-the-art results on both urban and rural datasets, with\nsignificantly less computation cost than CNN-based methods. It does not rely on\npolar transform and infers faster than CNN-based methods. Code is available at\nhttps://github.com/Jeff-Zilence/TransGeo2022.",
    "published": "2022-03-31T21:19:41Z",
    "updated": "2022-03-31T21:19:41Z",
    "authors": [
      "Sijie Zhu",
      "Mubarak Shah",
      "Chen Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.05585v1",
    "title": "SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient\n  object detection",
    "summary": "Convolutional neural networks (CNNs) are good at extracting contexture\nfeatures within certain receptive fields, while transformers can model the\nglobal long-range dependency features. By absorbing the advantage of\ntransformer and the merit of CNN, Swin Transformer shows strong feature\nrepresentation ability. Based on it, we propose a cross-modality fusion model\nSwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin\nTransformer to extract the hierarchical features, boosted by attention\nmechanism to bridge the gap between two modalities, and guided by edge\ninformation to sharp the contour of salient object. To be specific, two-stream\nSwin Transformer encoder first extracts multi-modality features, and then\nspatial alignment and channel re-calibration module is presented to optimize\nintra-level cross-modality features. To clarify the fuzzy boundary, edge-guided\ndecoder achieves inter-level cross-modality fusion under the guidance of edge\nfeatures. The proposed model outperforms the state-of-the-art models on RGB-D\nand RGB-T datasets, showing that it provides more insight into the\ncross-modality complementarity task.",
    "published": "2022-04-12T07:37:39Z",
    "updated": "2022-04-12T07:37:39Z",
    "authors": [
      "Zhengyi Liu",
      "Yacheng Tan",
      "Qian He",
      "Yun Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.07258v2",
    "title": "Causal Transformer for Estimating Counterfactual Outcomes",
    "summary": "Estimating counterfactual outcomes over time from observational data is\nrelevant for many applications (e.g., personalized medicine). Yet,\nstate-of-the-art methods build upon simple long short-term memory (LSTM)\nnetworks, thus rendering inferences for complex, long-range dependencies\nchallenging. In this paper, we develop a novel Causal Transformer for\nestimating counterfactual outcomes over time. Our model is specifically\ndesigned to capture complex, long-range dependencies among time-varying\nconfounders. For this, we combine three transformer subnetworks with separate\ninputs for time-varying covariates, previous treatments, and previous outcomes\ninto a joint network with in-between cross-attentions. We further develop a\ncustom, end-to-end training procedure for our Causal Transformer. Specifically,\nwe propose a novel counterfactual domain confusion loss to address confounding\nbias: it aims to learn adversarial balanced representations, so that they are\npredictive of the next outcome but non-predictive of the current treatment\nassignment. We evaluate our Causal Transformer based on synthetic and\nreal-world datasets, where it achieves superior performance over current\nbaselines. To the best of our knowledge, this is the first work proposing\ntransformer-based architecture for estimating counterfactual outcomes from\nlongitudinal data.",
    "published": "2022-04-14T22:40:09Z",
    "updated": "2022-06-03T15:57:34Z",
    "authors": [
      "Valentyn Melnychuk",
      "Dennis Frauen",
      "Stefan Feuerriegel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.12997v2",
    "title": "DearKD: Data-Efficient Early Knowledge Distillation for Vision\n  Transformers",
    "summary": "Transformers are successfully applied to computer vision due to their\npowerful modeling capacity with self-attention. However, the excellent\nperformance of transformers heavily depends on enormous training images. Thus,\na data-efficient transformer solution is urgently needed. In this work, we\npropose an early knowledge distillation framework, which is termed as DearKD,\nto improve the data efficiency required by transformers. Our DearKD is a\ntwo-stage framework that first distills the inductive biases from the early\nintermediate layers of a CNN and then gives the transformer full play by\ntraining without distillation. Further, our DearKD can be readily applied to\nthe extreme data-free case where no real images are available. In this case, we\npropose a boundary-preserving intra-divergence loss based on DeepInversion to\nfurther close the performance gap against the full-data counterpart. Extensive\nexperiments on ImageNet, partial ImageNet, data-free setting and other\ndownstream tasks prove the superiority of DearKD over its baselines and\nstate-of-the-art methods.",
    "published": "2022-04-27T15:11:04Z",
    "updated": "2022-04-28T14:36:21Z",
    "authors": [
      "Xianing Chen",
      "Qiong Cao",
      "Yujie Zhong",
      "Jing Zhang",
      "Shenghua Gao",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1808.05598v3",
    "title": "Palatini frames in scalar-tensor theories of gravity",
    "summary": "A new systematic approach extending the notion of frames to the Palatini\nscalar-tensor theories of gravity in various dimensions n>2 is proposed. We\nimpose frame transformation induced by the group action which includes\nalmost-geodesic and conformal transformations. We characterize theories\ninvariant with respect to these transformations dividing them up into\nsolution-equivalent subclasses (group orbits). To this end, invariant\ncharacteristics have been introduced. Unlike in the metric case, it turns out\nthat the dimension four admitting the largest transformation group is rather\nspecial for such theories. The formalism provides new frames that incorporate\nnon-metricity. The case of Palatini F(R)-gravity is considered in more detail.",
    "published": "2018-08-16T17:40:58Z",
    "updated": "2019-03-15T14:33:16Z",
    "authors": [
      "Aleksander Kozak",
      "Andrzej Borowiec"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.14438v1",
    "title": "Gated Transformer Networks for Multivariate Time Series Classification",
    "summary": "Deep learning model (primarily convolutional networks and LSTM) for time\nseries classification has been studied broadly by the community with the wide\napplications in different domains like healthcare, finance, industrial\nengineering and IoT. Meanwhile, Transformer Networks recently achieved frontier\nperformance on various natural language processing and computer vision tasks.\nIn this work, we explored a simple extension of the current Transformer\nNetworks with gating, named Gated Transformer Networks (GTN) for the\nmultivariate time series classification problem. With the gating that merges\ntwo towers of Transformer which model the channel-wise and step-wise\ncorrelations respectively, we show how GTN is naturally and effectively\nsuitable for the multivariate time series classification task. We conduct\ncomprehensive experiments on thirteen dataset with full ablation study. Our\nresults show that GTN is able to achieve competing results with current\nstate-of-the-art deep learning models. We also explored the attention map for\nthe natural interpretability of GTN on time series modeling. Our preliminary\nresults provide a strong baseline for the Transformer Networks on multivariate\ntime series classification task and grounds the foundation for future research.",
    "published": "2021-03-26T12:43:32Z",
    "updated": "2021-03-26T12:43:32Z",
    "authors": [
      "Minghao Liu",
      "Shengqi Ren",
      "Siyuan Ma",
      "Jiahui Jiao",
      "Yizhou Chen",
      "Zhiguang Wang",
      "Wei Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.17084v2",
    "title": "DA-DETR: Domain Adaptive Detection Transformer with Information Fusion",
    "summary": "The recent detection transformer (DETR) simplifies the object detection\npipeline by removing hand-crafted designs and hyperparameters as employed in\nconventional two-stage object detectors. However, how to leverage the simple\nyet effective DETR architecture in domain adaptive object detection is largely\nneglected. Inspired by the unique DETR attention mechanisms, we design DA-DETR,\na domain adaptive object detection transformer that introduces information\nfusion for effective transfer from a labeled source domain to an unlabeled\ntarget domain. DA-DETR introduces a novel CNN-Transformer Blender (CTBlender)\nthat fuses the CNN features and Transformer features ingeniously for effective\nfeature alignment and knowledge transfer across domains. Specifically,\nCTBlender employs the Transformer features to modulate the CNN features across\nmultiple scales where the high-level semantic information and the low-level\nspatial information are fused for accurate object identification and\nlocalization. Extensive experiments show that DA-DETR achieves superior\ndetection performance consistently across multiple widely adopted domain\nadaptation benchmarks.",
    "published": "2021-03-31T13:55:56Z",
    "updated": "2023-03-22T05:15:36Z",
    "authors": [
      "Jingyi Zhang",
      "Jiaxing Huang",
      "Zhipeng Luo",
      "Gongjie Zhang",
      "Xiaoqin Zhang",
      "Shijian Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.03861v1",
    "title": "Towards Light Weight Object Detection System",
    "summary": "Transformers are a popular choice for classification tasks and as backbones\nfor object detection tasks. However, their high latency brings challenges in\ntheir adaptation to lightweight object detection systems. We present an\napproximation of the self-attention layers used in the transformer\narchitecture. This approximation reduces the latency of the classification\nsystem while incurring minimal loss in accuracy. We also present a method that\nuses a transformer encoder layer for multi-resolution feature fusion. This\nfeature fusion improves the accuracy of the state-of-the-art lightweight object\ndetection system without significantly increasing the number of parameters.\nFinally, we provide an abstraction for the transformer architecture called\nGeneralized Transformer (gFormer) that can guide the design of novel\ntransformer-like architectures.",
    "published": "2022-10-08T00:55:15Z",
    "updated": "2022-10-08T00:55:15Z",
    "authors": [
      "Dharma KC",
      "Venkata Ravi Kiran Dayana",
      "Meng-Lin Wu",
      "Venkateswara Rao Cherukuri",
      "Hau Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.07240v1",
    "title": "How to Train Vision Transformer on Small-scale Datasets?",
    "summary": "Vision Transformer (ViT), a radically different architecture than\nconvolutional neural networks offers multiple advantages including design\nsimplicity, robustness and state-of-the-art performance on many vision tasks.\nHowever, in contrast to convolutional neural networks, Vision Transformer lacks\ninherent inductive biases. Therefore, successful training of such models is\nmainly attributed to pre-training on large-scale datasets such as ImageNet with\n1.2M or JFT with 300M images. This hinders the direct adaption of Vision\nTransformer for small-scale datasets. In this work, we show that\nself-supervised inductive biases can be learned directly from small-scale\ndatasets and serve as an effective weight initialization scheme for\nfine-tuning. This allows to train these models without large-scale\npre-training, changes to model architecture or loss functions. We present\nthorough experiments to successfully train monolithic and non-monolithic Vision\nTransformers on five small datasets including CIFAR10/100, CINIC10, SVHN,\nTiny-ImageNet and two fine-grained datasets: Aircraft and Cars. Our approach\nconsistently improves the performance of Vision Transformers while retaining\ntheir properties such as attention to salient regions and higher robustness.\nOur codes and pre-trained models are available at:\nhttps://github.com/hananshafi/vits-for-small-scale-datasets.",
    "published": "2022-10-13T17:59:19Z",
    "updated": "2022-10-13T17:59:19Z",
    "authors": [
      "Hanan Gani",
      "Muzammal Naseer",
      "Mohammad Yaqub"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.12786v2",
    "title": "When Can Transformers Ground and Compose: Insights from Compositional\n  Generalization Benchmarks",
    "summary": "Humans can reason compositionally whilst grounding language utterances to the\nreal world. Recent benchmarks like ReaSCAN use navigation tasks grounded in a\ngrid world to assess whether neural models exhibit similar capabilities. In\nthis work, we present a simple transformer-based model that outperforms\nspecialized architectures on ReaSCAN and a modified version of gSCAN. On\nanalyzing the task, we find that identifying the target location in the grid\nworld is the main challenge for the models. Furthermore, we show that a\nparticular split in ReaSCAN, which tests depth generalization, is unfair. On an\namended version of this split, we show that transformers can generalize to\ndeeper input structures. Finally, we design a simpler grounded compositional\ngeneralization task, RefEx, to investigate how transformers reason\ncompositionally. We show that a single self-attention layer with a single head\ngeneralizes to novel combinations of object attributes. Moreover, we derive a\nprecise mathematical construction of the transformer's computations from the\nlearned network. Overall, we provide valuable insights about the grounded\ncompositional generalization task and the behaviour of transformers on it,\nwhich would be useful for researchers working in this area.",
    "published": "2022-10-23T17:03:55Z",
    "updated": "2022-10-31T03:50:30Z",
    "authors": [
      "Ankur Sikarwar",
      "Arkil Patel",
      "Navin Goyal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1901.02273v1",
    "title": "Long Short-Term Memory Spatial Transformer Network",
    "summary": "Spatial transformer network has been used in a layered form in conjunction\nwith a convolutional network to enable the model to transform data spatially.\nIn this paper, we propose a combined spatial transformer network (STN) and a\nLong Short-Term Memory network (LSTM) to classify digits in sequences formed by\nMINST elements. This LSTM-STN model has a top-down attention mechanism profit\nfrom LSTM layer, so that the STN layer can perform short-term independent\nelements for the statement in the process of spatial transformation, thus\navoiding the distortion that may be caused when the entire sequence is\nspatially transformed. It also avoids the influence of this distortion on the\nsubsequent classification process using convolutional neural networks and\nachieves a single digit error of 1.6\\% compared with 2.2\\% of Convolutional\nNeural Network with STN layer.",
    "published": "2019-01-08T12:08:32Z",
    "updated": "2019-01-08T12:08:32Z",
    "authors": [
      "Shiyang Feng",
      "Tianyue Chen",
      "Hao Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1904.09408v2",
    "title": "Language Models with Transformers",
    "summary": "The Transformer architecture is superior to RNN-based models in computational\nefficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer\nmodels on various NLP tasks using pre-trained language models on large-scale\ncorpora. Surprisingly, these Transformer architectures are suboptimal for\nlanguage model itself. Neither self-attention nor the positional encoding in\nthe Transformer is able to efficiently incorporate the word-level sequential\ncontext crucial to language modeling.\n  In this paper, we explore effective Transformer architectures for language\nmodel, including adding additional LSTM layers to better capture the sequential\ncontext while still keeping the computation efficient. We propose Coordinate\nArchitecture Search (CAS) to find an effective architecture through iterative\nrefinement of the model. Experimental results on the PTB, WikiText-2, and\nWikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all\nproblems, i.e. on average an improvement of 12.0 perplexity units compared to\nstate-of-the-art LSTMs. The source code is publicly available.",
    "published": "2019-04-20T06:43:14Z",
    "updated": "2019-10-17T04:25:15Z",
    "authors": [
      "Chenguang Wang",
      "Mu Li",
      "Alexander J. Smola"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2201.05920v1",
    "title": "ViTBIS: Vision Transformer for Biomedical Image Segmentation",
    "summary": "In this paper, we propose a novel network named Vision Transformer for\nBiomedical Image Segmentation (ViTBIS). Our network splits the input feature\nmaps into three parts with $1\\times 1$, $3\\times 3$ and $5\\times 5$\nconvolutions in both encoder and decoder. Concat operator is used to merge the\nfeatures before being fed to three consecutive transformer blocks with\nattention mechanism embedded inside it. Skip connections are used to connect\nencoder and decoder transformer blocks. Similarly, transformer blocks and multi\nscale architecture is used in decoder before being linearly projected to\nproduce the output segmentation map. We test the performance of our network\nusing Synapse multi-organ segmentation dataset, Automated cardiac diagnosis\nchallenge dataset, Brain tumour MRI segmentation dataset and Spleen CT\nsegmentation dataset. Without bells and whistles, our network outperforms most\nof the previous state of the art CNN and transformer based models using Dice\nscore and the Hausdorff distance as the evaluation metrics.",
    "published": "2022-01-15T20:44:45Z",
    "updated": "2022-01-15T20:44:45Z",
    "authors": [
      "Abhinav Sagar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.10802v1",
    "title": "SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal\n  Brain MRI",
    "summary": "Volumetric reconstruction of fetal brains from multiple stacks of MR slices,\nacquired in the presence of almost unpredictable and often severe subject\nmotion, is a challenging task that is highly sensitive to the initialization of\nslice-to-volume transformations. We propose a novel slice-to-volume\nregistration method using Transformers trained on synthetically transformed\ndata, which model multiple stacks of MR slices as a sequence. With the\nattention mechanism, our model automatically detects the relevance between\nslices and predicts the transformation of one slice using information from\nother slices. We also estimate the underlying 3D volume to assist\nslice-to-volume registration and update the volume and transformations\nalternately to improve accuracy. Results on synthetic data show that our method\nachieves lower registration error and better reconstruction quality compared\nwith existing state-of-the-art methods. Experiments with real-world MRI data\nare also performed to demonstrate the ability of the proposed model to improve\nthe quality of 3D reconstruction under severe fetal motion.",
    "published": "2022-06-22T01:55:42Z",
    "updated": "2022-06-22T01:55:42Z",
    "authors": [
      "Junshen Xu",
      "Daniel Moyer",
      "P. Ellen Grant",
      "Polina Golland",
      "Juan Eugenio Iglesias",
      "Elfar Adalsteinsson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.04365v1",
    "title": "SANDFORMER: CNN and Transformer under Gated Fusion for Sand Dust Image\n  Restoration",
    "summary": "Although Convolutional Neural Networks (CNN) have made good progress in image\nrestoration, the intrinsic equivalence and locality of convolutions still\nconstrain further improvements in image quality. Recent vision transformer and\nself-attention have achieved promising results on various computer vision\ntasks. However, directly utilizing Transformer for image restoration is a\nchallenging task. In this paper, we introduce an effective hybrid architecture\nfor sand image restoration tasks, which leverages local features from CNN and\nlong-range dependencies captured by transformer to improve the results further.\nWe propose an efficient hybrid structure for sand dust image restoration to\nsolve the feature inconsistency issue between Transformer and CNN. The\nframework complements each representation by modulating features from the\nCNN-based and Transformer-based branches rather than simply adding or\nconcatenating features. Experiments demonstrate that SandFormer achieves\nsignificant performance improvements in synthetic and real dust scenes compared\nto previous sand image restoration methods.",
    "published": "2023-03-08T04:12:32Z",
    "updated": "2023-03-08T04:12:32Z",
    "authors": [
      "Jun Shi",
      "Bingcai Wei",
      "Gang Zhou",
      "Liye Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.13827v2",
    "title": "Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact\n  Deformable Convolutional Transformers",
    "summary": "Manufacturing wafers is an intricate task involving thousands of steps.\nDefect Pattern Recognition (DPR) of wafer maps is crucial to find the root\ncause of the issue and further improving the yield in the wafer foundry.\nMixed-type DPR is much more complicated compared to single-type DPR due to\nvaried spatial features, the uncertainty of defects, and the number of defects\npresent. To accurately predict the number of defects as well as the types of\ndefects, we propose a novel compact deformable convolutional transformer (DC\nTransformer). Specifically, DC Transformer focuses on the global features\npresent in the wafer map by virtue of learnable deformable kernels and\nmulti-head attention to the global features. The proposed method succinctly\nmodels the internal relationship between the wafer maps and the defects. DC\nTransformer is evaluated on a real dataset containing 38 defect patterns.\nExperimental results show that DC Transformer performs exceptionally well in\nrecognizing both single and mixed-type defects. The proposed method outperforms\nthe current state of the models by a considerable margin",
    "published": "2023-03-24T06:24:07Z",
    "updated": "2023-10-17T03:26:07Z",
    "authors": [
      "Nitish Shukla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1909.02635v1",
    "title": "Effective Use of Transformer Networks for Entity Tracking",
    "summary": "Tracking entities in procedural language requires understanding the\ntransformations arising from actions on entities as well as those entities'\ninteractions. While self-attention-based pre-trained language encoders like GPT\nand BERT have been successfully applied across a range of natural language\nunderstanding tasks, their ability to handle the nuances of procedural texts is\nstill untested. In this paper, we explore the use of pre-trained transformer\nnetworks for entity tracking tasks in procedural text. First, we test standard\nlightweight approaches for prediction with pre-trained transformers, and find\nthat these approaches underperform even simple baselines. We show that much\nstronger results can be attained by restructuring the input to guide the\ntransformer model to focus on a particular entity. Second, we assess the degree\nto which transformer networks capture the process dynamics, investigating such\nfactors as merged entities and oblique entity references. On two different\ntasks, ingredient detection in recipes and QA over scientific processes, we\nachieve state-of-the-art results, but our models still largely attend to\nshallow context clues and do not form complex representations of intermediate\nentity or process state.",
    "published": "2019-09-05T21:13:37Z",
    "updated": "2019-09-05T21:13:37Z",
    "authors": [
      "Aditya Gupta",
      "Greg Durrett"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2003.13215v3",
    "title": "Some Conclusions on Markov Matrices and Transformations",
    "summary": "Markov matrices have an important role in the filed of stochastic processes.\nIn this paper, we will show and prove a series of conclusions on Markov\nmatrices and transformations rather than pay attention to stochastic processes\nalthough these conclusions are useful for studying stochastic processes. These\nconclusions we come to, which will make us have a deeper understanding of\nMarkov matrices and transformations, refer to eigenvalues, eigenvectors and the\nstructure of invariant subspaces. At the same time, we account for the\ncorresponding significances of the conclusions. For any Markov matrix and the\ncorresponding transformation, we decompose the space as a direct sum of an\neigenvector and an invariant subspace. Enlightened by this, we achieve two\ntheorems about Markov matrices and transformations inspired by which we\nconclude that Markov transformations may be a defective matrix--in other words,\nmay be a nondiagonalizable one. Specifically, we construct a nondiagonalizable\nMarkov matrix to exhibit our train of thought.",
    "published": "2020-03-30T04:38:05Z",
    "updated": "2022-12-30T07:47:06Z",
    "authors": [
      "Chengshen Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.09286v3",
    "title": "On the Computational Power of Transformers and its Implications in\n  Sequence Modeling",
    "summary": "Transformers are being used extensively across several sequence modeling\ntasks. Significant research effort has been devoted to experimentally probe the\ninner workings of Transformers. However, our conceptual and theoretical\nunderstanding of their power and inherent limitations is still nascent. In\nparticular, the roles of various components in Transformers such as positional\nencodings, attention heads, residual connections, and feedforward networks, are\nnot clear. In this paper, we take a step towards answering these questions. We\nanalyze the computational power as captured by Turing-completeness. We first\nprovide an alternate and simpler proof to show that vanilla Transformers are\nTuring-complete and then we prove that Transformers with only positional\nmasking and without any positional encoding are also Turing-complete. We\nfurther analyze the necessity of each component for the Turing-completeness of\nthe network; interestingly, we find that a particular type of residual\nconnection is necessary. We demonstrate the practical implications of our\nresults via experiments on machine translation and synthetic tasks.",
    "published": "2020-06-16T16:27:56Z",
    "updated": "2020-10-10T13:34:20Z",
    "authors": [
      "Satwik Bhattamishra",
      "Arkil Patel",
      "Navin Goyal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2006.16722v1",
    "title": "Correction of Faulty Background Knowledge based on Condition Aware and\n  Revise Transformer for Question Answering",
    "summary": "The study of question answering has received increasing attention in recent\nyears. This work focuses on providing an answer that compatible with both user\nintent and conditioning information corresponding to the question, such as\ndelivery status and stock information in e-commerce. However, these conditions\nmay be wrong or incomplete in real-world applications. Although existing\nquestion answering systems have considered the external information, such as\ncategorical attributes and triples in knowledge base, they all assume that the\nexternal information is correct and complete. To alleviate the effect of\ndefective condition values, this paper proposes condition aware and revise\nTransformer (CAR-Transformer). CAR-Transformer (1) revises each condition value\nbased on the whole conversation and original conditions values, and (2) it\nencodes the revised conditions and utilizes the conditions embedding to select\nan answer. Experimental results on a real-world customer service dataset\ndemonstrate that the CAR-Transformer can still select an appropriate reply when\nconditions corresponding to the question exist wrong or missing values, and\nsubstantially outperforms baseline models on automatic and human evaluations.\nThe proposed CAR-Transformer can be extended to other NLP tasks which need to\nconsider conditioning information.",
    "published": "2020-06-30T12:24:35Z",
    "updated": "2020-06-30T12:24:35Z",
    "authors": [
      "Xinyan Zhao",
      "Xiao Feng",
      "Haoming Zhong",
      "Jun Yao",
      "Huanhuan Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2008.04481v1",
    "title": "Transformer with Bidirectional Decoder for Speech Recognition",
    "summary": "Attention-based models have made tremendous progress on end-to-end automatic\nspeech recognition(ASR) recently. However, the conventional transformer-based\napproaches usually generate the sequence results token by token from left to\nright, leaving the right-to-left contexts unexploited. In this work, we\nintroduce a bidirectional speech transformer to utilize the different\ndirectional contexts simultaneously. Specifically, the outputs of our proposed\ntransformer include a left-to-right target, and a right-to-left target. In\ninference stage, we use the introduced bidirectional beam search method, which\ncan not only generate left-to-right candidates but also generate right-to-left\ncandidates, and determine the best hypothesis by the score.\n  To demonstrate our proposed speech transformer with a bidirectional\ndecoder(STBD), we conduct extensive experiments on the AISHELL-1 dataset. The\nresults of experiments show that STBD achieves a 3.6\\% relative CER\nreduction(CERR) over the unidirectional speech transformer baseline. Besides,\nthe strongest model in this paper called STBD-Big can achieve 6.64\\% CER on the\ntest set, without language model rescoring and any extra data augmentation\nstrategies.",
    "published": "2020-08-11T02:12:42Z",
    "updated": "2020-08-11T02:12:42Z",
    "authors": [
      "Xi Chen",
      "Songyang Zhang",
      "Dandan Song",
      "Peng Ouyang",
      "Shouyi Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2009.11264v2",
    "title": "On the Ability and Limitations of Transformers to Recognize Formal\n  Languages",
    "summary": "Transformers have supplanted recurrent models in a large number of NLP tasks.\nHowever, the differences in their abilities to model different syntactic\nproperties remain largely unknown. Past works suggest that LSTMs generalize\nvery well on regular languages and have close connections with counter\nlanguages. In this work, we systematically study the ability of Transformers to\nmodel such languages as well as the role of its individual components in doing\nso. We first provide a construction of Transformers for a subclass of counter\nlanguages, including well-studied languages such as n-ary Boolean Expressions,\nDyck-1, and its generalizations. In experiments, we find that Transformers do\nwell on this subclass, and their learned mechanism strongly correlates with our\nconstruction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well\nonly on a subset of regular languages with degrading performance as we make\nlanguages more complex according to a well-known measure of complexity. Our\nanalysis also provides insights on the role of self-attention mechanism in\nmodeling certain behaviors and the influence of positional encoding schemes on\nthe learning and generalization abilities of the model.",
    "published": "2020-09-23T17:21:33Z",
    "updated": "2020-10-08T12:55:37Z",
    "authors": [
      "Satwik Bhattamishra",
      "Kabir Ahuja",
      "Navin Goyal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.00931v2",
    "title": "Point Transformer",
    "summary": "In this work, we present Point Transformer, a deep neural network that\noperates directly on unordered and unstructured point sets. We design Point\nTransformer to extract local and global features and relate both\nrepresentations by introducing the local-global attention mechanism, which aims\nto capture spatial point relations and shape information. For that purpose, we\npropose SortNet, as part of the Point Transformer, which induces input\npermutation invariance by selecting points based on a learned score. The output\nof Point Transformer is a sorted and permutation invariant feature list that\ncan directly be incorporated into common computer vision applications. We\nevaluate our approach on standard classification and part segmentation\nbenchmarks to demonstrate competitive results compared to the prior work. Code\nis publicly available at: https://github.com/engelnico/point-transformer",
    "published": "2020-11-02T12:26:14Z",
    "updated": "2021-10-14T10:51:39Z",
    "authors": [
      "Nico Engel",
      "Vasileios Belagiannis",
      "Klaus Dietmayer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.04432v2",
    "title": "Colorization Transformer",
    "summary": "We present the Colorization Transformer, a novel approach for diverse high\nfidelity image colorization based on self-attention. Given a grayscale image,\nthe colorization proceeds in three steps. We first use a conditional\nautoregressive transformer to produce a low resolution coarse coloring of the\ngrayscale image. Our architecture adopts conditional transformer layers to\neffectively condition grayscale input. Two subsequent fully parallel networks\nupsample the coarse colored low resolution image into a finely colored high\nresolution image. Sampling from the Colorization Transformer produces diverse\ncolorings whose fidelity outperforms the previous state-of-the-art on\ncolorising ImageNet based on FID results and based on a human evaluation in a\nMechanical Turk test. Remarkably, in more than 60% of cases human evaluators\nprefer the highest rated among three generated colorings over the ground truth.\nThe code and pre-trained checkpoints for Colorization Transformer are publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/coltran",
    "published": "2021-02-08T18:45:06Z",
    "updated": "2021-03-07T08:38:49Z",
    "authors": [
      "Manoj Kumar",
      "Dirk Weissenborn",
      "Nal Kalchbrenner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2102.04754v1",
    "title": "Bayesian Transformer Language Models for Speech Recognition",
    "summary": "State-of-the-art neural language models (LMs) represented by Transformers are\nhighly complex. Their use of fixed, deterministic parameter estimates fail to\naccount for model uncertainty and lead to over-fitting and poor generalization\nwhen given limited training data. In order to address these issues, this paper\nproposes a full Bayesian learning framework for Transformer LM estimation.\nEfficient variational inference based approaches are used to estimate the\nlatent parameter posterior distributions associated with different parts of the\nTransformer model architecture including multi-head self-attention, feed\nforward and embedding layers. Statistically significant word error rate (WER)\nreductions up to 0.5\\% absolute (3.18\\% relative) and consistent perplexity\ngains were obtained over the baseline Transformer LMs on state-of-the-art\nSwitchboard corpus trained LF-MMI factored TDNN systems with i-Vector speaker\nadaptation. Performance improvements were also obtained on a cross domain LM\nadaptation task requiring porting a Transformer LM trained on the Switchboard\nand Fisher data to a low-resource DementiaBank elderly speech corpus.",
    "published": "2021-02-09T10:55:27Z",
    "updated": "2021-02-09T10:55:27Z",
    "authors": [
      "Boyang Xue",
      "Jianwei Yu",
      "Junhao Xu",
      "Shansong Liu",
      "Shoukang Hu",
      "Zi Ye",
      "Mengzhe Geng",
      "Xunying Liu",
      "Helen Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.15168v3",
    "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating\n  Messenger Tokens",
    "summary": "Transformers have offered a new methodology of designing neural networks for\nvisual recognition. Compared to convolutional networks, Transformers enjoy the\nability of referring to global features at each stage, yet the attention module\nbrings higher computational overhead that obstructs the application of\nTransformers to process high-resolution visual data. This paper aims to\nalleviate the conflict between efficiency and flexibility, for which we propose\na specialized token for each region that serves as a messenger (MSG). Hence, by\nmanipulating these MSG tokens, one can flexibly exchange visual information\nacross regions and the computational complexity is reduced. We then integrate\nthe MSG token into a multi-scale architecture named MSG-Transformer. In\nstandard image classification and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU and CPU is accelerated.\nCode is available at https://github.com/hustvl/MSG-Transformer.",
    "published": "2021-05-31T17:16:42Z",
    "updated": "2022-03-25T13:25:08Z",
    "authors": [
      "Jiemin Fang",
      "Lingxi Xie",
      "Xinggang Wang",
      "Xiaopeng Zhang",
      "Wenyu Liu",
      "Qi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.03146v1",
    "title": "Oriented Object Detection with Transformer",
    "summary": "Object detection with Transformers (DETR) has achieved a competitive\nperformance over traditional detectors, such as Faster R-CNN. However, the\npotential of DETR remains largely unexplored for the more challenging task of\narbitrary-oriented object detection problem. We provide the first attempt and\nimplement Oriented Object DEtection with TRansformer ($\\bf O^2DETR$) based on\nan end-to-end network. The contributions of $\\rm O^2DETR$ include: 1) we\nprovide a new insight into oriented object detection, by applying Transformer\nto directly and efficiently localize objects without a tedious process of\nrotated anchors as in conventional detectors; 2) we design a simple but highly\nefficient encoder for Transformer by replacing the attention mechanism with\ndepthwise separable convolution, which can significantly reduce the memory and\ncomputational cost of using multi-scale features in the original Transformer;\n3) our $\\rm O^2DETR$ can be another new benchmark in the field of oriented\nobject detection, which achieves up to 3.85 mAP improvement over Faster R-CNN\nand RetinaNet. We simply fine-tune the head mounted on $\\rm O^2DETR$ in a\ncascaded architecture and achieve a competitive performance over SOTA in the\nDOTA dataset.",
    "published": "2021-06-06T14:57:17Z",
    "updated": "2021-06-06T14:57:17Z",
    "authors": [
      "Teli Ma",
      "Mingyuan Mao",
      "Honghui Zheng",
      "Peng Gao",
      "Xiaodi Wang",
      "Shumin Han",
      "Errui Ding",
      "Baochang Zhang",
      "David Doermann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.06295v2",
    "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
    "summary": "Transformers with linearised attention (''linear Transformers'') have\ndemonstrated the practical scalability and effectiveness of outer product-based\nFast Weight Programmers (FWPs) from the '90s. However, the original FWP\nformulation is more general than the one of linear Transformers: a slow neural\nnetwork (NN) continually reprograms the weights of a fast NN with arbitrary\narchitecture. In existing linear Transformers, both NNs are feedforward and\nconsist of a single layer. Here we explore new variations by adding recurrence\nto the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two\nsynthetic algorithmic tasks (code execution and sequential ListOps),\nWikitext-103 language models, and on the Atari 2600 2D game environment. Our\nmodels exhibit properties of Transformers and RNNs. In the reinforcement\nlearning setting, we report large improvements over LSTM in several Atari\ngames. Our code is public.",
    "published": "2021-06-11T10:32:11Z",
    "updated": "2021-10-26T19:41:12Z",
    "authors": [
      "Kazuki Irie",
      "Imanol Schlag",
      "RÃ³bert CsordÃ¡s",
      "JÃ¼rgen Schmidhuber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.07167v1",
    "title": "End-to-end Neural Diarization: From Transformer to Conformer",
    "summary": "We propose a new end-to-end neural diarization (EEND) system that is based on\nConformer, a recently proposed neural architecture that combines convolutional\nmappings and Transformer to model both local and global dependencies in speech.\nWe first show that data augmentation and convolutional subsampling layers\nenhance the original self-attentive EEND in the Transformer-based EEND, and\nthen Conformer gives an additional gain over the Transformer-based EEND.\nHowever, we notice that the Conformer-based EEND does not generalize as well\nfrom simulated to real conversation data as the Transformer-based model. This\nleads us to quantify the mismatch between simulated data and real speaker\nbehavior in terms of temporal statistics reflecting turn-taking between\nspeakers, and investigate its correlation with diarization error. By mixing\nsimulated and real data in EEND training, we mitigate the mismatch further,\nwith Conformer-based EEND achieving 24% error reduction over the baseline\nSA-EEND system, and 10% improvement over the best augmented Transformer-based\nsystem, on two-speaker CALLHOME data.",
    "published": "2021-06-14T05:21:08Z",
    "updated": "2021-06-14T05:21:08Z",
    "authors": [
      "Yi Chieh Liu",
      "Eunjung Han",
      "Chul Lee",
      "Andreas Stolcke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2107.02039v1",
    "title": "Power Law Graph Transformer for Machine Translation and Representation\n  Learning",
    "summary": "We present the Power Law Graph Transformer, a transformer model with well\ndefined deductive and inductive tasks for prediction and representation\nlearning. The deductive task learns the dataset level (global) and instance\nlevel (local) graph structures in terms of learnable power law distribution\nparameters. The inductive task outputs the prediction probabilities using the\ndeductive task output, similar to a transductive model. We trained our model\nwith Turkish-English and Portuguese-English datasets from TED talk transcripts\nfor machine translation and compared the model performance and characteristics\nto a transformer model with scaled dot product attention trained on the same\nexperimental setup. We report BLEU scores of $17.79$ and $28.33$ on the\nTurkish-English and Portuguese-English translation tasks with our model,\nrespectively. We also show how a duality between a quantization set and\nN-dimensional manifold representation can be leveraged to transform between\nlocal and global deductive-inductive outputs using successive application of\nlinear and non-linear transformations end-to-end.",
    "published": "2021-06-27T15:59:37Z",
    "updated": "2021-06-27T15:59:37Z",
    "authors": [
      "Burc Gokden"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2108.13015v1",
    "title": "Exploring and Improving Mobile Level Vision Transformers",
    "summary": "We study the vision transformer structure in the mobile level in this paper,\nand find a dramatic performance drop. We analyze the reason behind this\nphenomenon, and propose a novel irregular patch embedding module and adaptive\npatch fusion module to improve the performance. We conjecture that the vision\ntransformer blocks (which consist of multi-head attention and feed-forward\nnetwork) are more suitable to handle high-level information than low-level\nfeatures. The irregular patch embedding module extracts patches that contain\nrich high-level information with different receptive fields. The transformer\nblocks can obtain the most useful information from these irregular patches.\nThen the processed patches pass the adaptive patch merging module to get the\nfinal features for the classifier. With our proposed improvements, the\ntraditional uniform vision transformer structure can achieve state-of-the-art\nresults in mobile level. We improve the DeiT baseline by more than 9\\% under\nthe mobile-level settings and surpass other transformer architectures like Swin\nand CoaT by a large margin.",
    "published": "2021-08-30T06:42:49Z",
    "updated": "2021-08-30T06:42:49Z",
    "authors": [
      "Pengguang Chen",
      "Yixin Chen",
      "Shu Liu",
      "Mingchang Yang",
      "Jiaya Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.02270v1",
    "title": "Transformer Assisted Convolutional Network for Cell Instance\n  Segmentation",
    "summary": "Region proposal based methods like R-CNN and Faster R-CNN models have proven\nto be extremely successful in object detection and segmentation tasks.\nRecently, Transformers have also gained popularity in the domain of Computer\nVision, and are being utilised to improve the performance of conventional\nmodels. In this paper, we present a relatively new transformer based approach\nto enhance the performance of the conventional convolutional feature extractor\nin the existing region proposal based methods. Our approach merges the\nconvolutional feature maps with transformer-based token embeddings by applying\na projection operation similar to self-attention in transformers. The results\nof our experiments show that transformer assisted feature extractor achieves a\nsignificant improvement in mIoU (mean Intersection over Union) scores compared\nto vanilla convolutional backbone.",
    "published": "2021-10-05T18:18:31Z",
    "updated": "2021-10-05T18:18:31Z",
    "authors": [
      "Deepanshu Pandey",
      "Pradyumna Gupta",
      "Sumit Bhattacharya",
      "Aman Sinha",
      "Rohit Agarwal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.02402v1",
    "title": "Language Modeling using LMUs: 10x Better Data Efficiency or Improved\n  Scaling Compared to Transformers",
    "summary": "Recent studies have demonstrated that the performance of transformers on the\ntask of language modeling obeys a power-law relationship with model size over\nsix orders of magnitude. While transformers exhibit impressive scaling, their\nperformance hinges on processing large amounts of data, and their computational\nand memory requirements grow quadratically with sequence length. Motivated by\nthese considerations, we construct a Legendre Memory Unit based model that\nintroduces a general prior for sequence processing and exhibits an $O(n)$ and\n$O(n \\ln n)$ (or better) dependency for memory and computation respectively.\nOver three orders of magnitude, we show that our new architecture attains the\nsame accuracy as transformers with 10x fewer tokens. We also show that for the\nsame amount of training our model improves the loss over transformers about as\nmuch as transformers improve over LSTMs. Additionally, we demonstrate that\nadding global self-attention complements our architecture and the augmented\nmodel improves performance even further.",
    "published": "2021-10-05T23:20:37Z",
    "updated": "2021-10-05T23:20:37Z",
    "authors": [
      "Narsimha Chilkuri",
      "Eric Hunsberger",
      "Aaron Voelker",
      "Gurshaant Malik",
      "Chris Eliasmith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.04020v2",
    "title": "Pathologies in priors and inference for Bayesian transformers",
    "summary": "In recent years, the transformer has established itself as a workhorse in\nmany applications ranging from natural language processing to reinforcement\nlearning. Similarly, Bayesian deep learning has become the gold-standard for\nuncertainty estimation in safety-critical applications, where robustness and\ncalibration are crucial. Surprisingly, no successful attempts to improve\ntransformer models in terms of predictive uncertainty using Bayesian inference\nexist. In this work, we study this curiously underpopulated area of Bayesian\ntransformers. We find that weight-space inference in transformers does not work\nwell, regardless of the approximate posterior. We also find that the prior is\nat least partially at fault, but that it is very hard to find well-specified\nweight priors for these models. We hypothesize that these problems stem from\nthe complexity of obtaining a meaningful mapping from weight-space to\nfunction-space distributions in the transformer. Therefore, moving closer to\nfunction-space, we propose a novel method based on the implicit\nreparameterization of the Dirichlet distribution to apply variational inference\ndirectly to the attention weights. We find that this proposed method performs\ncompetitively with our baselines.",
    "published": "2021-10-08T10:35:27Z",
    "updated": "2021-10-15T09:44:21Z",
    "authors": [
      "Tristan Cinquin",
      "Alexander Immer",
      "Max Horn",
      "Vincent Fortuin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.12942v2",
    "title": "DocTr: Document Image Transformer for Geometric Unwarping and\n  Illumination Correction",
    "summary": "In this work, we propose a new framework, called Document Image Transformer\n(DocTr), to address the issue of geometry and illumination distortion of the\ndocument images. Specifically, DocTr consists of a geometric unwarping\ntransformer and an illumination correction transformer. By setting a set of\nlearned query embedding, the geometric unwarping transformer captures the\nglobal context of the document image by self-attention mechanism and decodes\nthe pixel-wise displacement solution to correct the geometric distortion. After\ngeometric unwarping, our illumination correction transformer further removes\nthe shading artifacts to improve the visual quality and OCR accuracy. Extensive\nevaluations are conducted on several datasets, and superior results are\nreported against the state-of-the-art methods. Remarkably, our DocTr achieves\n20.02% Character Error Rate (CER), a 15% absolute improvement over the\nstate-of-the-art methods. Moreover, it also shows high efficiency on running\ntime and parameter count. The results will be available at\nhttps://github.com/fh2019ustc/DocTr for further comparison.",
    "published": "2021-10-25T13:27:10Z",
    "updated": "2022-10-08T06:29:24Z",
    "authors": [
      "Hao Feng",
      "Yuechen Wang",
      "Wengang Zhou",
      "Jiajun Deng",
      "Houqiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.00633v2",
    "title": "Transformer Grammars: Augmenting Transformer Language Models with\n  Syntactic Inductive Biases at Scale",
    "summary": "We introduce Transformer Grammars (TGs), a novel class of Transformer\nlanguage models that combine (i) the expressive power, scalability, and strong\nperformance of Transformers and (ii) recursive syntactic compositions, which\nhere are implemented through a special attention mask and deterministic\ntransformation of the linearized tree. We find that TGs outperform various\nstrong baselines on sentence-level language modeling perplexity, as well as on\nmultiple syntax-sensitive language modeling evaluation metrics. Additionally,\nwe find that the recursive syntactic composition bottleneck which represents\neach sentence as a single vector harms perplexity on document-level language\nmodeling, providing evidence that a different kind of memory mechanism -- one\nthat is independent of composed syntactic representations -- plays an important\nrole in current successful models of long text.",
    "published": "2022-03-01T17:22:31Z",
    "updated": "2022-12-06T15:20:14Z",
    "authors": [
      "Laurent Sartran",
      "Samuel Barrett",
      "Adhiguna Kuncoro",
      "MiloÅ¡ StanojeviÄ",
      "Phil Blunsom",
      "Chris Dyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2203.06850v3",
    "title": "Efficient Language Modeling with Sparse all-MLP",
    "summary": "All-MLP architectures have attracted increasing interest as an alternative to\nattention-based models. In NLP, recent work like gMLP shows that all-MLPs can\nmatch Transformers in language modeling, but still lag behind in downstream\ntasks. In this work, we analyze the limitations of MLPs in expressiveness, and\npropose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature\nand input (token) dimensions. Such sparse all-MLPs significantly increase model\ncapacity and expressiveness while keeping the compute constant. We address\ncritical challenges in incorporating conditional computation with two routing\nstrategies. The proposed sparse all-MLP improves language modeling perplexity\nand obtains up to 2$\\times$ improvement in training efficiency compared to both\nTransformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH\nLayers) as well as dense Transformers and all-MLPs. Finally, we evaluate its\nzero-shot in-context learning performance on six downstream tasks, and find\nthat it surpasses Transformer-based MoEs and dense Transformers.",
    "published": "2022-03-14T04:32:19Z",
    "updated": "2022-05-31T20:28:09Z",
    "authors": [
      "Ping Yu",
      "Mikel Artetxe",
      "Myle Ott",
      "Sam Shleifer",
      "Hongyu Gong",
      "Ves Stoyanov",
      "Xian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.12602v1",
    "title": "VTP: Volumetric Transformer for Multi-view Multi-person 3D Pose\n  Estimation",
    "summary": "This paper presents Volumetric Transformer Pose estimator (VTP), the first 3D\nvolumetric transformer framework for multi-view multi-person 3D human pose\nestimation. VTP aggregates features from 2D keypoints in all camera views and\ndirectly learns the spatial relationships in the 3D voxel space in an\nend-to-end fashion. The aggregated 3D features are passed through 3D\nconvolutions before being flattened into sequential embeddings and fed into a\ntransformer. A residual structure is designed to further improve the\nperformance. In addition, the sparse Sinkhorn attention is empowered to reduce\nthe memory cost, which is a major bottleneck for volumetric representations,\nwhile also achieving excellent performance. The output of the transformer is\nagain concatenated with 3D convolutional features by a residual design. The\nproposed VTP framework integrates the high performance of the transformer with\nvolumetric representations, which can be used as a good alternative to the\nconvolutional backbones. Experiments on the Shelf, Campus and CMU Panoptic\nbenchmarks show promising results in terms of both Mean Per Joint Position\nError (MPJPE) and Percentage of Correctly estimated Parts (PCP). Our code will\nbe available.",
    "published": "2022-05-25T09:26:42Z",
    "updated": "2022-05-25T09:26:42Z",
    "authors": [
      "Yuxing Chen",
      "Renshu Gu",
      "Ouhan Huang",
      "Gangyong Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.06235v2",
    "title": "Entry-Flipped Transformer for Inference and Prediction of Participant\n  Behavior",
    "summary": "Some group activities, such as team sports and choreographed dances, involve\nclosely coupled interaction between participants. Here we investigate the tasks\nof inferring and predicting participant behavior, in terms of motion paths and\nactions, under such conditions. We narrow the problem to that of estimating how\na set target participants react to the behavior of other observed participants.\nOur key idea is to model the spatio-temporal relations among participants in a\nmanner that is robust to error accumulation during frame-wise inference and\nprediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),\nwhich models the relations of participants by attention mechanisms on both\nspatial and temporal domains. Unlike typical transformers, we tackle the\nproblem of error accumulation by flipping the order of query, key, and value\nentries, to increase the importance and fidelity of observed features in the\ncurrent frame. Comparative experiments show that our EF-Transformer achieves\nthe best performance on a newly-collected tennis doubles dataset, a Ceilidh\ndance dataset, and two pedestrian datasets. Furthermore, it is also\ndemonstrated that our EF-Transformer is better at limiting accumulated errors\nand recovering from wrong estimations.",
    "published": "2022-07-13T14:31:09Z",
    "updated": "2022-07-14T12:35:04Z",
    "authors": [
      "Bo Hu",
      "Tat-Jen Cham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.05654v1",
    "title": "Efficient Joint Detection and Multiple Object Tracking with Spatially\n  Aware Transformer",
    "summary": "We propose a light-weight and highly efficient Joint Detection and Tracking\npipeline for the task of Multi-Object Tracking using a fully-transformer\narchitecture. It is a modified version of TransTrack, which overcomes the\ncomputational bottleneck associated with its design, and at the same time,\nachieves state-of-the-art MOTA score of 73.20%. The model design is driven by a\ntransformer based backbone instead of CNN, which is highly scalable with the\ninput resolution. We also propose a drop-in replacement for Feed Forward\nNetwork of transformer encoder layer, by using Butterfly Transform Operation to\nperform channel fusion and depth-wise convolution to learn spatial context\nwithin the feature maps, otherwise missing within the attention maps of the\ntransformer. As a result of our modifications, we reduce the overall model size\nof TransTrack by 58.73% and the complexity by 78.72%. Therefore, we expect our\ndesign to provide novel perspectives for architecture optimization in future\nresearch related to multi-object tracking.",
    "published": "2022-11-09T07:19:33Z",
    "updated": "2022-11-09T07:19:33Z",
    "authors": [
      "Siddharth Sagar Nijhawan",
      "Leo Hoshikawa",
      "Atsushi Irie",
      "Masakazu Yoshimura",
      "Junji Otsuka",
      "Takeshi Ohashi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.11962v3",
    "title": "Transformation-Equivariant 3D Object Detection for Autonomous Driving",
    "summary": "3D object detection received increasing attention in autonomous driving\nrecently. Objects in 3D scenes are distributed with diverse orientations.\nOrdinary detectors do not explicitly model the variations of rotation and\nreflection transformations. Consequently, large networks and extensive data\naugmentation are required for robust detection. Recent equivariant networks\nexplicitly model the transformation variations by applying shared networks on\nmultiple transformed point clouds, showing great potential in object geometry\nmodeling. However, it is difficult to apply such networks to 3D object\ndetection in autonomous driving due to its large computation cost and slow\nreasoning speed. In this work, we present TED, an efficient\nTransformation-Equivariant 3D Detector to overcome the computation cost and\nspeed issues. TED first applies a sparse convolution backbone to extract\nmulti-channel transformation-equivariant voxel features; and then aligns and\naggregates these equivariant features into lightweight and compact\nrepresentations for high-performance 3D object detection. On the highly\ncompetitive KITTI 3D car detection leaderboard, TED ranked 1st among all\nsubmissions with competitive efficiency.",
    "published": "2022-11-22T02:51:56Z",
    "updated": "2022-12-01T08:00:16Z",
    "authors": [
      "Hai Wu",
      "Chenglu Wen",
      "Wei Li",
      "Xin Li",
      "Ruigang Yang",
      "Cheng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.14730v2",
    "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
    "summary": "We propose an efficient design of Transformer-based models for multivariate\ntime series forecasting and self-supervised representation learning. It is\nbased on two key components: (i) segmentation of time series into\nsubseries-level patches which are served as input tokens to Transformer; (ii)\nchannel-independence where each channel contains a single univariate time\nseries that shares the same embedding and Transformer weights across all the\nseries. Patching design naturally has three-fold benefit: local semantic\ninformation is retained in the embedding; computation and memory usage of the\nattention maps are quadratically reduced given the same look-back window; and\nthe model can attend longer history. Our channel-independent patch time series\nTransformer (PatchTST) can improve the long-term forecasting accuracy\nsignificantly when compared with that of SOTA Transformer-based models. We also\napply our model to self-supervised pre-training tasks and attain excellent\nfine-tuning performance, which outperforms supervised training on large\ndatasets. Transferring of masked pre-trained representation on one dataset to\nothers also produces SOTA forecasting accuracy. Code is available at:\nhttps://github.com/yuqinie98/PatchTST.",
    "published": "2022-11-27T05:15:42Z",
    "updated": "2023-03-05T22:11:56Z",
    "authors": [
      "Yuqi Nie",
      "Nam H. Nguyen",
      "Phanwadee Sinthong",
      "Jayant Kalagnanam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.11115v1",
    "title": "What Makes for Good Tokenizers in Vision Transformer?",
    "summary": "The architecture of transformers, which recently witness booming applications\nin vision tasks, has pivoted against the widespread convolutional paradigm.\nRelying on the tokenization process that splits inputs into multiple tokens,\ntransformers are capable of extracting their pairwise relationships using\nself-attention. While being the stemming building block of transformers, what\nmakes for a good tokenizer has not been well understood in computer vision. In\nthis work, we investigate this uncharted problem from an information trade-off\nperspective. In addition to unifying and understanding existing structural\nmodifications, our derivation leads to better design strategies for vision\ntokenizers. The proposed Modulation across Tokens (MoTo) incorporates\ninter-token modeling capability through normalization. Furthermore, a\nregularization objective TokenProp is embraced in the standard training regime.\nThrough extensive experiments on various transformer architectures, we observe\nboth improved performance and intriguing properties of these two plug-and-play\ndesigns with negligible computational overhead. These observations further\nindicate the importance of the commonly-omitted designs of tokenizers in vision\ntransformer.",
    "published": "2022-12-21T15:51:43Z",
    "updated": "2022-12-21T15:51:43Z",
    "authors": [
      "Shengju Qian",
      "Yi Zhu",
      "Wenbo Li",
      "Mu Li",
      "Jiaya Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.07434v1",
    "title": "Masked Pre-Training of Transformers for Histology Image Analysis",
    "summary": "In digital pathology, whole slide images (WSIs) are widely used for\napplications such as cancer diagnosis and prognosis prediction. Visual\ntransformer models have recently emerged as a promising method for encoding\nlarge regions of WSIs while preserving spatial relationships among patches.\nHowever, due to the large number of model parameters and limited labeled data,\napplying transformer models to WSIs remains challenging. Inspired by masked\nlanguage models, we propose a pretext task for training the transformer model\nwithout labeled data to address this problem. Our model, MaskHIT, uses the\ntransformer output to reconstruct masked patches and learn representative\nhistological features based on their positions and visual features. The\nexperimental results demonstrate that MaskHIT surpasses various multiple\ninstance learning approaches by 3% and 2% on survival prediction and cancer\nsubtype classification tasks, respectively. Furthermore, MaskHIT also\noutperforms two of the most recent state-of-the-art transformer-based methods.\nFinally, a comparison between the attention maps generated by the MaskHIT model\nwith pathologist's annotations indicates that the model can accurately identify\nclinically relevant histological structures in each task.",
    "published": "2023-04-14T23:56:49Z",
    "updated": "2023-04-14T23:56:49Z",
    "authors": [
      "Shuai Jiang",
      "Liesbeth Hondelink",
      "Arief A. Suriawinata",
      "Saeed Hassanpour"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.07521v1",
    "title": "AGFormer: Efficient Graph Representation with Anchor-Graph Transformer",
    "summary": "To alleviate the local receptive issue of GCN, Transformers have been\nexploited to capture the long range dependences of nodes for graph data\nrepresentation and learning. However, existing graph Transformers generally\nemploy regular self-attention module for all node-to-node message passing which\nneeds to learn the affinities/relationships between all node's pairs, leading\nto high computational cost issue. Also, they are usually sensitive to graph\nnoises. To overcome this issue, we propose a novel graph Transformer\narchitecture, termed Anchor Graph Transformer (AGFormer), by leveraging an\nanchor graph model. To be specific, AGFormer first obtains some representative\nanchors and then converts node-to-node message passing into anchor-to-anchor\nand anchor-to-node message passing process. Thus, AGFormer performs much more\nefficiently and also robustly than regular node-to-node Transformers. Extensive\nexperiments on several benchmark datasets demonstrate the effectiveness and\nbenefits of proposed AGFormer.",
    "published": "2023-05-12T14:35:42Z",
    "updated": "2023-05-12T14:35:42Z",
    "authors": [
      "Bo Jiang",
      "Fei Xu",
      "Ziyan Zhang",
      "Jin Tang",
      "Feiping Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.10502v2",
    "title": "EENED: End-to-End Neural Epilepsy Detection based on Convolutional\n  Transformer",
    "summary": "Recently Transformer and Convolution neural network (CNN) based models have\nshown promising results in EEG signal processing. Transformer models can\ncapture the global dependencies in EEG signals through a self-attention\nmechanism, while CNN models can capture local features such as sawtooth waves.\nIn this work, we propose an end-to-end neural epilepsy detection model, EENED,\nthat combines CNN and Transformer. Specifically, by introducing the convolution\nmodule into the Transformer encoder, EENED can learn the time-dependent\nrelationship of the patient's EEG signal features and notice local EEG abnormal\nmutations closely related to epilepsy, such as the appearance of spikes and the\nsprinkling of sharp and slow waves. Our proposed framework combines the ability\nof Transformer and CNN to capture different scale features of EEG signals and\nholds promise for improving the accuracy and reliability of epilepsy detection.\nOur source code will be released soon on GitHub.",
    "published": "2023-05-17T18:16:33Z",
    "updated": "2023-09-08T05:15:57Z",
    "authors": [
      "Chenyu Liu",
      "Xinliang Zhou",
      "Yang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.19365v1",
    "title": "Vision Transformers for Mobile Applications: A Short Survey",
    "summary": "Vision Transformers (ViTs) have demonstrated state-of-the-art performance on\nmany Computer Vision Tasks. Unfortunately, deploying these large-scale ViTs is\nresource-consuming and impossible for many mobile devices. While most in the\ncommunity are building for larger and larger ViTs, we ask a completely opposite\nquestion: How small can a ViT be within the tradeoffs of accuracy and inference\nlatency that make it suitable for mobile deployment? We look into a few ViTs\nspecifically designed for mobile applications and observe that they modify the\ntransformer's architecture or are built around the combination of CNN and\ntransformer. Recent work has also attempted to create sparse ViT networks and\nproposed alternatives to the attention module. In this paper, we study these\narchitectures, identify the challenges and analyze what really makes a vision\ntransformer suitable for mobile applications. We aim to serve as a baseline for\nfuture research direction and hopefully lay the foundation to choose the\nexemplary vision transformer architecture for your application running on\nmobile devices.",
    "published": "2023-05-30T19:12:08Z",
    "updated": "2023-05-30T19:12:08Z",
    "authors": [
      "Nahid Alam",
      "Steven Kolawole",
      "Simardeep Sethi",
      "Nishant Bansali",
      "Karina Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.17000v1",
    "title": "MotionTrack: End-to-End Transformer-based Multi-Object Tracing with\n  LiDAR-Camera Fusion",
    "summary": "Multiple Object Tracking (MOT) is crucial to autonomous vehicle perception.\nEnd-to-end transformer-based algorithms, which detect and track objects\nsimultaneously, show great potential for the MOT task. However, most existing\nmethods focus on image-based tracking with a single object category. In this\npaper, we propose an end-to-end transformer-based MOT algorithm (MotionTrack)\nwith multi-modality sensor inputs to track objects with multiple classes. Our\nobjective is to establish a transformer baseline for the MOT in an autonomous\ndriving environment. The proposed algorithm consists of a transformer-based\ndata association (DA) module and a transformer-based query enhancement module\nto achieve MOT and Multiple Object Detection (MOD) simultaneously. The\nMotionTrack and its variations achieve better results (AMOTA score at 0.55) on\nthe nuScenes dataset compared with other classical baseline models, such as the\nAB3DMOT, the CenterTrack, and the probabilistic 3D Kalman filter. In addition,\nwe prove that a modified attention mechanism can be utilized for DA to\naccomplish the MOT, and aggregate history features to enhance the MOD\nperformance.",
    "published": "2023-06-29T15:00:12Z",
    "updated": "2023-06-29T15:00:12Z",
    "authors": [
      "Ce Zhang",
      "Chengjie Zhang",
      "Yiluan Guo",
      "Lingji Chen",
      "Michael Happold"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.00293v2",
    "title": "AutoST: Training-free Neural Architecture Search for Spiking\n  Transformers",
    "summary": "Spiking Transformers have gained considerable attention because they achieve\nboth the energy efficiency of Spiking Neural Networks (SNNs) and the high\ncapacity of Transformers. However, the existing Spiking Transformer\narchitectures, derived from Artificial Neural Networks (ANNs), exhibit a\nnotable architectural gap, resulting in suboptimal performance compared to\ntheir ANN counterparts. Manually discovering optimal architectures is\ntime-consuming. To address these limitations, we introduce AutoST, a\ntraining-free NAS method for Spiking Transformers, to rapidly identify\nhigh-performance Spiking Transformer architectures. Unlike existing\ntraining-free NAS methods, which struggle with the non-differentiability and\nhigh sparsity inherent in SNNs, we propose to utilize Floating-Point Operations\n(FLOPs) as a performance metric, which is independent of model computations and\ntraining dynamics, leading to a stronger correlation with performance. Our\nextensive experiments show that AutoST models outperform state-of-the-art\nmanually or automatically designed SNN architectures on static and neuromorphic\ndatasets. Full code, model, and data are released for reproduction.",
    "published": "2023-07-01T10:19:52Z",
    "updated": "2023-12-14T00:58:03Z",
    "authors": [
      "Ziqing Wang",
      "Qidong Zhao",
      "Jinku Cui",
      "Xu Liu",
      "Dongkuan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.00197v1",
    "title": "Performance Evaluation of Swin Vision Transformer Model using Gradient\n  Accumulation Optimization Technique",
    "summary": "Vision Transformers (ViTs) have emerged as a promising approach for visual\nrecognition tasks, revolutionizing the field by leveraging the power of\ntransformer-based architectures. Among the various ViT models, Swin\nTransformers have gained considerable attention due to their hierarchical\ndesign and ability to capture both local and global visual features\neffectively. This paper evaluates the performance of Swin ViT model using\ngradient accumulation optimization (GAO) technique. We investigate the impact\nof gradient accumulation optimization technique on the model's accuracy and\ntraining time. Our experiments show that applying the GAO technique leads to a\nsignificant decrease in the accuracy of the Swin ViT model, compared to the\nstandard Swin Transformer model. Moreover, we detect a significant increase in\nthe training time of the Swin ViT model when GAO model is applied. These\nfindings suggest that applying the GAO technique may not be suitable for the\nSwin ViT model, and concern should be undertaken when using GAO technique for\nother transformer-based models.",
    "published": "2023-07-31T23:30:16Z",
    "updated": "2023-07-31T23:30:16Z",
    "authors": [
      "Sanad Aburass",
      "Osama Dorgham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.09293v1",
    "title": "How important are specialized transforms in Neural Operators?",
    "summary": "Simulating physical systems using Partial Differential Equations (PDEs) has\nbecome an indispensible part of modern industrial process optimization.\nTraditionally, numerical solvers have been used to solve the associated PDEs,\nhowever recently Transform-based Neural Operators such as the Fourier Neural\nOperator and Wavelet Neural Operator have received a lot of attention for their\npotential to provide fast solutions for systems of PDEs. In this work, we\ninvestigate the importance of the transform layers to the reported success of\ntransform based neural operators. In particular, we record the cost in terms of\nperformance, if all the transform layers are replaced by learnable linear\nlayers. Surprisingly, we observe that linear layers suffice to provide\nperformance comparable to the best-known transform-based layers and seem to do\nso with a compute time advantage as well. We believe that this observation can\nhave significant implications for future work on Neural Operators, and might\npoint to other sources of efficiencies for these architectures.",
    "published": "2023-08-18T04:35:13Z",
    "updated": "2023-08-18T04:35:13Z",
    "authors": [
      "Ritam Majumdar",
      "Shirish Karande",
      "Lovekesh Vig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.10061v1",
    "title": "Transformed-Linear Innovations Algorithm for Modeling and Forecasting of\n  Time Series Extremes",
    "summary": "The innovations algorithm is a classical recursive forecasting algorithm used\nin time series analysis. We develop the innovations algorithm for a class of\nnonnegative regularly varying time series models constructed via\ntransformed-linear arithmetic. In addition to providing the best linear\npredictor, the algorithm also enables us to estimate parameters of\ntransformed-linear regularly-varying moving average (MA) models, thus providing\na tool for modeling.\n  We first construct an inner product space of transformed-linear combinations\nof nonnegative regularly-varying random variables and prove its link to a\nHilbert space which allows us to employ the projection theorem, from which we\ndevelop the transformed-linear innovations algorithm. Turning our attention to\nthe class of transformed linear MA($\\infty$) models, we give results on\nparameter estimation and also show that this class of models is dense in the\nclass of possible tail pairwise dependence functions (TPDFs). We also develop\nan extremes analogue of the classical Wold decomposition. Simulation study\nshows that our class of models captures tail dependence for the GARCH(1,1)\nmodel and a Markov time series model, both of which are outside our class of\nmodels.",
    "published": "2023-09-18T18:17:07Z",
    "updated": "2023-09-18T18:17:07Z",
    "authors": [
      "Nehali Mhatre",
      "Daniel Cooley"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.01906v2",
    "title": "Simplifying Transformer Blocks",
    "summary": "A simple design recipe for deep Transformers is to compose identical building\nblocks. But standard transformer blocks are far from simple, interweaving\nattention and MLP sub-blocks with skip connections & normalisation layers in\nprecise arrangements. This complexity leads to brittle architectures, where\nseemingly minor changes can significantly reduce training speed, or render\nmodels untrainable.\n  In this work, we ask to what extent the standard transformer block can be\nsimplified? Combining signal propagation theory and empirical observations, we\nmotivate modifications that allow many block components to be removed with no\nloss of training speed, including skip connections, projection or value\nparameters, sequential sub-blocks and normalisation layers. In experiments on\nboth autoregressive decoder-only and BERT encoder-only models, our simplified\ntransformers emulate the per-update training speed and performance of standard\ntransformers, while enjoying 15% faster training throughput, and using 15%\nfewer parameters.",
    "published": "2023-11-03T13:30:52Z",
    "updated": "2024-05-31T11:14:16Z",
    "authors": [
      "Bobby He",
      "Thomas Hofmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.10763v1",
    "title": "Comparing Generalization in Learning with Limited Numbers of Exemplars:\n  Transformer vs. RNN in Attractor Dynamics",
    "summary": "ChatGPT, a widely-recognized large language model (LLM), has recently gained\nsubstantial attention for its performance scaling, attributed to the billions\nof web-sourced natural language sentences used for training. Its underlying\narchitecture, Transformer, has found applications across diverse fields,\nincluding video, audio signals, and robotic movement. %The crucial question\nthis raises concerns the Transformer's generalization-in-learning (GIL)\ncapacity. However, this raises a crucial question about Transformer's\ngeneralization in learning (GIL) capacity. Is ChatGPT's success chiefly due to\nthe vast dataset used for training, or is there more to the story? To\ninvestigate this, we compared Transformer's GIL capabilities with those of a\ntraditional Recurrent Neural Network (RNN) in tasks involving attractor\ndynamics learning. For performance evaluation, the Dynamic Time Warping (DTW)\nmethod has been employed. Our simulation results suggest that under conditions\nof limited data availability, Transformer's GIL abilities are markedly inferior\nto those of RNN.",
    "published": "2023-11-15T00:37:49Z",
    "updated": "2023-11-15T00:37:49Z",
    "authors": [
      "Rui Fukushima",
      "Jun Tani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.06937v3",
    "title": "Can a Transformer Represent a Kalman Filter?",
    "summary": "Transformers are a class of autoregressive deep learning architectures which\nhave recently achieved state-of-the-art performance in various vision,\nlanguage, and robotics tasks. We revisit the problem of Kalman Filtering in\nlinear dynamical systems and show that Transformers can approximate the Kalman\nFilter in a strong sense. Specifically, for any observable LTI system we\nconstruct an explicit causally-masked Transformer which implements the Kalman\nFilter, up to a small additive error which is bounded uniformly in time; we\ncall our construction the Transformer Filter. Our construction is based on a\ntwo-step reduction. We first show that a softmax self-attention block can\nexactly represent a Nadaraya-Watson kernel smoothing estimator with a Gaussian\nkernel. We then show that this estimator closely approximates the Kalman\nFilter. We also investigate how the Transformer Filter can be used for\nmeasurement-feedback control and prove that the resulting nonlinear controllers\nclosely approximate the performance of standard optimal control policies such\nas the LQG controller.",
    "published": "2023-12-12T02:13:50Z",
    "updated": "2024-05-18T03:53:15Z",
    "authors": [
      "Gautam Goel",
      "Peter Bartlett"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.08594v2",
    "title": "CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer",
    "summary": "Recent deep multi-view stereo (MVS) methods have widely incorporated\ntransformers into cascade network for high-resolution depth estimation,\nachieving impressive results. However, existing transformer-based methods are\nconstrained by their computational costs, preventing their extension to finer\nstages. In this paper, we propose a novel cross-scale transformer (CT) that\nprocesses feature representations at different stages without additional\ncomputation. Specifically, we introduce an adaptive matching-aware transformer\n(AMT) that employs different interactive attention combinations at multiple\nscales. This combined strategy enables our network to capture intra-image\ncontext information and enhance inter-image feature relationships. Besides, we\npresent a dual-feature guided aggregation (DFGA) that embeds the coarse global\nsemantic information into the finer cost volume construction to further\nstrengthen global and local feature awareness. Meanwhile, we design a feature\nmetric loss (FM Loss) that evaluates the feature bias before and after\ntransformation to reduce the impact of feature mismatch on depth estimation.\nExtensive experiments on DTU dataset and Tanks and Temples (T\\&T) benchmark\ndemonstrate that our method achieves state-of-the-art results. Code is\navailable at https://github.com/wscstrive/CT-MVSNet.",
    "published": "2023-12-14T01:33:18Z",
    "updated": "2024-02-02T01:41:21Z",
    "authors": [
      "Sicheng Wang",
      "Hao Jiang",
      "Lei Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.01855v1",
    "title": "Transformer Neural Autoregressive Flows",
    "summary": "Density estimation, a central problem in machine learning, can be performed\nusing Normalizing Flows (NFs). NFs comprise a sequence of invertible\ntransformations, that turn a complex target distribution into a simple one, by\nexploiting the change of variables theorem. Neural Autoregressive Flows (NAFs)\nand Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant\nmembers of the NF family. However, they suffer scalability issues and training\ninstability due to the constraints imposed on the network structure. In this\npaper, we propose a novel solution to these challenges by exploiting\ntransformers to define a new class of neural flows called Transformer Neural\nAutoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable\nas a separate input token, using attention masking to enforce an autoregressive\nconstraint. We take an amortization-inspired approach where the transformer\noutputs the parameters of an invertible transformation. The experimental\nresults demonstrate that T-NAFs consistently match or outperform NAFs and\nB-NAFs across multiple datasets from the UCI benchmark. Remarkably, T-NAFs\nachieve these results using an order of magnitude fewer parameters than\nprevious approaches, without composing multiple flows.",
    "published": "2024-01-03T17:51:16Z",
    "updated": "2024-01-03T17:51:16Z",
    "authors": [
      "Massimiliano Patacchiola",
      "Aliaksandra Shysheya",
      "Katja Hofmann",
      "Richard E. Turner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.09686v2",
    "title": "An Empirical Study on the Impact of Positional Encoding in\n  Transformer-based Monaural Speech Enhancement",
    "summary": "Transformer architecture has enabled recent progress in speech enhancement.\nSince Transformers are position-agostic, positional encoding is the de facto\nstandard component used to enable Transformers to distinguish the order of\nelements in a sequence. However, it remains unclear how positional encoding\nexactly impacts speech enhancement based on Transformer architectures. In this\npaper, we perform a comprehensive empirical study evaluating five positional\nencoding methods, i.e., Sinusoidal and learned absolute position embedding\n(APE), T5-RPE, KERPLE, as well as the Transformer without positional encoding\n(No-Pos), across both causal and noncausal configurations. We conduct extensive\nspeech enhancement experiments, involving spectral mapping and masking methods.\nOur findings establish that positional encoding is not quite helpful for the\nmodels in a causal configuration, which indicates that causal attention may\nimplicitly incorporate position information. In a noncausal configuration, the\nmodels significantly benefit from the use of positional encoding. In addition,\nwe find that among the four position embeddings, relative position embeddings\noutperform APEs.",
    "published": "2024-01-18T02:31:46Z",
    "updated": "2024-02-13T20:36:13Z",
    "authors": [
      "Qiquan Zhang",
      "Meng Ge",
      "Hongxu Zhu",
      "Eliathamby Ambikairajah",
      "Qi Song",
      "Zhaoheng Ni",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.05964v2",
    "title": "A Survey on Transformer Compression",
    "summary": "Transformer plays a vital role in the realms of natural language processing\n(NLP) and computer vision (CV), specially for constructing large language\nmodels (LLM) and large vision models (LVM). Model compression methods reduce\nthe memory and computational cost of Transformer, which is a necessary step to\nimplement large language/vision models on practical devices. Given the unique\narchitecture of Transformer, featuring alternative attention and feedforward\nneural network (FFN) modules, specific compression techniques are usually\nrequired. The efficiency of these compression methods is also paramount, as\nretraining large models on the entire training dataset is usually impractical.\nThis survey provides a comprehensive review of recent compression methods, with\na specific focus on their application to Transformer-based models. The\ncompression methods are primarily categorized into pruning, quantization,\nknowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV,\netc.). In each category, we discuss compression methods for both language and\nvision tasks, highlighting common underlying principles. Finally, we delve into\nthe relation between various compression methods, and discuss further\ndirections in this domain.",
    "published": "2024-02-05T12:16:28Z",
    "updated": "2024-04-07T13:03:58Z",
    "authors": [
      "Yehui Tang",
      "Yunhe Wang",
      "Jianyuan Guo",
      "Zhijun Tu",
      "Kai Han",
      "Hailin Hu",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.14180v2",
    "title": "Linear Transformers are Versatile In-Context Learners",
    "summary": "Recent research has demonstrated that transformers, particularly linear\nattention models, implicitly execute gradient-descent-like algorithms on data\nprovided in-context during their forward inference step. However, their\ncapability in handling more complex problems remains unexplored. In this paper,\nwe prove that each layer of a linear transformer maintains a weight vector for\nan implicit linear regression problem and can be interpreted as performing a\nvariant of preconditioned gradient descent. We also investigate the use of\nlinear transformers in a challenging scenario where the training data is\ncorrupted with different levels of noise. Remarkably, we demonstrate that for\nthis problem linear transformers discover an intricate and highly effective\noptimization algorithm, surpassing or matching in performance many reasonable\nbaselines. We analyze this algorithm and show that it is a novel approach\nincorporating momentum and adaptive rescaling based on noise levels. Our\nfindings show that even linear transformers possess the surprising ability to\ndiscover sophisticated optimization strategies.",
    "published": "2024-02-21T23:45:57Z",
    "updated": "2024-10-30T04:27:00Z",
    "authors": [
      "Max Vladymyrov",
      "Johannes von Oswald",
      "Mark Sandler",
      "Rong Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.15989v1",
    "title": "PIDformer: Transformer Meets Control Theory",
    "summary": "In this work, we address two main shortcomings of transformer architectures:\ninput corruption and rank collapse in their output representation. We unveil\nself-attention as an autonomous state-space model that inherently promotes\nsmoothness in its solutions, leading to lower-rank outputs and diminished\nrepresentation capacity. Moreover, the steady-state solution of the model is\nsensitive to input perturbations. We incorporate a\nProportional-Integral-Derivative (PID) closed-loop feedback control system with\na reference point into the model to improve robustness and representation\ncapacity. This integration aims to preserve high-frequency details while\nbolstering model stability, rendering it more noise-resilient. The resulting\ncontrolled state-space model is theoretically proven robust and adept at\naddressing the rank collapse. Motivated by this control framework, we derive a\nnovel class of transformers, PID-controlled Transformer (PIDformer), aimed at\nimproving robustness and mitigating the rank-collapse issue inherent in softmax\ntransformers. We empirically evaluate the model for advantages and robustness\nagainst baseline transformers across various practical tasks, including object\nclassification, image segmentation, and language modeling.",
    "published": "2024-02-25T05:04:51Z",
    "updated": "2024-02-25T05:04:51Z",
    "authors": [
      "Tam Nguyen",
      "CÃ©sar A. Uribe",
      "Tan M. Nguyen",
      "Richard G. Baraniuk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.19218v1",
    "title": "Memory-Augmented Generative Adversarial Transformers",
    "summary": "Conversational AI systems that rely on Large Language Models, like\nTransformers, have difficulty interweaving external data (like facts) with the\nlanguage they generate. Vanilla Transformer architectures are not designed for\nanswering factual questions with high accuracy. This paper investigates a\npossible route for addressing this problem. We propose to extend the standard\nTransformer architecture with an additional memory bank holding extra\ninformation (such as facts drawn from a knowledge base), and an extra attention\nlayer for addressing this memory. We add this augmented memory to a Generative\nAdversarial Network-inspired Transformer architecture. This setup allows for\nimplementing arbitrary felicity conditions on the generated language of the\nTransformer. We first demonstrate how this machinery can be deployed for\nhandling factual questions in goal-oriented dialogues. Secondly, we demonstrate\nthat our approach can be useful for applications like {\\it style adaptation} as\nwell: the adaptation of utterances according to certain stylistic (external)\nconstraints, like social properties of human interlocutors in dialogues.",
    "published": "2024-02-29T14:47:24Z",
    "updated": "2024-02-29T14:47:24Z",
    "authors": [
      "Stephan Raaijmakers",
      "Roos Bakker",
      "Anita Cremers",
      "Roy de Kleijn",
      "Tom Kouwenhoven",
      "Tessa Verhoef"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.02366v1",
    "title": "Human Evaluation of English--Irish Transformer-Based NMT",
    "summary": "In this study, a human evaluation is carried out on how hyperparameter\nsettings impact the quality of Transformer-based Neural Machine Translation\n(NMT) for the low-resourced English--Irish pair. SentencePiece models using\nboth Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations\nin model architectures included modifying the number of layers, evaluating the\noptimal number of heads for attention and testing various regularisation\ntechniques. The greatest performance improvement was recorded for a\nTransformer-optimized model with a 16k BPE subword model. Compared with a\nbaseline Recurrent Neural Network (RNN) model, a Transformer-optimized model\ndemonstrated a BLEU score improvement of 7.8 points. When benchmarked against\nGoogle Translate, our translation engines demonstrated significant\nimprovements. Furthermore, a quantitative fine-grained manual evaluation was\nconducted which compared the performance of machine translation systems. Using\nthe Multidimensional Quality Metrics (MQM) error taxonomy, a human evaluation\nof the error types generated by an RNN-based system and a Transformer-based\nsystem was explored. Our findings show the best-performing Transformer system\nsignificantly reduces both accuracy and fluency errors when compared with an\nRNN-based model.",
    "published": "2024-03-04T11:45:46Z",
    "updated": "2024-03-04T11:45:46Z",
    "authors": [
      "SÃ©amus Lankford",
      "Haithem Afli",
      "Andy Way"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.13347v2",
    "title": "vid-TLDR: Training Free Token merging for Light-weight Video Transformer",
    "summary": "Video Transformers have become the prevalent solution for various video\ndownstream tasks with superior expressive power and flexibility. However, these\nvideo transformers suffer from heavy computational costs induced by the massive\nnumber of tokens across the entire video frames, which has been the major\nbarrier to training the model. Further, the patches irrelevant to the main\ncontents, e.g., backgrounds, degrade the generalization performance of models.\nTo tackle these issues, we propose training free token merging for lightweight\nvideo Transformer (vid-TLDR) that aims to enhance the efficiency of video\nTransformers by merging the background tokens without additional training. For\nvid-TLDR, we introduce a novel approach to capture the salient regions in\nvideos only with the attention map. Further, we introduce the saliency-aware\ntoken merging strategy by dropping the background tokens and sharpening the\nobject scores. Our experiments show that vid-TLDR significantly mitigates the\ncomputational complexity of video Transformers while achieving competitive\nperformance compared to the base model without vid-TLDR. Code is available at\nhttps://github.com/mlvlab/vid-TLDR.",
    "published": "2024-03-20T07:15:22Z",
    "updated": "2024-03-30T09:45:38Z",
    "authors": [
      "Joonmyung Choi",
      "Sanghyeok Lee",
      "Jaewon Chu",
      "Minhyuk Choi",
      "Hyunwoo J. Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.12779v1",
    "title": "Transformer in Touch: A Survey",
    "summary": "The Transformer model, initially achieving significant success in the field\nof natural language processing, has recently shown great potential in the\napplication of tactile perception. This review aims to comprehensively outline\nthe application and development of Transformers in tactile technology. We first\nintroduce the two fundamental concepts behind the success of the Transformer:\nthe self-attention mechanism and large-scale pre-training. Then, we delve into\nthe application of Transformers in various tactile tasks, including but not\nlimited to object recognition, cross-modal generation, and object manipulation,\noffering a concise summary of the core methodologies, performance benchmarks,\nand design highlights. Finally, we suggest potential areas for further research\nand future work, aiming to generate more interest within the community, tackle\nexisting challenges, and encourage the use of Transformer models in the tactile\nfield.",
    "published": "2024-05-21T13:26:27Z",
    "updated": "2024-05-21T13:26:27Z",
    "authors": [
      "Jing Gao",
      "Ning Cheng",
      "Bin Fang",
      "Wenjuan Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.19501v1",
    "title": "MDS-ViTNet: Improving saliency prediction for Eye-Tracking with Vision\n  Transformer",
    "summary": "In this paper, we present a novel methodology we call MDS-ViTNet (Multi\nDecoder Saliency by Vision Transformer Network) for enhancing visual saliency\nprediction or eye-tracking. This approach holds significant potential for\ndiverse fields, including marketing, medicine, robotics, and retail. We propose\na network architecture that leverages the Vision Transformer, moving beyond the\nconventional ImageNet backbone. The framework adopts an encoder-decoder\nstructure, with the encoder utilizing a Swin transformer to efficiently embed\nmost important features. This process involves a Transfer Learning method,\nwherein layers from the Vision Transformer are converted by the Encoder\nTransformer and seamlessly integrated into a CNN Decoder. This methodology\nensures minimal information loss from the original input image. The decoder\nemploys a multi-decoding technique, utilizing dual decoders to generate two\ndistinct attention maps. These maps are subsequently combined into a singular\noutput via an additional CNN model. Our trained model MDS-ViTNet achieves\nstate-of-the-art results across several benchmarks. Committed to fostering\nfurther collaboration, we intend to make our code, models, and datasets\naccessible to the public.",
    "published": "2024-05-29T20:28:04Z",
    "updated": "2024-05-29T20:28:04Z",
    "authors": [
      "Polezhaev Ignat",
      "Goncharenko Igor",
      "Iurina Natalya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.03229v4",
    "title": "Global Clipper: Enhancing Safety and Reliability of Transformer-based\n  Object Detection Models",
    "summary": "As transformer-based object detection models progress, their impact in\ncritical sectors like autonomous vehicles and aviation is expected to grow.\nSoft errors causing bit flips during inference have significantly impacted DNN\nperformance, altering predictions. Traditional range restriction solutions for\nCNNs fall short for transformers. This study introduces the Global Clipper and\nGlobal Hybrid Clipper, effective mitigation strategies specifically designed\nfor transformer-based models. It significantly enhances their resilience to\nsoft errors and reduces faulty inferences to ~ 0\\%. We also detail extensive\ntesting across over 64 scenarios involving two transformer models (DINO-DETR\nand Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets,\ntotalling approximately 3.3 million inferences, to assess model robustness\ncomprehensively. Moreover, the paper explores unique aspects of attention\nblocks in transformers and their operational differences from CNNs.",
    "published": "2024-06-05T13:06:17Z",
    "updated": "2024-07-09T10:23:53Z",
    "authors": [
      "Qutub Syed Sha",
      "Michael Paulitsch",
      "Karthik Pattabiraman",
      "Korbinian Hagn",
      "Fabian Oboril",
      "Cornelius Buerkle",
      "Kay-Ulrich Scholl",
      "Gereon Hinz",
      "Alois Knoll"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.11401v1",
    "title": "An Exploration of Length Generalization in Transformer-Based Speech\n  Enhancement",
    "summary": "The use of Transformer architectures has facilitated remarkable progress in\nspeech enhancement. Training Transformers using substantially long speech\nutterances is often infeasible as self-attention suffers from quadratic\ncomplexity. It is a critical and unexplored challenge for a Transformer-based\nspeech enhancement model to learn from short speech utterances and generalize\nto longer ones. In this paper, we conduct comprehensive experiments to explore\nthe length generalization problem in speech enhancement with Transformer. Our\nfindings first establish that position embedding provides an effective\ninstrument to alleviate the impact of utterance length on Transformer-based\nspeech enhancement. Specifically, we explore four different position embedding\nschemes to enable length generalization. The results confirm the superiority of\nrelative position embeddings (RPEs) over absolute PE (APEs) in length\ngeneralization.",
    "published": "2024-06-17T10:44:29Z",
    "updated": "2024-06-17T10:44:29Z",
    "authors": [
      "Qiquan Zhang",
      "Hongxu Zhu",
      "Xinyuan Qian",
      "Eliathamby Ambikairajah",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.16134v2",
    "title": "Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory\n  for Gaussian Process Data",
    "summary": "Diffusion Transformer, the backbone of Sora for video generation,\nsuccessfully scales the capacity of diffusion models, pioneering new avenues\nfor high-fidelity sequential data generation. Unlike static data such as\nimages, sequential data consists of consecutive data frames indexed by time,\nexhibiting rich spatial and temporal dependencies. These dependencies represent\nthe underlying dynamic model and are critical to validate the generated data.\nIn this paper, we make the first theoretical step towards bridging diffusion\ntransformers for capturing spatial-temporal dependencies. Specifically, we\nestablish score approximation and distribution estimation guarantees of\ndiffusion transformers for learning Gaussian process data with covariance\nfunctions of various decay patterns. We highlight how the spatial-temporal\ndependencies are captured and affect learning efficiency. Our study proposes a\nnovel transformer approximation theory, where the transformer acts to unroll an\nalgorithm. We support our theoretical results by numerical experiments,\nproviding strong evidence that spatial-temporal dependencies are captured\nwithin attention layers, aligning with our approximation theory.",
    "published": "2024-07-23T02:42:43Z",
    "updated": "2025-02-04T16:00:20Z",
    "authors": [
      "Hengyu Fu",
      "Zehao Dou",
      "Jiawei Guo",
      "Mengdi Wang",
      "Minshuo Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.14991v1",
    "title": "Speech Recognition Transformers: Topological-lingualism Perspective",
    "summary": "Transformers have evolved with great success in various artificial\nintelligence tasks. Thanks to our recent prevalence of self-attention\nmechanisms, which capture long-term dependency, phenomenal outcomes in speech\nprocessing and recognition tasks have been produced. The paper presents a\ncomprehensive survey of transformer techniques oriented in speech modality. The\nmain contents of this survey include (1) background of traditional ASR,\nend-to-end transformer ecosystem, and speech transformers (2) foundational\nmodels in a speech via lingualism paradigm, i.e., monolingual, bilingual,\nmultilingual, and cross-lingual (3) dataset and languages, acoustic features,\narchitecture, decoding, and evaluation metric from a specific topological\nlingualism perspective (4) popular speech transformer toolkit for building\nend-to-end ASR systems. Finally, highlight the discussion of open challenges\nand potential research directions for the community to conduct further research\nin this domain.",
    "published": "2024-08-27T12:15:43Z",
    "updated": "2024-08-27T12:15:43Z",
    "authors": [
      "Shruti Singh",
      "Muskaan Singh",
      "Virender Kadyan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.23182v1",
    "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
    "summary": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains.",
    "published": "2024-10-30T16:38:09Z",
    "updated": "2024-10-30T16:38:09Z",
    "authors": [
      "Zhichao Hou",
      "Weizhi Gao",
      "Yuchen Shen",
      "Feiyi Wang",
      "Xiaorui Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03606v1",
    "title": "Advanced Risk Prediction and Stability Assessment of Banks Using Time\n  Series Transformer Models",
    "summary": "This paper aims to study the prediction of the bank stability index based on\nthe Time Series Transformer model. The bank stability index is an important\nindicator to measure the health status and risk resistance of financial\ninstitutions. Traditional prediction methods are difficult to adapt to complex\nmarket changes because they rely on single-dimensional macroeconomic data. This\npaper proposes a prediction framework based on the Time Series Transformer,\nwhich uses the self-attention mechanism of the model to capture the complex\ntemporal dependencies and nonlinear relationships in financial data. Through\nexperiments, we compare the model with LSTM, GRU, CNN, TCN and RNN-Transformer\nmodels. The experimental results show that the Time Series Transformer model\noutperforms other models in both mean square error (MSE) and mean absolute\nerror (MAE) evaluation indicators, showing strong prediction ability. This\nshows that the Time Series Transformer model can better handle multidimensional\ntime series data in bank stability prediction, providing new technical\napproaches and solutions for financial risk management.",
    "published": "2024-12-04T08:15:27Z",
    "updated": "2024-12-04T08:15:27Z",
    "authors": [
      "Wenying Sun",
      "Zhen Xu",
      "Wenqing Zhang",
      "Kunyuan Ma",
      "You Wu",
      "Mengfang Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.05505v1",
    "title": "Trimming Down Large Spiking Vision Transformers via Heterogeneous\n  Quantization Search",
    "summary": "Spiking Neural Networks (SNNs) are amenable to deployment on edge devices and\nneuromorphic hardware due to their lower dissipation. Recently, SNN-based\ntransformers have garnered significant interest, incorporating attention\nmechanisms akin to their counterparts in Artificial Neural Networks (ANNs)\nwhile demonstrating excellent performance. However, deploying large spiking\ntransformer models on resource-constrained edge devices such as mobile phones,\nstill poses significant challenges resulted from the high computational demands\nof large uncompressed high-precision models. In this work, we introduce a novel\nheterogeneous quantization method for compressing spiking transformers through\nlayer-wise quantization. Our approach optimizes the quantization of each layer\nusing one of two distinct quantization schemes, i.e., uniform or power-of-two\nquantification, with mixed bit resolutions. Our heterogeneous quantization\ndemonstrates the feasibility of maintaining high performance for spiking\ntransformers while utilizing an average effective resolution of 3.14-3.67 bits\nwith less than a 1% accuracy drop on DVS Gesture and CIFAR10-DVS datasets. It\nattains a model compression rate of 8.71x-10.19x for standard floating-point\nspiking transformers. Moreover, the proposed approach achieves a significant\nenergy reduction of 5.69x, 8.72x, and 10.2x while maintaining high accuracy\nlevels of 85.3%, 97.57%, and 80.4% on N-Caltech101, DVS-Gesture, and\nCIFAR10-DVS datasets, respectively.",
    "published": "2024-12-07T02:34:02Z",
    "updated": "2024-12-07T02:34:02Z",
    "authors": [
      "Boxun Xu",
      "Yufei Song",
      "Peng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.15656v2",
    "title": "Classifying Deepfakes Using Swin Transformers",
    "summary": "The proliferation of deepfake technology poses significant challenges to the\nauthenticity and trustworthiness of digital media, necessitating the\ndevelopment of robust detection methods. This study explores the application of\nSwin Transformers, a state-of-the-art architecture leveraging shifted windows\nfor self-attention, in detecting and classifying deepfake images. Using the\nReal and Fake Face Detection dataset by Yonsei University's Computational\nIntelligence Photography Lab, we evaluate the Swin Transformer and hybrid\nmodels such as Swin-ResNet and Swin-KNN, focusing on their ability to identify\nsubtle manipulation artifacts. Our results demonstrate that the Swin\nTransformer outperforms conventional CNN-based architectures, including VGG16,\nResNet18, and AlexNet, achieving a test accuracy of 71.29%. Additionally, we\npresent insights into hybrid model design, highlighting the complementary\nstrengths of transformer and CNN-based approaches in deepfake detection. This\nstudy underscores the potential of transformer-based architectures for\nimproving accuracy and generalizability in image-based manipulation detection,\npaving the way for more effective countermeasures against deepfake threats.",
    "published": "2025-01-26T19:35:46Z",
    "updated": "2025-01-31T16:16:30Z",
    "authors": [
      "Aprille J. Xi",
      "Eason Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09503v2",
    "title": "AttentionSmithy: A Modular Framework for Rapid Transformer Development\n  and Customization",
    "summary": "Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.",
    "published": "2025-02-13T17:15:26Z",
    "updated": "2025-02-14T21:38:39Z",
    "authors": [
      "Caleb Cranney",
      "Jesse G. Meyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.17196v2",
    "title": "Disentangling Visual Transformers: Patch-level Interpretability for\n  Image Classification",
    "summary": "Visual transformers have achieved remarkable performance in image\nclassification tasks, but this performance gain has come at the cost of\ninterpretability. One of the main obstacles to the interpretation of\ntransformers is the self-attention mechanism, which mixes visual information\nacross the whole image in a complex way. In this paper, we propose Hindered\nTransformer (HiT), a novel interpretable by design architecture inspired by\nvisual transformers. Our proposed architecture rethinks the design of\ntransformers to better disentangle patch influences at the classification\nstage. Ultimately, HiT can be interpreted as a linear combination of\npatch-level information. We show that the advantages of our approach in terms\nof explicability come with a reasonable trade-off in performance, making it an\nattractive alternative for applications where interpretability is paramount.",
    "published": "2025-02-24T14:30:29Z",
    "updated": "2025-04-24T12:21:25Z",
    "authors": [
      "Guillaume Jeanneret",
      "LoÃ¯c Simon",
      "FrÃ©dÃ©ric Jurie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01888v1",
    "title": "Enhancing Transformer with GNN Structural Knowledge via Distillation: A\n  Novel Approach",
    "summary": "Integrating the structural inductive biases of Graph Neural Networks (GNNs)\nwith the global contextual modeling capabilities of Transformers represents a\npivotal challenge in graph representation learning. While GNNs excel at\ncapturing localized topological patterns through message-passing mechanisms,\ntheir inherent limitations in modeling long-range dependencies and\nparallelizability hinder their deployment in large-scale scenarios. Conversely,\nTransformers leverage self-attention mechanisms to achieve global receptive\nfields but struggle to inherit the intrinsic graph structural priors of GNNs.\nThis paper proposes a novel knowledge distillation framework that\nsystematically transfers multiscale structural knowledge from GNN teacher\nmodels to Transformer student models, offering a new perspective on addressing\nthe critical challenges in cross-architectural distillation. The framework\neffectively bridges the architectural gap between GNNs and Transformers through\nmicro-macro distillation losses and multiscale feature alignment. This work\nestablishes a new paradigm for inheriting graph structural biases in\nTransformer architectures, with broad application prospects.",
    "published": "2025-02-27T05:14:47Z",
    "updated": "2025-02-27T05:14:47Z",
    "authors": [
      "Zhihua Duan",
      "Jialin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.04416v2",
    "title": "Learning Transformer-based World Models with Contrastive Predictive\n  Coding",
    "summary": "The DreamerV3 algorithm recently obtained remarkable performance across\ndiverse environment domains by learning an accurate world model based on\nRecurrent Neural Networks (RNNs). Following the success of model-based\nreinforcement learning algorithms and the rapid adoption of the Transformer\narchitecture for its superior training efficiency and favorable scaling\nproperties, recent works such as STORM have proposed replacing RNN-based world\nmodels with Transformer-based world models using masked self-attention.\nHowever, despite the improved training efficiency of these methods, their\nimpact on performance remains limited compared to the Dreamer algorithm,\nstruggling to learn competitive Transformer-based world models. In this work,\nwe show that the next state prediction objective adopted in previous approaches\nis insufficient to fully exploit the representation capabilities of\nTransformers. We propose to extend world model predictions to longer time\nhorizons by introducing TWISTER (Transformer-based World model wIth contraSTivE\nRepresentations), a world model using action-conditioned Contrastive Predictive\nCoding to learn high-level temporal feature representations and improve the\nagent performance. TWISTER achieves a human-normalized mean score of 162% on\nthe Atari 100k benchmark, setting a new record among state-of-the-art methods\nthat do not employ look-ahead search.",
    "published": "2025-03-06T13:18:37Z",
    "updated": "2025-05-25T17:33:51Z",
    "authors": [
      "Maxime Burchi",
      "Radu Timofte"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18862v1",
    "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
    "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
    "published": "2025-03-24T16:38:31Z",
    "updated": "2025-03-24T16:38:31Z",
    "authors": [
      "DeShin Hwa",
      "Tobias Holmes",
      "Klaus Drechsler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.22196v1",
    "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
    "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
    "published": "2025-03-28T07:26:37Z",
    "updated": "2025-03-28T07:26:37Z",
    "authors": [
      "Jiyu Chen",
      "Shuang Peng",
      "Daxiong Luo",
      "Fan Yang",
      "Renshou Wu",
      "Fangyuan Li",
      "Xiaoxin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20770v1",
    "title": "JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular\n  Generation",
    "summary": "The discovery of new molecules based on the original chemical molecule\ndistributions is of great importance in medicine. The graph transformer, with\nits advantages of high performance and scalability compared to traditional\ngraph networks, has been widely explored in recent research for applications of\ngraph structures. However, current transformer-based graph decoders struggle to\neffectively utilize graph information, which limits their capacity to leverage\nonly sequences of nodes rather than the complex topological structures of\nmolecule graphs. This paper focuses on building a graph transformer-based\nframework for molecular generation, which we call \\textbf{JTreeformer} as it\ntransforms graph generation into junction tree generation. It combines GCN\nparallel with multi-head attention as the encoder. It integrates a directed\nacyclic GCN into a graph-based Transformer to serve as a decoder, which can\niteratively synthesize the entire molecule by leveraging information from the\npartially constructed molecular structure at each step. In addition, a\ndiffusion model is inserted in the latent space generated by the encoder, to\nenhance the efficiency and effectiveness of sampling further. The empirical\nresults demonstrate that our novel framework outperforms existing molecule\ngeneration methods, thus offering a promising tool to advance drug discovery\n(https://anonymous.4open.science/r/JTreeformer-C74C).",
    "published": "2025-04-29T13:51:07Z",
    "updated": "2025-04-29T13:51:07Z",
    "authors": [
      "Ji Shi",
      "Chengxun Xie",
      "Zhonghao Li",
      "Xinming Zhang",
      "Miao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.05709v1",
    "title": "Token Transforming: A Unified and Training-Free Token Compression\n  Framework for Vision Transformer Acceleration",
    "summary": "Vision transformers have been widely explored in various vision tasks. Due to\nheavy computational cost, much interest has aroused for compressing vision\ntransformer dynamically in the aspect of tokens. Current methods mainly pay\nattention to token pruning or merging to reduce token numbers, in which tokens\nare compressed exclusively, causing great information loss and therefore\npost-training is inevitably required to recover the performance. In this paper,\nwe rethink token reduction and unify the process as an explicit form of token\nmatrix transformation, in which all existing methods are constructing special\nforms of matrices within the framework. Furthermore, we propose a many-to-many\nToken Transforming framework that serves as a generalization of all existing\nmethods and reserves the most information, even enabling training-free\nacceleration. We conduct extensive experiments to validate our framework.\nSpecifically, we reduce 40% FLOPs and accelerate DeiT-S by $\\times$1.5 with\nmarginal 0.1% accuracy drop. Furthermore, we extend the method to dense\nprediction tasks including segmentation, object detection, depth estimation,\nand language model generation. Results demonstrate that the proposed method\nconsistently achieves substantial improvements, offering a better\ncomputation-performance trade-off, impressive budget reduction and inference\nacceleration.",
    "published": "2025-06-06T03:18:11Z",
    "updated": "2025-06-06T03:18:11Z",
    "authors": [
      "Fanhu Zeng",
      "Deli Yu",
      "Zhenglun Kong",
      "Hao Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06398v1",
    "title": "Theoretical Analysis of Positional Encodings in Transformer Models:\n  Impact on Expressiveness and Generalization",
    "summary": "Positional encodings are a core part of transformer-based models, enabling\nprocessing of sequential data without recurrence. This paper presents a\ntheoretical framework to analyze how various positional encoding methods,\nincluding sinusoidal, learned, relative, and bias-based methods like Attention\nwith Linear Biases (ALiBi), impact a transformer's expressiveness,\ngeneralization ability, and extrapolation to longer sequences. Expressiveness\nis defined via function approximation, generalization bounds are established\nusing Rademacher complexity, and new encoding methods based on orthogonal\nfunctions, such as wavelets and Legendre polynomials, are proposed. The\nextrapolation capacity of existing and proposed encodings is analyzed,\nextending ALiBi's biasing approach to a unified theoretical context.\nExperimental evaluation on synthetic sequence-to-sequence tasks shows that\northogonal transform-based encodings outperform traditional sinusoidal\nencodings in generalization and extrapolation. This work addresses a critical\ngap in transformer theory, providing insights for design choices in natural\nlanguage processing, computer vision, and other transformer applications.",
    "published": "2025-06-05T23:02:18Z",
    "updated": "2025-06-05T23:02:18Z",
    "authors": [
      "Yin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13354v2",
    "title": "Physical models realizing the transformer architecture of large language\n  models",
    "summary": "The introduction of the transformer architecture in 2017 marked the most\nstriking advancement in natural language processing. The transformer is a model\narchitecture relying entirely on an attention mechanism to draw global\ndependencies between input and output. However, we believe there is a gap in\nour theoretical understanding of what the transformer is, and how it works\nphysically. From a physical perspective on modern chips, such as those chips\nunder 28nm, modern intelligent machines should be regarded as open quantum\nsystems beyond conventional statistical systems. Thereby, in this paper, we\nconstruct physical models realizing large language models based on a\ntransformer architecture as open quantum systems in the Fock space over the\nHilbert space of tokens. Our physical models underlie the transformer\narchitecture for large language models.",
    "published": "2025-05-21T10:53:05Z",
    "updated": "2025-07-22T09:01:10Z",
    "authors": [
      "Zeqian Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03653v1",
    "title": "Optimized imaging prefiltering for enhanced image segmentation",
    "summary": "The Box-Cox transformation, introduced in 1964, is a widely used statistical\ntool for stabilizing variance and improving normality in data analysis. Its\napplication in image processing, particularly for image enhancement, has gained\nincreasing attention in recent years. This paper investigates the use of the\nBox-Cox transformation as a preprocessing step for image segmentation, with a\nfocus on the estimation of the transformation parameter. We evaluate the\neffectiveness of the transformation by comparing various segmentation methods,\nhighlighting its advantages for traditional machine learning\ntechniques-especially in situations where no training data is available. The\nresults demonstrate that the transformation enhances feature separability and\ncomputational efficiency, making it particularly beneficial for models like\ndiscriminant analysis. In contrast, deep learning models did not show\nconsistent improvements, underscoring the differing impacts of the\ntransformation across model types and image characteristics.",
    "published": "2025-08-05T17:02:30Z",
    "updated": "2025-08-05T17:02:30Z",
    "authors": [
      "Ronny Vallejos",
      "Felipe Osorio",
      "Sebastian Vidal",
      "Grisel Britos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.14418v1",
    "title": "Adaptive Interpolating Quantum Transform: A Quantum-Native Framework for\n  Efficient Transform Learning",
    "summary": "Machine learning on quantum computers has attracted attention for its\npotential to deliver computational speedups in different tasks. However, deep\nvariational quantum circuits require a large number of trainable parameters\nthat grows with both qubit count and circuit depth, often rendering training\ninfeasible. In this study, we introduce the Adaptive Interpolating Quantum\nTransform (AIQT), a quantum-native framework for flexible and efficient\nlearning. AIQT defines a trainable unitary that interpolates between quantum\ntransforms, such as the Hadamard and quantum Fourier transforms. This approach\nenables expressive quantum state manipulation while controlling parameter\noverhead. It also allows AIQT to inherit any quantum advantages present in its\nconstituent transforms. Our results show that AIQT achieves high performance\nwith minimal parameter count, offering a scalable and interpretable alternative\nto deep variational circuits.",
    "published": "2025-08-20T04:26:52Z",
    "updated": "2025-08-20T04:26:52Z",
    "authors": [
      "Gekko Budiutama",
      "Shunsuke Daimon",
      "Hirofumi Nishi",
      "Ryui Kaneko",
      "Tomi Ohtsuki",
      "Yu-ichiro Matsushita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10790v1",
    "title": "GoldenTransformer: A Modular Fault Injection Framework for Transformer\n  Robustness Research",
    "summary": "Transformers have become the foundation for a wide range of\nstate--of--the--art models across natural language processing, computer vision,\nand other machine learning domains. Despite their widespread deployment, the\nrobustness of these models under fault conditions remains underexplored. We\npresent GoldenTransformer, a modular and extensible fault injection framework\ndesigned to evaluate the resiliency of Large Language Models to induced\nhardware faults. GoldenTransformer offers a unified Python-based platform for\ninjecting diverse classes of faults--such as weight corruption, activation\ninjections, and attention--level disruptions--into pretrained\ntransformer--based models. Inspired by the GoldenEye simulator for DNNs, our\nframework focuses on the unique challenges of working with large transformer\narchitectures, including considerations such as structural complexity, latent\ndependencies, and nonuniform layer definitions. GoldenTransformer is built atop\nPyTorch and HuggingFace Transformers, and it supports experiment\nreproducibility, metric logging, and visualization out of the box. We detail\nthe technical design and use of GoldenTransformer and demonstrate through\nseveral example experiments on classification and generation tasks. By enabling\ncontrolled injection of faults at multiple logical and structural points in a\ntransformer, GoldenTransformer offers researchers and practitioners a valuable\ntool for model robustness analysis and for guiding dependable system design in\nreal-world LLM applications.",
    "published": "2025-09-13T02:52:08Z",
    "updated": "2025-09-13T02:52:08Z",
    "authors": [
      "Luke Howard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25001v1",
    "title": "LVT: Large-Scale Scene Reconstruction via Local View Transformers",
    "summary": "Large transformer models are proving to be a powerful tool for 3D vision and\nnovel view synthesis. However, the standard Transformer's well-known quadratic\ncomplexity makes it difficult to scale these methods to large scenes. To\naddress this challenge, we propose the Local View Transformer (LVT), a\nlarge-scale scene reconstruction and novel view synthesis architecture that\ncircumvents the need for the quadratic attention operation. Motivated by the\ninsight that spatially nearby views provide more useful signal about the local\nscene composition than distant views, our model processes all information in a\nlocal neighborhood around each view. To attend to tokens in nearby views, we\nleverage a novel positional encoding that conditions on the relative geometric\ntransformation between the query and nearby views. We decode the output of our\nmodel into a 3D Gaussian Splat scene representation that includes both color\nand opacity view-dependence. Taken together, the Local View Transformer enables\nreconstruction of arbitrarily large, high-resolution scenes in a single forward\npass. See our project page for results and interactive demos\nhttps://toobaimt.github.io/lvt/.",
    "published": "2025-09-29T16:24:34Z",
    "updated": "2025-09-29T16:24:34Z",
    "authors": [
      "Tooba Imtiaz",
      "Lucy Chai",
      "Kathryn Heal",
      "Xuan Luo",
      "Jungyeon Park",
      "Jennifer Dy",
      "John Flynn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2012.07436v3",
    "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series\n  Forecasting",
    "summary": "Many real-world applications require the prediction of long sequence\ntime-series, such as electricity consumption planning. Long sequence\ntime-series forecasting (LSTF) demands a high prediction capacity of the model,\nwhich is the ability to capture precise long-range dependency coupling between\noutput and input efficiently. Recent studies have shown the potential of\nTransformer to increase the prediction capacity. However, there are several\nsevere issues with Transformer that prevent it from being directly applicable\nto LSTF, including quadratic time complexity, high memory usage, and inherent\nlimitation of the encoder-decoder architecture. To address these issues, we\ndesign an efficient transformer-based model for LSTF, named Informer, with\nthree distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism,\nwhich achieves $O(L \\log L)$ in time complexity and memory usage, and has\ncomparable performance on sequences' dependency alignment. (ii) the\nself-attention distilling highlights dominating attention by halving cascading\nlayer input, and efficiently handles extreme long input sequences. (iii) the\ngenerative style decoder, while conceptually simple, predicts the long\ntime-series sequences at one forward operation rather than a step-by-step way,\nwhich drastically improves the inference speed of long-sequence predictions.\nExtensive experiments on four large-scale datasets demonstrate that Informer\nsignificantly outperforms existing methods and provides a new solution to the\nLSTF problem.",
    "published": "2020-12-14T11:43:09Z",
    "updated": "2021-03-28T14:45:04Z",
    "authors": [
      "Haoyi Zhou",
      "Shanghang Zhang",
      "Jieqi Peng",
      "Shuai Zhang",
      "Jianxin Li",
      "Hui Xiong",
      "Wancai Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2103.16553v1",
    "title": "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with\n  Transformers",
    "summary": "Our objective is language-based search of large-scale image and video\ndatasets. For this task, the approach that consists of independently mapping\ntext and vision to a joint embedding space, a.k.a. dual encoders, is attractive\nas retrieval scales and is efficient for billions of images using approximate\nnearest neighbour search. An alternative approach of using vision-text\ntransformers with cross-attention gives considerable improvements in accuracy\nover the joint embeddings, but is often inapplicable in practice for\nlarge-scale retrieval given the cost of the cross-attention mechanisms required\nfor each sample at test time. This work combines the best of both worlds. We\nmake the following three contributions. First, we equip transformer-based\nmodels with a new fine-grained cross-attention architecture, providing\nsignificant improvements in retrieval accuracy whilst preserving scalability.\nSecond, we introduce a generic approach for combining a Fast dual encoder model\nwith our Slow but accurate transformer-based model via distillation and\nre-ranking. Finally, we validate our approach on the Flickr30K image dataset\nwhere we show an increase in inference speed by several orders of magnitude\nwhile having results competitive to the state of the art. We also extend our\nmethod to the video domain, improving the state of the art on the VATEX\ndataset.",
    "published": "2021-03-30T17:57:08Z",
    "updated": "2021-03-30T17:57:08Z",
    "authors": [
      "Antoine Miech",
      "Jean-Baptiste Alayrac",
      "Ivan Laptev",
      "Josef Sivic",
      "Andrew Zisserman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2208.03486v3",
    "title": "HaloAE: An HaloNet based Local Transformer Auto-Encoder for Anomaly\n  Detection and Localization",
    "summary": "Unsupervised anomaly detection and localization is a crucial task as it is\nimpossible to collect and label all possible anomalies. Many studies have\nemphasized the importance of integrating local and global information to\nachieve accurate segmentation of anomalies. To this end, there has been a\ngrowing interest in Transformer, which allows modeling long-range content\ninteractions. However, global interactions through self attention are generally\ntoo expensive for most image scales. In this study, we introduce HaloAE, the\nfirst auto-encoder based on a local 2D version of Transformer with HaloNet.\nWith HaloAE, we have created a hybrid model that combines convolution and local\n2D block-wise self-attention layers and jointly performs anomaly detection and\nsegmentation through a single model. We achieved competitive results on the\nMVTec dataset, suggesting that vision models incorporating Transformer could\nbenefit from a local computation of the self-attention operation, and pave the\nway for other applications.",
    "published": "2022-08-06T09:52:32Z",
    "updated": "2022-09-26T13:37:53Z",
    "authors": [
      "E. Mathian",
      "H. Liu",
      "L. Fernandez-Cuesta",
      "D. Samaras",
      "M. Foll",
      "L. Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.11860v1",
    "title": "Online Transformers with Spiking Neurons for Fast Prosthetic Hand\n  Control",
    "summary": "Transformers are state-of-the-art networks for most sequence processing\ntasks. However, the self-attention mechanism often used in Transformers\nrequires large time windows for each computation step and thus makes them less\nsuitable for online signal processing compared to Recurrent Neural Networks\n(RNNs). In this paper, instead of the self-attention mechanism, we use a\nsliding window attention mechanism. We show that this mechanism is more\nefficient for continuous signals with finite-range dependencies between input\nand target, and that we can use it to process sequences element-by-element,\nthis making it compatible with online processing. We test our model on a finger\nposition regression dataset (NinaproDB8) with Surface Electromyographic (sEMG)\nsignals measured on the forearm skin to estimate muscle activities. Our\napproach sets the new state-of-the-art in terms of accuracy on this dataset\nwhile requiring only very short time windows of 3.5 ms at each inference step.\nMoreover, we increase the sparsity of the network using Leaky-Integrate and\nFire (LIF) units, a bio-inspired neuron model that activates sparsely in time\nsolely when crossing a threshold. We thus reduce the number of synaptic\noperations up to a factor of $\\times5.3$ without loss of accuracy. Our results\nhold great promises for accurate and fast online processing of sEMG signals for\nsmooth prosthetic hand control and is a step towards Transformers and Spiking\nNeural Networks (SNNs) co-integration for energy efficient temporal signal\nprocessing.",
    "published": "2023-03-21T13:59:35Z",
    "updated": "2023-03-21T13:59:35Z",
    "authors": [
      "Nathan Leroux",
      "Jan Finkbeiner",
      "Emre Neftci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/1910.11218v1",
    "title": "Promoting the Knowledge of Source Syntax in Transformer NMT Is Not\n  Needed",
    "summary": "The utility of linguistic annotation in neural machine translation seemed to\nhad been established in past papers. The experiments were however limited to\nrecurrent sequence-to-sequence architectures and relatively small data\nsettings. We focus on the state-of-the-art Transformer model and use comparably\nlarger corpora. Specifically, we try to promote the knowledge of source-side\nsyntax using multi-task learning either through simple data manipulation\ntechniques or through a dedicated model component. In particular, we train one\nof Transformer attention heads to produce source-side dependency tree. Overall,\nour results cast some doubt on the utility of multi-task setups with linguistic\ninformation. The data manipulation techniques, recommended in previous works,\nprove ineffective in large data settings. The treatment of self-attention as\ndependencies seems much more promising: it helps in translation and reveals\nthat Transformer model can very easily grasp the syntactic structure. An\nimportant but curious result is, however, that identical gains are obtained by\nusing trivial \"linear trees\" instead of true dependencies. The reason for the\ngain thus may not be coming from the added linguistic knowledge but from some\nsimpler regularizing effect we induced on self-attention matrices.",
    "published": "2019-10-24T15:23:08Z",
    "updated": "2019-10-24T15:23:08Z",
    "authors": [
      "Thuong-Hai Pham",
      "Dominik MachÃ¡Äek",
      "OndÅej Bojar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.08480v1",
    "title": "s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis",
    "summary": "Neural end-to-end text-to-speech (TTS) , which adopts either a recurrent\nmodel, e.g. Tacotron, or an attention one, e.g. Transformer, to characterize a\nspeech utterance, has achieved significant improvement of speech synthesis.\nHowever, it is still very challenging to deal with different sentence lengths,\nparticularly, for long sentences where sequence model has limitation of the\neffective context length. We propose a novel segment-Transformer\n(s-Transformer), which models speech at segment level where recurrence is\nreused via cached memories for both the encoder and decoder. Long-range\ncontexts can be captured by the extended memory, meanwhile, the encoder-decoder\nattention on segment which is much easier to handle. In addition, we employ a\nmodified relative positional self attention to generalize sequence length\nbeyond a period possibly unseen in the training data. By comparing the proposed\ns-Transformer with the standard Transformer, on short sentences, both achieve\nthe same MOS scores of 4.29, which is very close to 4.32 by the recordings;\nsimilar scores of 4.22 vs 4.2 on long sentences, and significantly better for\nextra-long sentences with a gain of 0.2 in MOS. Since the cached memory is\nupdated with time, the s-Transformer generates rather natural and coherent\nspeech for a long period of time.",
    "published": "2020-11-17T07:24:04Z",
    "updated": "2020-11-17T07:24:04Z",
    "authors": [
      "Xi Wang",
      "Huaiping Ming",
      "Lei He",
      "Frank K. Soong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2105.14138v1",
    "title": "Transformer-Based Source-Free Domain Adaptation",
    "summary": "In this paper, we study the task of source-free domain adaptation (SFDA),\nwhere the source data are not available during target adaptation. Previous\nworks on SFDA mainly focus on aligning the cross-domain distributions. However,\nthey ignore the generalization ability of the pretrained source model, which\nlargely influences the initial target outputs that are vital to the target\nadaptation stage. To address this, we make the interesting observation that the\nmodel accuracy is highly correlated with whether or not attention is focused on\nthe objects in an image. To this end, we propose a generic and effective\nframework based on Transformer, named TransDA, for learning a generalized model\nfor SFDA. Specifically, we apply the Transformer as the attention module and\ninject it into a convolutional network. By doing so, the model is encouraged to\nturn attention towards the object regions, which can effectively improve the\nmodel's generalization ability on the target domains. Moreover, a novel\nself-supervised knowledge distillation approach is proposed to adapt the\nTransformer with target pseudo-labels, thus further encouraging the network to\nfocus on the object regions. Experiments on three domain adaptation tasks,\nincluding closed-set, partial-set, and open-set adaption, demonstrate that\nTransDA can greatly improve the adaptation accuracy and produce\nstate-of-the-art results. The source code and trained models are available at\nhttps://github.com/ygjwd12345/TransDA.",
    "published": "2021-05-28T23:06:26Z",
    "updated": "2021-05-28T23:06:26Z",
    "authors": [
      "Guanglei Yang",
      "Hao Tang",
      "Zhun Zhong",
      "Mingli Ding",
      "Ling Shao",
      "Nicu Sebe",
      "Elisa Ricci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.03893v3",
    "title": "Rethinking Graph Transformers with Spectral Attention",
    "summary": "In recent years, the Transformer architecture has proven to be very\nsuccessful in sequence processing, but its application to other data\nstructures, such as graphs, has remained limited due to the difficulty of\nproperly defining positions. Here, we present the $\\textit{Spectral Attention\nNetwork}$ (SAN), which uses a learned positional encoding (LPE) that can take\nadvantage of the full Laplacian spectrum to learn the position of each node in\na given graph. This LPE is then added to the node features of the graph and\npassed to a fully-connected Transformer. By leveraging the full spectrum of the\nLaplacian, our model is theoretically powerful in distinguishing graphs, and\ncan better detect similar sub-structures from their resonance. Further, by\nfully connecting the graph, the Transformer does not suffer from\nover-squashing, an information bottleneck of most GNNs, and enables better\nmodeling of physical phenomenons such as heat transfer and electric\ninteraction. When tested empirically on a set of 4 standard datasets, our model\nperforms on par or better than state-of-the-art GNNs, and outperforms any\nattention-based model by a wide margin, becoming the first fully-connected\narchitecture to perform well on graph benchmarks.",
    "published": "2021-06-07T18:11:11Z",
    "updated": "2021-10-27T16:09:50Z",
    "authors": [
      "Devin Kreuzer",
      "Dominique Beaini",
      "William L. Hamilton",
      "Vincent LÃ©tourneau",
      "Prudencio Tossou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.13008v5",
    "title": "Autoformer: Decomposition Transformers with Auto-Correlation for\n  Long-Term Series Forecasting",
    "summary": "Extending the forecasting time is a critical demand for real applications,\nsuch as extreme weather early warning and long-term energy consumption\nplanning. This paper studies the long-term forecasting problem of time series.\nPrior Transformer-based models adopt various self-attention mechanisms to\ndiscover the long-range dependencies. However, intricate temporal patterns of\nthe long-term future prohibit the model from finding reliable dependencies.\nAlso, Transformers have to adopt the sparse versions of point-wise\nself-attentions for long series efficiency, resulting in the information\nutilization bottleneck. Going beyond Transformers, we design Autoformer as a\nnovel decomposition architecture with an Auto-Correlation mechanism. We break\nwith the pre-processing convention of series decomposition and renovate it as a\nbasic inner block of deep models. This design empowers Autoformer with\nprogressive decomposition capacities for complex time series. Further, inspired\nby the stochastic process theory, we design the Auto-Correlation mechanism\nbased on the series periodicity, which conducts the dependencies discovery and\nrepresentation aggregation at the sub-series level. Auto-Correlation\noutperforms self-attention in both efficiency and accuracy. In long-term\nforecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative\nimprovement on six benchmarks, covering five practical applications: energy,\ntraffic, economics, weather and disease. Code is available at this repository:\n\\url{https://github.com/thuml/Autoformer}.",
    "published": "2021-06-24T13:43:43Z",
    "updated": "2022-01-07T13:39:16Z",
    "authors": [
      "Haixu Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.02442v4",
    "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
    "summary": "Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.",
    "published": "2021-10-06T01:07:54Z",
    "updated": "2023-05-22T09:07:17Z",
    "authors": [
      "Chao-Hong Tan",
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Zhen-Hua Ling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2110.03183v5",
    "title": "Attention is All You Need? Good Embeddings with Statistics are\n  enough:Large Scale Audio Understanding without Transformers/ Convolutions/\n  BERTs/ Mixers/ Attention/ RNNs or ....",
    "summary": "This paper presents a way of doing large scale audio understanding without\ntraditional state of the art neural architectures. Ever since the introduction\nof deep learning for understanding audio signals in the past decade,\nconvolutional architectures have been able to achieve state of the art results\nsurpassing traditional hand-crafted features. In the recent past, there has\nbeen a similar shift away from traditional convolutional and recurrent neural\nnetworks towards purely end-to-end Transformer architectures. We, in this work,\nexplore an approach, based on Bag-of-Words model. Our approach does not have\nany convolutions, recurrence, attention, transformers or other approaches such\nas BERT. We utilize micro and macro level clustered vanilla embeddings, and use\na MLP head for classification. We only use feed-forward encoder-decoder models\nto get the bottlenecks of spectral envelops, spectral patches and slices as\nwell as multi-resolution spectra. A classification head (a feed-forward layer),\nsimilar to the approach in SimCLR is trained on a learned representation. Using\nsimple codes learned on latent representations, we show how we surpass\ntraditional convolutional neural network architectures, and come strikingly\nclose to outperforming powerful Transformer architectures. This work hopefully\nwould pave way for exciting advancements in the field of representation\nlearning without massive, end-to-end neural architectures.",
    "published": "2021-10-07T05:00:26Z",
    "updated": "2022-01-30T02:25:10Z",
    "authors": [
      "Prateek Verma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.03420v1",
    "title": "Sampling Equivariant Self-attention Networks for Object Detection in\n  Aerial Images",
    "summary": "Objects in aerial images have greater variations in scale and orientation\nthan in typical images, so detection is more difficult. Convolutional neural\nnetworks use a variety of frequency- and orientation-specific kernels to\nidentify objects subject to different transformations; these require many\nparameters. Sampling equivariant networks can adjust sampling from input\nfeature maps according to the transformation of the object, allowing a kernel\nto extract features of an object under different transformations. Doing so\nrequires fewer parameters, and makes the network more suitable for representing\ndeformable objects, like those in aerial images. However, methods like\ndeformable convolutional networks can only provide sampling equivariance under\ncertain circumstances, because of the locations used for sampling. We propose\nsampling equivariant self-attention networks which consider self-attention\nrestricted to a local image patch as convolution sampling with masks instead of\nlocations, and design a transformation embedding module to further improve the\nequivariant sampling ability. We also use a novel randomized normalization\nmodule to tackle overfitting due to limited aerial image data. We show that our\nmodel (i) provides significantly better sampling equivariance than existing\nmethods, without additional supervision, (ii) provides improved classification\non ImageNet, and (iii) achieves state-of-the-art results on the DOTA dataset,\nwithout increased computation.",
    "published": "2021-11-05T11:48:04Z",
    "updated": "2021-11-05T11:48:04Z",
    "authors": [
      "Guo-Ye Yang",
      "Xiang-Li Li",
      "Ralph R. Martin",
      "Shi-Min Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.03937v2",
    "title": "Transformer Based Bengali Chatbot Using General Knowledge Dataset",
    "summary": "An AI chatbot provides an impressive response after learning from the trained\ndataset. In this decade, most of the research work demonstrates that deep\nneural models superior to any other model. RNN model regularly used for\ndetermining the sequence-related problem like a question and it answers. This\napproach acquainted with everyone as seq2seq learning. In a seq2seq model\nmechanism, it has encoder and decoder. The encoder embedded any input sequence,\nand the decoder embedded output sequence. For reinforcing the seq2seq model\nperformance, attention mechanism added into the encoder and decoder. After\nthat, the transformer model has introduced itself as a high-performance model\nwith multiple attention mechanism for solving the sequence-related dilemma.\nThis model reduces training time compared with RNN based model and also\nachieved state-of-the-art performance for sequence transduction. In this\nresearch, we applied the transformer model for Bengali general knowledge\nchatbot based on the Bengali general knowledge Question Answer (QA) dataset. It\nscores 85.0 BLEU on the applied QA data. To check the comparison of the\ntransformer model performance, we trained the seq2seq model with attention on\nour dataset that scores 23.5 BLEU.",
    "published": "2021-11-06T18:33:20Z",
    "updated": "2021-11-09T04:42:19Z",
    "authors": [
      "Abu Kaisar Mohammad Masum",
      "Sheikh Abujar",
      "Sharmin Akter",
      "Nushrat Jahan Ria",
      "Syed Akhter Hossain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2111.10659v2",
    "title": "Are Vision Transformers Robust to Patch Perturbations?",
    "summary": "Recent advances in Vision Transformer (ViT) have demonstrated its impressive\nperformance in image classification, which makes it a promising alternative to\nConvolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image\nas a sequence of image patches. The patch-based input image representation\nmakes the following question interesting: How does ViT perform when individual\ninput image patches are perturbed with natural corruptions or adversarial\nperturbations, compared to CNNs? In this work, we study the robustness of ViT\nto patch-wise perturbations. Surprisingly, we find that ViTs are more robust to\nnaturally corrupted patches than CNNs, whereas they are more vulnerable to\nadversarial patches. Furthermore, we discover that the attention mechanism\ngreatly affects the robustness of vision transformers. Specifically, the\nattention module can help improve the robustness of ViT by effectively ignoring\nnatural corrupted patches. However, when ViTs are attacked by an adversary, the\nattention mechanism can be easily fooled to focus more on the adversarially\nperturbed patches and cause a mistake. Based on our analysis, we propose a\nsimple temperature-scaling based method to improve the robustness of ViT\nagainst adversarial patches. Extensive qualitative and quantitative experiments\nare performed to support our findings, understanding, and improvement of ViT\nrobustness to patch-wise perturbations across a set of transformer-based\narchitectures.",
    "published": "2021-11-20T19:00:51Z",
    "updated": "2022-07-18T17:24:18Z",
    "authors": [
      "Jindong Gu",
      "Volker Tresp",
      "Yao Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.04976v2",
    "title": "Dual Vision Transformer",
    "summary": "Prior works have proposed several strategies to reduce the computational cost\nof self-attention mechanism. Many of these works consider decomposing the\nself-attention procedure into regional and local feature extraction procedures\nthat each incurs a much smaller computational complexity. However, regional\ninformation is typically only achieved at the expense of undesirable\ninformation lost owing to down-sampling. In this paper, we propose a novel\nTransformer architecture that aims to mitigate the cost issue, named Dual\nVision Transformer (Dual-ViT). The new architecture incorporates a critical\nsemantic pathway that can more efficiently compress token vectors into global\nsemantics with reduced order of complexity. Such compressed global semantics\nthen serve as useful prior information in learning finer pixel level details,\nthrough another constructed pixel pathway. The semantic pathway and pixel\npathway are then integrated together and are jointly trained, spreading the\nenhanced self-attention information in parallel through both of the pathways.\nDual-ViT is henceforth able to reduce the computational complexity without\ncompromising much accuracy. We empirically demonstrate that Dual-ViT provides\nsuperior accuracy than SOTA Transformer architectures with reduced training\ncomplexity. Source code is available at\n\\url{https://github.com/YehLi/ImageNetModel}.",
    "published": "2022-07-11T16:03:44Z",
    "updated": "2022-07-12T08:26:22Z",
    "authors": [
      "Ting Yao",
      "Yehao Li",
      "Yingwei Pan",
      "Yu Wang",
      "Xiao-Ping Zhang",
      "Tao Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.01209v1",
    "title": "FECAM: Frequency Enhanced Channel Attention Mechanism for Time Series\n  Forecasting",
    "summary": "Time series forecasting is a long-standing challenge due to the real-world\ninformation is in various scenario (e.g., energy, weather, traffic, economics,\nearthquake warning). However some mainstream forecasting model forecasting\nresult is derailed dramatically from ground truth. We believe it's the reason\nthat model's lacking ability of capturing frequency information which richly\ncontains in real world datasets. At present, the mainstream frequency\ninformation extraction methods are Fourier transform(FT) based. However, use of\nFT is problematic due to Gibbs phenomenon. If the values on both sides of\nsequences differ significantly, oscillatory approximations are observed around\nboth sides and high frequency noise will be introduced. Therefore We propose a\nnovel frequency enhanced channel attention that adaptively modelling frequency\ninterdependencies between channels based on Discrete Cosine Transform which\nwould intrinsically avoid high frequency noise caused by problematic periodity\nduring Fourier Transform, which is defined as Gibbs Phenomenon. We show that\nthis network generalize extremely effectively across six real-world datasets\nand achieve state-of-the-art performance, we further demonstrate that frequency\nenhanced channel attention mechanism module can be flexibly applied to\ndifferent networks. This module can improve the prediction ability of existing\nmainstream networks, which reduces 35.99% MSE on LSTM, 10.01% on Reformer,\n8.71% on Informer, 8.29% on Autoformer, 8.06% on Transformer, etc., at a slight\ncomputational cost ,with just a few line of code. Our codes and data are\navailable at https://github.com/Zero-coder/FECAM.",
    "published": "2022-12-02T14:40:55Z",
    "updated": "2022-12-02T14:40:55Z",
    "authors": [
      "Maowei Jiang",
      "Pengyu Zeng",
      "Kai Wang",
      "Huan Liu",
      "Wenbo Chen",
      "Haoran Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2212.03968v1",
    "title": "Multimodal Vision Transformers with Forced Attention for Behavior\n  Analysis",
    "summary": "Human behavior understanding requires looking at minute details in the large\ncontext of a scene containing multiple input modalities. It is necessary as it\nallows the design of more human-like machines. While transformer approaches\nhave shown great improvements, they face multiple challenges such as lack of\ndata or background noise. To tackle these, we introduce the Forced Attention\n(FAt) Transformer which utilize forced attention with a modified backbone for\ninput encoding and a use of additional inputs. In addition to improving the\nperformance on different tasks and inputs, the modification requires less time\nand memory resources. We provide a model for a generalised feature extraction\nfor tasks concerning social signals and behavior analysis. Our focus is on\nunderstanding behavior in videos where people are interacting with each other\nor talking into the camera which simulates the first person point of view in\nsocial interaction. FAt Transformers are applied to two downstream tasks:\npersonality recognition and body language recognition. We achieve\nstate-of-the-art results for Udiva v0.5, First Impressions v2 and MPII Group\nInteraction datasets. We further provide an extensive ablation study of the\nproposed architecture.",
    "published": "2022-12-07T21:56:50Z",
    "updated": "2022-12-07T21:56:50Z",
    "authors": [
      "Tanay Agrawal",
      "Michal Balazia",
      "Philipp MÃ¼ller",
      "FranÃ§ois BrÃ©mond"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2302.06461v1",
    "title": "A Study on ReLU and Softmax in Transformer",
    "summary": "The Transformer architecture consists of self-attention and feed-forward\nnetworks (FFNs) which can be viewed as key-value memories according to previous\nworks. However, FFN and traditional memory utilize different activation\nfunctions (i.e., ReLU and Softmax respectively), which makes them not\nequivalent. In this paper, we first rebuild the connections between FFN and\nkey-value memory by conducting extensive studies on ReLU and Softmax, and find\nthey are equivalent when adding an additional layer normalization module on\nSoftmax. In addition, ReLU outperforms Softmax on both FFN and key-value memory\nwhen the number of value slots is large. We analyze the reasons and then\nexplore this good property of ReLU on the self-attention network where the\noriginal Softmax activation performs poorly on long input sequences. We then\npropose a full ReLU architecture named ReLUFormer which performs better than\nthe baseline Transformer on long sequence tasks such as document translation.\nThis paper sheds light on the following points: 1) Softmax and ReLU use\ndifferent normalization methods over elements which lead to different variances\nof results, and ReLU is good at dealing with a large number of key-value slots;\n2) FFN and key-value memory are equivalent, and thus the Transformer can be\nviewed as a memory network where FFNs and self-attention networks are both\nkey-value memories.",
    "published": "2023-02-13T15:41:20Z",
    "updated": "2023-02-13T15:41:20Z",
    "authors": [
      "Kai Shen",
      "Junliang Guo",
      "Xu Tan",
      "Siliang Tang",
      "Rui Wang",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.01452v1",
    "title": "Attention Map Guided Transformer Pruning for Edge Device",
    "summary": "Due to its significant capability of modeling long-range dependencies, vision\ntransformer (ViT) has achieved promising success in both holistic and occluded\nperson re-identification (Re-ID) tasks. However, the inherent problems of\ntransformers such as the huge computational cost and memory footprint are still\ntwo unsolved issues that will block the deployment of ViT based person Re-ID\nmodels on resource-limited edge devices. Our goal is to reduce both the\ninference complexity and model size without sacrificing the comparable accuracy\non person Re-ID, especially for tasks with occlusion. To this end, we propose a\nnovel attention map guided (AMG) transformer pruning method, which removes both\nredundant tokens and heads with the guidance of the attention map in a\nhardware-friendly way. We first calculate the entropy in the key dimension and\nsum it up for the whole map, and the corresponding head parameters of maps with\nhigh entropy will be removed for model size reduction. Then we combine the\nsimilarity and first-order gradients of key tokens along the query dimension\nfor token importance estimation and remove redundant key and value tokens to\nfurther reduce the inference complexity. Comprehensive experiments on Occluded\nDukeMTMC and Market-1501 demonstrate the effectiveness of our proposals. For\nexample, our proposed pruning strategy on ViT-Base enjoys\n\\textup{\\textbf{29.4\\%}} \\textup{\\textbf{FLOPs}} savings with\n\\textup{\\textbf{0.2\\%}} drop on Rank-1 and \\textup{\\textbf{0.4\\%}} improvement\non mAP, respectively.",
    "published": "2023-04-04T01:51:53Z",
    "updated": "2023-04-04T01:51:53Z",
    "authors": [
      "Junzhu Mao",
      "Yazhou Yao",
      "Zeren Sun",
      "Xingguo Huang",
      "Fumin Shen",
      "Heng-Tao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.02860v2",
    "title": "Towards an Effective and Efficient Transformer for Rain-by-snow Weather\n  Removal",
    "summary": "Rain-by-snow weather removal is a specialized task in weather-degraded image\nrestoration aiming to eliminate coexisting rain streaks and snow particles. In\nthis paper, we propose RSFormer, an efficient and effective Transformer that\naddresses this challenge. Initially, we explore the proximity of convolution\nnetworks (ConvNets) and vision Transformers (ViTs) in hierarchical\narchitectures and experimentally find they perform approximately at intra-stage\nfeature learning. On this basis, we utilize a Transformer-like convolution\nblock (TCB) that replaces the computationally expensive self-attention while\npreserving attention characteristics for adapting to input content. We also\ndemonstrate that cross-stage progression is critical for performance\nimprovement, and propose a global-local self-attention sampling mechanism\n(GLASM) that down-/up-samples features while capturing both global and local\ndependencies. Finally, we synthesize two novel rain-by-snow datasets,\nRSCityScape and RS100K, to evaluate our proposed RSFormer. Extensive\nexperiments verify that RSFormer achieves the best trade-off between\nperformance and time-consumption compared to other restoration methods. For\ninstance, it outperforms Restormer with a 1.53% reduction in the number of\nparameters and a 15.6% reduction in inference time. Datasets, source code and\npre-trained models are available at \\url{https://github.com/chdwyb/RSFormer}.",
    "published": "2023-04-06T04:39:23Z",
    "updated": "2023-10-27T09:45:18Z",
    "authors": [
      "Tao Gao",
      "Yuanbo Wen",
      "Kaihao Zhang",
      "Peng Cheng",
      "Ting Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.07549v1",
    "title": "MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing",
    "summary": "The existing multi-modal face anti-spoofing (FAS) frameworks are designed\nbased on two strategies: halfway and late fusion. However, the former requires\ntest modalities consistent with the training input, which seriously limits its\ndeployment scenarios. And the latter is built on multiple branches to process\ndifferent modalities independently, which limits their use in applications with\nlow memory or fast execution requirements. In this work, we present a single\nbranch based Transformer framework, namely Modality-Agnostic Vision Transformer\n(MA-ViT), which aims to improve the performance of arbitrary modal attacks with\nthe help of multi-modal data. Specifically, MA-ViT adopts the early fusion to\naggregate all the available training modalities data and enables flexible\ntesting of any given modal samples. Further, we develop the Modality-Agnostic\nTransformer Block (MATB) in MA-ViT, which consists of two stacked attentions\nnamed Modal-Disentangle Attention (MDA) and Cross-Modal Attention (CMA), to\neliminate modality-related information for each modal sequences and supplement\nmodality-agnostic liveness features from another modal sequences, respectively.\nExperiments demonstrate that the single model trained based on MA-ViT can not\nonly flexibly evaluate different modal samples, but also outperforms existing\nsingle-modal frameworks by a large margin, and approaches the multi-modal\nframeworks introduced with smaller FLOPs and model parameters.",
    "published": "2023-04-15T13:03:44Z",
    "updated": "2023-04-15T13:03:44Z",
    "authors": [
      "Ajian Liu",
      "Yanyan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.03277v1",
    "title": "FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing",
    "summary": "The availability of handy multi-modal (i.e., RGB-D) sensors has brought about\na surge of face anti-spoofing research. However, the current multi-modal face\npresentation attack detection (PAD) has two defects: (1) The framework based on\nmulti-modal fusion requires providing modalities consistent with the training\ninput, which seriously limits the deployment scenario. (2) The performance of\nConvNet-based model on high fidelity datasets is increasingly limited. In this\nwork, we present a pure transformer-based framework, dubbed the Flexible Modal\nVision Transformer (FM-ViT), for face anti-spoofing to flexibly target any\nsingle-modal (i.e., RGB) attack scenarios with the help of available\nmulti-modal data. Specifically, FM-ViT retains a specific branch for each\nmodality to capture different modal information and introduces the Cross-Modal\nTransformer Block (CMTB), which consists of two cascaded attentions named\nMulti-headed Mutual-Attention (MMA) and Fusion-Attention (MFA) to guide each\nmodal branch to mine potential features from informative patch tokens, and to\nlearn modality-agnostic liveness features by enriching the modal information of\nown CLS token, respectively. Experiments demonstrate that the single model\ntrained based on FM-ViT can not only flexibly evaluate different modal samples,\nbut also outperforms existing single-modal frameworks by a large margin, and\napproaches the multi-modal frameworks introduced with smaller FLOPs and model\nparameters.",
    "published": "2023-05-05T04:28:48Z",
    "updated": "2023-05-05T04:28:48Z",
    "authors": [
      "Ajian Liu",
      "Zichang Tan",
      "Zitong Yu",
      "Chenxu Zhao",
      "Jun Wan",
      "Yanyan Liang",
      "Zhen Lei",
      "Du Zhang",
      "Stan Z. Li",
      "Guodong Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.00396v2",
    "title": "Lightweight Vision Transformer with Bidirectional Interaction",
    "summary": "Recent advancements in vision backbones have significantly improved their\nperformance by simultaneously modeling images' local and global contexts.\nHowever, the bidirectional interaction between these two contexts has not been\nwell explored and exploited, which is important in the human visual system.\nThis paper proposes a Fully Adaptive Self-Attention (FASA) mechanism for vision\ntransformer to model the local and global information as well as the\nbidirectional interaction between them in context-aware ways. Specifically,\nFASA employs self-modulated convolutions to adaptively extract local\nrepresentation while utilizing self-attention in down-sampled space to extract\nglobal representation. Subsequently, it conducts a bidirectional adaptation\nprocess between local and global representation to model their interaction. In\naddition, we introduce a fine-grained downsampling strategy to enhance the\ndown-sampled self-attention mechanism for finer-grained global perception\ncapability. Based on FASA, we develop a family of lightweight vision backbones,\nFully Adaptive Transformer (FAT) family. Extensive experiments on multiple\nvision tasks demonstrate that FAT achieves impressive performance. Notably, FAT\naccomplishes a 77.6% accuracy on ImageNet-1K using only 4.5M parameters and\n0.7G FLOPs, which surpasses the most advanced ConvNets and Transformers with\nsimilar model size and computational costs. Moreover, our model exhibits faster\nspeed on modern GPU compared to other models. Code will be available at\nhttps://github.com/qhfan/FAT.",
    "published": "2023-06-01T06:56:41Z",
    "updated": "2025-02-27T03:16:17Z",
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Xiaoqiang Zhou",
      "Ran He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.00371v5",
    "title": "Learning Content-enhanced Mask Transformer for Domain Generalized\n  Urban-Scene Segmentation",
    "summary": "Domain-generalized urban-scene semantic segmentation (USSS) aims to learn\ngeneralized semantic predictions across diverse urban-scene styles. Unlike\ndomain gap challenges, USSS is unique in that the semantic categories are often\nsimilar in different urban scenes, while the styles can vary significantly due\nto changes in urban landscapes, weather conditions, lighting, and other\nfactors. Existing approaches typically rely on convolutional neural networks\n(CNNs) to learn the content of urban scenes.\n  In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for\ndomain-generalized USSS. The main idea is to enhance the focus of the\nfundamental component, the mask attention mechanism, in Transformer\nsegmentation models on content information. To achieve this, we introduce a\nnovel content-enhanced mask attention mechanism. It learns mask queries from\nboth the image feature and its down-sampled counterpart, as lower-resolution\nimage features usually contain more robust content information and are less\nsensitive to style variations. These features are fused into a Transformer\ndecoder and integrated into a multi-resolution content-enhanced mask attention\nlearning scheme.\n  Extensive experiments conducted on various domain-generalized urban-scene\nsegmentation datasets demonstrate that the proposed CMFormer significantly\noutperforms existing CNN-based methods for domain-generalized semantic\nsegmentation, achieving improvements of up to 14.00\\% in terms of mIoU (mean\nintersection over union). The source code is publicly available at\n\\url{https://github.com/BiQiWHU/CMFormer}.",
    "published": "2023-07-01T15:48:33Z",
    "updated": "2023-12-17T15:50:36Z",
    "authors": [
      "Qi Bi",
      "Shaodi You",
      "Theo Gevers"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.01115v1",
    "title": "MeT: A Graph Transformer for Semantic Segmentation of 3D Meshes",
    "summary": "Polygonal meshes have become the standard for discretely approximating 3D\nshapes, thanks to their efficiency and high flexibility in capturing\nnon-uniform shapes. This non-uniformity, however, leads to irregularity in the\nmesh structure, making tasks like segmentation of 3D meshes particularly\nchallenging. Semantic segmentation of 3D mesh has been typically addressed\nthrough CNN-based approaches, leading to good accuracy. Recently, transformers\nhave gained enough momentum both in NLP and computer vision fields, achieving\nperformance at least on par with CNN models, supporting the long-sought\narchitecture universalism. Following this trend, we propose a transformer-based\nmethod for semantic segmentation of 3D mesh motivated by a better modeling of\nthe graph structure of meshes, by means of global attention mechanisms. In\norder to address the limitations of standard transformer architectures in\nmodeling relative positions of non-sequential data, as in the case of 3D\nmeshes, as well as in capturing the local context, we perform positional\nencoding by means the Laplacian eigenvectors of the adjacency matrix, replacing\nthe traditional sinusoidal positional encodings, and by introducing\nclustering-based features into the self-attention and cross-attention\noperators. Experimental results, carried out on three sets of the Shape COSEG\nDataset, on the human segmentation dataset proposed in Maron et al., 2017 and\non the ShapeNet benchmark, show how the proposed approach yields\nstate-of-the-art performance on semantic segmentation of 3D meshes.",
    "published": "2023-07-03T15:45:14Z",
    "updated": "2023-07-03T15:45:14Z",
    "authors": [
      "Giuseppe Vecchio",
      "Luca Prezzavento",
      "Carmelo Pino",
      "Francesco Rundo",
      "Simone Palazzo",
      "Concetto Spampinato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.03576v1",
    "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner\n  with One Layer of Linear Self-Attention",
    "summary": "Recent works have empirically analyzed in-context learning and shown that\ntransformers trained on synthetic linear regression tasks can learn to\nimplement ridge regression, which is the Bayes-optimal predictor, given\nsufficient capacity [Aky\\\"urek et al., 2023], while one-layer transformers with\nlinear self-attention and no MLP layer will learn to implement one step of\ngradient descent (GD) on a least-squares linear regression objective [von\nOswald et al., 2022]. However, the theory behind these observations remains\npoorly understood. We theoretically study transformers with a single layer of\nlinear self-attention, trained on synthetic noisy linear regression data.\nFirst, we mathematically show that when the covariates are drawn from a\nstandard Gaussian distribution, the one-layer transformer which minimizes the\npre-training loss will implement a single step of GD on the least-squares\nlinear regression objective. Then, we find that changing the distribution of\nthe covariates and weight vector to a non-isotropic Gaussian distribution has a\nstrong impact on the learned algorithm: the global minimizer of the\npre-training loss now implements a single step of $\\textit{pre-conditioned}$\nGD. However, if only the distribution of the responses is changed, then this\ndoes not have a large effect on the learned algorithm: even when the response\ncomes from a more general family of $\\textit{nonlinear}$ functions, the global\nminimizer of the pre-training loss still implements a single step of GD on a\nleast-squares linear regression objective.",
    "published": "2023-07-07T13:09:18Z",
    "updated": "2023-07-07T13:09:18Z",
    "authors": [
      "Arvind Mahankali",
      "Tatsunori B. Hashimoto",
      "Tengyu Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.03274v1",
    "title": "DSformer: A Double Sampling Transformer for Multivariate Time Series\n  Long-term Prediction",
    "summary": "Multivariate time series long-term prediction, which aims to predict the\nchange of data in a long time, can provide references for decision-making.\nAlthough transformer-based models have made progress in this field, they\nusually do not make full use of three features of multivariate time series:\nglobal information, local information, and variables correlation. To\neffectively mine the above three features and establish a high-precision\nprediction model, we propose a double sampling transformer (DSformer), which\nconsists of the double sampling (DS) block and the temporal variable attention\n(TVA) block. Firstly, the DS block employs down sampling and piecewise sampling\nto transform the original series into feature vectors that focus on global\ninformation and local information respectively. Then, TVA block uses temporal\nattention and variable attention to mine these feature vectors from different\ndimensions and extract key information. Finally, based on a parallel structure,\nDSformer uses multiple TVA blocks to mine and integrate different features\nobtained from DS blocks respectively. The integrated feature information is\npassed to the generative decoder based on a multi-layer perceptron to realize\nmultivariate time series long-term prediction. Experimental results on nine\nreal-world datasets show that DSformer can outperform eight existing baselines.",
    "published": "2023-08-07T03:32:39Z",
    "updated": "2023-08-07T03:32:39Z",
    "authors": [
      "Chengqing Yu",
      "Fei Wang",
      "Zezhi Shao",
      "Tao Sun",
      "Lin Wu",
      "Yongjun Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.04637v1",
    "title": "Sparse Binary Transformers for Multivariate Time Series Modeling",
    "summary": "Compressed Neural Networks have the potential to enable deep learning across\nnew applications and smaller computational environments. However, understanding\nthe range of learning tasks in which such models can succeed is not well\nstudied. In this work, we apply sparse and binary-weighted Transformers to\nmultivariate time series problems, showing that the lightweight models achieve\naccuracy comparable to that of dense floating-point Transformers of the same\nstructure. Our model achieves favorable results across three time series\nlearning tasks: classification, anomaly detection, and single-step forecasting.\nAdditionally, to reduce the computational complexity of the attention\nmechanism, we apply two modifications, which show little to no decline in model\nperformance: 1) in the classification task, we apply a fixed mask to the query,\nkey, and value activations, and 2) for forecasting and anomaly detection, which\nrely on predicting outputs at a single point in time, we propose an attention\nmask to allow computation only at the current time step. Together, each\ncompression technique and attention modification substantially reduces the\nnumber of non-zero operations necessary in the Transformer. We measure the\ncomputational savings of our approach over a range of metrics including\nparameter count, bit size, and floating point operation (FLOPs) count, showing\nup to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.",
    "published": "2023-08-09T00:23:04Z",
    "updated": "2023-08-09T00:23:04Z",
    "authors": [
      "Matt Gorbett",
      "Hossein Shirazi",
      "Indrakshi Ray"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.06009v1",
    "title": "ViGT: Proposal-free Video Grounding with Learnable Token in Transformer",
    "summary": "The video grounding (VG) task aims to locate the queried action or event in\nan untrimmed video based on rich linguistic descriptions. Existing\nproposal-free methods are trapped in complex interaction between video and\nquery, overemphasizing cross-modal feature fusion and feature correlation for\nVG. In this paper, we propose a novel boundary regression paradigm that\nperforms regression token learning in a transformer. Particularly, we present a\nsimple but effective proposal-free framework, namely Video Grounding\nTransformer (ViGT), which predicts the temporal boundary using a learnable\nregression token rather than multi-modal or cross-modal features. In ViGT, the\nbenefits of a learnable token are manifested as follows. (1) The token is\nunrelated to the video or the query and avoids data bias toward the original\nvideo and query. (2) The token simultaneously performs global context\naggregation from video and query features. First, we employed a sharing feature\nencoder to project both video and query into a joint feature space before\nperforming cross-modal co-attention (i.e., video-to-query attention and\nquery-to-video attention) to highlight discriminative features in each\nmodality. Furthermore, we concatenated a learnable regression token [REG] with\nthe video and query features as the input of a vision-language transformer.\nFinally, we utilized the token [REG] to predict the target moment and visual\nfeatures to constrain the foreground and background probabilities at each\ntimestamp. The proposed ViGT performed well on three public datasets: ANet\nCaptions, TACoS and YouCookII. Extensive ablation studies and qualitative\nanalysis further validated the interpretability of ViGT.",
    "published": "2023-08-11T08:30:08Z",
    "updated": "2023-08-11T08:30:08Z",
    "authors": [
      "Kun Li",
      "Dan Guo",
      "Meng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.15881v2",
    "title": "Attention-aware Social Graph Transformer Networks for Stochastic\n  Trajectory Prediction",
    "summary": "Trajectory prediction is fundamental to various intelligent technologies,\nsuch as autonomous driving and robotics. The motion prediction of pedestrians\nand vehicles helps emergency braking, reduces collisions, and improves traffic\nsafety. Current trajectory prediction research faces problems of complex social\ninteractions, high dynamics and multi-modality. Especially, it still has\nlimitations in long-time prediction. We propose Attention-aware Social Graph\nTransformer Networks for multi-modal trajectory prediction. We combine Graph\nConvolutional Networks and Transformer Networks by generating stable resolution\npseudo-images from Spatio-temporal graphs through a designed stacking and\ninterception method. Furthermore, we design the attention-aware module to\nhandle social interaction information in scenarios involving mixed\npedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and\nTransformer, i.e., the ability to aggregate information over an arbitrary\nnumber of neighbors and the ability to perform complex time-dependent data\nprocessing. We conduct experiments on datasets involving pedestrian, vehicle,\nand mixed trajectories, respectively. Our results demonstrate that our model\nminimizes displacement errors across various metrics and significantly reduces\nthe likelihood of collisions. It is worth noting that our model effectively\nreduces the final displacement error, illustrating the ability of our model to\npredict for a long time.",
    "published": "2023-12-26T04:24:01Z",
    "updated": "2024-05-11T14:38:52Z",
    "authors": [
      "Yao Liu",
      "Binghao Li",
      "Xianzhi Wang",
      "Claude Sammut",
      "Lina Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.16033v2",
    "title": "Exploiting Regional Information Transformer for Single Image Deraining",
    "summary": "Transformer-based Single Image Deraining (SID) methods have achieved\nremarkable success, primarily attributed to their robust capability in\ncapturing long-range interactions. However, we've noticed that current methods\nhandle rain-affected and unaffected regions concurrently, overlooking the\ndisparities between these areas, resulting in confusion between rain streaks\nand background parts, and inabilities to obtain effective interactions,\nultimately resulting in suboptimal deraining outcomes. To address the above\nissue, we introduce the Region Transformer (Regformer), a novel SID method that\nunderlines the importance of independently processing rain-affected and\nunaffected regions while considering their combined impact for high-quality\nimage reconstruction. The crux of our method is the innovative Region\nTransformer Block (RTB), which integrates a Region Masked Attention (RMA)\nmechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention\nselection of rain-affected and unaffected regions and local modeling of mixed\nscales. The RMA generates attention maps tailored to these two regions and\ntheir interactions, enabling our model to capture comprehensive features\nessential for rain removal. To better recover high-frequency textures and\ncapture more local details, we develop the MGFB as a compensation module to\ncomplete local mixed scale modeling. Extensive experiments demonstrate that our\nmodel reaches state-of-the-art performance, significantly improving the image\nderaining quality. Our code and trained models are publicly available.",
    "published": "2024-02-25T09:09:30Z",
    "updated": "2024-08-04T17:37:02Z",
    "authors": [
      "Baiang Li",
      "Zhao Zhang",
      "Huan Zheng",
      "Xiaogang Xu",
      "Yanyan Wei",
      "Jingyi Zhang",
      "Jicong Fan",
      "Meng Wang"
    ]
  }
]